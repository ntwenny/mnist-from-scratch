{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f532c213",
   "metadata": {},
   "source": [
    "This is the notebook for training a neural network from scratch with only numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ef183bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906ba58",
   "metadata": {},
   "source": [
    "Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f79bb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(name=\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist.data.astype(np.float32)\n",
    "y = mnist.target.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d6fe2",
   "metadata": {},
   "source": [
    "Data exploration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7ccbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (70000, 784)\n",
      "y.shape: (70000,)\n",
      "X.dtype: float32\n",
      "y.dtype: int64\n",
      "Min pixel value: 0.0\n",
      "Max pixel value: 255.0\n",
      "Unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "Any NaNs in X: False\n",
      "Any NaNs in y: False\n"
     ]
    }
   ],
   "source": [
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "\n",
    "print(\"X.dtype:\", X.dtype)\n",
    "print(\"y.dtype:\", y.dtype)\n",
    "\n",
    "print(\"Min pixel value:\", X.min())\n",
    "print(\"Max pixel value:\", X.max())\n",
    "\n",
    "print(\"Unique labels:\", np.unique(y))\n",
    "\n",
    "print(\"Any NaNs in X:\", np.isnan(X).any())\n",
    "print(\"Any NaNs in y:\", np.isnan(y).any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc8dde",
   "metadata": {},
   "source": [
    "Plotting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29e85b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting pixel value distribution\n",
    "\n",
    "plt.hist(X.flatten(), bins=50)\n",
    "plt.title(\"Pixel Value Distribution\")\n",
    "plt.xlabel(\"Pixel intensity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting label distribution\n",
    "plt.hist(y, bins=10)\n",
    "plt.title(\"Pixel Value Distribution\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d78c45",
   "metadata": {},
   "source": [
    "Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e681bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = X / 255\n",
    "\n",
    "train_end = int(X.shape[0] * 0.8)\n",
    "cv_end = int(X.shape[0] * 0.9)\n",
    "\n",
    "randomized_indices = np.random.permutation(X.shape[0])\n",
    "\n",
    "train_indices = randomized_indices[0 : train_end]\n",
    "cv_indices = randomized_indices[train_end : cv_end]\n",
    "test_indices = randomized_indices[cv_end:]\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "X_cv = X[cv_indices]\n",
    "y_cv = y[cv_indices]\n",
    "\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print(X_train[np.random.choice(X_train.shape[0], size=10, replace=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32757980",
   "metadata": {},
   "source": [
    "Weight Initialization (Using the He Initialization method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f83f39a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightInit(*, n_input=784, n1=512, n2=256, n3=128, n_out=10, X_train=X_train, y_train=y_train):\n",
    "    W1 = np.random.randn(n_input, n1) * np.sqrt(2.0 / n_input)\n",
    "    b1 = np.zeros(n1)\n",
    "    print(\"W1.shape=\", W1.shape)\n",
    "    print(\"b1.shape=\", b1.shape)\n",
    "    # print(W1[np.random.choice(W1.shape[0], size=10, replace=False)])\n",
    "\n",
    "    W2 = np.random.randn(n1, n2) * np.sqrt(2.0 / n1)\n",
    "    b2 = np.zeros(n2)\n",
    "    print(\"W2.shape=\", W2.shape)\n",
    "    print(\"b2.shape=\", b2.shape)\n",
    "\n",
    "    W3 = np.random.randn(n2, n3) * np.sqrt(2.0 / n2)\n",
    "    b3 = np.random.randn(n3)\n",
    "    print(\"W3.shape=\", W3.shape)\n",
    "    print(\"b3.shape=\", b3.shape)\n",
    "\n",
    "    W4 = np.random.randn(n3, n_out) * np.sqrt(2.0 / n3)\n",
    "    b4 = np.random.randn(n_out)\n",
    "    print(\"W3.shape=\", W3.shape)\n",
    "    print(\"b3.shape=\", b3.shape)\n",
    "\n",
    "    # use smaller set at first to test, comment out when using full dataset\n",
    "    idx = np.random.choice(X_train.shape[0], size=200, replace=False)\n",
    "    X_train = X_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "\n",
    "    return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'W3': W3,\n",
    "        'b3': b3,\n",
    "        'W4': W4,\n",
    "        'b4': b4\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "abe5b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape= (784, 512)\n",
      "b1.shape= (512,)\n",
      "W2.shape= (512, 256)\n",
      "b2.shape= (256,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n"
     ]
    }
   ],
   "source": [
    "n_input = 784\n",
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 128\n",
    "n_out = 10\n",
    "\n",
    "# Weights as a dictionary\n",
    "weights = weightInit(n_input=n_input, n1=n1, n2=n2, n3=n3, n_out=n_out, X_train=X_train, y_train=y_train)\n",
    "W1 = weights['W1']\n",
    "b1 = weights['b1']\n",
    "W2 = weights['W2']\n",
    "b2 = weights['b2']\n",
    "W3 = weights['W3']\n",
    "b3 = weights['b3']\n",
    "W4 = weights['W4']\n",
    "b4 = weights['b4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a0953",
   "metadata": {},
   "source": [
    "Now we'll begin programming the forward pass\n",
    "\n",
    "First Hidden Layer (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3bee27ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 512)\n"
     ]
    }
   ],
   "source": [
    "z_1 = X_train @ W1 + b1\n",
    "a_1 = np.maximum(0, z_1)\n",
    "print(a_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c8046",
   "metadata": {},
   "source": [
    "Second Hidden Layer (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8072b2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 256)\n"
     ]
    }
   ],
   "source": [
    "z_2 = a_1 @ W2 + b2\n",
    "a_2 = np.maximum(0, z_2)\n",
    "print(a_2.shape)\n",
    "# print(a_2[np.random.choice(a_2.shape[0], size=1, replace=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392327d7",
   "metadata": {},
   "source": [
    "Third Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4de99851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 128)\n"
     ]
    }
   ],
   "source": [
    "z_3 = a_2 @ W3 + b3\n",
    "a_3 = np.maximum(0, z_3)\n",
    "print(a_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17b9a1",
   "metadata": {},
   "source": [
    "The logits before the softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7cf56f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_4 = a_3 @ W4 + b4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a58047",
   "metadata": {},
   "source": [
    "Softmax activation function applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0ddd303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 10)\n",
      "[0.13128377 0.09975014 0.58818054 0.01025408 0.06229953 0.04632771\n",
      " 0.04736766 0.01179882 0.00101539 0.00172237]\n",
      "1.0\n",
      "2\n",
      "vs\n",
      "Correct Image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "z_4_exp = np.exp(shifted)\n",
    "p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "print(p.shape)\n",
    "print(p[0])\n",
    "print(p[0].sum())\n",
    "print(np.argmax(p[0]))\n",
    "print(\"vs\")\n",
    "print(\"Correct Image\")\n",
    "\n",
    "\n",
    "img = X_train[0].reshape(28, 28)\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label: {y_train[0]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf517a",
   "metadata": {},
   "source": [
    "Loss Function for Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e92510a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.360170785142378\n"
     ]
    }
   ],
   "source": [
    "p = np.clip(p, 1e-12, 1.0)\n",
    "loss = -np.mean(np.log(p[np.arange(y_train.shape[0]), y_train]))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60070c1f",
   "metadata": {},
   "source": [
    "Repeatable function for forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "10c2c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(*, X_train=X_train, y_train=y_train, weights=weights, lmbda=0):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    # First Hidden Layer\n",
    "    z_1 = X_train @ W1 + b1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    z_2 = a_1 @ W2 + b2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    z_3 = a_2 @ W3 + b3\n",
    "    a_3 = np.maximum(0, z_3)\n",
    "\n",
    "    # Logits before softmax\n",
    "    z_4 = a_3 @ W4 + b4\n",
    "\n",
    "    # Softmax activation applied\n",
    "    shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "    z_4_exp = np.exp(shifted)\n",
    "    p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "\n",
    "    # loss function for softmax\n",
    "    p = np.clip(p, 1e-12, 1.0)\n",
    "    loss = (\n",
    "        -np.mean(np.log(p[np.arange(y_train.shape[0]), y_train])) \n",
    "            + (lmbda/(2 * X_train.shape[0])) * ((W1**2).sum() + (W2**2).sum() + (W3**2).sum() + (W4**2).sum())\n",
    "        )\n",
    "    return loss, {\n",
    "        \"a_1\": a_1,\n",
    "        \"a_2\": a_2,\n",
    "        \"a_3\": a_3,\n",
    "        \"p\": p,\n",
    "        \"z_1\": z_1,\n",
    "        \"z_2\": z_2,\n",
    "        \"z_3\": z_3,\n",
    "        \"z_4\": z_4\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738dae8",
   "metadata": {},
   "source": [
    "Repeatable Function for backward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "6d386147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(*, cache=None, X_train=None, y_train=None, weights=None, lmbda=0):\n",
    "\n",
    "    a_1 = cache[\"a_1\"]\n",
    "    a_2 = cache[\"a_2\"]\n",
    "    a_3 = cache[\"a_3\"]\n",
    "    p = cache[\"p\"]\n",
    "    z_1 = cache[\"z_1\"]\n",
    "    z_2 = cache[\"z_2\"]\n",
    "    z_3 = cache[\"z_3\"]\n",
    "    z_4 = cache[\"z_4\"]\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    y_1he = np.zeros((len(y_train), 10))\n",
    "    y_1he[np.arange(len(y_train)), y_train] = 1\n",
    "\n",
    "    dJ_dz4 = p - y_1he # Gradient of loss for each logit, shape = (200, 10)\n",
    "\n",
    "    dJ_dw4 = (a_3.T @ dJ_dz4) / a_3.shape[0]# Gradients for output layer\n",
    "    dJ_db4 = np.mean(dJ_dz4, axis=0)\n",
    "\n",
    "    dJ_da3 = dJ_dz4 @ W4.T # Gradients for third hidden layer\n",
    "\n",
    "    da3_dz3 = np.where(z_3 > 0, 1, 0)\n",
    "    dJ_dz3 = da3_dz3 * dJ_da3\n",
    "\n",
    "\n",
    "    dJ_dw3 = (a_2.T @ dJ_dz3) / a_2.shape[0]\n",
    "    dJ_db3 = np.mean(dJ_dz3, axis=0)\n",
    "\n",
    "\n",
    "    dJ_da2 = dJ_dz3 @ W3.T # Gradients for second hidden layer\n",
    "\n",
    "    da2_dz2 = np.where(z_2 > 0, 1, 0)\n",
    "    dJ_dz2 = da2_dz2 * dJ_da2\n",
    "\n",
    "    dJ_dw2 = (a_1.T @ dJ_dz2) / a_1.shape[0]\n",
    "    dJ_db2 = np.mean(dJ_dz2, axis=0)\n",
    "\n",
    "    dJ_da1 = dJ_dz2 @ W2.T # Gradients for first hidden layer\n",
    "\n",
    "    da1_dz1 = np.where(z_1 > 0, 1, 0)\n",
    "    dJ_dz1 = da1_dz1 * dJ_da1\n",
    "\n",
    "    dJ_dw1 = (X_train.T @ dJ_dz1) / X_train.shape[0]\n",
    "    dJ_db1 = np.mean(dJ_dz1, axis=0)\n",
    "\n",
    "    # L2 regularization\n",
    "    dJ_dw4 += (lmbda / X_train.shape[0]) * W4\n",
    "    dJ_dw3 += (lmbda / X_train.shape[0]) * W3\n",
    "    dJ_dw2 += (lmbda / X_train.shape[0]) * W2\n",
    "    dJ_dw1 += (lmbda / X_train.shape[0]) * W1\n",
    "\n",
    "    return {\"dJ_dw4\": dJ_dw4, \"dJ_db4\": dJ_db4, \"dJ_dw3\": dJ_dw3, \"dJ_db3\": dJ_db3,\n",
    "            \"dJ_dw2\": dJ_dw2, \"dJ_db2\": dJ_db2, \"dJ_dw1\": dJ_dw1, \"dJ_db1\": dJ_db1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70956097",
   "metadata": {},
   "source": [
    "Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8191e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(*, alpha = 0.01, cache = None, weights=None, grads=None):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    W4 -= alpha * grads[\"dJ_dw4\"]\n",
    "    b4 -= alpha * grads[\"dJ_db4\"]\n",
    "\n",
    "    W3 -= alpha * grads[\"dJ_dw3\"]\n",
    "    b3 -= alpha * grads[\"dJ_db3\"]\n",
    "\n",
    "    W2 -= alpha * grads[\"dJ_dw2\"]\n",
    "    b2 -= alpha * grads[\"dJ_db2\"]\n",
    "\n",
    "    W1 -= alpha * grads[\"dJ_dw1\"]\n",
    "    b1 -= alpha * grads[\"dJ_db1\"]\n",
    "\n",
    "    return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'W3': W3,\n",
    "        'b3': b3,\n",
    "        'W4': W4,\n",
    "        'b4': b4\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d672345",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "baa17d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape= (784, 512)\n",
      "b1.shape= (512,)\n",
      "W2.shape= (512, 256)\n",
      "b2.shape= (256,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "weights = weightInit(n_input=n_input, n1=n1, n2=n2, n3=n3, n_out=n_out, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63e9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV loss=1.3504427625526594(Epoch= 1)\n",
      "train loss=1.3430494735002165(Epoch= 1)\n",
      "CV loss=0.8079563506698999(Epoch= 2)\n",
      "train loss=0.8019445328539454(Epoch= 2)\n",
      "CV loss=0.5916857296517306(Epoch= 3)\n",
      "train loss=0.5873784536136601(Epoch= 3)\n",
      "CV loss=0.4904477336254391(Epoch= 4)\n",
      "train loss=0.4872418976452036(Epoch= 4)\n",
      "CV loss=0.4334837020509727(Epoch= 5)\n",
      "train loss=0.430540892498526(Epoch= 5)\n",
      "CV loss=0.3962133203853904(Epoch= 6)\n",
      "train loss=0.39324319818471004(Epoch= 6)\n",
      "CV loss=0.36995502660512636(Epoch= 7)\n",
      "train loss=0.36731867931497164(Epoch= 7)\n",
      "CV loss=0.3499206970155987(Epoch= 8)\n",
      "train loss=0.3472384900614383(Epoch= 8)\n",
      "CV loss=0.33452820385327314(Epoch= 9)\n",
      "train loss=0.3313469499560693(Epoch= 9)\n",
      "CV loss=0.32143892640568567(Epoch= 10)\n",
      "train loss=0.31818009525343316(Epoch= 10)\n",
      "CV loss=0.3104458061294173(Epoch= 11)\n",
      "train loss=0.30654514921422366(Epoch= 11)\n",
      "CV loss=0.300831170077246(Epoch= 12)\n",
      "train loss=0.2964646136566384(Epoch= 12)\n",
      "CV loss=0.29331534408014326(Epoch= 13)\n",
      "train loss=0.2877265822817977(Epoch= 13)\n",
      "CV loss=0.2845593280992875(Epoch= 14)\n",
      "train loss=0.27930013462913056(Epoch= 14)\n",
      "CV loss=0.2778273740141482(Epoch= 15)\n",
      "train loss=0.2721892886358992(Epoch= 15)\n",
      "CV loss=0.2719167948369837(Epoch= 16)\n",
      "train loss=0.2651916432064021(Epoch= 16)\n",
      "CV loss=0.2662381016616631(Epoch= 17)\n",
      "train loss=0.2589785654923519(Epoch= 17)\n",
      "CV loss=0.2610398820892535(Epoch= 18)\n",
      "train loss=0.25360719758273287(Epoch= 18)\n",
      "CV loss=0.25530715395471915(Epoch= 19)\n",
      "train loss=0.2477552536264627(Epoch= 19)\n",
      "CV loss=0.25043891263577334(Epoch= 20)\n",
      "train loss=0.24253540927803183(Epoch= 20)\n",
      "CV loss=0.24621201453260536(Epoch= 21)\n",
      "train loss=0.2376286121744522(Epoch= 21)\n",
      "CV loss=0.24240095292243538(Epoch= 22)\n",
      "train loss=0.23329532001282444(Epoch= 22)\n",
      "CV loss=0.23803572105079981(Epoch= 23)\n",
      "train loss=0.22875288760235948(Epoch= 23)\n",
      "CV loss=0.23404190564459154(Epoch= 24)\n",
      "train loss=0.22411647163423576(Epoch= 24)\n",
      "CV loss=0.2311210388265161(Epoch= 25)\n",
      "train loss=0.22020562244427472(Epoch= 25)\n",
      "CV loss=0.22720161571170244(Epoch= 26)\n",
      "train loss=0.2162931340018652(Epoch= 26)\n",
      "CV loss=0.22337243026736814(Epoch= 27)\n",
      "train loss=0.2123854476157429(Epoch= 27)\n",
      "CV loss=0.2204225080890551(Epoch= 28)\n",
      "train loss=0.20889711477173367(Epoch= 28)\n",
      "CV loss=0.2170839707897303(Epoch= 29)\n",
      "train loss=0.2056396617407382(Epoch= 29)\n",
      "CV loss=0.21405386811949517(Epoch= 30)\n",
      "train loss=0.20203914384195726(Epoch= 30)\n",
      "CV loss=0.21095582477267888(Epoch= 31)\n",
      "train loss=0.1986628690996655(Epoch= 31)\n",
      "CV loss=0.20795915917349467(Epoch= 32)\n",
      "train loss=0.195696161637495(Epoch= 32)\n",
      "CV loss=0.20521129157910653(Epoch= 33)\n",
      "train loss=0.19271921575524212(Epoch= 33)\n",
      "CV loss=0.20304060705226018(Epoch= 34)\n",
      "train loss=0.1895577859005004(Epoch= 34)\n",
      "CV loss=0.20083550029283176(Epoch= 35)\n",
      "train loss=0.18753108334302546(Epoch= 35)\n",
      "CV loss=0.19715236815129927(Epoch= 36)\n",
      "train loss=0.18412959052261735(Epoch= 36)\n",
      "CV loss=0.19506678940328986(Epoch= 37)\n",
      "train loss=0.18122953233059064(Epoch= 37)\n",
      "CV loss=0.19322448783277021(Epoch= 38)\n",
      "train loss=0.1788792629657522(Epoch= 38)\n",
      "CV loss=0.19054026135934932(Epoch= 39)\n",
      "train loss=0.17603730592618916(Epoch= 39)\n",
      "CV loss=0.18859475249552252(Epoch= 40)\n",
      "train loss=0.17377917407325186(Epoch= 40)\n",
      "CV loss=0.18614853359888175(Epoch= 41)\n",
      "train loss=0.17119547270908222(Epoch= 41)\n",
      "CV loss=0.18395889182440733(Epoch= 42)\n",
      "train loss=0.1687464689405491(Epoch= 42)\n",
      "CV loss=0.18283909605324028(Epoch= 43)\n",
      "train loss=0.1668006678007588(Epoch= 43)\n",
      "CV loss=0.18137818834363648(Epoch= 44)\n",
      "train loss=0.16488309698885892(Epoch= 44)\n",
      "CV loss=0.17844640111564258(Epoch= 45)\n",
      "train loss=0.16214563843443983(Epoch= 45)\n",
      "CV loss=0.1763309329638679(Epoch= 46)\n",
      "train loss=0.15995758529007195(Epoch= 46)\n",
      "CV loss=0.17466548850915042(Epoch= 47)\n",
      "train loss=0.15824588231019518(Epoch= 47)\n",
      "CV loss=0.17306729058592438(Epoch= 48)\n",
      "train loss=0.15609826339604216(Epoch= 48)\n",
      "CV loss=0.17091063366169273(Epoch= 49)\n",
      "train loss=0.15387860064159797(Epoch= 49)\n",
      "CV loss=0.16887692121976522(Epoch= 50)\n",
      "train loss=0.1521496313021721(Epoch= 50)\n",
      "CV loss=0.16834850193797962(Epoch= 51)\n",
      "train loss=0.15031921166552462(Epoch= 51)\n",
      "CV loss=0.16605847579263788(Epoch= 52)\n",
      "train loss=0.14802854542404015(Epoch= 52)\n",
      "CV loss=0.1646218387783153(Epoch= 53)\n",
      "train loss=0.14697507321236467(Epoch= 53)\n",
      "CV loss=0.16290778795826422(Epoch= 54)\n",
      "train loss=0.14443137554897387(Epoch= 54)\n",
      "CV loss=0.16070240920745021(Epoch= 55)\n",
      "train loss=0.14295884148138172(Epoch= 55)\n",
      "CV loss=0.15927745742668178(Epoch= 56)\n",
      "train loss=0.14101427644763864(Epoch= 56)\n",
      "CV loss=0.15836045384657674(Epoch= 57)\n",
      "train loss=0.13977490503911816(Epoch= 57)\n",
      "CV loss=0.1563539282280228(Epoch= 58)\n",
      "train loss=0.1379917797247472(Epoch= 58)\n",
      "CV loss=0.15560670790595127(Epoch= 59)\n",
      "train loss=0.13628236194285964(Epoch= 59)\n",
      "CV loss=0.15376876692574576(Epoch= 60)\n",
      "train loss=0.13457380762018406(Epoch= 60)\n",
      "CV loss=0.15246301329418052(Epoch= 61)\n",
      "train loss=0.132888909406896(Epoch= 61)\n",
      "CV loss=0.15250847507801657(Epoch= 62)\n",
      "train loss=0.1319668755346796(Epoch= 62)\n",
      "CV loss=0.150525061718465(Epoch= 63)\n",
      "train loss=0.12993527408998556(Epoch= 63)\n",
      "CV loss=0.1489310628132395(Epoch= 64)\n",
      "train loss=0.12862336978837396(Epoch= 64)\n",
      "CV loss=0.1476221623778074(Epoch= 65)\n",
      "train loss=0.127023678667558(Epoch= 65)\n",
      "CV loss=0.14609491361610355(Epoch= 66)\n",
      "train loss=0.12561512180583465(Epoch= 66)\n",
      "CV loss=0.145489637320586(Epoch= 67)\n",
      "train loss=0.12444504081027717(Epoch= 67)\n",
      "CV loss=0.14394436573471797(Epoch= 68)\n",
      "train loss=0.1229913806110525(Epoch= 68)\n",
      "CV loss=0.14278792857046757(Epoch= 69)\n",
      "train loss=0.12163855745537387(Epoch= 69)\n",
      "CV loss=0.14195230275433374(Epoch= 70)\n",
      "train loss=0.12067007189027439(Epoch= 70)\n",
      "CV loss=0.14077202291604282(Epoch= 71)\n",
      "train loss=0.11898399999913181(Epoch= 71)\n",
      "CV loss=0.13959105965752444(Epoch= 72)\n",
      "train loss=0.11770384832649058(Epoch= 72)\n",
      "CV loss=0.138814950271328(Epoch= 73)\n",
      "train loss=0.11650550458377257(Epoch= 73)\n",
      "CV loss=0.13801267778238804(Epoch= 74)\n",
      "train loss=0.11539871127058593(Epoch= 74)\n",
      "CV loss=0.13665813111839104(Epoch= 75)\n",
      "train loss=0.11402766725169178(Epoch= 75)\n",
      "CV loss=0.13591904119378054(Epoch= 76)\n",
      "train loss=0.11313014764382602(Epoch= 76)\n",
      "CV loss=0.13509112162821937(Epoch= 77)\n",
      "train loss=0.11192743169677521(Epoch= 77)\n",
      "CV loss=0.1340279222426554(Epoch= 78)\n",
      "train loss=0.11089037164685732(Epoch= 78)\n",
      "CV loss=0.13301795497100846(Epoch= 79)\n",
      "train loss=0.10987447547157196(Epoch= 79)\n",
      "CV loss=0.13224666887880904(Epoch= 80)\n",
      "train loss=0.10896616797884792(Epoch= 80)\n",
      "CV loss=0.1312327915158682(Epoch= 81)\n",
      "train loss=0.10737116490179384(Epoch= 81)\n",
      "CV loss=0.1298206616332888(Epoch= 82)\n",
      "train loss=0.10637513185778354(Epoch= 82)\n",
      "CV loss=0.12970101124340405(Epoch= 83)\n",
      "train loss=0.10531815542186422(Epoch= 83)\n",
      "CV loss=0.12857814877321275(Epoch= 84)\n",
      "train loss=0.10411058379915096(Epoch= 84)\n",
      "CV loss=0.12836748899251493(Epoch= 85)\n",
      "train loss=0.10337690082164727(Epoch= 85)\n",
      "CV loss=0.12706095560024752(Epoch= 86)\n",
      "train loss=0.10222328645697494(Epoch= 86)\n",
      "CV loss=0.12605240664340767(Epoch= 87)\n",
      "train loss=0.1011920539729844(Epoch= 87)\n",
      "CV loss=0.12573595046761832(Epoch= 88)\n",
      "train loss=0.10038986366732423(Epoch= 88)\n",
      "CV loss=0.12472289538961477(Epoch= 89)\n",
      "train loss=0.0992601280032865(Epoch= 89)\n",
      "CV loss=0.12472576269384968(Epoch= 90)\n",
      "train loss=0.09854109812252931(Epoch= 90)\n",
      "CV loss=0.12316955891276919(Epoch= 91)\n",
      "train loss=0.09745008836744626(Epoch= 91)\n",
      "CV loss=0.12315513951433903(Epoch= 92)\n",
      "train loss=0.09654666071765883(Epoch= 92)\n",
      "CV loss=0.1216867092892231(Epoch= 93)\n",
      "train loss=0.09571787669960771(Epoch= 93)\n",
      "CV loss=0.12083711083155763(Epoch= 94)\n",
      "train loss=0.09482775815364662(Epoch= 94)\n",
      "CV loss=0.12135439835682499(Epoch= 95)\n",
      "train loss=0.0943648876616352(Epoch= 95)\n",
      "CV loss=0.12020126891048674(Epoch= 96)\n",
      "train loss=0.09294147816447292(Epoch= 96)\n",
      "CV loss=0.11873820545710945(Epoch= 97)\n",
      "train loss=0.09220105230909423(Epoch= 97)\n",
      "CV loss=0.11843650683542127(Epoch= 98)\n",
      "train loss=0.09114554753981347(Epoch= 98)\n",
      "CV loss=0.1184301153708038(Epoch= 99)\n",
      "train loss=0.090365644971857(Epoch= 99)\n",
      "CV loss=0.1165750560047497(Epoch= 100)\n",
      "train loss=0.08964027629315742(Epoch= 100)\n",
      "CV loss=0.11594934878374624(Epoch= 101)\n",
      "train loss=0.08892429492876272(Epoch= 101)\n",
      "CV loss=0.11555905354552519(Epoch= 102)\n",
      "train loss=0.08797761826763453(Epoch= 102)\n",
      "CV loss=0.11513208905227479(Epoch= 103)\n",
      "train loss=0.0871059119863192(Epoch= 103)\n",
      "CV loss=0.11483422082298855(Epoch= 104)\n",
      "train loss=0.08632204339831995(Epoch= 104)\n",
      "CV loss=0.11509334245218365(Epoch= 105)\n",
      "train loss=0.08593237054913148(Epoch= 105)\n",
      "CV loss=0.11319815179671577(Epoch= 106)\n",
      "train loss=0.08474419567474245(Epoch= 106)\n",
      "CV loss=0.11296435431168697(Epoch= 107)\n",
      "train loss=0.08397261858158656(Epoch= 107)\n",
      "CV loss=0.11219961928864192(Epoch= 108)\n",
      "train loss=0.08350823507758984(Epoch= 108)\n",
      "CV loss=0.11132594989053285(Epoch= 109)\n",
      "train loss=0.0824954383803401(Epoch= 109)\n",
      "CV loss=0.11164447736278796(Epoch= 110)\n",
      "train loss=0.08197481026042887(Epoch= 110)\n",
      "CV loss=0.1105313527008154(Epoch= 111)\n",
      "train loss=0.08144468129512897(Epoch= 111)\n",
      "CV loss=0.11021918095048071(Epoch= 112)\n",
      "train loss=0.08037223745951279(Epoch= 112)\n",
      "CV loss=0.11052051364260032(Epoch= 113)\n",
      "train loss=0.08007023730390678(Epoch= 113)\n",
      "CV loss=0.10986647494616346(Epoch= 114)\n",
      "train loss=0.0793320138459981(Epoch= 114)\n",
      "CV loss=0.10908252810221784(Epoch= 115)\n",
      "train loss=0.07838105445385853(Epoch= 115)\n",
      "CV loss=0.10845474624714009(Epoch= 116)\n",
      "train loss=0.07771178476472833(Epoch= 116)\n",
      "CV loss=0.10830718029987312(Epoch= 117)\n",
      "train loss=0.07712771029086188(Epoch= 117)\n",
      "CV loss=0.10787112860640545(Epoch= 118)\n",
      "train loss=0.07655346166325286(Epoch= 118)\n",
      "CV loss=0.10716406893344108(Epoch= 119)\n",
      "train loss=0.07586108269685668(Epoch= 119)\n",
      "CV loss=0.10675320553083968(Epoch= 120)\n",
      "train loss=0.07536050924116028(Epoch= 120)\n",
      "CV loss=0.10602873148098353(Epoch= 121)\n",
      "train loss=0.07447795829575993(Epoch= 121)\n",
      "CV loss=0.10573148448849702(Epoch= 122)\n",
      "train loss=0.0738816339410889(Epoch= 122)\n",
      "CV loss=0.10518593980170689(Epoch= 123)\n",
      "train loss=0.07323414448165194(Epoch= 123)\n",
      "CV loss=0.10447684698719525(Epoch= 124)\n",
      "train loss=0.07256269416880823(Epoch= 124)\n",
      "CV loss=0.10453550425942255(Epoch= 125)\n",
      "train loss=0.07220285956184172(Epoch= 125)\n",
      "CV loss=0.10382474976264613(Epoch= 126)\n",
      "train loss=0.07142607777030155(Epoch= 126)\n",
      "CV loss=0.10354370459033414(Epoch= 127)\n",
      "train loss=0.07093090954236576(Epoch= 127)\n",
      "CV loss=0.10412317912028801(Epoch= 128)\n",
      "train loss=0.07058577767728784(Epoch= 128)\n",
      "CV loss=0.102482553888933(Epoch= 129)\n",
      "train loss=0.06985329065761894(Epoch= 129)\n",
      "CV loss=0.10259176348448278(Epoch= 130)\n",
      "train loss=0.06901372861351862(Epoch= 130)\n",
      "CV loss=0.10210612621832925(Epoch= 131)\n",
      "train loss=0.06860482945104326(Epoch= 131)\n",
      "CV loss=0.10165733880532622(Epoch= 132)\n",
      "train loss=0.06818900102250283(Epoch= 132)\n",
      "CV loss=0.10145175539729862(Epoch= 133)\n",
      "train loss=0.06752704593872566(Epoch= 133)\n",
      "CV loss=0.10073127046543826(Epoch= 134)\n",
      "train loss=0.06686745025665983(Epoch= 134)\n",
      "CV loss=0.10075563899158954(Epoch= 135)\n",
      "train loss=0.0668486166890309(Epoch= 135)\n",
      "CV loss=0.09953945311789947(Epoch= 136)\n",
      "train loss=0.06605003946848495(Epoch= 136)\n",
      "CV loss=0.09994916528091596(Epoch= 137)\n",
      "train loss=0.06534001382549377(Epoch= 137)\n",
      "CV loss=0.09911798156352805(Epoch= 138)\n",
      "train loss=0.06481064133251674(Epoch= 138)\n",
      "CV loss=0.09901693811885828(Epoch= 139)\n",
      "train loss=0.06430838668724728(Epoch= 139)\n",
      "CV loss=0.09909038748823643(Epoch= 140)\n",
      "train loss=0.0638739944273884(Epoch= 140)\n",
      "CV loss=0.09810454915165438(Epoch= 141)\n",
      "train loss=0.06333430165923658(Epoch= 141)\n",
      "CV loss=0.09889743601464347(Epoch= 142)\n",
      "train loss=0.06310261969537699(Epoch= 142)\n",
      "CV loss=0.09773576975933283(Epoch= 143)\n",
      "train loss=0.06247396532446146(Epoch= 143)\n",
      "CV loss=0.09791359658718252(Epoch= 144)\n",
      "train loss=0.061818033310483085(Epoch= 144)\n",
      "CV loss=0.09741622101490643(Epoch= 145)\n",
      "train loss=0.06142654675253518(Epoch= 145)\n",
      "CV loss=0.09771354138099836(Epoch= 146)\n",
      "train loss=0.06088593829885531(Epoch= 146)\n",
      "CV loss=0.0972802659069582(Epoch= 147)\n",
      "train loss=0.06043740870956119(Epoch= 147)\n",
      "CV loss=0.09675487591015612(Epoch= 148)\n",
      "train loss=0.06000881080301231(Epoch= 148)\n",
      "CV loss=0.09633435430524656(Epoch= 149)\n",
      "train loss=0.05937823301995317(Epoch= 149)\n",
      "CV loss=0.09551240194226154(Epoch= 150)\n",
      "train loss=0.05891094489463028(Epoch= 150)\n",
      "CV loss=0.09521891670895566(Epoch= 151)\n",
      "train loss=0.05855962723061964(Epoch= 151)\n",
      "CV loss=0.09514186018892468(Epoch= 152)\n",
      "train loss=0.05805556580524046(Epoch= 152)\n",
      "CV loss=0.09557180743278804(Epoch= 153)\n",
      "train loss=0.05772055640252408(Epoch= 153)\n",
      "CV loss=0.09484860863439817(Epoch= 154)\n",
      "train loss=0.05735411031194886(Epoch= 154)\n",
      "CV loss=0.09427750252266154(Epoch= 155)\n",
      "train loss=0.056846168065277654(Epoch= 155)\n",
      "CV loss=0.09427154231472787(Epoch= 156)\n",
      "train loss=0.056275952053380325(Epoch= 156)\n",
      "CV loss=0.09370767804125192(Epoch= 157)\n",
      "train loss=0.055912349619928296(Epoch= 157)\n",
      "CV loss=0.09362197771413315(Epoch= 158)\n",
      "train loss=0.055401720067515106(Epoch= 158)\n",
      "CV loss=0.09333858638130511(Epoch= 159)\n",
      "train loss=0.05502599271718143(Epoch= 159)\n",
      "CV loss=0.09373156346365197(Epoch= 160)\n",
      "train loss=0.05466061059630857(Epoch= 160)\n",
      "CV loss=0.09281832211930814(Epoch= 161)\n",
      "train loss=0.05423495112049059(Epoch= 161)\n",
      "CV loss=0.09298883138553186(Epoch= 162)\n",
      "train loss=0.053828879410460946(Epoch= 162)\n",
      "CV loss=0.09221843406038602(Epoch= 163)\n",
      "train loss=0.053369641240931465(Epoch= 163)\n",
      "CV loss=0.09221347000396295(Epoch= 164)\n",
      "train loss=0.05302321730624177(Epoch= 164)\n",
      "CV loss=0.09141411250520619(Epoch= 165)\n",
      "train loss=0.05260703187970645(Epoch= 165)\n",
      "CV loss=0.09164558418603098(Epoch= 166)\n",
      "train loss=0.05225004230200136(Epoch= 166)\n",
      "CV loss=0.09163393378461457(Epoch= 167)\n",
      "train loss=0.05185386624220504(Epoch= 167)\n",
      "CV loss=0.09113657465172537(Epoch= 168)\n",
      "train loss=0.05148117816469265(Epoch= 168)\n",
      "CV loss=0.09159852938434714(Epoch= 169)\n",
      "train loss=0.051080364224460856(Epoch= 169)\n",
      "CV loss=0.09074619421510302(Epoch= 170)\n",
      "train loss=0.050609290320920375(Epoch= 170)\n",
      "CV loss=0.09108191949889777(Epoch= 171)\n",
      "train loss=0.05023473701234023(Epoch= 171)\n",
      "CV loss=0.0906307932553957(Epoch= 172)\n",
      "train loss=0.04997431271025649(Epoch= 172)\n",
      "CV loss=0.09038530214980439(Epoch= 173)\n",
      "train loss=0.04962267011857578(Epoch= 173)\n",
      "CV loss=0.08963016494093212(Epoch= 174)\n",
      "train loss=0.04909037006345646(Epoch= 174)\n",
      "CV loss=0.09045515824160615(Epoch= 175)\n",
      "train loss=0.04878560745753511(Epoch= 175)\n",
      "CV loss=0.08982803525317556(Epoch= 176)\n",
      "train loss=0.04835345839783353(Epoch= 176)\n",
      "CV loss=0.08923619974673097(Epoch= 177)\n",
      "train loss=0.04816071563865862(Epoch= 177)\n",
      "CV loss=0.08930774564422068(Epoch= 178)\n",
      "train loss=0.0476794256975949(Epoch= 178)\n",
      "CV loss=0.08925940508679851(Epoch= 179)\n",
      "train loss=0.04733459954705068(Epoch= 179)\n",
      "CV loss=0.08948984370908701(Epoch= 180)\n",
      "train loss=0.047040760776915366(Epoch= 180)\n",
      "CV loss=0.08928060691917128(Epoch= 181)\n",
      "train loss=0.046925494009098054(Epoch= 181)\n",
      "CV loss=0.08816242968483257(Epoch= 182)\n",
      "train loss=0.04631210723558282(Epoch= 182)\n",
      "CV loss=0.08871668546697961(Epoch= 183)\n",
      "train loss=0.04608008491984642(Epoch= 183)\n",
      "CV loss=0.08823984780454697(Epoch= 184)\n",
      "train loss=0.04556214107494979(Epoch= 184)\n",
      "CV loss=0.08785498742344887(Epoch= 185)\n",
      "train loss=0.04554012782582494(Epoch= 185)\n",
      "CV loss=0.08771443915249849(Epoch= 186)\n",
      "train loss=0.04503170314880209(Epoch= 186)\n",
      "CV loss=0.0876452284550127(Epoch= 187)\n",
      "train loss=0.04469921600195373(Epoch= 187)\n",
      "CV loss=0.08790200126578732(Epoch= 188)\n",
      "train loss=0.044498021293724724(Epoch= 188)\n",
      "CV loss=0.08790568388473916(Epoch= 189)\n",
      "train loss=0.04405527635407953(Epoch= 189)\n",
      "CV loss=0.08683535026310782(Epoch= 190)\n",
      "train loss=0.04377105825485358(Epoch= 190)\n",
      "CV loss=0.08718565291826424(Epoch= 191)\n",
      "train loss=0.04336623039709962(Epoch= 191)\n",
      "CV loss=0.08714350051468221(Epoch= 192)\n",
      "train loss=0.04312853593724638(Epoch= 192)\n",
      "CV loss=0.08760708665812043(Epoch= 193)\n",
      "train loss=0.042936604800029594(Epoch= 193)\n",
      "CV loss=0.0868133689500234(Epoch= 194)\n",
      "train loss=0.0427181013091556(Epoch= 194)\n",
      "CV loss=0.08680721314756618(Epoch= 195)\n",
      "train loss=0.04215747016123573(Epoch= 195)\n",
      "CV loss=0.08661228065763467(Epoch= 196)\n",
      "train loss=0.04179348417039639(Epoch= 196)\n",
      "CV loss=0.08641979312139367(Epoch= 197)\n",
      "train loss=0.04148584012986569(Epoch= 197)\n",
      "CV loss=0.08585861141461465(Epoch= 198)\n",
      "train loss=0.04119861555251585(Epoch= 198)\n",
      "CV loss=0.08671018819091228(Epoch= 199)\n",
      "train loss=0.041333549125342324(Epoch= 199)\n",
      "CV loss=0.0854366815137645(Epoch= 200)\n",
      "train loss=0.04065846798364433(Epoch= 200)\n",
      "CV loss=0.08604504663998491(Epoch= 201)\n",
      "train loss=0.04044486253355109(Epoch= 201)\n",
      "CV loss=0.08565495619446277(Epoch= 202)\n",
      "train loss=0.040030039434461755(Epoch= 202)\n",
      "CV loss=0.08475113070917001(Epoch= 203)\n",
      "train loss=0.039810306337848496(Epoch= 203)\n",
      "CV loss=0.08539973793810479(Epoch= 204)\n",
      "train loss=0.03951563939401635(Epoch= 204)\n",
      "CV loss=0.08521842748992882(Epoch= 205)\n",
      "train loss=0.03916946071058681(Epoch= 205)\n",
      "CV loss=0.08514433484538411(Epoch= 206)\n",
      "train loss=0.03903713366938109(Epoch= 206)\n",
      "CV loss=0.08450883793847093(Epoch= 207)\n",
      "train loss=0.03871181065863173(Epoch= 207)\n",
      "CV loss=0.08439047920016664(Epoch= 208)\n",
      "train loss=0.03833928332033127(Epoch= 208)\n",
      "CV loss=0.08554724479763302(Epoch= 209)\n",
      "train loss=0.038433182795629524(Epoch= 209)\n",
      "CV loss=0.0844253693660916(Epoch= 210)\n",
      "train loss=0.03779956885151631(Epoch= 210)\n",
      "CV loss=0.08398297077302859(Epoch= 211)\n",
      "train loss=0.03753334335073055(Epoch= 211)\n",
      "CV loss=0.08501158897737915(Epoch= 212)\n",
      "train loss=0.03753005632140175(Epoch= 212)\n",
      "CV loss=0.08388833633460148(Epoch= 213)\n",
      "train loss=0.03718444385116939(Epoch= 213)\n",
      "CV loss=0.08427674232927154(Epoch= 214)\n",
      "train loss=0.03696109286800986(Epoch= 214)\n",
      "CV loss=0.08390263369832493(Epoch= 215)\n",
      "train loss=0.03649710171429219(Epoch= 215)\n",
      "CV loss=0.08380668385308(Epoch= 216)\n",
      "train loss=0.03624517865548489(Epoch= 216)\n",
      "CV loss=0.08358989922120938(Epoch= 217)\n",
      "train loss=0.03601442242043475(Epoch= 217)\n",
      "CV loss=0.08333101464862246(Epoch= 218)\n",
      "train loss=0.03592668297978791(Epoch= 218)\n",
      "CV loss=0.08292974276987428(Epoch= 219)\n",
      "train loss=0.0355124205732972(Epoch= 219)\n",
      "CV loss=0.08303528163639678(Epoch= 220)\n",
      "train loss=0.03519307429356975(Epoch= 220)\n",
      "CV loss=0.08398530901592745(Epoch= 221)\n",
      "train loss=0.03515113438376758(Epoch= 221)\n",
      "CV loss=0.08362665008085751(Epoch= 222)\n",
      "train loss=0.03505006630742099(Epoch= 222)\n",
      "CV loss=0.08271224295563748(Epoch= 223)\n",
      "train loss=0.034570881136474725(Epoch= 223)\n",
      "CV loss=0.08293803373933362(Epoch= 224)\n",
      "train loss=0.03423708127565253(Epoch= 224)\n",
      "CV loss=0.08307185338229939(Epoch= 225)\n",
      "train loss=0.03403144696453037(Epoch= 225)\n",
      "CV loss=0.08335561886158777(Epoch= 226)\n",
      "train loss=0.03387287278683865(Epoch= 226)\n",
      "CV loss=0.08314166832143481(Epoch= 227)\n",
      "train loss=0.033677937008589905(Epoch= 227)\n",
      "CV loss=0.08179781034616768(Epoch= 228)\n",
      "train loss=0.03357422304154288(Epoch= 228)\n",
      "CV loss=0.08234060901953465(Epoch= 229)\n",
      "train loss=0.033049486763617665(Epoch= 229)\n",
      "CV loss=0.08314736534762258(Epoch= 230)\n",
      "train loss=0.03302736677251955(Epoch= 230)\n",
      "CV loss=0.08216091332317761(Epoch= 231)\n",
      "train loss=0.032627271935840536(Epoch= 231)\n",
      "CV loss=0.08210672146486502(Epoch= 232)\n",
      "train loss=0.032352431109040056(Epoch= 232)\n",
      "CV loss=0.08180195502809928(Epoch= 233)\n",
      "train loss=0.032163943528027296(Epoch= 233)\n",
      "CV loss=0.08215241272087274(Epoch= 234)\n",
      "train loss=0.0320515352899628(Epoch= 234)\n",
      "CV loss=0.08176682207892627(Epoch= 235)\n",
      "train loss=0.0319163830264924(Epoch= 235)\n",
      "CV loss=0.08213901406569636(Epoch= 236)\n",
      "train loss=0.031548793381626514(Epoch= 236)\n",
      "CV loss=0.08170531656046469(Epoch= 237)\n",
      "train loss=0.031296890664669945(Epoch= 237)\n",
      "CV loss=0.08175204730809496(Epoch= 238)\n",
      "train loss=0.031133636365804997(Epoch= 238)\n",
      "CV loss=0.08152803902059531(Epoch= 239)\n",
      "train loss=0.031093910274402926(Epoch= 239)\n",
      "CV loss=0.08096130894060703(Epoch= 240)\n",
      "train loss=0.030625313873525267(Epoch= 240)\n",
      "CV loss=0.08102112031649521(Epoch= 241)\n",
      "train loss=0.030470510918773112(Epoch= 241)\n",
      "CV loss=0.08104186197628402(Epoch= 242)\n",
      "train loss=0.030249022437886942(Epoch= 242)\n",
      "CV loss=0.08067675985580584(Epoch= 243)\n",
      "train loss=0.03011677154124479(Epoch= 243)\n",
      "CV loss=0.0803400918566724(Epoch= 244)\n",
      "train loss=0.02988319490195155(Epoch= 244)\n",
      "CV loss=0.0811166256509525(Epoch= 245)\n",
      "train loss=0.02967585634218695(Epoch= 245)\n",
      "CV loss=0.08143991410753555(Epoch= 246)\n",
      "train loss=0.029529348415707535(Epoch= 246)\n",
      "CV loss=0.0806933719128866(Epoch= 247)\n",
      "train loss=0.029277005291010197(Epoch= 247)\n",
      "CV loss=0.08103009101607121(Epoch= 248)\n",
      "train loss=0.02915549257707262(Epoch= 248)\n",
      "CV loss=0.08114478382097381(Epoch= 249)\n",
      "train loss=0.028937043438549213(Epoch= 249)\n",
      "CV loss=0.08056901615029817(Epoch= 250)\n",
      "train loss=0.028775563490238167(Epoch= 250)\n",
      "CV loss=0.08010092013357524(Epoch= 251)\n",
      "train loss=0.02844678082108397(Epoch= 251)\n",
      "CV loss=0.08059728596270442(Epoch= 252)\n",
      "train loss=0.028325969742897444(Epoch= 252)\n",
      "CV loss=0.08025873216393649(Epoch= 253)\n",
      "train loss=0.027997405666376932(Epoch= 253)\n",
      "CV loss=0.08010854548246725(Epoch= 254)\n",
      "train loss=0.027952205903410687(Epoch= 254)\n",
      "CV loss=0.08120547461457338(Epoch= 255)\n",
      "train loss=0.02779379777971062(Epoch= 255)\n",
      "CV loss=0.08002216130226472(Epoch= 256)\n",
      "train loss=0.027560212928066267(Epoch= 256)\n",
      "CV loss=0.07992567762805604(Epoch= 257)\n",
      "train loss=0.027436488533657676(Epoch= 257)\n",
      "CV loss=0.08001906572114399(Epoch= 258)\n",
      "train loss=0.02709677593782673(Epoch= 258)\n",
      "CV loss=0.0798440299942408(Epoch= 259)\n",
      "train loss=0.02697990889273188(Epoch= 259)\n",
      "CV loss=0.0801842207029999(Epoch= 260)\n",
      "train loss=0.026765854077287217(Epoch= 260)\n",
      "CV loss=0.07968159611248106(Epoch= 261)\n",
      "train loss=0.026541563315288685(Epoch= 261)\n",
      "CV loss=0.07984649530295113(Epoch= 262)\n",
      "train loss=0.026379401886793417(Epoch= 262)\n",
      "CV loss=0.07931256797225847(Epoch= 263)\n",
      "train loss=0.026489483515329437(Epoch= 263)\n",
      "CV loss=0.07908658754415722(Epoch= 264)\n",
      "train loss=0.026167462575649277(Epoch= 264)\n",
      "CV loss=0.07954002673608132(Epoch= 265)\n",
      "train loss=0.025888911173165875(Epoch= 265)\n",
      "CV loss=0.07935728506653306(Epoch= 266)\n",
      "train loss=0.025640674852184655(Epoch= 266)\n",
      "CV loss=0.07952660672772523(Epoch= 267)\n",
      "train loss=0.02553082313833668(Epoch= 267)\n",
      "CV loss=0.07949983438202246(Epoch= 268)\n",
      "train loss=0.025375817389948534(Epoch= 268)\n",
      "CV loss=0.0791897704962679(Epoch= 269)\n",
      "train loss=0.025155100371087046(Epoch= 269)\n",
      "CV loss=0.07937919817488494(Epoch= 270)\n",
      "train loss=0.024977635713527103(Epoch= 270)\n",
      "CV loss=0.07908402718758624(Epoch= 271)\n",
      "train loss=0.024910817840165522(Epoch= 271)\n",
      "CV loss=0.07936212403570796(Epoch= 272)\n",
      "train loss=0.024701831127020034(Epoch= 272)\n",
      "CV loss=0.07933107002852581(Epoch= 273)\n",
      "train loss=0.02449185667305776(Epoch= 273)\n",
      "CV loss=0.07863527926888607(Epoch= 274)\n",
      "train loss=0.024411726745592797(Epoch= 274)\n",
      "CV loss=0.07918547349475183(Epoch= 275)\n",
      "train loss=0.024368326311363715(Epoch= 275)\n",
      "CV loss=0.07906680812165623(Epoch= 276)\n",
      "train loss=0.023993690324707603(Epoch= 276)\n",
      "CV loss=0.07921756451075473(Epoch= 277)\n",
      "train loss=0.023963281124476623(Epoch= 277)\n",
      "CV loss=0.07916426754528919(Epoch= 278)\n",
      "train loss=0.023907807109128206(Epoch= 278)\n",
      "CV loss=0.07838901897505517(Epoch= 279)\n",
      "train loss=0.023715108916057666(Epoch= 279)\n",
      "CV loss=0.07890594617777454(Epoch= 280)\n",
      "train loss=0.02338281810135185(Epoch= 280)\n",
      "CV loss=0.07870868488825969(Epoch= 281)\n",
      "train loss=0.023271890003897015(Epoch= 281)\n",
      "CV loss=0.07913394998290924(Epoch= 282)\n",
      "train loss=0.02329804493033261(Epoch= 282)\n",
      "CV loss=0.07845690435674328(Epoch= 283)\n",
      "train loss=0.02293584778057503(Epoch= 283)\n",
      "CV loss=0.07863338200502462(Epoch= 284)\n",
      "train loss=0.022732674029618562(Epoch= 284)\n",
      "CV loss=0.07873350693706528(Epoch= 285)\n",
      "train loss=0.022652448181490707(Epoch= 285)\n",
      "CV loss=0.07841980070275424(Epoch= 286)\n",
      "train loss=0.022501249214675813(Epoch= 286)\n",
      "CV loss=0.07851668814628117(Epoch= 287)\n",
      "train loss=0.022357900682083334(Epoch= 287)\n",
      "CV loss=0.078405623850976(Epoch= 288)\n",
      "train loss=0.022174916500862483(Epoch= 288)\n",
      "CV loss=0.0784065237171559(Epoch= 289)\n",
      "train loss=0.022020445814448064(Epoch= 289)\n",
      "CV loss=0.07852204240744835(Epoch= 290)\n",
      "train loss=0.021862201026151572(Epoch= 290)\n",
      "CV loss=0.07819407024947106(Epoch= 291)\n",
      "train loss=0.021779927192390306(Epoch= 291)\n",
      "CV loss=0.0784477652997439(Epoch= 292)\n",
      "train loss=0.0216050453416558(Epoch= 292)\n",
      "CV loss=0.07807011378420319(Epoch= 293)\n",
      "train loss=0.02146732684575975(Epoch= 293)\n",
      "CV loss=0.07791924437620712(Epoch= 294)\n",
      "train loss=0.021327611703073575(Epoch= 294)\n",
      "CV loss=0.07816323004918146(Epoch= 295)\n",
      "train loss=0.021213778900986892(Epoch= 295)\n",
      "CV loss=0.0782710663317255(Epoch= 296)\n",
      "train loss=0.021034498452655342(Epoch= 296)\n",
      "CV loss=0.07785545768748177(Epoch= 297)\n",
      "train loss=0.020897957772957713(Epoch= 297)\n",
      "CV loss=0.07857570179257384(Epoch= 298)\n",
      "train loss=0.020897752560063383(Epoch= 298)\n",
      "CV loss=0.0776725066946704(Epoch= 299)\n",
      "train loss=0.02076416283034728(Epoch= 299)\n",
      "CV loss=0.07778379473287819(Epoch= 300)\n",
      "train loss=0.02065313093664031(Epoch= 300)\n",
      "CV loss=0.07792758566330385(Epoch= 301)\n",
      "train loss=0.0204225424601521(Epoch= 301)\n",
      "CV loss=0.07802899789291931(Epoch= 302)\n",
      "train loss=0.020313213772060697(Epoch= 302)\n",
      "CV loss=0.07751390088809425(Epoch= 303)\n",
      "train loss=0.020129068398714967(Epoch= 303)\n",
      "CV loss=0.07809587125283442(Epoch= 304)\n",
      "train loss=0.0199948343960688(Epoch= 304)\n",
      "CV loss=0.07816753817418604(Epoch= 305)\n",
      "train loss=0.01984966832988876(Epoch= 305)\n",
      "CV loss=0.07771347580890331(Epoch= 306)\n",
      "train loss=0.01976686384844888(Epoch= 306)\n",
      "CV loss=0.07824936763594047(Epoch= 307)\n",
      "train loss=0.019788972332130023(Epoch= 307)\n",
      "CV loss=0.0775780966417633(Epoch= 308)\n",
      "train loss=0.019469914091146727(Epoch= 308)\n",
      "CV loss=0.07826992121146305(Epoch= 309)\n",
      "train loss=0.01937862313022066(Epoch= 309)\n",
      "CV loss=0.07804425574795489(Epoch= 310)\n",
      "train loss=0.019217122062983205(Epoch= 310)\n",
      "CV loss=0.07746593268506775(Epoch= 311)\n",
      "train loss=0.019078740894537226(Epoch= 311)\n",
      "CV loss=0.0776259840667303(Epoch= 312)\n",
      "train loss=0.018988032784155444(Epoch= 312)\n",
      "CV loss=0.07771711117174215(Epoch= 313)\n",
      "train loss=0.01890829366479295(Epoch= 313)\n",
      "CV loss=0.07752288822127987(Epoch= 314)\n",
      "train loss=0.018723203033892407(Epoch= 314)\n",
      "CV loss=0.0780857125391033(Epoch= 315)\n",
      "train loss=0.018639447420624827(Epoch= 315)\n",
      "CV loss=0.07729543291631805(Epoch= 316)\n",
      "train loss=0.018541013823011546(Epoch= 316)\n",
      "CV loss=0.07789302250684706(Epoch= 317)\n",
      "train loss=0.018433761645049035(Epoch= 317)\n",
      "CV loss=0.07735418804270193(Epoch= 318)\n",
      "train loss=0.01824142686202459(Epoch= 318)\n",
      "CV loss=0.07756813316944237(Epoch= 319)\n",
      "train loss=0.018158420236863735(Epoch= 319)\n",
      "CV loss=0.07724956141992724(Epoch= 320)\n",
      "train loss=0.018067323640550352(Epoch= 320)\n",
      "CV loss=0.0777863625258684(Epoch= 321)\n",
      "train loss=0.017945356318347278(Epoch= 321)\n",
      "CV loss=0.0773246827850898(Epoch= 322)\n",
      "train loss=0.017798228977914405(Epoch= 322)\n",
      "CV loss=0.07740039697732326(Epoch= 323)\n",
      "train loss=0.017747154120294567(Epoch= 323)\n",
      "CV loss=0.07733556250483352(Epoch= 324)\n",
      "train loss=0.01762144505593688(Epoch= 324)\n",
      "CV loss=0.07749924947329288(Epoch= 325)\n",
      "train loss=0.017484168326872676(Epoch= 325)\n",
      "CV loss=0.07728417852897473(Epoch= 326)\n",
      "train loss=0.017380050660874266(Epoch= 326)\n",
      "CV loss=0.07743875081061015(Epoch= 327)\n",
      "train loss=0.017392880570032215(Epoch= 327)\n",
      "CV loss=0.07686183560715931(Epoch= 328)\n",
      "train loss=0.017191987564177846(Epoch= 328)\n",
      "CV loss=0.07735040970368291(Epoch= 329)\n",
      "train loss=0.01715613928361387(Epoch= 329)\n",
      "CV loss=0.07732212623988624(Epoch= 330)\n",
      "train loss=0.017047980285333903(Epoch= 330)\n",
      "CV loss=0.07715611035124609(Epoch= 331)\n",
      "train loss=0.0169268426406008(Epoch= 331)\n",
      "CV loss=0.07733921658700108(Epoch= 332)\n",
      "train loss=0.016774772933501568(Epoch= 332)\n",
      "CV loss=0.07722705984548847(Epoch= 333)\n",
      "train loss=0.01663991459952294(Epoch= 333)\n",
      "CV loss=0.07718291091823036(Epoch= 334)\n",
      "train loss=0.016551494949874852(Epoch= 334)\n",
      "CV loss=0.07709921239970595(Epoch= 335)\n",
      "train loss=0.016438002742142008(Epoch= 335)\n",
      "CV loss=0.07701500834521728(Epoch= 336)\n",
      "train loss=0.01642372312247904(Epoch= 336)\n",
      "CV loss=0.0778974205078109(Epoch= 337)\n",
      "train loss=0.01628678796649845(Epoch= 337)\n",
      "CV loss=0.0765804259925893(Epoch= 338)\n",
      "train loss=0.016232432255205403(Epoch= 338)\n",
      "CV loss=0.0775158957338512(Epoch= 339)\n",
      "train loss=0.016084139772453796(Epoch= 339)\n",
      "CV loss=0.07719742549446916(Epoch= 340)\n",
      "train loss=0.0159120238653261(Epoch= 340)\n",
      "CV loss=0.07715256280966692(Epoch= 341)\n",
      "train loss=0.01581984518316947(Epoch= 341)\n",
      "CV loss=0.07772338684469794(Epoch= 342)\n",
      "train loss=0.015789561362213142(Epoch= 342)\n",
      "CV loss=0.07760659649019287(Epoch= 343)\n",
      "train loss=0.01571228536529357(Epoch= 343)\n",
      "CV loss=0.07709097287970382(Epoch= 344)\n",
      "train loss=0.015531048674094794(Epoch= 344)\n",
      "CV loss=0.07678260814761209(Epoch= 345)\n",
      "train loss=0.01551476984107119(Epoch= 345)\n",
      "CV loss=0.07700840526685276(Epoch= 346)\n",
      "train loss=0.01537075719868644(Epoch= 346)\n",
      "CV loss=0.07733807482234238(Epoch= 347)\n",
      "train loss=0.01528483642437287(Epoch= 347)\n",
      "CV loss=0.07672111944189494(Epoch= 348)\n",
      "train loss=0.015244255331985776(Epoch= 348)\n",
      "CV loss=0.07660522377856585(Epoch= 349)\n",
      "train loss=0.015179832360166326(Epoch= 349)\n",
      "CV loss=0.07705996834700997(Epoch= 350)\n",
      "train loss=0.014979898024703205(Epoch= 350)\n",
      "CV loss=0.07700325016802494(Epoch= 351)\n",
      "train loss=0.01489203125954104(Epoch= 351)\n",
      "CV loss=0.0768427320308385(Epoch= 352)\n",
      "train loss=0.014787457238611511(Epoch= 352)\n",
      "CV loss=0.07734752761563332(Epoch= 353)\n",
      "train loss=0.014742233977151338(Epoch= 353)\n",
      "CV loss=0.0776169344406145(Epoch= 354)\n",
      "train loss=0.014757672404446453(Epoch= 354)\n",
      "CV loss=0.0767130243780483(Epoch= 355)\n",
      "train loss=0.014556506962699509(Epoch= 355)\n",
      "CV loss=0.0766366905643591(Epoch= 356)\n",
      "train loss=0.01456919717452118(Epoch= 356)\n",
      "CV loss=0.07720860149055465(Epoch= 357)\n",
      "train loss=0.014370406946114626(Epoch= 357)\n",
      "CV loss=0.07653534121095329(Epoch= 358)\n",
      "train loss=0.014286485545440904(Epoch= 358)\n",
      "CV loss=0.07643371828512631(Epoch= 359)\n",
      "train loss=0.014225717831026698(Epoch= 359)\n",
      "CV loss=0.07675794481572928(Epoch= 360)\n",
      "train loss=0.014116430752699166(Epoch= 360)\n",
      "CV loss=0.07663176265540882(Epoch= 361)\n",
      "train loss=0.014101699981678918(Epoch= 361)\n",
      "CV loss=0.07673425036505178(Epoch= 362)\n",
      "train loss=0.013938776156849457(Epoch= 362)\n",
      "CV loss=0.07703443724836659(Epoch= 363)\n",
      "train loss=0.013874602475977063(Epoch= 363)\n",
      "CV loss=0.0770594018620655(Epoch= 364)\n",
      "train loss=0.013807388957039925(Epoch= 364)\n",
      "CV loss=0.0768260385636249(Epoch= 365)\n",
      "train loss=0.013703530903707557(Epoch= 365)\n",
      "CV loss=0.07683400721928071(Epoch= 366)\n",
      "train loss=0.013635642149958901(Epoch= 366)\n",
      "CV loss=0.0771239940961111(Epoch= 367)\n",
      "train loss=0.01359827510380392(Epoch= 367)\n",
      "CV loss=0.07694405652390875(Epoch= 368)\n",
      "train loss=0.013516463776010804(Epoch= 368)\n",
      "CV loss=0.0765153792347732(Epoch= 369)\n",
      "train loss=0.013404454559877762(Epoch= 369)\n",
      "CV loss=0.07718669876132843(Epoch= 370)\n",
      "train loss=0.013347900191978304(Epoch= 370)\n",
      "CV loss=0.07654543692293779(Epoch= 371)\n",
      "train loss=0.013247924491566277(Epoch= 371)\n",
      "CV loss=0.0770154823023562(Epoch= 372)\n",
      "train loss=0.01312983572578157(Epoch= 372)\n",
      "CV loss=0.07653655034562007(Epoch= 373)\n",
      "train loss=0.013141801779060847(Epoch= 373)\n",
      "CV loss=0.07711464558982332(Epoch= 374)\n",
      "train loss=0.013032984352619853(Epoch= 374)\n",
      "CV loss=0.07665161935917436(Epoch= 375)\n",
      "train loss=0.012974828311283172(Epoch= 375)\n",
      "CV loss=0.07666758905261047(Epoch= 376)\n",
      "train loss=0.012895728408381489(Epoch= 376)\n",
      "CV loss=0.07682217646932062(Epoch= 377)\n",
      "train loss=0.012768887791144448(Epoch= 377)\n",
      "CV loss=0.0769091618600429(Epoch= 378)\n",
      "train loss=0.012775341014267095(Epoch= 378)\n",
      "CV loss=0.0770272677679643(Epoch= 379)\n",
      "train loss=0.012615177916622419(Epoch= 379)\n",
      "CV loss=0.07673235957455264(Epoch= 380)\n",
      "train loss=0.01257682969809817(Epoch= 380)\n",
      "CV loss=0.07699647259926967(Epoch= 381)\n",
      "train loss=0.01254953719994255(Epoch= 381)\n",
      "CV loss=0.07693106480188164(Epoch= 382)\n",
      "train loss=0.012422520089637498(Epoch= 382)\n",
      "CV loss=0.0771445063162222(Epoch= 383)\n",
      "train loss=0.012375282010664075(Epoch= 383)\n",
      "CV loss=0.07713071538489943(Epoch= 384)\n",
      "train loss=0.01226714004401738(Epoch= 384)\n",
      "CV loss=0.07721025060882299(Epoch= 385)\n",
      "train loss=0.01224097339785964(Epoch= 385)\n",
      "CV loss=0.07661862252089871(Epoch= 386)\n",
      "train loss=0.012126556712725067(Epoch= 386)\n",
      "CV loss=0.0768609555068552(Epoch= 387)\n",
      "train loss=0.012041702646135808(Epoch= 387)\n",
      "CV loss=0.07680594001553134(Epoch= 388)\n",
      "train loss=0.012020811337977217(Epoch= 388)\n",
      "CV loss=0.0769296375717764(Epoch= 389)\n",
      "train loss=0.011899317645746095(Epoch= 389)\n",
      "CV loss=0.07673535932976243(Epoch= 390)\n",
      "train loss=0.01183727358107654(Epoch= 390)\n",
      "CV loss=0.07666214306595394(Epoch= 391)\n",
      "train loss=0.01177609331553568(Epoch= 391)\n",
      "CV loss=0.07678335209729341(Epoch= 392)\n",
      "train loss=0.011753997675905816(Epoch= 392)\n",
      "CV loss=0.07685318230546603(Epoch= 393)\n",
      "train loss=0.011647843557386295(Epoch= 393)\n",
      "CV loss=0.07706411282105653(Epoch= 394)\n",
      "train loss=0.011599752224385798(Epoch= 394)\n",
      "CV loss=0.07698803308499766(Epoch= 395)\n",
      "train loss=0.011509846952448752(Epoch= 395)\n",
      "CV loss=0.07673897804989961(Epoch= 396)\n",
      "train loss=0.011457718047327802(Epoch= 396)\n",
      "CV loss=0.0769351313054252(Epoch= 397)\n",
      "train loss=0.011411781254223824(Epoch= 397)\n",
      "CV loss=0.07687966229164994(Epoch= 398)\n",
      "train loss=0.011345335749523492(Epoch= 398)\n",
      "CV loss=0.07702343341799153(Epoch= 399)\n",
      "train loss=0.011275886960544199(Epoch= 399)\n",
      "CV loss=0.07672946666684993(Epoch= 400)\n",
      "train loss=0.01120558855303853(Epoch= 400)\n",
      "CV loss=0.07703640479196305(Epoch= 401)\n",
      "train loss=0.011141901939750096(Epoch= 401)\n",
      "CV loss=0.07705521339990368(Epoch= 402)\n",
      "train loss=0.011083045226419841(Epoch= 402)\n",
      "CV loss=0.07683340269669589(Epoch= 403)\n",
      "train loss=0.01101934537369502(Epoch= 403)\n",
      "CV loss=0.07677604192605876(Epoch= 404)\n",
      "train loss=0.01097594776141656(Epoch= 404)\n",
      "CV loss=0.07667199883703964(Epoch= 405)\n",
      "train loss=0.010906364968138101(Epoch= 405)\n",
      "CV loss=0.07679639389041526(Epoch= 406)\n",
      "train loss=0.010837074914072002(Epoch= 406)\n",
      "CV loss=0.07668347667598449(Epoch= 407)\n",
      "train loss=0.010798046850973977(Epoch= 407)\n",
      "CV loss=0.07674222277772261(Epoch= 408)\n",
      "train loss=0.01073595398967273(Epoch= 408)\n",
      "CV loss=0.07679551110099003(Epoch= 409)\n",
      "train loss=0.010689836312969768(Epoch= 409)\n",
      "CV loss=0.07688714885634677(Epoch= 410)\n",
      "train loss=0.010601724718802257(Epoch= 410)\n",
      "CV loss=0.076799942150318(Epoch= 411)\n",
      "train loss=0.010572591047282297(Epoch= 411)\n",
      "CV loss=0.07698762662125007(Epoch= 412)\n",
      "train loss=0.010502009373489406(Epoch= 412)\n",
      "CV loss=0.0768855422269145(Epoch= 413)\n",
      "train loss=0.01042160692236884(Epoch= 413)\n",
      "CV loss=0.07679547204767506(Epoch= 414)\n",
      "train loss=0.010383843312688667(Epoch= 414)\n",
      "CV loss=0.07696165405330531(Epoch= 415)\n",
      "train loss=0.010334379359390836(Epoch= 415)\n",
      "CV loss=0.07694496673471074(Epoch= 416)\n",
      "train loss=0.010250723089248733(Epoch= 416)\n",
      "CV loss=0.07719085027465108(Epoch= 417)\n",
      "train loss=0.010237335717084487(Epoch= 417)\n",
      "CV loss=0.0770191193596221(Epoch= 418)\n",
      "train loss=0.01014003330837638(Epoch= 418)\n",
      "CV loss=0.07705934698341567(Epoch= 419)\n",
      "train loss=0.010124419585937837(Epoch= 419)\n",
      "CV loss=0.07683877908849386(Epoch= 420)\n",
      "train loss=0.010087255723104616(Epoch= 420)\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "batch_size = 64\n",
    "loss = float(\"inf\")\n",
    "epsilon = 0.0001\n",
    "\n",
    "n_input = 784\n",
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 128\n",
    "n_out = 10\n",
    "\n",
    "# early stop logic\n",
    "best_cv_loss = float(\"inf\")\n",
    "patience = 100\n",
    "counter = 0\n",
    "tol = 1e-6 # accounts for floating point noise\n",
    "\n",
    "training_loss_history = []\n",
    "cv_loss_history = []\n",
    "\n",
    "i = 0\n",
    "while i < 2000: # epochs\n",
    "    randomized_indices = np.random.permutation(X_train.shape[0])\n",
    "    \n",
    "    for start_of_batch in range(0, X_train.shape[0], batch_size):\n",
    "        batch_indices = randomized_indices[start_of_batch : start_of_batch + batch_size]\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        loss, cache = forwardPass(X_train=X_batch, y_train=y_batch, weights=weights, lmbda=0)\n",
    "        grads = backpropagation(cache=cache, X_train=X_batch, y_train=y_batch, weights=weights, lmbda=0)\n",
    "        weights = update(alpha=0.001, cache = cache, weights=weights, grads=grads)\n",
    "    cv_loss, cache = forwardPass(X_train=X_cv, y_train=y_cv, weights=weights)\n",
    "    print(\"CV loss=\", cv_loss, \"(Epoch= \", i + 1, \")\", sep=\"\")\n",
    "    train_loss, cache = forwardPass(X_train=X_train, y_train=y_train, weights=weights)\n",
    "    print(\"train loss=\", train_loss, \"(Epoch= \", i + 1, \")\", sep=\"\")\n",
    "    training_loss_history.append(train_loss)\n",
    "    cv_loss_history.append(cv_loss)\n",
    "\n",
    "    if cv_loss < best_cv_loss - tol:\n",
    "        best_cv_loss = cv_loss\n",
    "        best_weights = copy.deepcopy(weights)\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"early stop\")\n",
    "        weights = best_weights\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_loss_history, label=\"Training Loss\")\n",
    "plt.plot(cv_loss_history, label=\"CV Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed810c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.99464285714285 %\n",
      "CV accuracy: 97.65714285714286 %\n",
      "relative gap between CV and Train loss: 335.56533272093526 %\n",
      "\n",
      "Training error rate is approximately 0.005357142857150166 %\n",
      "CV error rate is approximately 2.3428571428571416 %\n",
      "Bayes/human error rate is approximately 1%\n"
     ]
    }
   ],
   "source": [
    "def inference(X):\n",
    "\n",
    "    # First Hidden Layer\n",
    "    z_1 = X @ W1 + b1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    z_2 = a_1 @ W2 + b2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    z_3 = a_2 @ W3 + b3\n",
    "    a_3 = np.maximum(0, z_3)\n",
    "\n",
    "    # Logits before softmax\n",
    "    z_4 = a_3 @ W4 + b4\n",
    "\n",
    "    # Softmax activation applied\n",
    "    shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "    z_4_exp = np.exp(shifted)\n",
    "    p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "\n",
    "    return np.argmax(p, axis=1)\n",
    "\n",
    "train_accuracy = np.mean(inference(X_train) == y_train)\n",
    "print(\"Training accuracy:\", train_accuracy*100, \"%\")\n",
    "\n",
    "cv_accuracy = np.mean(inference(X_cv) == y_cv)\n",
    "print(\"CV accuracy:\", cv_accuracy*100, \"%\")\n",
    "\n",
    "gap = cv_loss - train_loss\n",
    "gap_percentage = (gap / train_loss) * 100\n",
    "print(\"relative gap between CV and Train loss:\", gap_percentage, \"%\")\n",
    "print()\n",
    "\n",
    "print(\"Training error rate is approximately\", 100 - train_accuracy*100, \"%\")\n",
    "print(\"CV error rate is approximately\", 100 - cv_accuracy*100, \"%\")\n",
    "print(\"Bayes/human error rate for mnist datset is approximately 1%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9e158f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 2\n",
      "Index = 115\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(X_train.shape[0])\n",
    "\n",
    "img = X_train[idx].reshape(28, 28)\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label: {y_train[idx]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Predicted Label:\", inference(X_train[0:1])[idx])\n",
    "print(\"Index =\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0869e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
