{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f532c213",
   "metadata": {},
   "source": [
    "This is the notebook for training a neural network from scratch with only numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef183bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906ba58",
   "metadata": {},
   "source": [
    "Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79bb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(name=\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist.data.astype(np.float32)\n",
    "y = mnist.target.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d6fe2",
   "metadata": {},
   "source": [
    "Data exploration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ccbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "\n",
    "print(\"X.dtype:\", X.dtype)\n",
    "print(\"y.dtype:\", y.dtype)\n",
    "\n",
    "print(\"Min pixel value:\", X.min())\n",
    "print(\"Max pixel value:\", X.max())\n",
    "\n",
    "print(\"Unique labels:\", np.unique(y))\n",
    "\n",
    "print(\"Any NaNs in X:\", np.isnan(X).any())\n",
    "print(\"Any NaNs in y:\", np.isnan(y).any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc8dde",
   "metadata": {},
   "source": [
    "Plotting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting pixel value distribution\n",
    "\n",
    "plt.hist(X.flatten(), bins=50)\n",
    "plt.title(\"Pixel Value Distribution\")\n",
    "plt.xlabel(\"Pixel intensity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting label distribution\n",
    "plt.hist(y, bins=10)\n",
    "plt.title(\"Pixel Value Distribution\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d78c45",
   "metadata": {},
   "source": [
    "Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e681bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255\n",
    "\n",
    "train_end = int(X.shape[0] * 0.8)\n",
    "cv_end = int(X.shape[0] * 0.9)\n",
    "\n",
    "randomized_indices = np.random.permutation(X.shape[0])\n",
    "\n",
    "train_indices = randomized_indices[0 : train_end]\n",
    "cv_indices = randomized_indices[train_end : cv_end]\n",
    "test_indices = randomized_indices[cv_end:]\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "X_cv = X[cv_indices]\n",
    "y_cv = y[cv_indices]\n",
    "\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print(X_train[np.random.choice(X_train.shape[0], size=10, replace=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32757980",
   "metadata": {},
   "source": [
    "Weight Initialization (Using the He Initialization method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f39a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightInit(*, n_input=784, n1=512, n2=256, n3=128, n_out=10, X_train=X_train, y_train=y_train):\n",
    "    W1 = np.random.randn(n_input, n1) * np.sqrt(2.0 / n_input)\n",
    "    b1 = np.zeros(n1)\n",
    "    print(\"W1.shape=\", W1.shape)\n",
    "    print(\"b1.shape=\", b1.shape)\n",
    "    # print(W1[np.random.choice(W1.shape[0], size=10, replace=False)])\n",
    "\n",
    "    W2 = np.random.randn(n1, n2) * np.sqrt(2.0 / n1)\n",
    "    b2 = np.zeros(n2)\n",
    "    print(\"W2.shape=\", W2.shape)\n",
    "    print(\"b2.shape=\", b2.shape)\n",
    "\n",
    "    W3 = np.random.randn(n2, n3) * np.sqrt(2.0 / n2)\n",
    "    b3 = np.random.randn(n3)\n",
    "    print(\"W3.shape=\", W3.shape)\n",
    "    print(\"b3.shape=\", b3.shape)\n",
    "\n",
    "    W4 = np.random.randn(n3, n_out) * np.sqrt(2.0 / n3)\n",
    "    b4 = np.random.randn(n_out)\n",
    "    print(\"W3.shape=\", W3.shape)\n",
    "    print(\"b3.shape=\", b3.shape)\n",
    "\n",
    "    # use smaller set at first to test, comment out when using full dataset\n",
    "    # idx = np.random.choice(X_train.shape[0], size=200, replace=False)\n",
    "    # X_train = X_train[idx]\n",
    "    # y_train = y_train[idx]\n",
    "\n",
    "    return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'W3': W3,\n",
    "        'b3': b3,\n",
    "        'W4': W4,\n",
    "        'b4': b4\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 128\n",
    "n_out = 10\n",
    "\n",
    "# Weights as a dictionary\n",
    "weights = weightInit(n_input=n_input, n1=n1, n2=n2, n3=n3, n_out=n_out, X_train=X_train, y_train=y_train)\n",
    "W1 = weights['W1']\n",
    "b1 = weights['b1']\n",
    "W2 = weights['W2']\n",
    "b2 = weights['b2']\n",
    "W3 = weights['W3']\n",
    "b3 = weights['b3']\n",
    "W4 = weights['W4']\n",
    "b4 = weights['b4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a0953",
   "metadata": {},
   "source": [
    "Now we'll begin programming the forward pass\n",
    "\n",
    "First Hidden Layer (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1 = X_train @ W1 + b1\n",
    "a_1 = np.maximum(0, z_1)\n",
    "print(a_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c8046",
   "metadata": {},
   "source": [
    "Second Hidden Layer (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_2 = a_1 @ W2 + b2\n",
    "a_2 = np.maximum(0, z_2)\n",
    "print(a_2.shape)\n",
    "# print(a_2[np.random.choice(a_2.shape[0], size=1, replace=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392327d7",
   "metadata": {},
   "source": [
    "Third Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de99851",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_3 = a_2 @ W3 + b3\n",
    "a_3 = np.maximum(0, z_3)\n",
    "print(a_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17b9a1",
   "metadata": {},
   "source": [
    "The logits before the softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf56f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_4 = a_3 @ W4 + b4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a58047",
   "metadata": {},
   "source": [
    "Softmax activation function applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "z_4_exp = np.exp(shifted)\n",
    "p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "print(p.shape)\n",
    "print(p[0])\n",
    "print(p[0].sum())\n",
    "print(np.argmax(p[0]))\n",
    "print(\"vs\")\n",
    "print(\"Correct Image\")\n",
    "\n",
    "\n",
    "img = X_train[0].reshape(28, 28)\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label: {y_train[0]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf517a",
   "metadata": {},
   "source": [
    "Loss Function for Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92510a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.clip(p, 1e-12, 1.0)\n",
    "loss = -np.mean(np.log(p[np.arange(y_train.shape[0]), y_train]))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60070c1f",
   "metadata": {},
   "source": [
    "Repeatable function for forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(*, X_train=X_train, y_train=y_train, weights=weights, lmbda=0):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    # First Hidden Layer\n",
    "    z_1 = X_train @ W1 + b1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    z_2 = a_1 @ W2 + b2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    z_3 = a_2 @ W3 + b3\n",
    "    a_3 = np.maximum(0, z_3)\n",
    "\n",
    "    # Logits before softmax\n",
    "    z_4 = a_3 @ W4 + b4\n",
    "\n",
    "    # Softmax activation applied\n",
    "    shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "    z_4_exp = np.exp(shifted)\n",
    "    p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "\n",
    "    # loss function for softmax\n",
    "    p = np.clip(p, 1e-12, 1.0)\n",
    "    loss = (\n",
    "        -np.mean(np.log(p[np.arange(y_train.shape[0]), y_train])) \n",
    "            + (lmbda/(2)) * ((W1**2).sum() + (W2**2).sum() + (W3**2).sum() + (W4**2).sum())\n",
    "        )\n",
    "    return loss, {\n",
    "        \"a_1\": a_1,\n",
    "        \"a_2\": a_2,\n",
    "        \"a_3\": a_3,\n",
    "        \"p\": p,\n",
    "        \"z_1\": z_1,\n",
    "        \"z_2\": z_2,\n",
    "        \"z_3\": z_3,\n",
    "        \"z_4\": z_4\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738dae8",
   "metadata": {},
   "source": [
    "Repeatable Function for backward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d386147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(*, cache=None, X_train=None, y_train=None, weights=None, lmbda=0):\n",
    "\n",
    "    a_1 = cache[\"a_1\"]\n",
    "    a_2 = cache[\"a_2\"]\n",
    "    a_3 = cache[\"a_3\"]\n",
    "    p = cache[\"p\"]\n",
    "    z_1 = cache[\"z_1\"]\n",
    "    z_2 = cache[\"z_2\"]\n",
    "    z_3 = cache[\"z_3\"]\n",
    "    z_4 = cache[\"z_4\"]\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    y_1he = np.zeros((len(y_train), 10))\n",
    "    y_1he[np.arange(len(y_train)), y_train] = 1\n",
    "\n",
    "    dJ_dz4 = p - y_1he # Gradient of loss for each logit, shape = (200, 10)\n",
    "\n",
    "    dJ_dw4 = (a_3.T @ dJ_dz4) / a_3.shape[0]# Gradients for output layer\n",
    "    dJ_db4 = np.mean(dJ_dz4, axis=0)\n",
    "\n",
    "    dJ_da3 = dJ_dz4 @ W4.T # Gradients for third hidden layer\n",
    "\n",
    "    da3_dz3 = np.where(z_3 > 0, 1, 0)\n",
    "    dJ_dz3 = da3_dz3 * dJ_da3\n",
    "\n",
    "\n",
    "    dJ_dw3 = (a_2.T @ dJ_dz3) / a_2.shape[0]\n",
    "    dJ_db3 = np.mean(dJ_dz3, axis=0)\n",
    "\n",
    "\n",
    "    dJ_da2 = dJ_dz3 @ W3.T # Gradients for second hidden layer\n",
    "\n",
    "    da2_dz2 = np.where(z_2 > 0, 1, 0)\n",
    "    dJ_dz2 = da2_dz2 * dJ_da2\n",
    "\n",
    "    dJ_dw2 = (a_1.T @ dJ_dz2) / a_1.shape[0]\n",
    "    dJ_db2 = np.mean(dJ_dz2, axis=0)\n",
    "\n",
    "    dJ_da1 = dJ_dz2 @ W2.T # Gradients for first hidden layer\n",
    "\n",
    "    da1_dz1 = np.where(z_1 > 0, 1, 0)\n",
    "    dJ_dz1 = da1_dz1 * dJ_da1\n",
    "\n",
    "    dJ_dw1 = (X_train.T @ dJ_dz1) / X_train.shape[0]\n",
    "    dJ_db1 = np.mean(dJ_dz1, axis=0)\n",
    "\n",
    "    # L2 regularization\n",
    "    dJ_dw4 += (lmbda) * W4\n",
    "    dJ_dw3 += (lmbda) * W3\n",
    "    dJ_dw2 += (lmbda) * W2\n",
    "    dJ_dw1 += (lmbda) * W1\n",
    "\n",
    "    return {\"dJ_dw4\": dJ_dw4, \"dJ_db4\": dJ_db4, \"dJ_dw3\": dJ_dw3, \"dJ_db3\": dJ_db3,\n",
    "            \"dJ_dw2\": dJ_dw2, \"dJ_db2\": dJ_db2, \"dJ_dw1\": dJ_dw1, \"dJ_db1\": dJ_db1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70956097",
   "metadata": {},
   "source": [
    "Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(*, alpha = 0.01, cache = None, weights=None, grads=None):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    W4 -= alpha * grads[\"dJ_dw4\"]\n",
    "    b4 -= alpha * grads[\"dJ_db4\"]\n",
    "\n",
    "    W3 -= alpha * grads[\"dJ_dw3\"]\n",
    "    b3 -= alpha * grads[\"dJ_db3\"]\n",
    "\n",
    "    W2 -= alpha * grads[\"dJ_dw2\"]\n",
    "    b2 -= alpha * grads[\"dJ_db2\"]\n",
    "\n",
    "    W1 -= alpha * grads[\"dJ_dw1\"]\n",
    "    b1 -= alpha * grads[\"dJ_db1\"]\n",
    "\n",
    "    return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'W3': W3,\n",
    "        'b3': b3,\n",
    "        'W4': W4,\n",
    "        'b4': b4\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d672345",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa17d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "weights = weightInit(n_input=n_input, n1=n1, n2=n2, n3=n3, n_out=n_out, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "batch_size = 64\n",
    "loss = float(\"inf\")\n",
    "epsilon = 0.0001\n",
    "\n",
    "n_input = 784\n",
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 128\n",
    "n_out = 10\n",
    "\n",
    "# early stop logic\n",
    "best_cv_loss = float(\"inf\")\n",
    "patience = 30\n",
    "counter = 0\n",
    "tol = 1e-6 # accounts for floating point noise\n",
    "\n",
    "training_loss_history = []\n",
    "cv_loss_history = []\n",
    "\n",
    "i = 0\n",
    "while i < 10000: # epochs\n",
    "    randomized_indices = np.random.permutation(X_train.shape[0])\n",
    "    \n",
    "    for start_of_batch in range(0, X_train.shape[0], batch_size):\n",
    "        batch_indices = randomized_indices[start_of_batch : start_of_batch + batch_size]\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        loss, cache = forwardPass(X_train=X_batch, y_train=y_batch, weights=weights, lmbda=0.0005)\n",
    "        grads = backpropagation(cache=cache, X_train=X_batch, y_train=y_batch, weights=weights, lmbda=0.0005)\n",
    "        weights = update(alpha=0.001, cache = cache, weights=weights, grads=grads)\n",
    "\n",
    "        \n",
    "    cv_loss, cache = forwardPass(X_train=X_cv, y_train=y_cv, weights=weights, lmbda=0.0005)\n",
    "    print(\"CV loss=\", cv_loss, \"(Epoch= \", i + 1, \")\", sep=\"\")\n",
    "    train_loss, cache = forwardPass(X_train=X_train, y_train=y_train, weights=weights, lmbda=0.0005)\n",
    "    print(\"train loss=\", train_loss, \"(Epoch= \", i + 1, \")\", sep=\"\")\n",
    "    training_loss_history.append(train_loss)\n",
    "    cv_loss_history.append(cv_loss)\n",
    "\n",
    "    if cv_loss < best_cv_loss - tol:\n",
    "        best_cv_loss = cv_loss\n",
    "        best_weights = copy.deepcopy(weights)\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"early stop\")\n",
    "        weights = best_weights\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392f703",
   "metadata": {},
   "source": [
    "Plot training and cross validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ffb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_loss_history, label=\"Training Loss, lambda=0.0005\")\n",
    "plt.plot(cv_loss_history, label=\"CV Loss\")\n",
    "\n",
    "# Mark the best CV loss point\n",
    "best_epoch = cv_loss_history.index(min(cv_loss_history))\n",
    "plt.scatter(best_epoch, best_cv_loss, color='red', s=100, zorder=5, label=f'Best CV Loss: {best_cv_loss:.4f} (epoch {best_epoch + 1})')\n",
    "plt.axvline(x=best_epoch, color='red', linestyle='--', alpha=0.3, label='Best epoch')\n",
    "\n",
    "# Mark where training actually stopped\n",
    "plt.axvline(x=len(cv_loss_history) - 1, color='orange', linestyle='--', alpha=0.5, label=f'Stopped at epoch {len(cv_loss_history)}')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Cross-Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed810c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(X, *, weights=weights):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4'] \n",
    "\n",
    "    # First Hidden Layer\n",
    "    z_1 = X @ W1 + b1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    z_2 = a_1 @ W2 + b2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    z_3 = a_2 @ W3 + b3\n",
    "    a_3 = np.maximum(0, z_3)\n",
    "\n",
    "    # Logits before softmax\n",
    "    z_4 = a_3 @ W4 + b4\n",
    "\n",
    "    # Softmax activation applied\n",
    "    shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "    z_4_exp = np.exp(shifted)\n",
    "    p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "\n",
    "    return np.argmax(p, axis=1)\n",
    "\n",
    "train_accuracy = np.mean(inference(X_train, weights=weights) == y_train)\n",
    "print(\"Training accuracy:\", train_accuracy*100, \"%\")\n",
    "\n",
    "cv_accuracy = np.mean(inference(X_cv, weights=weights) == y_cv)\n",
    "print(\"CV accuracy:\", cv_accuracy*100, \"%\")\n",
    "\n",
    "gap = cv_loss - train_loss\n",
    "gap_percentage = (gap / train_loss) * 100\n",
    "print(\"relative gap between CV and Train loss:\", gap_percentage, \"%\")\n",
    "print()\n",
    "\n",
    "print(\"Training error rate is approximately\", 100 - train_accuracy*100, \"%\")\n",
    "print(\"CV error rate is approximately\", 100 - cv_accuracy*100, \"%\")\n",
    "print(\"Bayes/human error rate for mnist datset is approximately 0.2%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5974d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"CV set size:\", X_cv.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e158f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(X_train.shape[0])\n",
    "\n",
    "img = X_train[idx].reshape(28, 28)\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label: {y_train[idx]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Predicted Label:\", inference(X_train)[idx])\n",
    "print(\"Index =\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a328b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking accuracy on test set\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
