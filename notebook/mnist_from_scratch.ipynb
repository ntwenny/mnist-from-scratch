{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f532c213",
   "metadata": {},
   "source": [
    "This is the notebook for training a neural network from scratch with only numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ef183bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906ba58",
   "metadata": {},
   "source": [
    "Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f79bb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(name=\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist.data.astype(np.float32)\n",
    "y = mnist.target.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d6fe2",
   "metadata": {},
   "source": [
    "Data exploration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7ccbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (70000, 784)\n",
      "y.shape: (70000,)\n",
      "X.dtype: float32\n",
      "y.dtype: int64\n",
      "Min pixel value: 0.0\n",
      "Max pixel value: 255.0\n",
      "Unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "Any NaNs in X: False\n",
      "Any NaNs in y: False\n"
     ]
    }
   ],
   "source": [
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "\n",
    "print(\"X.dtype:\", X.dtype)\n",
    "print(\"y.dtype:\", y.dtype)\n",
    "\n",
    "print(\"Min pixel value:\", X.min())\n",
    "print(\"Max pixel value:\", X.max())\n",
    "\n",
    "print(\"Unique labels:\", np.unique(y))\n",
    "\n",
    "print(\"Any NaNs in X:\", np.isnan(X).any())\n",
    "print(\"Any NaNs in y:\", np.isnan(y).any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc8dde",
   "metadata": {},
   "source": [
    "Plotting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29e85b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting pixel value distribution\n",
    "\n",
    "plt.hist(X.flatten(), bins=50)\n",
    "plt.title(\"Pixel Value Distribution\")\n",
    "plt.xlabel(\"Pixel intensity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting label distribution\n",
    "plt.hist(y, bins=10)\n",
    "plt.title(\"Pixel Value Distribution\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d78c45",
   "metadata": {},
   "source": [
    "Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e681bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = X / 255\n",
    "\n",
    "train_end = int(X.shape[0] * 0.8)\n",
    "cv_end = int(X.shape[0] * 0.9)\n",
    "\n",
    "randomized_indices = np.random.permutation(X.shape[0])\n",
    "\n",
    "train_indices = randomized_indices[0 : train_end]\n",
    "cv_indices = randomized_indices[train_end : cv_end]\n",
    "test_indices = randomized_indices[cv_end:]\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "X_cv = X[cv_indices]\n",
    "y_cv = y[cv_indices]\n",
    "\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print(X_train[np.random.choice(X_train.shape[0], size=10, replace=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32757980",
   "metadata": {},
   "source": [
    "Weight Initialization (Using the He Initialization method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f83f39a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightInit(*, n_input=784, n1=512, n2=256, n3=128, n_out=10, X_train=X_train, y_train=y_train):\n",
    "    W1 = np.random.randn(n_input, n1) * np.sqrt(2.0 / n_input)\n",
    "    b1 = np.zeros(n1)\n",
    "    print(\"W1.shape=\", W1.shape)\n",
    "    print(\"b1.shape=\", b1.shape)\n",
    "    # print(W1[np.random.choice(W1.shape[0], size=10, replace=False)])\n",
    "\n",
    "    W2 = np.random.randn(n1, n2) * np.sqrt(2.0 / n1)\n",
    "    b2 = np.zeros(n2)\n",
    "    print(\"W2.shape=\", W2.shape)\n",
    "    print(\"b2.shape=\", b2.shape)\n",
    "\n",
    "    W3 = np.random.randn(n2, n3) * np.sqrt(2.0 / n2)\n",
    "    b3 = np.random.randn(n3)\n",
    "    print(\"W3.shape=\", W3.shape)\n",
    "    print(\"b3.shape=\", b3.shape)\n",
    "\n",
    "    W4 = np.random.randn(n3, n_out) * np.sqrt(2.0 / n3)\n",
    "    b4 = np.random.randn(n_out)\n",
    "    print(\"W3.shape=\", W3.shape)\n",
    "    print(\"b3.shape=\", b3.shape)\n",
    "\n",
    "    # use smaller set at first to test, comment out when using full dataset\n",
    "    idx = np.random.choice(X_train.shape[0], size=200, replace=False)\n",
    "    X_train = X_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "\n",
    "    return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'W3': W3,\n",
    "        'b3': b3,\n",
    "        'W4': W4,\n",
    "        'b4': b4\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "abe5b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape= (784, 512)\n",
      "b1.shape= (512,)\n",
      "W2.shape= (512, 256)\n",
      "b2.shape= (256,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n"
     ]
    }
   ],
   "source": [
    "n_input = 784\n",
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 128\n",
    "n_out = 10\n",
    "\n",
    "# Weights as a dictionary\n",
    "weights = weightInit(n_input=n_input, n1=n1, n2=n2, n3=n3, n_out=n_out, X_train=X_train, y_train=y_train)\n",
    "W1 = weights['W1']\n",
    "b1 = weights['b1']\n",
    "W2 = weights['W2']\n",
    "b2 = weights['b2']\n",
    "W3 = weights['W3']\n",
    "b3 = weights['b3']\n",
    "W4 = weights['W4']\n",
    "b4 = weights['b4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a0953",
   "metadata": {},
   "source": [
    "Now we'll begin programming the forward pass\n",
    "\n",
    "First Hidden Layer (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3bee27ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 512)\n"
     ]
    }
   ],
   "source": [
    "z_1 = X_train @ W1 + b1\n",
    "a_1 = np.maximum(0, z_1)\n",
    "print(a_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c8046",
   "metadata": {},
   "source": [
    "Second Hidden Layer (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8072b2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 256)\n"
     ]
    }
   ],
   "source": [
    "z_2 = a_1 @ W2 + b2\n",
    "a_2 = np.maximum(0, z_2)\n",
    "print(a_2.shape)\n",
    "# print(a_2[np.random.choice(a_2.shape[0], size=1, replace=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392327d7",
   "metadata": {},
   "source": [
    "Third Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4de99851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 128)\n"
     ]
    }
   ],
   "source": [
    "z_3 = a_2 @ W3 + b3\n",
    "a_3 = np.maximum(0, z_3)\n",
    "print(a_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17b9a1",
   "metadata": {},
   "source": [
    "The logits before the softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7cf56f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_4 = a_3 @ W4 + b4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a58047",
   "metadata": {},
   "source": [
    "Softmax activation function applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0ddd303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 10)\n",
      "[0.13128377 0.09975014 0.58818054 0.01025408 0.06229953 0.04632771\n",
      " 0.04736766 0.01179882 0.00101539 0.00172237]\n",
      "1.0\n",
      "2\n",
      "vs\n",
      "Correct Image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "z_4_exp = np.exp(shifted)\n",
    "p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "print(p.shape)\n",
    "print(p[0])\n",
    "print(p[0].sum())\n",
    "print(np.argmax(p[0]))\n",
    "print(\"vs\")\n",
    "print(\"Correct Image\")\n",
    "\n",
    "\n",
    "img = X_train[0].reshape(28, 28)\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label: {y_train[0]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf517a",
   "metadata": {},
   "source": [
    "Loss Function for Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e92510a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.360170785142378\n"
     ]
    }
   ],
   "source": [
    "p = np.clip(p, 1e-12, 1.0)\n",
    "loss = -np.mean(np.log(p[np.arange(y_train.shape[0]), y_train]))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60070c1f",
   "metadata": {},
   "source": [
    "Repeatable function for forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(*, X_train=X_train, y_train=y_train, weights=weights, lmbda=0):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    # First Hidden Layer\n",
    "    z_1 = X_train @ W1 + b1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    z_2 = a_1 @ W2 + b2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    z_3 = a_2 @ W3 + b3\n",
    "    a_3 = np.maximum(0, z_3)\n",
    "\n",
    "    # Logits before softmax\n",
    "    z_4 = a_3 @ W4 + b4\n",
    "\n",
    "    # Softmax activation applied\n",
    "    shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "    z_4_exp = np.exp(shifted)\n",
    "    p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "\n",
    "    # loss function for softmax\n",
    "    p = np.clip(p, 1e-12, 1.0)\n",
    "    loss = (\n",
    "        -np.mean(np.log(p[np.arange(y_train.shape[0]), y_train])) \n",
    "            + (lmbda/(2 * X_train.shape[0])) * ((W1**2).sum() + (W2**2).sum() + (W3**2).sum() + (W4**2).sum())\n",
    "        )\n",
    "    return loss, {\n",
    "        \"a_1\": a_1,\n",
    "        \"a_2\": a_2,\n",
    "        \"a_3\": a_3,\n",
    "        \"p\": p,\n",
    "        \"z_1\": z_1,\n",
    "        \"z_2\": z_2,\n",
    "        \"z_3\": z_3,\n",
    "        \"z_4\": z_4\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738dae8",
   "metadata": {},
   "source": [
    "Repeatable Function for backward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d386147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(*, cache=None, X_train=None, y_train=None, weights=None, lmbda=0):\n",
    "\n",
    "    a_1 = cache[\"a_1\"]\n",
    "    a_2 = cache[\"a_2\"]\n",
    "    a_3 = cache[\"a_3\"]\n",
    "    p = cache[\"p\"]\n",
    "    z_1 = cache[\"z_1\"]\n",
    "    z_2 = cache[\"z_2\"]\n",
    "    z_3 = cache[\"z_3\"]\n",
    "    z_4 = cache[\"z_4\"]\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    y_1he = np.zeros((len(y_train), 10))\n",
    "    y_1he[np.arange(len(y_train)), y_train] = 1\n",
    "\n",
    "    dJ_dz4 = p - y_1he # Gradient of loss for each logit, shape = (200, 10)\n",
    "\n",
    "    dJ_dw4 = (a_3.T @ dJ_dz4) / a_3.shape[0]# Gradients for output layer\n",
    "    dJ_db4 = np.mean(dJ_dz4, axis=0)\n",
    "\n",
    "    dJ_da3 = dJ_dz4 @ W4.T # Gradients for third hidden layer\n",
    "\n",
    "    da3_dz3 = np.where(z_3 > 0, 1, 0)\n",
    "    dJ_dz3 = da3_dz3 * dJ_da3\n",
    "\n",
    "\n",
    "    dJ_dw3 = (a_2.T @ dJ_dz3) / a_2.shape[0]\n",
    "    dJ_db3 = np.mean(dJ_dz3, axis=0)\n",
    "\n",
    "\n",
    "    dJ_da2 = dJ_dz3 @ W3.T # Gradients for second hidden layer\n",
    "\n",
    "    da2_dz2 = np.where(z_2 > 0, 1, 0)\n",
    "    dJ_dz2 = da2_dz2 * dJ_da2\n",
    "\n",
    "    dJ_dw2 = (a_1.T @ dJ_dz2) / a_1.shape[0]\n",
    "    dJ_db2 = np.mean(dJ_dz2, axis=0)\n",
    "\n",
    "    dJ_da1 = dJ_dz2 @ W2.T # Gradients for first hidden layer\n",
    "\n",
    "    da1_dz1 = np.where(z_1 > 0, 1, 0)\n",
    "    dJ_dz1 = da1_dz1 * dJ_da1\n",
    "\n",
    "    dJ_dw1 = (X_train.T @ dJ_dz1) / X_train.shape[0]\n",
    "    dJ_db1 = np.mean(dJ_dz1, axis=0)\n",
    "\n",
    "    # L2 regularization\n",
    "    dJ_dw4 += (lmbda / X_train.shape[0]) * W4\n",
    "    dJ_dw3 += (lmbda / X_train.shape[0]) * W3\n",
    "    dJ_dw2 += (lmbda / X_train.shape[0]) * W2\n",
    "    dJ_dw1 += (lmbda / X_train.shape[0]) * W1\n",
    "\n",
    "    return {\"dJ_dw4\": dJ_dw4, \"dJ_db4\": dJ_db4, \"dJ_dw3\": dJ_dw3, \"dJ_db3\": dJ_db3,\n",
    "            \"dJ_dw2\": dJ_dw2, \"dJ_db2\": dJ_db2, \"dJ_dw1\": dJ_dw1, \"dJ_db1\": dJ_db1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70956097",
   "metadata": {},
   "source": [
    "Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(*, alpha = 0.01, cache = None, weights=None, grads=None):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    W4 -= alpha * grads[\"dJ_dw4\"]\n",
    "    b4 -= alpha * grads[\"dJ_db4\"]\n",
    "\n",
    "    W3 -= alpha * grads[\"dJ_dw3\"]\n",
    "    b3 -= alpha * grads[\"dJ_db3\"]\n",
    "\n",
    "    W2 -= alpha * grads[\"dJ_dw2\"]\n",
    "    b2 -= alpha * grads[\"dJ_db2\"]\n",
    "\n",
    "    W1 -= alpha * grads[\"dJ_dw1\"]\n",
    "    b1 -= alpha * grads[\"dJ_db1\"]\n",
    "\n",
    "    return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'W3': W3,\n",
    "        'b3': b3,\n",
    "        'W4': W4,\n",
    "        'b4': b4\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d672345",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "baa17d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape= (784, 512)\n",
      "b1.shape= (512,)\n",
      "W2.shape= (512, 256)\n",
      "b2.shape= (256,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "weights = weightInit(n_input=n_input, n1=n1, n2=n2, n3=n3, n_out=n_out, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63e9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV loss=1.407115851189552(Epoch= 1)\n",
      "train loss=1.412135930255188(Epoch= 1)\n",
      "CV loss=0.8369309952779564(Epoch= 2)\n",
      "train loss=0.8422082573283406(Epoch= 2)\n",
      "CV loss=0.6079293828066321(Epoch= 3)\n",
      "train loss=0.6128231157911253(Epoch= 3)\n",
      "CV loss=0.5006664367947479(Epoch= 4)\n",
      "train loss=0.5048487273469209(Epoch= 4)\n",
      "CV loss=0.43974053068503116(Epoch= 5)\n",
      "train loss=0.44309556486443297(Epoch= 5)\n",
      "CV loss=0.40078123224171874(Epoch= 6)\n",
      "train loss=0.40247834529198434(Epoch= 6)\n",
      "CV loss=0.37294788353415714(Epoch= 7)\n",
      "train loss=0.3734000211820465(Epoch= 7)\n",
      "CV loss=0.35215973462463535(Epoch= 8)\n",
      "train loss=0.35165867209015(Epoch= 8)\n",
      "CV loss=0.3361136513261152(Epoch= 9)\n",
      "train loss=0.3346594459193897(Epoch= 9)\n",
      "CV loss=0.32350941450391135(Epoch= 10)\n",
      "train loss=0.32025237816973945(Epoch= 10)\n",
      "CV loss=0.3121288943182861(Epoch= 11)\n",
      "train loss=0.3080404293788641(Epoch= 11)\n",
      "CV loss=0.3025360901688171(Epoch= 12)\n",
      "train loss=0.29787702342368866(Epoch= 12)\n",
      "CV loss=0.2940364637885725(Epoch= 13)\n",
      "train loss=0.28857397922316885(Epoch= 13)\n",
      "CV loss=0.2863910696184289(Epoch= 14)\n",
      "train loss=0.28052486806916443(Epoch= 14)\n",
      "CV loss=0.27997854931390065(Epoch= 15)\n",
      "train loss=0.27316230776123335(Epoch= 15)\n",
      "CV loss=0.2739884611340032(Epoch= 16)\n",
      "train loss=0.26623299440265213(Epoch= 16)\n",
      "CV loss=0.26849458035077733(Epoch= 17)\n",
      "train loss=0.2601644801766941(Epoch= 17)\n",
      "CV loss=0.2627589009163547(Epoch= 18)\n",
      "train loss=0.2540679902346282(Epoch= 18)\n",
      "CV loss=0.2577384682912933(Epoch= 19)\n",
      "train loss=0.24886196672230376(Epoch= 19)\n",
      "CV loss=0.25328547072640833(Epoch= 20)\n",
      "train loss=0.2436404770848442(Epoch= 20)\n",
      "CV loss=0.2486504141284715(Epoch= 21)\n",
      "train loss=0.23904052194912873(Epoch= 21)\n",
      "CV loss=0.2449922717934775(Epoch= 22)\n",
      "train loss=0.23443315185512587(Epoch= 22)\n",
      "CV loss=0.24115413998857993(Epoch= 23)\n",
      "train loss=0.23001477818951174(Epoch= 23)\n",
      "CV loss=0.23743285925906513(Epoch= 24)\n",
      "train loss=0.22596276351057687(Epoch= 24)\n",
      "CV loss=0.23414686024437248(Epoch= 25)\n",
      "train loss=0.22255226281943571(Epoch= 25)\n",
      "CV loss=0.23070965170710198(Epoch= 26)\n",
      "train loss=0.21872940703833807(Epoch= 26)\n",
      "CV loss=0.22779332804562422(Epoch= 27)\n",
      "train loss=0.21516374596482687(Epoch= 27)\n",
      "CV loss=0.22464388788472778(Epoch= 28)\n",
      "train loss=0.21155266583667004(Epoch= 28)\n",
      "CV loss=0.22116860661006096(Epoch= 29)\n",
      "train loss=0.2082919327484646(Epoch= 29)\n",
      "CV loss=0.21870460839456357(Epoch= 30)\n",
      "train loss=0.2049091661465823(Epoch= 30)\n",
      "CV loss=0.21656190009739826(Epoch= 31)\n",
      "train loss=0.2022273807529124(Epoch= 31)\n",
      "CV loss=0.21264945073063576(Epoch= 32)\n",
      "train loss=0.19862204999391686(Epoch= 32)\n",
      "CV loss=0.21060527030874387(Epoch= 33)\n",
      "train loss=0.19578869598878704(Epoch= 33)\n",
      "CV loss=0.20804263653950986(Epoch= 34)\n",
      "train loss=0.1929675833065174(Epoch= 34)\n",
      "CV loss=0.20570257763233535(Epoch= 35)\n",
      "train loss=0.19026297624578936(Epoch= 35)\n",
      "CV loss=0.20351704376264868(Epoch= 36)\n",
      "train loss=0.18782225789405246(Epoch= 36)\n",
      "CV loss=0.20133875581707916(Epoch= 37)\n",
      "train loss=0.1852086989488654(Epoch= 37)\n",
      "CV loss=0.19890902656042234(Epoch= 38)\n",
      "train loss=0.18255076397037862(Epoch= 38)\n",
      "CV loss=0.19688198885939992(Epoch= 39)\n",
      "train loss=0.18021662315407194(Epoch= 39)\n",
      "CV loss=0.19478073825744832(Epoch= 40)\n",
      "train loss=0.17781079202219227(Epoch= 40)\n",
      "CV loss=0.19262878663273098(Epoch= 41)\n",
      "train loss=0.17549802502679515(Epoch= 41)\n",
      "CV loss=0.19058196883352477(Epoch= 42)\n",
      "train loss=0.1732951791021925(Epoch= 42)\n",
      "CV loss=0.1883770181570364(Epoch= 43)\n",
      "train loss=0.17084326822092905(Epoch= 43)\n",
      "CV loss=0.18657206095156384(Epoch= 44)\n",
      "train loss=0.16880685415749597(Epoch= 44)\n",
      "CV loss=0.1849929717349311(Epoch= 45)\n",
      "train loss=0.16675035689992163(Epoch= 45)\n",
      "CV loss=0.183179869133379(Epoch= 46)\n",
      "train loss=0.1646971563879747(Epoch= 46)\n",
      "CV loss=0.18161934126880788(Epoch= 47)\n",
      "train loss=0.16249488746992577(Epoch= 47)\n",
      "CV loss=0.1808992570994529(Epoch= 48)\n",
      "train loss=0.16142085263062758(Epoch= 48)\n",
      "CV loss=0.17789400901438535(Epoch= 49)\n",
      "train loss=0.15866735225994485(Epoch= 49)\n",
      "CV loss=0.1758974397690693(Epoch= 50)\n",
      "train loss=0.15686738981641313(Epoch= 50)\n",
      "CV loss=0.1751600781797702(Epoch= 51)\n",
      "train loss=0.1552385980668809(Epoch= 51)\n",
      "CV loss=0.17325519446422927(Epoch= 52)\n",
      "train loss=0.15338533613027033(Epoch= 52)\n",
      "CV loss=0.1712280343232849(Epoch= 53)\n",
      "train loss=0.15151947983312494(Epoch= 53)\n",
      "CV loss=0.1703638273720484(Epoch= 54)\n",
      "train loss=0.149803200988699(Epoch= 54)\n",
      "CV loss=0.16838361039648636(Epoch= 55)\n",
      "train loss=0.14799970330043896(Epoch= 55)\n",
      "CV loss=0.1672533833498708(Epoch= 56)\n",
      "train loss=0.1463125081159648(Epoch= 56)\n",
      "CV loss=0.16521192689674913(Epoch= 57)\n",
      "train loss=0.14465992189392432(Epoch= 57)\n",
      "CV loss=0.16435943586078425(Epoch= 58)\n",
      "train loss=0.14323968848877394(Epoch= 58)\n",
      "CV loss=0.16352750135072944(Epoch= 59)\n",
      "train loss=0.14202641765352317(Epoch= 59)\n",
      "CV loss=0.16168952302472878(Epoch= 60)\n",
      "train loss=0.1406623361085068(Epoch= 60)\n",
      "CV loss=0.16042231802567727(Epoch= 61)\n",
      "train loss=0.1386380968129788(Epoch= 61)\n",
      "CV loss=0.1590786413477945(Epoch= 62)\n",
      "train loss=0.13730705021857306(Epoch= 62)\n",
      "CV loss=0.15781062699228188(Epoch= 63)\n",
      "train loss=0.13580805166983634(Epoch= 63)\n",
      "CV loss=0.15701039662439897(Epoch= 64)\n",
      "train loss=0.13437556887809804(Epoch= 64)\n",
      "CV loss=0.15589602241267458(Epoch= 65)\n",
      "train loss=0.13323953514981884(Epoch= 65)\n",
      "CV loss=0.1541585253729074(Epoch= 66)\n",
      "train loss=0.13144969799220207(Epoch= 66)\n",
      "CV loss=0.15299149660976744(Epoch= 67)\n",
      "train loss=0.13011904454565726(Epoch= 67)\n",
      "CV loss=0.15176876351438837(Epoch= 68)\n",
      "train loss=0.12895529572362122(Epoch= 68)\n",
      "CV loss=0.15090655922813748(Epoch= 69)\n",
      "train loss=0.12765365862313785(Epoch= 69)\n",
      "CV loss=0.14966405854890982(Epoch= 70)\n",
      "train loss=0.12676561317791948(Epoch= 70)\n",
      "CV loss=0.1491491564724049(Epoch= 71)\n",
      "train loss=0.12533169717352885(Epoch= 71)\n",
      "CV loss=0.14865737090619238(Epoch= 72)\n",
      "train loss=0.12396208746455312(Epoch= 72)\n",
      "CV loss=0.14681231206225548(Epoch= 73)\n",
      "train loss=0.12268821287072014(Epoch= 73)\n",
      "CV loss=0.14609629917015582(Epoch= 74)\n",
      "train loss=0.12159146495623725(Epoch= 74)\n",
      "CV loss=0.14515712627263205(Epoch= 75)\n",
      "train loss=0.12017820526433987(Epoch= 75)\n",
      "CV loss=0.14358080729265257(Epoch= 76)\n",
      "train loss=0.11910114514005421(Epoch= 76)\n",
      "CV loss=0.1431939617246402(Epoch= 77)\n",
      "train loss=0.11806834137356606(Epoch= 77)\n",
      "CV loss=0.14219856164170824(Epoch= 78)\n",
      "train loss=0.1168155982556718(Epoch= 78)\n",
      "CV loss=0.14108639413731616(Epoch= 79)\n",
      "train loss=0.11561469979849268(Epoch= 79)\n",
      "CV loss=0.13962589202331047(Epoch= 80)\n",
      "train loss=0.11473707146526588(Epoch= 80)\n",
      "CV loss=0.1396876584252929(Epoch= 81)\n",
      "train loss=0.1135642979050722(Epoch= 81)\n",
      "CV loss=0.13834089433652805(Epoch= 82)\n",
      "train loss=0.11258030233901077(Epoch= 82)\n",
      "CV loss=0.1375362615738165(Epoch= 83)\n",
      "train loss=0.11173769647375582(Epoch= 83)\n",
      "CV loss=0.13681617282054673(Epoch= 84)\n",
      "train loss=0.11068119699736131(Epoch= 84)\n",
      "CV loss=0.13606513730292955(Epoch= 85)\n",
      "train loss=0.1095711079796305(Epoch= 85)\n",
      "CV loss=0.1352008982536979(Epoch= 86)\n",
      "train loss=0.10851560677974152(Epoch= 86)\n",
      "CV loss=0.133817202852858(Epoch= 87)\n",
      "train loss=0.10736644067638708(Epoch= 87)\n",
      "CV loss=0.13369816471235238(Epoch= 88)\n",
      "train loss=0.10682820826878593(Epoch= 88)\n",
      "CV loss=0.13236573185523723(Epoch= 89)\n",
      "train loss=0.10552317496031856(Epoch= 89)\n",
      "CV loss=0.13175361467534716(Epoch= 90)\n",
      "train loss=0.10470890867607108(Epoch= 90)\n",
      "CV loss=0.13068327875944075(Epoch= 91)\n",
      "train loss=0.10377323869756842(Epoch= 91)\n",
      "CV loss=0.1307732917627152(Epoch= 92)\n",
      "train loss=0.10326318029952894(Epoch= 92)\n",
      "CV loss=0.12979005081983114(Epoch= 93)\n",
      "train loss=0.10201211591678622(Epoch= 93)\n",
      "CV loss=0.1288137805723751(Epoch= 94)\n",
      "train loss=0.10133779539494625(Epoch= 94)\n",
      "CV loss=0.1279797004324526(Epoch= 95)\n",
      "train loss=0.10008143270255944(Epoch= 95)\n",
      "CV loss=0.12851420752911583(Epoch= 96)\n",
      "train loss=0.09960346292092027(Epoch= 96)\n",
      "CV loss=0.1268397162373091(Epoch= 97)\n",
      "train loss=0.09849838835510893(Epoch= 97)\n",
      "CV loss=0.12598577628392296(Epoch= 98)\n",
      "train loss=0.09741105724207849(Epoch= 98)\n",
      "CV loss=0.12533522234791303(Epoch= 99)\n",
      "train loss=0.09671352788313824(Epoch= 99)\n",
      "CV loss=0.1247938934877924(Epoch= 100)\n",
      "train loss=0.09569731521609152(Epoch= 100)\n",
      "CV loss=0.1242118042106146(Epoch= 101)\n",
      "train loss=0.09496907823161231(Epoch= 101)\n",
      "CV loss=0.12448250876943495(Epoch= 102)\n",
      "train loss=0.09447047496479359(Epoch= 102)\n",
      "CV loss=0.1232505608405903(Epoch= 103)\n",
      "train loss=0.09351826809026256(Epoch= 103)\n",
      "CV loss=0.1232684217141296(Epoch= 104)\n",
      "train loss=0.092954811908663(Epoch= 104)\n",
      "CV loss=0.12266285309317794(Epoch= 105)\n",
      "train loss=0.09201915722109344(Epoch= 105)\n",
      "CV loss=0.12113350305735386(Epoch= 106)\n",
      "train loss=0.09106671658424663(Epoch= 106)\n",
      "CV loss=0.12075753301850987(Epoch= 107)\n",
      "train loss=0.0903842605185247(Epoch= 107)\n",
      "CV loss=0.12052165154890514(Epoch= 108)\n",
      "train loss=0.08975559626577166(Epoch= 108)\n",
      "CV loss=0.11981599602143186(Epoch= 109)\n",
      "train loss=0.08897312723420309(Epoch= 109)\n",
      "CV loss=0.11900504803039813(Epoch= 110)\n",
      "train loss=0.0880689849129887(Epoch= 110)\n",
      "CV loss=0.11818985452264164(Epoch= 111)\n",
      "train loss=0.08746130188511815(Epoch= 111)\n",
      "CV loss=0.11765930819270136(Epoch= 112)\n",
      "train loss=0.08654370251989049(Epoch= 112)\n",
      "CV loss=0.11752789728400567(Epoch= 113)\n",
      "train loss=0.08605506576788635(Epoch= 113)\n",
      "CV loss=0.11680044612756761(Epoch= 114)\n",
      "train loss=0.08510709811461728(Epoch= 114)\n",
      "CV loss=0.11596407852459775(Epoch= 115)\n",
      "train loss=0.08447627627792596(Epoch= 115)\n",
      "CV loss=0.11592791263573937(Epoch= 116)\n",
      "train loss=0.08389883038724162(Epoch= 116)\n",
      "CV loss=0.11533399758026001(Epoch= 117)\n",
      "train loss=0.08346225876627089(Epoch= 117)\n",
      "CV loss=0.11487179482112574(Epoch= 118)\n",
      "train loss=0.08272932637423738(Epoch= 118)\n",
      "CV loss=0.1146111126255363(Epoch= 119)\n",
      "train loss=0.08183841583628235(Epoch= 119)\n",
      "CV loss=0.11420786014763569(Epoch= 120)\n",
      "train loss=0.08132469110125141(Epoch= 120)\n",
      "CV loss=0.11372221289201936(Epoch= 121)\n",
      "train loss=0.08057605676001621(Epoch= 121)\n",
      "CV loss=0.11320889978746505(Epoch= 122)\n",
      "train loss=0.07993764618520757(Epoch= 122)\n",
      "CV loss=0.11273648812161415(Epoch= 123)\n",
      "train loss=0.07937142225864448(Epoch= 123)\n",
      "CV loss=0.11143757371803668(Epoch= 124)\n",
      "train loss=0.07872762417751325(Epoch= 124)\n",
      "CV loss=0.112302783429858(Epoch= 125)\n",
      "train loss=0.07829069349248918(Epoch= 125)\n",
      "CV loss=0.11086516568448596(Epoch= 126)\n",
      "train loss=0.07759565320527875(Epoch= 126)\n",
      "CV loss=0.11115377144021985(Epoch= 127)\n",
      "train loss=0.07689775058401431(Epoch= 127)\n",
      "CV loss=0.11030303018634541(Epoch= 128)\n",
      "train loss=0.0762466947928961(Epoch= 128)\n",
      "CV loss=0.11020821621664335(Epoch= 129)\n",
      "train loss=0.07564049600898795(Epoch= 129)\n",
      "CV loss=0.10982733777632663(Epoch= 130)\n",
      "train loss=0.07497023312854882(Epoch= 130)\n",
      "CV loss=0.10887937968990673(Epoch= 131)\n",
      "train loss=0.07448227391138341(Epoch= 131)\n",
      "CV loss=0.10836722789556198(Epoch= 132)\n",
      "train loss=0.07373120266023693(Epoch= 132)\n",
      "CV loss=0.10829470339890697(Epoch= 133)\n",
      "train loss=0.07326176590533819(Epoch= 133)\n",
      "CV loss=0.10862216339762788(Epoch= 134)\n",
      "train loss=0.0729850389569557(Epoch= 134)\n",
      "CV loss=0.10725064476420662(Epoch= 135)\n",
      "train loss=0.07214993895811925(Epoch= 135)\n",
      "CV loss=0.1073271428300812(Epoch= 136)\n",
      "train loss=0.07160272864678735(Epoch= 136)\n",
      "CV loss=0.10632958490709478(Epoch= 137)\n",
      "train loss=0.07109298020213282(Epoch= 137)\n",
      "CV loss=0.10696502739949(Epoch= 138)\n",
      "train loss=0.07066512856506774(Epoch= 138)\n",
      "CV loss=0.1054318061981037(Epoch= 139)\n",
      "train loss=0.06989630067631976(Epoch= 139)\n",
      "CV loss=0.10590128308138823(Epoch= 140)\n",
      "train loss=0.06947798619115461(Epoch= 140)\n",
      "CV loss=0.10517826583052699(Epoch= 141)\n",
      "train loss=0.0688206386684246(Epoch= 141)\n",
      "CV loss=0.10472699305233177(Epoch= 142)\n",
      "train loss=0.06843811870855326(Epoch= 142)\n",
      "CV loss=0.10570518823599193(Epoch= 143)\n",
      "train loss=0.06829841142604907(Epoch= 143)\n",
      "CV loss=0.1039283266747266(Epoch= 144)\n",
      "train loss=0.06744100556227274(Epoch= 144)\n",
      "CV loss=0.10388893403188594(Epoch= 145)\n",
      "train loss=0.06703334816897065(Epoch= 145)\n",
      "CV loss=0.10310681657525646(Epoch= 146)\n",
      "train loss=0.06650975771943757(Epoch= 146)\n",
      "CV loss=0.10407853715506514(Epoch= 147)\n",
      "train loss=0.06646775397500286(Epoch= 147)\n",
      "CV loss=0.10263548164055016(Epoch= 148)\n",
      "train loss=0.06572248914744913(Epoch= 148)\n",
      "CV loss=0.10264406578145337(Epoch= 149)\n",
      "train loss=0.06497849307435676(Epoch= 149)\n",
      "CV loss=0.10354685166790532(Epoch= 150)\n",
      "train loss=0.06506215115260218(Epoch= 150)\n",
      "CV loss=0.10188461767687626(Epoch= 151)\n",
      "train loss=0.0638846803843276(Epoch= 151)\n",
      "CV loss=0.10134588532199015(Epoch= 152)\n",
      "train loss=0.06344846125265916(Epoch= 152)\n",
      "CV loss=0.10154738069080542(Epoch= 153)\n",
      "train loss=0.06300543905219834(Epoch= 153)\n",
      "CV loss=0.10101194597936208(Epoch= 154)\n",
      "train loss=0.06246776606430283(Epoch= 154)\n",
      "CV loss=0.10032769026429407(Epoch= 155)\n",
      "train loss=0.06190793997325608(Epoch= 155)\n",
      "CV loss=0.1004621804329268(Epoch= 156)\n",
      "train loss=0.061745142054242266(Epoch= 156)\n",
      "CV loss=0.10032039172272997(Epoch= 157)\n",
      "train loss=0.061299477501824365(Epoch= 157)\n",
      "CV loss=0.10012438539692028(Epoch= 158)\n",
      "train loss=0.06079514899697283(Epoch= 158)\n",
      "CV loss=0.09922328776532637(Epoch= 159)\n",
      "train loss=0.0605173118954649(Epoch= 159)\n",
      "CV loss=0.09893026653461293(Epoch= 160)\n",
      "train loss=0.05986212562486479(Epoch= 160)\n",
      "CV loss=0.09860083193909151(Epoch= 161)\n",
      "train loss=0.05935486425613111(Epoch= 161)\n",
      "CV loss=0.09865989897009518(Epoch= 162)\n",
      "train loss=0.05890976598118668(Epoch= 162)\n",
      "CV loss=0.09884252687968306(Epoch= 163)\n",
      "train loss=0.05863368045045524(Epoch= 163)\n",
      "CV loss=0.0977241649784422(Epoch= 164)\n",
      "train loss=0.058107062176107824(Epoch= 164)\n",
      "CV loss=0.09772395514069883(Epoch= 165)\n",
      "train loss=0.05761481031739371(Epoch= 165)\n",
      "CV loss=0.09736948634873613(Epoch= 166)\n",
      "train loss=0.057455600578998636(Epoch= 166)\n",
      "CV loss=0.09735020969360088(Epoch= 167)\n",
      "train loss=0.05681242711354527(Epoch= 167)\n",
      "CV loss=0.09726671127817729(Epoch= 168)\n",
      "train loss=0.05639431646037972(Epoch= 168)\n",
      "CV loss=0.09697278585725913(Epoch= 169)\n",
      "train loss=0.05619935842371338(Epoch= 169)\n",
      "CV loss=0.09648238104881833(Epoch= 170)\n",
      "train loss=0.055547780111115504(Epoch= 170)\n",
      "CV loss=0.09620461698562771(Epoch= 171)\n",
      "train loss=0.0551536067792136(Epoch= 171)\n",
      "CV loss=0.09593673596016775(Epoch= 172)\n",
      "train loss=0.055003850066825885(Epoch= 172)\n",
      "CV loss=0.09541073739826057(Epoch= 173)\n",
      "train loss=0.05442405454878428(Epoch= 173)\n",
      "CV loss=0.09580371193771012(Epoch= 174)\n",
      "train loss=0.05413098473249403(Epoch= 174)\n",
      "CV loss=0.09500081721087533(Epoch= 175)\n",
      "train loss=0.05371098249054369(Epoch= 175)\n",
      "CV loss=0.09467541461140618(Epoch= 176)\n",
      "train loss=0.053284905154313916(Epoch= 176)\n",
      "CV loss=0.0947020935981992(Epoch= 177)\n",
      "train loss=0.053048405506691425(Epoch= 177)\n",
      "CV loss=0.0945571645119369(Epoch= 178)\n",
      "train loss=0.052535471866107665(Epoch= 178)\n",
      "CV loss=0.09414053299542906(Epoch= 179)\n",
      "train loss=0.05207719150341202(Epoch= 179)\n",
      "CV loss=0.09404070498368526(Epoch= 180)\n",
      "train loss=0.05162933135892928(Epoch= 180)\n",
      "CV loss=0.09399025261219858(Epoch= 181)\n",
      "train loss=0.05125000314428572(Epoch= 181)\n",
      "CV loss=0.09388628017701083(Epoch= 182)\n",
      "train loss=0.050919410503970905(Epoch= 182)\n",
      "CV loss=0.09294134879062513(Epoch= 183)\n",
      "train loss=0.050585384806094445(Epoch= 183)\n",
      "CV loss=0.09291625443679687(Epoch= 184)\n",
      "train loss=0.05065118407064488(Epoch= 184)\n",
      "CV loss=0.0925749972796731(Epoch= 185)\n",
      "train loss=0.05005463715616096(Epoch= 185)\n",
      "CV loss=0.09303625516348518(Epoch= 186)\n",
      "train loss=0.04943445042200832(Epoch= 186)\n",
      "CV loss=0.09257661230667111(Epoch= 187)\n",
      "train loss=0.04925624480917983(Epoch= 187)\n",
      "CV loss=0.0920865955509765(Epoch= 188)\n",
      "train loss=0.04882257611795803(Epoch= 188)\n",
      "CV loss=0.09204171082343207(Epoch= 189)\n",
      "train loss=0.04867317660346843(Epoch= 189)\n",
      "CV loss=0.09172018511205553(Epoch= 190)\n",
      "train loss=0.04816317526363277(Epoch= 190)\n",
      "CV loss=0.09223888970521031(Epoch= 191)\n",
      "train loss=0.04793806843736105(Epoch= 191)\n",
      "CV loss=0.09117053798206975(Epoch= 192)\n",
      "train loss=0.04745124778086202(Epoch= 192)\n",
      "CV loss=0.0915703973874399(Epoch= 193)\n",
      "train loss=0.04720069025383909(Epoch= 193)\n",
      "CV loss=0.09087373286207184(Epoch= 194)\n",
      "train loss=0.04677205913692999(Epoch= 194)\n",
      "CV loss=0.09060082630973589(Epoch= 195)\n",
      "train loss=0.0465461828029983(Epoch= 195)\n",
      "CV loss=0.09092883640204112(Epoch= 196)\n",
      "train loss=0.04608101404970031(Epoch= 196)\n",
      "CV loss=0.09074312053706587(Epoch= 197)\n",
      "train loss=0.0459229172272888(Epoch= 197)\n",
      "CV loss=0.09022409326334106(Epoch= 198)\n",
      "train loss=0.04548285418156607(Epoch= 198)\n",
      "CV loss=0.09028764017464214(Epoch= 199)\n",
      "train loss=0.04530089454571421(Epoch= 199)\n",
      "CV loss=0.08994640259615047(Epoch= 200)\n",
      "train loss=0.04478487032879852(Epoch= 200)\n",
      "CV loss=0.08955799361528716(Epoch= 201)\n",
      "train loss=0.044615731264029006(Epoch= 201)\n",
      "CV loss=0.08951826460927928(Epoch= 202)\n",
      "train loss=0.04418746648262918(Epoch= 202)\n",
      "CV loss=0.08989877237169988(Epoch= 203)\n",
      "train loss=0.04405933212435736(Epoch= 203)\n",
      "CV loss=0.08967756468744606(Epoch= 204)\n",
      "train loss=0.04362374656929328(Epoch= 204)\n",
      "CV loss=0.0890005826148799(Epoch= 205)\n",
      "train loss=0.04331061296761696(Epoch= 205)\n",
      "CV loss=0.0888989126500305(Epoch= 206)\n",
      "train loss=0.04299114741541369(Epoch= 206)\n",
      "CV loss=0.08899958737000677(Epoch= 207)\n",
      "train loss=0.04275447664151821(Epoch= 207)\n",
      "CV loss=0.08882387328875524(Epoch= 208)\n",
      "train loss=0.04237311682416159(Epoch= 208)\n",
      "CV loss=0.08883189202162571(Epoch= 209)\n",
      "train loss=0.0420894924501399(Epoch= 209)\n",
      "CV loss=0.08817181278766352(Epoch= 210)\n",
      "train loss=0.04169312581484249(Epoch= 210)\n",
      "CV loss=0.088686533859504(Epoch= 211)\n",
      "train loss=0.04151111066096543(Epoch= 211)\n",
      "CV loss=0.08854526560579164(Epoch= 212)\n",
      "train loss=0.04128717133092741(Epoch= 212)\n",
      "CV loss=0.08765488500306236(Epoch= 213)\n",
      "train loss=0.04087767480045308(Epoch= 213)\n",
      "CV loss=0.08774188985107598(Epoch= 214)\n",
      "train loss=0.04058838475091363(Epoch= 214)\n",
      "CV loss=0.08794291633839961(Epoch= 215)\n",
      "train loss=0.04042276638735208(Epoch= 215)\n",
      "CV loss=0.08764381462318484(Epoch= 216)\n",
      "train loss=0.040121059378751045(Epoch= 216)\n",
      "CV loss=0.08723752409960607(Epoch= 217)\n",
      "train loss=0.03984443495351488(Epoch= 217)\n",
      "CV loss=0.087104976516294(Epoch= 218)\n",
      "train loss=0.03960953268397402(Epoch= 218)\n",
      "CV loss=0.08705344766162913(Epoch= 219)\n",
      "train loss=0.039143318403846154(Epoch= 219)\n",
      "CV loss=0.08641539130240562(Epoch= 220)\n",
      "train loss=0.03892978214496865(Epoch= 220)\n",
      "CV loss=0.08645917925786732(Epoch= 221)\n",
      "train loss=0.03862725136219812(Epoch= 221)\n",
      "CV loss=0.08644362958029758(Epoch= 222)\n",
      "train loss=0.0387088729002352(Epoch= 222)\n",
      "CV loss=0.0864762501694674(Epoch= 223)\n",
      "train loss=0.03815915431900425(Epoch= 223)\n",
      "CV loss=0.0862604329336197(Epoch= 224)\n",
      "train loss=0.03799729806670404(Epoch= 224)\n",
      "CV loss=0.08618862431000805(Epoch= 225)\n",
      "train loss=0.03769006446151346(Epoch= 225)\n",
      "CV loss=0.08538698221433466(Epoch= 226)\n",
      "train loss=0.037730082540249815(Epoch= 226)\n",
      "CV loss=0.08553278638484095(Epoch= 227)\n",
      "train loss=0.037180558277712494(Epoch= 227)\n",
      "CV loss=0.08544970709067067(Epoch= 228)\n",
      "train loss=0.036919346087149994(Epoch= 228)\n",
      "CV loss=0.08551303094275337(Epoch= 229)\n",
      "train loss=0.03650809728462702(Epoch= 229)\n",
      "CV loss=0.08525205748500307(Epoch= 230)\n",
      "train loss=0.036264812540825274(Epoch= 230)\n",
      "CV loss=0.08602793828473956(Epoch= 231)\n",
      "train loss=0.03630354868666279(Epoch= 231)\n",
      "CV loss=0.0855705125162557(Epoch= 232)\n",
      "train loss=0.0358063341024594(Epoch= 232)\n",
      "CV loss=0.0850022263682772(Epoch= 233)\n",
      "train loss=0.035547361214349156(Epoch= 233)\n",
      "CV loss=0.0848775326543396(Epoch= 234)\n",
      "train loss=0.035436691326618384(Epoch= 234)\n",
      "CV loss=0.08542809690215855(Epoch= 235)\n",
      "train loss=0.03540461875217919(Epoch= 235)\n",
      "CV loss=0.08505229513744685(Epoch= 236)\n",
      "train loss=0.034919534002931525(Epoch= 236)\n",
      "CV loss=0.0847100255256913(Epoch= 237)\n",
      "train loss=0.03476554735901569(Epoch= 237)\n",
      "CV loss=0.08462902354620769(Epoch= 238)\n",
      "train loss=0.034358941510772685(Epoch= 238)\n",
      "CV loss=0.08477164412072882(Epoch= 239)\n",
      "train loss=0.0342190090709959(Epoch= 239)\n",
      "CV loss=0.08446772030476363(Epoch= 240)\n",
      "train loss=0.03396483725077309(Epoch= 240)\n",
      "CV loss=0.08401146474818265(Epoch= 241)\n",
      "train loss=0.033627665497052095(Epoch= 241)\n",
      "CV loss=0.08413910451175129(Epoch= 242)\n",
      "train loss=0.03366450189889052(Epoch= 242)\n",
      "CV loss=0.08371811749005992(Epoch= 243)\n",
      "train loss=0.03317094289468387(Epoch= 243)\n",
      "CV loss=0.08416176385359189(Epoch= 244)\n",
      "train loss=0.033115599817263384(Epoch= 244)\n",
      "CV loss=0.08339675375211046(Epoch= 245)\n",
      "train loss=0.03282750607514945(Epoch= 245)\n",
      "CV loss=0.0831532150600556(Epoch= 246)\n",
      "train loss=0.03258965362567727(Epoch= 246)\n",
      "CV loss=0.08330227235493069(Epoch= 247)\n",
      "train loss=0.03227422981277138(Epoch= 247)\n",
      "CV loss=0.08318516064726389(Epoch= 248)\n",
      "train loss=0.03215990279296272(Epoch= 248)\n",
      "CV loss=0.08306881193685986(Epoch= 249)\n",
      "train loss=0.03195774199238096(Epoch= 249)\n",
      "CV loss=0.08370387340633978(Epoch= 250)\n",
      "train loss=0.03166642848513253(Epoch= 250)\n",
      "CV loss=0.08359745161887197(Epoch= 251)\n",
      "train loss=0.031491864438069804(Epoch= 251)\n",
      "CV loss=0.08365050719470063(Epoch= 252)\n",
      "train loss=0.031289327828434826(Epoch= 252)\n",
      "CV loss=0.08301663143345984(Epoch= 253)\n",
      "train loss=0.031054469899049558(Epoch= 253)\n",
      "CV loss=0.08244566700281027(Epoch= 254)\n",
      "train loss=0.0309522898149827(Epoch= 254)\n",
      "CV loss=0.08284986288046961(Epoch= 255)\n",
      "train loss=0.03066758948593867(Epoch= 255)\n",
      "CV loss=0.08241307896949508(Epoch= 256)\n",
      "train loss=0.030376916111827235(Epoch= 256)\n",
      "CV loss=0.08228226149945665(Epoch= 257)\n",
      "train loss=0.030200937696079302(Epoch= 257)\n",
      "CV loss=0.0821205279117328(Epoch= 258)\n",
      "train loss=0.030015567425151494(Epoch= 258)\n",
      "CV loss=0.08265526860382616(Epoch= 259)\n",
      "train loss=0.02994590987204946(Epoch= 259)\n",
      "CV loss=0.0821306948945257(Epoch= 260)\n",
      "train loss=0.02950195259602282(Epoch= 260)\n",
      "CV loss=0.08219604302290737(Epoch= 261)\n",
      "train loss=0.029446174853592046(Epoch= 261)\n",
      "CV loss=0.08179400306430704(Epoch= 262)\n",
      "train loss=0.029158642671419318(Epoch= 262)\n",
      "CV loss=0.08261369491608966(Epoch= 263)\n",
      "train loss=0.029083585801690265(Epoch= 263)\n",
      "CV loss=0.08203949897789188(Epoch= 264)\n",
      "train loss=0.028842685387370864(Epoch= 264)\n",
      "CV loss=0.08161412619388846(Epoch= 265)\n",
      "train loss=0.028617210073561425(Epoch= 265)\n",
      "CV loss=0.0817505114033885(Epoch= 266)\n",
      "train loss=0.028509943641599266(Epoch= 266)\n",
      "CV loss=0.08173130527614883(Epoch= 267)\n",
      "train loss=0.02817851645673348(Epoch= 267)\n",
      "CV loss=0.08139208294905569(Epoch= 268)\n",
      "train loss=0.028124901849722942(Epoch= 268)\n",
      "CV loss=0.08204372709693876(Epoch= 269)\n",
      "train loss=0.028001085288380377(Epoch= 269)\n",
      "CV loss=0.08173554846474089(Epoch= 270)\n",
      "train loss=0.027759983664397244(Epoch= 270)\n",
      "CV loss=0.08140240717296124(Epoch= 271)\n",
      "train loss=0.027446924107699708(Epoch= 271)\n",
      "CV loss=0.08176208525402635(Epoch= 272)\n",
      "train loss=0.02735385347525964(Epoch= 272)\n",
      "CV loss=0.08112732727181707(Epoch= 273)\n",
      "train loss=0.027156879540029644(Epoch= 273)\n",
      "CV loss=0.08189857906551322(Epoch= 274)\n",
      "train loss=0.02701806887814763(Epoch= 274)\n",
      "CV loss=0.08133286179720088(Epoch= 275)\n",
      "train loss=0.02683075675638884(Epoch= 275)\n",
      "CV loss=0.08108419740132931(Epoch= 276)\n",
      "train loss=0.026556146715249945(Epoch= 276)\n",
      "CV loss=0.08117200914643112(Epoch= 277)\n",
      "train loss=0.02649120353287713(Epoch= 277)\n",
      "CV loss=0.08089862882797251(Epoch= 278)\n",
      "train loss=0.026220695645238094(Epoch= 278)\n",
      "CV loss=0.08055739408012202(Epoch= 279)\n",
      "train loss=0.025999474785987926(Epoch= 279)\n",
      "CV loss=0.0807977207839257(Epoch= 280)\n",
      "train loss=0.025909900283129617(Epoch= 280)\n",
      "CV loss=0.08119149434567537(Epoch= 281)\n",
      "train loss=0.02573220694663604(Epoch= 281)\n",
      "CV loss=0.08078746924368267(Epoch= 282)\n",
      "train loss=0.025565142493909888(Epoch= 282)\n",
      "CV loss=0.0800580460357936(Epoch= 283)\n",
      "train loss=0.02540653787432156(Epoch= 283)\n",
      "CV loss=0.08015847244694094(Epoch= 284)\n",
      "train loss=0.0251970462329997(Epoch= 284)\n",
      "CV loss=0.0811064474991239(Epoch= 285)\n",
      "train loss=0.025208125587138565(Epoch= 285)\n",
      "CV loss=0.07992344466444647(Epoch= 286)\n",
      "train loss=0.024817831623338713(Epoch= 286)\n",
      "CV loss=0.08054818217183116(Epoch= 287)\n",
      "train loss=0.024688949139747268(Epoch= 287)\n",
      "CV loss=0.0803059480278443(Epoch= 288)\n",
      "train loss=0.024472476087899497(Epoch= 288)\n",
      "CV loss=0.08030422422584547(Epoch= 289)\n",
      "train loss=0.024352714565797604(Epoch= 289)\n",
      "CV loss=0.08013477500744944(Epoch= 290)\n",
      "train loss=0.024192942837046937(Epoch= 290)\n",
      "CV loss=0.08006823961535918(Epoch= 291)\n",
      "train loss=0.02399026767557354(Epoch= 291)\n",
      "CV loss=0.0802120629960627(Epoch= 292)\n",
      "train loss=0.02392076675641403(Epoch= 292)\n",
      "CV loss=0.08042258774037578(Epoch= 293)\n",
      "train loss=0.023863515138815655(Epoch= 293)\n",
      "CV loss=0.08019862050956311(Epoch= 294)\n",
      "train loss=0.0236971304655028(Epoch= 294)\n",
      "CV loss=0.07987534321030651(Epoch= 295)\n",
      "train loss=0.023413351265677007(Epoch= 295)\n",
      "CV loss=0.07955647305913233(Epoch= 296)\n",
      "train loss=0.023396457471542582(Epoch= 296)\n",
      "CV loss=0.07990618821466415(Epoch= 297)\n",
      "train loss=0.023121676272030357(Epoch= 297)\n",
      "CV loss=0.07981973492833341(Epoch= 298)\n",
      "train loss=0.022953722468789208(Epoch= 298)\n",
      "CV loss=0.07985901296332293(Epoch= 299)\n",
      "train loss=0.022900676852020345(Epoch= 299)\n",
      "CV loss=0.07956017111602692(Epoch= 300)\n",
      "train loss=0.02275069324027745(Epoch= 300)\n",
      "CV loss=0.07974523302100524(Epoch= 301)\n",
      "train loss=0.022518683902831658(Epoch= 301)\n",
      "CV loss=0.08021217452365148(Epoch= 302)\n",
      "train loss=0.022511402003524212(Epoch= 302)\n",
      "CV loss=0.07905868353309824(Epoch= 303)\n",
      "train loss=0.022256693219557974(Epoch= 303)\n",
      "CV loss=0.07912640389855444(Epoch= 304)\n",
      "train loss=0.022050619137510035(Epoch= 304)\n",
      "CV loss=0.07903329084213184(Epoch= 305)\n",
      "train loss=0.02206682634139849(Epoch= 305)\n",
      "CV loss=0.07914043779019682(Epoch= 306)\n",
      "train loss=0.021863375057841295(Epoch= 306)\n",
      "CV loss=0.07917423655912469(Epoch= 307)\n",
      "train loss=0.021690535481131146(Epoch= 307)\n",
      "CV loss=0.07957114280465885(Epoch= 308)\n",
      "train loss=0.02153249221561333(Epoch= 308)\n",
      "CV loss=0.07901426129697339(Epoch= 309)\n",
      "train loss=0.021377821589608118(Epoch= 309)\n",
      "CV loss=0.07913294597655669(Epoch= 310)\n",
      "train loss=0.021283206728358563(Epoch= 310)\n",
      "CV loss=0.07897383730839705(Epoch= 311)\n",
      "train loss=0.021168904139924158(Epoch= 311)\n",
      "CV loss=0.07895722727815212(Epoch= 312)\n",
      "train loss=0.020972920592046144(Epoch= 312)\n",
      "CV loss=0.07924308139682111(Epoch= 313)\n",
      "train loss=0.02099881481241291(Epoch= 313)\n",
      "CV loss=0.07921634938977026(Epoch= 314)\n",
      "train loss=0.020722823848285745(Epoch= 314)\n",
      "CV loss=0.07951130768688051(Epoch= 315)\n",
      "train loss=0.02067814077936625(Epoch= 315)\n",
      "CV loss=0.07910981434267839(Epoch= 316)\n",
      "train loss=0.020405576657582775(Epoch= 316)\n",
      "CV loss=0.07878551815393758(Epoch= 317)\n",
      "train loss=0.020278219119016075(Epoch= 317)\n",
      "CV loss=0.07909016349614212(Epoch= 318)\n",
      "train loss=0.020197044870210918(Epoch= 318)\n",
      "CV loss=0.07933767852165363(Epoch= 319)\n",
      "train loss=0.020140269913013955(Epoch= 319)\n",
      "CV loss=0.07838922482394219(Epoch= 320)\n",
      "train loss=0.02004095675696654(Epoch= 320)\n",
      "CV loss=0.07872432465213398(Epoch= 321)\n",
      "train loss=0.01987082196763779(Epoch= 321)\n",
      "CV loss=0.0786740050569597(Epoch= 322)\n",
      "train loss=0.019714789561753897(Epoch= 322)\n",
      "CV loss=0.07904052200150953(Epoch= 323)\n",
      "train loss=0.019665204031450275(Epoch= 323)\n",
      "CV loss=0.07843133135148216(Epoch= 324)\n",
      "train loss=0.01947442423405007(Epoch= 324)\n",
      "CV loss=0.07852540062749888(Epoch= 325)\n",
      "train loss=0.019312265451236103(Epoch= 325)\n",
      "CV loss=0.07820894484207731(Epoch= 326)\n",
      "train loss=0.01928421188320478(Epoch= 326)\n",
      "CV loss=0.0785161055955913(Epoch= 327)\n",
      "train loss=0.019084651303132617(Epoch= 327)\n",
      "CV loss=0.0783819093112577(Epoch= 328)\n",
      "train loss=0.019032413879990828(Epoch= 328)\n",
      "CV loss=0.07901841997796286(Epoch= 329)\n",
      "train loss=0.018888982928310368(Epoch= 329)\n",
      "CV loss=0.0783996246683225(Epoch= 330)\n",
      "train loss=0.018768837500435738(Epoch= 330)\n",
      "CV loss=0.07843348607637785(Epoch= 331)\n",
      "train loss=0.018629376823862336(Epoch= 331)\n",
      "CV loss=0.07844917869413477(Epoch= 332)\n",
      "train loss=0.018534146374582967(Epoch= 332)\n",
      "CV loss=0.07839655666367852(Epoch= 333)\n",
      "train loss=0.018359267165229023(Epoch= 333)\n",
      "CV loss=0.07866013413504155(Epoch= 334)\n",
      "train loss=0.01827819880679202(Epoch= 334)\n",
      "CV loss=0.07823031531095102(Epoch= 335)\n",
      "train loss=0.018106587476707(Epoch= 335)\n",
      "CV loss=0.07865826930969888(Epoch= 336)\n",
      "train loss=0.018058891146898248(Epoch= 336)\n",
      "early stop\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "batch_size = 64\n",
    "loss = float(\"inf\")\n",
    "epsilon = 0.0001\n",
    "\n",
    "n_input = 784\n",
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 128\n",
    "n_out = 10\n",
    "\n",
    "# early stop logic\n",
    "best_cv_loss = float(\"inf\")\n",
    "patience = 100\n",
    "counter = 0\n",
    "tol = 1e-6 # accounts for floating point noise\n",
    "\n",
    "training_loss_history = []\n",
    "cv_loss_history = []\n",
    "\n",
    "i = 0\n",
    "while i < 2000: # epochs\n",
    "    randomized_indices = np.random.permutation(X_train.shape[0])\n",
    "    \n",
    "    for start_of_batch in range(0, X_train.shape[0], batch_size):\n",
    "        batch_indices = randomized_indices[start_of_batch : start_of_batch + batch_size]\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        loss, cache = forwardPass(X_train=X_batch, y_train=y_batch, weights=weights, lmbda=0)\n",
    "        grads = backpropagation(cache=cache, X_train=X_batch, y_train=y_batch, weights=weights, lmbda=0)\n",
    "        weights = update(alpha=0.001, cache = cache, weights=weights, grads=grads)\n",
    "    cv_loss, cache = forwardPass(X_train=X_cv, y_train=y_cv, weights=weights)\n",
    "    print(\"CV loss=\", cv_loss, \"(Epoch= \", i + 1, \")\", sep=\"\")\n",
    "    train_loss, cache = forwardPass(X_train=X_train, y_train=y_train, weights=weights)\n",
    "    print(\"train loss=\", train_loss, \"(Epoch= \", i + 1, \")\", sep=\"\")\n",
    "    training_loss_history.append(train_loss)\n",
    "    cv_loss_history.append(cv_loss)\n",
    "\n",
    "    if cv_loss < best_cv_loss - tol:\n",
    "        best_cv_loss = cv_loss\n",
    "        best_weights = copy.deepcopy(weights)\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"early stop\")\n",
    "        weights = best_weights\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_loss_history, label=\"Training Loss\")\n",
    "plt.plot(cv_loss_history, label=\"CV Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed810c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.99464285714285 %\n",
      "CV accuracy: 97.65714285714286 %\n",
      "relative gap between CV and Train loss: 335.56533272093526 %\n",
      "\n",
      "Training error rate is approximately 0.005357142857150166 %\n",
      "CV error rate is approximately 2.3428571428571416 %\n",
      "Bayes/human error rate is approximately 1%\n"
     ]
    }
   ],
   "source": [
    "def inference(X):\n",
    "\n",
    "    # First Hidden Layer\n",
    "    z_1 = X @ W1 + b1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    z_2 = a_1 @ W2 + b2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    z_3 = a_2 @ W3 + b3\n",
    "    a_3 = np.maximum(0, z_3)\n",
    "\n",
    "    # Logits before softmax\n",
    "    z_4 = a_3 @ W4 + b4\n",
    "\n",
    "    # Softmax activation applied\n",
    "    shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "    z_4_exp = np.exp(shifted)\n",
    "    p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "\n",
    "    return np.argmax(p, axis=1)\n",
    "\n",
    "train_accuracy = np.mean(inference(X_train) == y_train)\n",
    "print(\"Training accuracy:\", train_accuracy*100, \"%\")\n",
    "\n",
    "cv_accuracy = np.mean(inference(X_cv) == y_cv)\n",
    "print(\"CV accuracy:\", cv_accuracy*100, \"%\")\n",
    "\n",
    "gap = cv_loss - train_loss\n",
    "gap_percentage = (gap / train_loss) * 100\n",
    "print(\"relative gap between CV and Train loss:\", gap_percentage, \"%\")\n",
    "print()\n",
    "\n",
    "print(\"Training error rate is approximately\", 100 - train_accuracy*100, \"%\")\n",
    "print(\"CV error rate is approximately\", 100 - cv_accuracy*100, \"%\")\n",
    "print(\"Bayes/human error rate for mnist datset is approximately 1%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9e158f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 2\n",
      "Index = 115\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(X_train.shape[0])\n",
    "\n",
    "img = X_train[idx].reshape(28, 28)\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label: {y_train[idx]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Predicted Label:\", inference(X_train[0:1])[idx])\n",
    "print(\"Index =\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0869e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
