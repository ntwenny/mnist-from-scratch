{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f532c213",
   "metadata": {},
   "source": [
    "This is the notebook for training a neural network from scratch with only numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef183bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906ba58",
   "metadata": {},
   "source": [
    "Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f79bb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(name=\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist.data.astype(np.float32)\n",
    "y = mnist.target.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d6fe2",
   "metadata": {},
   "source": [
    "Data exploration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f7ccbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (70000, 784)\n",
      "y.shape: (70000,)\n",
      "X.dtype: float32\n",
      "y.dtype: int64\n",
      "Min pixel value: 0.0\n",
      "Max pixel value: 255.0\n",
      "Unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "Any NaNs in X: False\n",
      "Any NaNs in y: False\n"
     ]
    }
   ],
   "source": [
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "\n",
    "print(\"X.dtype:\", X.dtype)\n",
    "print(\"y.dtype:\", y.dtype)\n",
    "\n",
    "print(\"Min pixel value:\", X.min())\n",
    "print(\"Max pixel value:\", X.max())\n",
    "\n",
    "print(\"Unique labels:\", np.unique(y))\n",
    "\n",
    "print(\"Any NaNs in X:\", np.isnan(X).any())\n",
    "print(\"Any NaNs in y:\", np.isnan(y).any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc8dde",
   "metadata": {},
   "source": [
    "Plotting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a29e85b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHHCAYAAACRAnNyAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALc9JREFUeJzt3Qd4FFXb//E7EAgEMFTpEKQpICAgPgjSOw9SLDQlIKI0FSliXgtiC6ig2IDHR5rSRAEVBQSkKh0pgiAgVUCahCahZN7rPv9397+bnk3Zk+T7ua5hs7uT2bMnw85vT5kJcBzHEQAAAAtl83cBAAAA4kNQAQAA1iKoAAAAaxFUAACAtQgqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABUkmvXr0kNDQ0zepz6tSpEhAQIIcOHZL0tHLlSvO6epsRNG7c2CzpQevllVdecd/Xn/WxM2fOpMvr6/6m+x2QmRFUgCQGBNeSK1cuqVSpkgwaNEj++usvq+qvevXqUqZMGUnoyhj169eXokWLyo0bN8R2ehD2rPu8efPKbbfdJg8++KB89dVXEh0dnSqv8/PPP5uQcf78ebGNzWUD0kNgurwKkAm8+uqrUq5cObl69aqsXbtWJkyYIN9//738+uuvEhwcLJ988kmqHTh91aNHD3n++edlzZo10rBhw1jPa2vMunXrTMgKDMwY//2DgoLkv//9r/n5n3/+kcOHD8u3335rwoq2nHz99ddyyy23uNf/4YcffAoDo0aNMsEof/78Sf49LU9a12NCZdu7d69ky8b3TWRu7OFAErVp00YeeeQRefzxx00ry+DBg+XgwYPmQKly5MhhDqr+1L17d9PyMHPmzDifnzVrlmlt0UCTUWgQ0HrXpW/fvvL666/L9u3bJSIiwnRH6WOecubMaZa0omFUw6rS1jV/Bj7d33S/AzIzggrgo6ZNm5pbDStxjVEZOXKk+ba7fPlyr9974oknzIFUD7YuGzZskNatW0tISIhpnWnUqJH89NNPyS5T6dKlTUvKl19+KdevX4/1vAaY8uXLyz333GNaJgYMGCCVK1eW3LlzS6FCheShhx5K0hiY+MZGxDU+JCoqytRFhQoVzIFVy/jcc8+Zx1NCW45atmwpc+fOld9//z3BMnzwwQdStWpVU7cFChSQOnXquMOcdqsMHz7c/KwtZq5uJlc96M/aAjVjxgyzDX0PixcvjnOMiouOUXn44YdNS4/W6zPPPOMON0q3rb+rgTcmz20mVra4/g5//PGH+TsWLFjQvN9//etf8t1338U57uiLL76QN954Q0qVKmVCV7NmzWT//v3J/EsAaStjtP0CFjpw4IC51QNRXF588UXTRdGnTx/ZuXOn5MuXT5YsWWK6iF577TWpUaOGWe/HH380rTW1a9d2h5spU6aYIKRdOHXr1k1WubS1RMOQvta///1v9+NaBu2mevnll839TZs2mW6Frl27mgOVHvy0O0sP8rt37zYHudRofbj//vtNV5mW6Y477jDlePfdd024WLBgQYq2/+ijj5qunqVLl5pxQ3HR+n766adNV5ErMOzYscOEQ22B6ty5symLtjZpuQoXLmx+r0iRIu5t6N9ID+oaWPT5xAZNa0jRdbTVZ/369fL+++/L33//LdOnT0/W+0tK2TzpmKl7771Xrly5Yt6z7pvTpk0zfwMNr506dfJaf/To0WZ/GzZsmERGRspbb71l9h+tG8AaDoAETZkyRUemOsuWLXNOnz7tHD161Jk9e7ZTqFAhJ3fu3M6xY8fMemFhYU7ZsmW9fnfnzp1Ozpw5nccff9z5+++/nZIlSzp16tRxrl+/bp6Pjo52Klas6LRq1cr87HLlyhWnXLlyTosWLWKV4+DBgwmW99y5c05QUJDTrVs3r8eff/558/t79+51v0ZM69atM+tMnz7d/diKFSvMY3rrou9T329MjRo1MovLZ5995mTLls1Zs2aN13oTJ0402/zpp58SfC/6Gnny5In3+V9++cVs59lnn423DB06dHCqVq2a4Ou8/fbb8datPq7vYdeuXXE+N3LkSPd9/Vkfu//++73WGzBggHl8+/bt5r6+jt7Xv2li20yobDH/DoMHDzbretb3xYsXzb4UGhrq3Lx50+tvescddzhRUVHudcePH28e1/0WsAVdP0ASNW/e3HyT1a4LbYXQGSjz58+XkiVLxvs71apVMwMhdTBoq1atTJeAfsN1jWvYtm2b7Nu3z3yzP3v2rHlel8uXL5tm+NWrVyd7gK52bbRt21a++eYbs53/+0Iis2fPNl0erpYH7e5x0W4ifX3tntEBm1u3bk2V/UK7ZbQV5fbbb3e/N11c3WYrVqxI0fb1b6AuXrwY7zr6fo4dO2ZakHylXXFVqlRJ8voDBw70uv/UU0+ZWx18nZZ0+9oC16BBA6860tYsbTHTljJPvXv39hrPc99997m7jwBbZJqgoh/o7du3lxIlSpi+1+Q2KbvOfxBzyZMnT5qVGRnLRx99ZLoY9OCqH/j6Ya7hIzE6xkC7eTZu3Gi6djwPeBpSVFhYmAlBnouGGx3HoU3yyaXN9xpSXAN9tYtHD1Seg2h1xop2A2nw0nEX2q2gr6vTYH15zbjo+9u1a1es9+YKS6dOnUrR9i9dumRutVstPiNGjDAHaz2AV6xY0YSI5I7/0fEhyaGv40nHBWkXS1qfA0fHHemYo5g0LLqe96RT2WOGXKXdVIAtMs0YFf1Q1oPBY489Zvp1k0v7aPv16+f1mH6jvfvuu1OxlMjI9ECnLRLJpYHGFUh0fIYnV2vJ22+/LTVr1kyw1SA5dGyKDszVAaPaWqO32bNnNy1Bnt/ydSyMzl6qV6+eWV/Dua6TWCuOrheXmzdvmtfxfH933nmnjBs3Ls71NSSlhI65UdoSFB89SOs03oULF5pBsHr+lY8//tiENG3tSgrP1idfxKyvhOovPXn+rTwldB4eIL1lmqCigxF1iY9+M33hhRfMoDT9xqhN8mPGjHHPDtCDgecBQWdk6LfmiRMnpkv5kTnpgVpnZejsDw0Eb775phnU6QrT+k1b6fPatZRatIVEX0cHb+oAS+2C0e6WYsWKudfRwZXakjN27Fj3YzrQNCknFtNv3nGtp9/Y9YRsLvr+9P+Shv74Ds4p8dlnn5nttmjRIsH1tGW0S5cuZrl27Zqpf53tEh4ebma7pHbZNJh6tsLoTBrdF1yDcF0tFzHrMGaLh0pO2cqWLWtCWUx79uxxPw9kNJmm6ycxOlpfT3Sl/fQ64l+n7+l0UNc33Zi02V2bp119toAvtCVBu13+85//mJk+OiOjf//+7lOs60wfPZi/88477m4MT6dPn/a54rWbR8eePPnkk2Y7Mc+dot+mY35z1mm8SflWr2XW2Sx60HfRFoujR4/Gmv3y559/mpk3MWnXk2sMjS90xorO+NHwEbOrxZOOvfGkYzK0+03fu2sKt6uLN7XO/qrdhDHrVbm+TGkw1a427bL2pC09MSWnbDo2SbsY9bPORetY9z8NSckZZwPYItO0qCTkyJEjpolbb3UMi6urR5uB9XH9lutJv1XqORP0PA2Ar3777Td56aWXTIuKjp9Set4M7eLR85fodFcdt6ChWA9geo4OHdyog3P14K5jYfSAplOcfR0AqtOOdZyKdl3E7BLV7iFtkdAuHz2A6cFt2bJl8U639qQnvdMWGQ37GkZ0qvbnn3/ubiHynD6s71O7VfX96On7NQjpN3x9XKdQJ9adpqf61227/m9qq4MOFNYvHE2aNDEH4YTouVa0Jcl16QD9u3z44YfSrl0799gWDYxKW12160tPoqZ/M1/HqOm5dXRKsNaP1quWX7vgXFPSXXWoYUtvtQ40tHieD8YlOWXTzyxtNdb9Sacn67lUdPC2lke7vDiLLTIkJxPStzV//nz3/YULF5rHdJqj5xIYGOg8/PDDsX5/5syZ5rmTJ0+mc8lhI9e04E2bNiW4nuf05Bs3bjh33323U6pUKef8+fNe67mmgM6ZM8drmm3nzp3NlGedWqzb0X1z+fLlyZ6e7Gn48OHmd+Laz3W6dO/evZ3ChQs7efPmNVOk9+zZE2vKa1zTk9XYsWPNdGstb/369Z3NmzfHmhqsrl275owZM8ZMEdZ1CxQo4NSuXdsZNWqUExkZmWid6mu7luDgYDPN9oEHHnC+/PJL93RbTzHLMGnSJKdhw4buui1fvrypl5iv/dprr5n3o1ORPetZfx44cGCc5YtvevLu3budBx980MmXL595v4MGDXL++ecfr9/V6eF9+vRxQkJCzHr6Nzp16lSsbSZUtrimiR84cMC8dv78+Z1cuXI5devWNZ+Bnlx/07lz53o9ntC0acBfAvQfyWS0T1enjXbs2NHcnzNnjmn21tkHMQeP6bgUz357pf3p+k1WtwEAAPwnS3T93HXXXaa5WadCJjbmRJtItYlam5YBAIB/ZZqgogMRPa9RoYFDT6alfbQ6KFZbVHr27GlmOGhw0cGFeg2W6tWrm75ql8mTJ0vx4sUTnEEEAADSR6bp+tGLbOnAuph0+qUOYNTR/XrVVZ2uqQMVdcS9XqxLz6Og53lQOn1Qp+9poNGpiwAAwL8yTVABAACZT5Y5jwoAAMh4CCoAAMBaGXowrY4pOX78uDlpU1qcnhsAAKQ+HXWiVz3Xk7AmdiLCDB1UNKSk9KJmAADAP/SyG3oG7UwbVFynv9Y3qidoAwAA9rtw4YJpaHAdxzNtUHF192hIIagAAJCxJGXYBoNpAQCAtQgqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANYK9HcBbBb6/HeJrnNodLt0KQsAAFkRLSoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANYiqAAAAGsRVAAAgLUIKgAAwFoEFQAAYC2CCgAAsBZBBQAAWIugAgAArEVQAQAA1iKoAAAAaxFUAACAtQgqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANYiqAAAAGtZE1RGjx4tAQEBMnjwYH8XBQAAWMKKoLJp0yaZNGmSVK9e3d9FAQAAFvF7ULl06ZL06NFDPvnkEylQoIC/iwMAACzi96AycOBAadeunTRv3jzRdaOiouTChQteCwAAyLwC/fnis2fPlq1bt5qun6SIiIiQUaNGpXm5AABAFm9ROXr0qDzzzDMyY8YMyZUrV5J+Jzw8XCIjI92LbgMAAGRefmtR2bJli5w6dUpq1arlfuzmzZuyevVq+fDDD003T/bs2b1+JygoyCwAACBr8FtQadasmezcudPrsd69e8vtt98uI0aMiBVSAABA1uO3oJIvXz6pVq2a12N58uSRQoUKxXocAABkTX6f9QMAAGDlrJ+YVq5c6e8iAAAAi9CiAgAArEVQAQAA1iKoAAAAaxFUAACAtQgqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANYiqAAAAGsRVAAAgLUIKgAAwFoEFQAAYC2CCgAAsBZBBQAAWIugAgAArEVQAQAA1iKoAAAAaxFUAACAtQgqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANYiqAAAAGsRVAAAgLUIKgAAwFoEFQAAYC2CCgAAsBZBBQAAWIugAgAArEVQAQAA1iKoAAAAaxFUAACAtQgqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANYiqAAAAGsRVAAAgLX8GlQmTJgg1atXl1tuucUs9erVk0WLFvmzSAAAwCJ+DSqlSpWS0aNHy5YtW2Tz5s3StGlT6dChg+zatcufxQIAAJYI9OeLt2/f3uv+G2+8YVpZ1q9fL1WrVvVbuQAAgB38GlQ83bx5U+bOnSuXL182XUBxiYqKMovLhQsX0rGEAAAgyw2m3blzp+TNm1eCgoKkX79+Mn/+fKlSpUqc60ZEREhISIh7KV26dLqXFwAAZKGgUrlyZdm2bZts2LBB+vfvL2FhYbJ79+441w0PD5fIyEj3cvTo0XQvLwAAyEJdPzlz5pQKFSqYn2vXri2bNm2S8ePHy6RJk2Ktq60uugAAgKzB7y0qMUVHR3uNQwEAAFmXX1tUtCunTZs2UqZMGbl48aLMnDlTVq5cKUuWLPFnsQAAgCX8GlROnTolPXv2lBMnTpjBsXryNw0pLVq08GexAACAJfwaVD799FN/vjwAALCcdWNUAAAAXAgqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAAMhcQeWPP/5I/ZIAAACkRlCpUKGCNGnSRD7//HO5evWqL5sAAABIm6CydetWqV69ugwZMkSKFSsmTz75pGzcuNGXTQEAAKRuUKlZs6aMHz9ejh8/LpMnT5YTJ05IgwYNpFq1ajJu3Dg5ffq0L5sFAABIvcG0gYGB0rlzZ5k7d66MGTNG9u/fL8OGDZPSpUtLz549TYABAADwS1DZvHmzDBgwQIoXL25aUjSkHDhwQJYuXWpaWzp06JCSzQMAgCwu0Jdf0lAyZcoU2bt3r7Rt21amT59ubrNl+3+5p1y5cjJ16lQJDQ1N7fICAIAsxKegMmHCBHnsscekV69epjUlLrfeeqt8+umnKS0fAADIwnwKKvv27Ut0nZw5c0pYWJgvmwcAAPB9jIp2++gA2pj0sWnTpvmySQAAgNQJKhEREVK4cOE4u3vefPNNXzYJAACQOkHlyJEjZsBsTGXLljXPAQAA+C2oaMvJjh07Yj2+fft2KVSoUGqUCwAAwLeg0q1bN3n66adlxYoVcvPmTbP8+OOP8swzz0jXrl2pVgAA4L9ZP6+99pocOnRImjVrZs5Oq6Kjo83ZaBmjAgAA/BpUdOrxnDlzTGDR7p7cuXPLnXfeacaoAAAA+DWouFSqVMksAAAA1gQVHZOip8hfvny5nDp1ynT7eNLxKgAAAH4JKjpoVoNKu3btpFq1ahIQEJDiggAAAKRKUJk9e7Z88cUX5kKEAAAAVk1P1sG0FSpUSP3SAAAApDSoDB06VMaPHy+O4/jy6wAAAGnX9bN27VpzsrdFixZJ1apVJUeOHF7Pz5s3z5fNAgAApDyo5M+fXzp16uTLrwIAAKRtUJkyZYovvwYAAJD2Y1TUjRs3ZNmyZTJp0iS5ePGieez48eNy6dIlXzcJAACQ8haVw4cPS+vWreXIkSMSFRUlLVq0kHz58smYMWPM/YkTJ/qyWQAAgJS3qOgJ3+rUqSN///23uc6Pi45b0bPVAgAA+K1FZc2aNfLzzz+b86l4Cg0NlT///DNVCgYAAOBTi4pe20ev9xPTsWPHTBcQAACA34JKy5Yt5b333nPf12v96CDakSNHclp9AADg366fsWPHSqtWraRKlSpy9epV6d69u+zbt08KFy4ss2bNSr3SAQCALM2noFKqVCnZvn27uTjhjh07TGtKnz59pEePHl6DawEAANI9qJhfDAyURx55JEUvDgAAkOpBZfr06Qk+37NnT182CwAAkPKgoudR8XT9+nW5cuWKma4cHBxMUAEAAP6b9aMnevNcdIzK3r17pUGDBgymBQAA/r/WT0wVK1aU0aNHx2ptAQAA8HtQcQ2w1QsTAgAA+G2MyjfffON133EcOXHihHz44YdSv379VCkYAACAT0GlY8eOXvf1zLRFihSRpk2bmpPBAQAA+C2o6LV+AAAAMtQYFQAAAL+3qAwZMiTJ644bN86XlwAAAPAtqPzyyy9m0RO9Va5c2Tz2+++/S/bs2aVWrVpeY1cAAADSNai0b99e8uXLJ9OmTZMCBQqYx/TEb71795b77rtPhg4d6nOBAAAAUjRGRWf2REREuEOK0p9ff/11Zv0AAAD/BpULFy7I6dOnYz2uj128eDE1ygUAAOBbUOnUqZPp5pk3b54cO3bMLF999ZX06dNHOnfuTLUCAAD/jVGZOHGiDBs2TLp3724G1JoNBQaaoPL222+nTskAAECW51NQCQ4Olo8//tiEkgMHDpjHypcvL3ny5MnyFQoAACw54Zte30cXvXKyhhS95g8AAIBfg8rZs2elWbNmUqlSJWnbtq0JK0q7fpiaDAAA/BpUnn32WcmRI4ccOXLEdAO5dOnSRRYvXpxqhQMAAFmbT2NUfvjhB1myZImUKlXK63HtAjp8+HBqlQ0AAGRxPrWoXL582aslxeXcuXMSFBSU5O3oSePuvvtuc5bbW2+9VTp27Ch79+71pUgAACAT8imo6Gnyp0+f7nVNn+joaHnrrbekSZMmSd7OqlWrZODAgbJ+/XpZunSpmercsmVLE4QAAAB86vrRQKKDaTdv3izXrl2T5557Tnbt2mVaVH766ackbyfmeJapU6ealpUtW7ZIw4YN+esAAJDF+dSiUq1aNXO15AYNGkiHDh1MC4iekVavqKznU/FVZGSkuS1YsKDP2wAAAFm4RUW7Z1q3bm3OTvvCCy+kWkG062jw4MFSv359E4TiEhUVZRbPaw4BAIDMK9ktKjoteceOHaleEB2r8uuvv8rs2bMTHHwbEhLiXkqXLp3q5QAAABm86+eRRx6RTz/9NNUKMWjQIFm4cKGsWLEi1pRnT+Hh4aZ7yLUcPXo01coAAAAyyWDaGzduyOTJk2XZsmVSu3btWNf4GTduXJK2o6fcf+qpp2T+/PmycuVKKVeuXILr69Tn5Ex/BgAAWSio/PHHHxIaGmq6aGrVqmUe00G1nnSqcnK6e2bOnClff/21OZfKyZMnzeParZM7d+7kFA0AAGT1oKJnntXr+mgXjeuU+e+//74ULVrUpxefMGGCuW3cuLHX41OmTJFevXr5tE0AAJBFg0rMqyMvWrQoRSdn42rLAAAg1QfTuhA0AACANUFFx5/EHIOSnDEpAAAAadr1o2NHXDNvrl69Kv369Ys162fevHnJKgQAAECKg0pYWFis86kAAABYEVR0Ng4AAECGGEwLAACQlggqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANYiqAAAAGsRVAAAgLUIKgAAwFoEFQAAYC2CCgAAsBZBBQAAWIugAgAArEVQAQAA1iKoAAAAaxFUAACAtQgqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANYiqAAAAGsRVAAAgLUIKgAAwFoEFQAAYC2CCgAAsBZBBQAAWIugAgAArEVQAQAA1iKoAAAAaxFUAACAtQgqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANbya1BZvXq1tG/fXkqUKCEBAQGyYMECfxYHAABYxq9B5fLly1KjRg356KOP/FkMAABgqUB/vnibNm3MAgAAEBfGqAAAAGv5tUUluaKioszicuHCBb+WBwAApK0M1aISEREhISEh7qV06dL+LhIAAEhDGSqohIeHS2RkpHs5evSov4sEAADSUIbq+gkKCjILAADIGvwaVC5duiT79+933z948KBs27ZNChYsKGXKlPFn0QAAQFYPKps3b5YmTZq47w8ZMsTchoWFydSpU/1YMgAAIFk9qDRu3Fgcx/FnEQAAgMUy1GBaAACQtRBUAACAtQgqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANYiqAAAAGsRVAAAgLUIKgAAwFoEFQAAYC2CCgAAsBZBBQAAWIugAgAArEVQAQAA1iKoAAAAaxFUAACAtQgqAADAWgQVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANYiqAAAAGsF+rsAAADAP0Kf/y7RdQ6Nbif+RIsKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAFiLoAIAAKxFUAEAANYiqAAAAGsRVAAAgLUIKgAAwFoEFQAAYC2CCgAAsBZBBQAAWIugAgAArEVQAQAA1iKoAAAAaxFUAACAtQgqAADAWgQVAABgLYIKAACwFkEFAABYy4qg8tFHH0loaKjkypVL7rnnHtm4caO/iwQAACzg96AyZ84cGTJkiIwcOVK2bt0qNWrUkFatWsmpU6f8XTQAAOBnfg8q48aNk759+0rv3r2lSpUqMnHiRAkODpbJkyf7u2gAAMDPAv354teuXZMtW7ZIeHi4+7Fs2bJJ8+bNZd26dZIRhD7/nWQ0h0a383cRgFT7/8X+DBtkxGNBRuHXoHLmzBm5efOmFC1a1Otxvb9nz55Y60dFRZnFJTIy0txeuHAhTcoXHXVFMqMyz871dxGAVMP+DKSttDjGurbpOI7dQSW5IiIiZNSoUbEeL126tF/KAwBAZhfyXtpt++LFixISEmJvUClcuLBkz55d/vrrL6/H9X6xYsVira9dRDrw1iU6OlrOnTsnhQoVkoCAgFRPexqAjh49KrfcckuqbhvUb3pgH6Z+MzL238xdx47jmJBSokSJRNf1a1DJmTOn1K5dW5YvXy4dO3Z0hw+9P2jQoFjrBwUFmcVT/vz507SM+scjqFC/GRn7MPWbkbH/Zt46TqwlxZquH20hCQsLkzp16kjdunXlvffek8uXL5tZQAAAIGvze1Dp0qWLnD59Wl5++WU5efKk1KxZUxYvXhxrgC0AAMh6/B5UlHbzxNXV40/axaQnoYvZ1QTqN6NgH6Z+MzL2X+rYJcBJytwgAACArHhmWgAAgPgQVAAAgLUIKgAAwFoEFQAAYC2CShw++ugjCQ0NlVy5csk999wjGzduTP+/TCbwyiuvmDMGey633367+/mrV6/KwIEDzZmF8+bNKw888ECssxTD2+rVq6V9+/bmbI5anwsWLPB6XsfG61T/4sWLS+7cuc0FPvft2+e1jp7NuUePHuYET3rCxD59+silS5eo6iTUb69evWLt061bt6Z+k0gvg3L33XdLvnz55NZbbzUn+ty7d6/XOkn5XDhy5Ii0a9dOgoODzXaGDx8uN27cYB+WpNVx48aNY+3H/fr1s7aOCSoxzJkzx5yETqcmb926VWrUqCGtWrWSU6dO+eUPlNFVrVpVTpw44V7Wrl3rfu7ZZ5+Vb7/9VubOnSurVq2S48ePS+fOnf1aXtvpyRB1n9QwHZe33npL3n//fZk4caJs2LBB8uTJY/Zf/fB30ZCya9cuWbp0qSxcuNAcnJ944ol0fBcZt36VBhPPfXrWrFlez1O/8dP/5xpC1q9fb/a/69evS8uWLU29J/VzQS9kqwfQa9euyc8//yzTpk2TqVOnmoAOSVIdq759+3rtx/rZYW0d6/Rk/H9169Z1Bg4c6L5/8+ZNp0SJEk5ERATVlEwjR450atSoEedz58+fd3LkyOHMnTvX/dhvv/2mU+WddevWUddJoHU1f/589/3o6GinWLFizttvv+1Vz0FBQc6sWbPM/d27d5vf27Rpk3udRYsWOQEBAc6ff/5JvSdQvyosLMzp0KFDvPVE/SbPqVOnTD2vWrUqyZ8L33//vZMtWzbn5MmT7nUmTJjg3HLLLU5UVBT7cCJ1rBo1auQ888wzTnxsq2NaVDxoetyyZYtpLnfJli2bub9u3Tp/5MgMT7sdtBn9tttuM980tTlRaT1r0vesa+0WKlOmDHXto4MHD5qzO3vWqV5LQ7svXfuv3mp3j16ywkXX1/1cW2CQuJUrV5qm8MqVK0v//v3l7Nmz7ueo3+SJjIw0twULFkzy54Le3nnnnV5nL9dWQ73AnrYUIuE6dpkxY4a5MHC1atXMBX+vXLnifs62OrbizLS2OHPmjGnyinn6fr2/Z88ev5Uro9IDpDYX6ge6Ni2OGjVK7rvvPvn111/NAVUvShnzopJa1/ocks9Vb3Htv67n9FYPsp4CAwPNhxj1njjt9tFuiHLlysmBAwfkf/7nf6RNmzbmg12vBE/9Jp1egHbw4MFSv359c7B07Z+JfS7obVz7uOf/AcRfx6p79+5StmxZ8yVyx44dMmLECDOOZd68eVbWMUEFaUY/wF2qV69ugov+5/jiiy/MQE8go+natav7Z/3Gqft1+fLlTStLs2bN/Fq2jEbHUeiXFs9xa0ifOn7CY0ya7sc6+F73Xw3fuj/bhq4fD9oMpt+KYo4w1/vFihVL779NpqPfkipVqiT79+839aldbefPn/dah7r2nWsfTWj/1duYA8N1JL/OBGIfTz7t0tTPDd2nqd+k02u76UDuFStWSKlSpbz24cQ+F/Q2rn3c9RwSruO46JdI5bkf21THBBUP2uRYu3ZtWb58uVfTmd6vV69euv9xMhudAquJXdO71nOOHDm86lqbHnUMC3XtG+2O0A8RzzrVPmUde+KqU73Vg4COBXD58ccfzX7u+rBC0h07dsyMUdF9mvpNnI5R1gPo/PnzzX6n+6ynpHwu6O3OnTu9ArfObtHp9lWqVMnyu6+TSB3HZdu2bebWcz+2qo7Tffiu5WbPnm1mSUydOtWM4H/iiSec/Pnze41+RtIMHTrUWblypXPw4EHnp59+cpo3b+4ULlzYjEJX/fr1c8qUKeP8+OOPzubNm5169eqZBfG7ePGi88svv5hF//uOGzfO/Hz48GHz/OjRo83++vXXXzs7duwwM1TKlSvn/PPPP+5ttG7d2rnrrrucDRs2OGvXrnUqVqzodOvWjWpPpH71uWHDhpnZJ7pPL1u2zKlVq5apv6tXr1K/SdC/f38nJCTEfC6cOHHCvVy5csW9TmKfCzdu3HCqVavmtGzZ0tm2bZuzePFip0iRIk54eDj7sJN4He/fv9959dVXTd3qfqyfFbfddpvTsGFDa+uYoBKHDz74wPxHyZkzp5muvH79+vT/y2QCXbp0cYoXL27qsWTJkua+/idx0YPngAEDnAIFCjjBwcFOp06dzH8oxG/FihXmABpz0WmzrinKL730klO0aFETuJs1a+bs3bvXaxtnz541wSRv3rxmumHv3r3NQRgJ169+0OsHt35g6xTasmXLOn379o31JYb6jV9cdavLlClTkvW5cOjQIadNmzZO7ty5zZcf/VJ0/fp1dmEn8To+cuSICSUFCxY0nxEVKlRwhg8f7kRGRlpbxwH/98YAAACswxgVAABgLYIKAACwFkEFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCpAFterVy/p2LFjqm1Pr5gd8+q3ng4dOiQBAQHu03ZnpboBkHwEFSCT04OtBgNd9HpWFSpUkFdffdVcjFCNHz/ehIv0Urp0aTlx4oTXZecT88orr0jNmjUlvcWsm8aNG8vgwYPTvRxAVhbo7wIASHutW7eWKVOmSFRUlHz//ffm8u968bfw8HAJCQlJ1z+BXqE8o1zlNr3rBkBstKgAWUBQUJAJB2XLlpX+/ftL8+bN5ZtvvonVvXH69Gmz3ptvvun+3Z9//tm0xLiuaKthZ9iwYVKyZEnJkyePueryypUrk1yWmF0/+rt6X7dfp04dCQ4OlnvvvddcNVdpi8aoUaNk+/bt7pYhVyuHXgn68ccflyJFipgruzZt2tSsF7Ml5rPPPpPQ0FATPLp27SoXL150r/Pll1/KnXfeKblz55ZChQqZurl8+XKsutGfV61aZVpZXOU4ePCgaaF65513vN6jvjd9fv/+/cn+WwHwRlABsiA9KF+7di3W43rAnzx5sjnAb9682RzQH330UXPZ+GbNmpl19Od169bJ7NmzZceOHfLQQw+ZFpt9+/alqEwvvPCCjB071rxuYGCgPPbYY+bxLl26yNChQ6Vq1aqmy0gXfUzpa+ul6BctWiRbtmyRWrVqmXKeO3fOvd0DBw7IggULZOHChWbRsDF69GjznG6rW7du5rV+++03E5o6d+6sF2uNVT4NKPXq1ZO+ffu6y1GmTBnzu9pa5UnvN2zY0IQYAClDUAGyED0AL1u2TJYsWWJaH+LStm1bczDu0aOH9OvXz7SaREREmOeOHDliDsJz586V++67T8qXL29aVxo0aBDrYJ1cb7zxhjRq1EiqVKkizz//vGnJuXr1qglVefPmNeFFW3t00cfWrl0rGzduNGXRlpiKFSualg0dyKutJC7R0dGmBUbHxGiZNXi5Woc0bOhYHQ0n2uKiLSsDBgwwrxeTtsZoy5K2+LjKod1Y2tKirT9aFnX9+nWZOXOmO2gBSBnGqABZgLYk6MFXD6J64O7evbtpNYmPHvD1wK4hQFsqtOtI7dy5U27evCmVKlXyWl+7g7TbJCWqV6/u/rl48eLmVltLtNUiLtrFc+nSpViv+88//5hWFBcNIPny5fPatm5X1ahRw7TAaEBp1aqVtGzZUh588EEpUKBAkstdokQJadeunWmJqlu3rnz77bemPrS1B0DKEVSALKBJkyYyYcIE0yKgB1ZtnUiIHuiPHz9uQo2OKdEDudJgoK0IGl701lNcrRDJoYN7XXR8h9LXj4+WRUNHXONjPKdHe27XtW3XdvU9LF261LTe/PDDD/LBBx+YLqgNGzZIuXLlklx2HSejLTXvvvuuaVnSrilteQGQcgQVIAvQ7pukjpfQsSuPPPKIOdhWrlzZHIS1JeXWW2+Vu+66y7SoaIuEdqOkFw1Y+rqedDzKyZMnTejSVhNfaXCpX7++WV5++WUz4Hj+/PkyZMiQJJXD1V2mdaxhcPHixbJ69WqfywPAG2NUAHjRFoXIyEh5//33ZcSIEaabxzXeQn/WsSs9e/aUefPmmVkvOjZDx7B89913aVaTGkT0tXQ2zZkzZ0zXis7O0cGtOitHW0O05UdbRrT8OiA3KbTlRGc46fo6/kbfk858uuOOO+Ith/6OvpaWw7NlRseq6HRvHSuj5QKQOggqANy0G+W9994z03l1um+2bNnMz2vWrDGtBUq7NjSo6EwcbXHRoLBp06Z4x5KkhgceeMDMLNIuLJ2ZNGvWLNMSoueE0dk1vXv3NiFKpx4fPnxYihYtmqTt6nvU1g9tEdHff/HFF83MozZt2sS5vg4c1lCiA361HBpuXPr06WNao7QsAFJPgBPXPDwAQLJomNOBuUePHk1yUAKQOIIKAKSAdkNpd1FYWJiZsjxjxgzqE0hFdP0AQApoN5QOwNWz5L711lvUJZDKaFEBAADWokUFAABYi6ACAACsRVABAADWIqgAAABrEVQAAIC1CCoAAMBaBBUAAGAtggoAALAWQQUAAIit/hetXa0tJccnzQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting pixel value distribution\n",
    "\n",
    "plt.hist(X.flatten(), bins=50)\n",
    "plt.title(\"Pixel Value Distribution\")\n",
    "plt.xlabel(\"Pixel intensity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting label distribution\n",
    "plt.hist(y, bins=10)\n",
    "plt.title(\"Pixel Value Distribution\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d78c45",
   "metadata": {},
   "source": [
    "Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e681bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = X / 255\n",
    "\n",
    "train_end = int(X.shape[0] * 0.8)\n",
    "cv_end = int(X.shape[0] * 0.9)\n",
    "\n",
    "randomized_indices = np.random.permutation(X.shape[0])\n",
    "\n",
    "train_indices = randomized_indices[0 : train_end]\n",
    "cv_indices = randomized_indices[train_end : cv_end]\n",
    "test_indices = randomized_indices[cv_end:]\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "X_cv = X[cv_indices]\n",
    "y_cv = y[cv_indices]\n",
    "\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print(X_train[np.random.choice(X_train.shape[0], size=10, replace=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32757980",
   "metadata": {},
   "source": [
    "Weight Initialization (Using the He Initialization method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f83f39a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightInit(*, n_input=784, n1=512, n2=256, n3=128, n_out=10, X_train=X_train, y_train=y_train):\n",
    "    W1 = np.random.randn(n_input, n1) * np.sqrt(2.0 / n_input)\n",
    "    b1 = np.zeros(n1)\n",
    "    print(\"W1.shape=\", W1.shape)\n",
    "    print(\"b1.shape=\", b1.shape)\n",
    "    # print(W1[np.random.choice(W1.shape[0], size=10, replace=False)])\n",
    "\n",
    "    W2 = np.random.randn(n1, n2) * np.sqrt(2.0 / n1)\n",
    "    b2 = np.zeros(n2)\n",
    "    print(\"W2.shape=\", W2.shape)\n",
    "    print(\"b2.shape=\", b2.shape)\n",
    "\n",
    "    W3 = np.random.randn(n2, n3) * np.sqrt(2.0 / n2)\n",
    "    b3 = np.random.randn(n3)\n",
    "    print(\"W3.shape=\", W3.shape)\n",
    "    print(\"b3.shape=\", b3.shape)\n",
    "\n",
    "    W4 = np.random.randn(n3, n_out) * np.sqrt(2.0 / n3)\n",
    "    b4 = np.random.randn(n_out)\n",
    "    print(\"W3.shape=\", W3.shape)\n",
    "    print(\"b3.shape=\", b3.shape)\n",
    "\n",
    "    # use smaller set at first to test, comment out when using full dataset\n",
    "    # idx = np.random.choice(X_train.shape[0], size=200, replace=False)\n",
    "    # X_train = X_train[idx]\n",
    "    # y_train = y_train[idx]\n",
    "\n",
    "    return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'W3': W3,\n",
    "        'b3': b3,\n",
    "        'W4': W4,\n",
    "        'b4': b4\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abe5b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape= (784, 512)\n",
      "b1.shape= (512,)\n",
      "W2.shape= (512, 256)\n",
      "b2.shape= (256,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n"
     ]
    }
   ],
   "source": [
    "n_input = 784\n",
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 128\n",
    "n_out = 10\n",
    "\n",
    "# Weights as a dictionary\n",
    "weights = weightInit(n_input=n_input, n1=n1, n2=n2, n3=n3, n_out=n_out, X_train=X_train, y_train=y_train)\n",
    "W1 = weights['W1']\n",
    "b1 = weights['b1']\n",
    "W2 = weights['W2']\n",
    "b2 = weights['b2']\n",
    "W3 = weights['W3']\n",
    "b3 = weights['b3']\n",
    "W4 = weights['W4']\n",
    "b4 = weights['b4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a0953",
   "metadata": {},
   "source": [
    "Now we'll begin programming the forward pass\n",
    "\n",
    "First Hidden Layer (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3bee27ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 512)\n"
     ]
    }
   ],
   "source": [
    "z_1 = X_train @ W1 + b1\n",
    "a_1 = np.maximum(0, z_1)\n",
    "print(a_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c8046",
   "metadata": {},
   "source": [
    "Second Hidden Layer (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8072b2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 256)\n"
     ]
    }
   ],
   "source": [
    "z_2 = a_1 @ W2 + b2\n",
    "a_2 = np.maximum(0, z_2)\n",
    "print(a_2.shape)\n",
    "# print(a_2[np.random.choice(a_2.shape[0], size=1, replace=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392327d7",
   "metadata": {},
   "source": [
    "Third Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4de99851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 128)\n"
     ]
    }
   ],
   "source": [
    "z_3 = a_2 @ W3 + b3\n",
    "a_3 = np.maximum(0, z_3)\n",
    "print(a_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17b9a1",
   "metadata": {},
   "source": [
    "The logits before the softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7cf56f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_4 = a_3 @ W4 + b4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a58047",
   "metadata": {},
   "source": [
    "Softmax activation function applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ddd303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 10)\n",
      "[0.11577247 0.01580383 0.05529895 0.22177    0.00992068 0.14714474\n",
      " 0.01875554 0.01575198 0.0813076  0.31847422]\n",
      "1.0\n",
      "9\n",
      "vs\n",
      "Correct Image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "z_4_exp = np.exp(shifted)\n",
    "p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "print(p.shape)\n",
    "print(p[0])\n",
    "print(p[0].sum())\n",
    "print(np.argmax(p[0]))\n",
    "print(\"vs\")\n",
    "print(\"Correct Image\")\n",
    "\n",
    "\n",
    "img = X_train[0].reshape(28, 28)\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label: {y_train[0]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf517a",
   "metadata": {},
   "source": [
    "Loss Function for Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e92510a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7622466611385104\n"
     ]
    }
   ],
   "source": [
    "p = np.clip(p, 1e-12, 1.0)\n",
    "loss = -np.mean(np.log(p[np.arange(y_train.shape[0]), y_train]))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60070c1f",
   "metadata": {},
   "source": [
    "Repeatable function for forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10c2c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(*, X_train=X_train, y_train=y_train, weights=weights, lmbda=0):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    # First Hidden Layer\n",
    "    z_1 = X_train @ W1 + b1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    z_2 = a_1 @ W2 + b2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    z_3 = a_2 @ W3 + b3\n",
    "    a_3 = np.maximum(0, z_3)\n",
    "\n",
    "    # Logits before softmax\n",
    "    z_4 = a_3 @ W4 + b4\n",
    "\n",
    "    # Softmax activation applied\n",
    "    shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "    z_4_exp = np.exp(shifted)\n",
    "    p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "\n",
    "    # loss function for softmax\n",
    "    p = np.clip(p, 1e-12, 1.0)\n",
    "    loss = (\n",
    "        -np.mean(np.log(p[np.arange(y_train.shape[0]), y_train])) \n",
    "            + (lmbda/(2)) * ((W1**2).sum() + (W2**2).sum() + (W3**2).sum() + (W4**2).sum())\n",
    "        )\n",
    "    return loss, {\n",
    "        \"a_1\": a_1,\n",
    "        \"a_2\": a_2,\n",
    "        \"a_3\": a_3,\n",
    "        \"p\": p,\n",
    "        \"z_1\": z_1,\n",
    "        \"z_2\": z_2,\n",
    "        \"z_3\": z_3,\n",
    "        \"z_4\": z_4\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738dae8",
   "metadata": {},
   "source": [
    "Repeatable Function for backward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6d386147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(*, cache=None, X_train=None, y_train=None, weights=None, lmbda=0):\n",
    "\n",
    "    a_1 = cache[\"a_1\"]\n",
    "    a_2 = cache[\"a_2\"]\n",
    "    a_3 = cache[\"a_3\"]\n",
    "    p = cache[\"p\"]\n",
    "    z_1 = cache[\"z_1\"]\n",
    "    z_2 = cache[\"z_2\"]\n",
    "    z_3 = cache[\"z_3\"]\n",
    "    z_4 = cache[\"z_4\"]\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    y_1he = np.zeros((len(y_train), 10))\n",
    "    y_1he[np.arange(len(y_train)), y_train] = 1\n",
    "\n",
    "    dJ_dz4 = p - y_1he # Gradient of loss for each logit, shape = (200, 10)\n",
    "\n",
    "    dJ_dw4 = (a_3.T @ dJ_dz4) / a_3.shape[0]# Gradients for output layer\n",
    "    dJ_db4 = np.mean(dJ_dz4, axis=0)\n",
    "\n",
    "    dJ_da3 = dJ_dz4 @ W4.T # Gradients for third hidden layer\n",
    "\n",
    "    da3_dz3 = np.where(z_3 > 0, 1, 0)\n",
    "    dJ_dz3 = da3_dz3 * dJ_da3\n",
    "\n",
    "\n",
    "    dJ_dw3 = (a_2.T @ dJ_dz3) / a_2.shape[0]\n",
    "    dJ_db3 = np.mean(dJ_dz3, axis=0)\n",
    "\n",
    "\n",
    "    dJ_da2 = dJ_dz3 @ W3.T # Gradients for second hidden layer\n",
    "\n",
    "    da2_dz2 = np.where(z_2 > 0, 1, 0)\n",
    "    dJ_dz2 = da2_dz2 * dJ_da2\n",
    "\n",
    "    dJ_dw2 = (a_1.T @ dJ_dz2) / a_1.shape[0]\n",
    "    dJ_db2 = np.mean(dJ_dz2, axis=0)\n",
    "\n",
    "    dJ_da1 = dJ_dz2 @ W2.T # Gradients for first hidden layer\n",
    "\n",
    "    da1_dz1 = np.where(z_1 > 0, 1, 0)\n",
    "    dJ_dz1 = da1_dz1 * dJ_da1\n",
    "\n",
    "    dJ_dw1 = (X_train.T @ dJ_dz1) / X_train.shape[0]\n",
    "    dJ_db1 = np.mean(dJ_dz1, axis=0)\n",
    "\n",
    "    # L2 regularization\n",
    "    dJ_dw4 += (lmbda) * W4\n",
    "    dJ_dw3 += (lmbda) * W3\n",
    "    dJ_dw2 += (lmbda) * W2\n",
    "    dJ_dw1 += (lmbda) * W1\n",
    "\n",
    "    return {\"dJ_dw4\": dJ_dw4, \"dJ_db4\": dJ_db4, \"dJ_dw3\": dJ_dw3, \"dJ_db3\": dJ_db3,\n",
    "            \"dJ_dw2\": dJ_dw2, \"dJ_db2\": dJ_db2, \"dJ_dw1\": dJ_dw1, \"dJ_db1\": dJ_db1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70956097",
   "metadata": {},
   "source": [
    "Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8191e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(*, alpha = 0.01, cache = None, weights=None, grads=None):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    W4 -= alpha * grads[\"dJ_dw4\"]\n",
    "    b4 -= alpha * grads[\"dJ_db4\"]\n",
    "\n",
    "    W3 -= alpha * grads[\"dJ_dw3\"]\n",
    "    b3 -= alpha * grads[\"dJ_db3\"]\n",
    "\n",
    "    W2 -= alpha * grads[\"dJ_dw2\"]\n",
    "    b2 -= alpha * grads[\"dJ_db2\"]\n",
    "\n",
    "    W1 -= alpha * grads[\"dJ_dw1\"]\n",
    "    b1 -= alpha * grads[\"dJ_db1\"]\n",
    "\n",
    "    return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'W3': W3,\n",
    "        'b3': b3,\n",
    "        'W4': W4,\n",
    "        'b4': b4\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d672345",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "baa17d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape= (784, 512)\n",
      "b1.shape= (512,)\n",
      "W2.shape= (512, 256)\n",
      "b2.shape= (256,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "weights = weightInit(n_input=n_input, n1=n1, n2=n2, n3=n3, n_out=n_out, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e63e9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV loss=8.827347580013534(Epoch= 1)\n",
      "train loss=8.816232883428347(Epoch= 1)\n",
      "train loss=8.816232883428347(Epoch= 1)\n",
      "CV loss=8.056229629343122(Epoch= 2)\n",
      "CV loss=8.056229629343122(Epoch= 2)\n",
      "train loss=8.04359060551963(Epoch= 2)\n",
      "train loss=8.04359060551963(Epoch= 2)\n",
      "CV loss=7.406143331070038(Epoch= 3)\n",
      "CV loss=7.406143331070038(Epoch= 3)\n",
      "train loss=7.391608887703857(Epoch= 3)\n",
      "train loss=7.391608887703857(Epoch= 3)\n",
      "CV loss=6.818732862729522(Epoch= 4)\n",
      "CV loss=6.818732862729522(Epoch= 4)\n",
      "train loss=6.803320663253974(Epoch= 4)\n",
      "train loss=6.803320663253974(Epoch= 4)\n",
      "CV loss=6.287351536135892(Epoch= 5)\n",
      "CV loss=6.287351536135892(Epoch= 5)\n",
      "train loss=6.272148607488173(Epoch= 5)\n",
      "train loss=6.272148607488173(Epoch= 5)\n",
      "CV loss=5.797939428476763(Epoch= 6)\n",
      "CV loss=5.797939428476763(Epoch= 6)\n",
      "train loss=5.7834050830249195(Epoch= 6)\n",
      "train loss=5.7834050830249195(Epoch= 6)\n",
      "CV loss=5.3554668846405935(Epoch= 7)\n",
      "CV loss=5.3554668846405935(Epoch= 7)\n",
      "train loss=5.3402872895477165(Epoch= 7)\n",
      "train loss=5.3402872895477165(Epoch= 7)\n",
      "CV loss=4.94986128103733(Epoch= 8)\n",
      "CV loss=4.94986128103733(Epoch= 8)\n",
      "train loss=4.935480980840004(Epoch= 8)\n",
      "train loss=4.935480980840004(Epoch= 8)\n",
      "CV loss=4.583560027701865(Epoch= 9)\n",
      "CV loss=4.583560027701865(Epoch= 9)\n",
      "train loss=4.568368126304217(Epoch= 9)\n",
      "train loss=4.568368126304217(Epoch= 9)\n",
      "CV loss=4.239873044106551(Epoch= 10)\n",
      "CV loss=4.239873044106551(Epoch= 10)\n",
      "train loss=4.225307249771515(Epoch= 10)\n",
      "train loss=4.225307249771515(Epoch= 10)\n",
      "CV loss=3.929918273142708(Epoch= 11)\n",
      "CV loss=3.929918273142708(Epoch= 11)\n",
      "train loss=3.9161468145569285(Epoch= 11)\n",
      "train loss=3.9161468145569285(Epoch= 11)\n",
      "CV loss=3.6470081388822946(Epoch= 12)\n",
      "CV loss=3.6470081388822946(Epoch= 12)\n",
      "train loss=3.6324877050519166(Epoch= 12)\n",
      "train loss=3.6324877050519166(Epoch= 12)\n",
      "CV loss=3.386360888539725(Epoch= 13)\n",
      "CV loss=3.386360888539725(Epoch= 13)\n",
      "train loss=3.373087086756682(Epoch= 13)\n",
      "train loss=3.373087086756682(Epoch= 13)\n",
      "CV loss=3.149733839349348(Epoch= 14)\n",
      "CV loss=3.149733839349348(Epoch= 14)\n",
      "train loss=3.135858988734519(Epoch= 14)\n",
      "train loss=3.135858988734519(Epoch= 14)\n",
      "CV loss=2.930421252909954(Epoch= 15)\n",
      "CV loss=2.930421252909954(Epoch= 15)\n",
      "train loss=2.916920954119691(Epoch= 15)\n",
      "train loss=2.916920954119691(Epoch= 15)\n",
      "CV loss=2.7326234158681486(Epoch= 16)\n",
      "CV loss=2.7326234158681486(Epoch= 16)\n",
      "train loss=2.719234305913012(Epoch= 16)\n",
      "train loss=2.719234305913012(Epoch= 16)\n",
      "CV loss=2.5480685227671485(Epoch= 17)\n",
      "CV loss=2.5480685227671485(Epoch= 17)\n",
      "train loss=2.535134870139954(Epoch= 17)\n",
      "train loss=2.535134870139954(Epoch= 17)\n",
      "CV loss=2.381346905747696(Epoch= 18)\n",
      "CV loss=2.381346905747696(Epoch= 18)\n",
      "train loss=2.3682964780731175(Epoch= 18)\n",
      "train loss=2.3682964780731175(Epoch= 18)\n",
      "CV loss=2.2273594390583424(Epoch= 19)\n",
      "CV loss=2.2273594390583424(Epoch= 19)\n",
      "train loss=2.2142995994714445(Epoch= 19)\n",
      "train loss=2.2142995994714445(Epoch= 19)\n",
      "CV loss=2.0873360986629463(Epoch= 20)\n",
      "CV loss=2.0873360986629463(Epoch= 20)\n",
      "train loss=2.074824527137955(Epoch= 20)\n",
      "train loss=2.074824527137955(Epoch= 20)\n",
      "CV loss=1.958302663367579(Epoch= 21)\n",
      "CV loss=1.958302663367579(Epoch= 21)\n",
      "train loss=1.9458753342882198(Epoch= 21)\n",
      "train loss=1.9458753342882198(Epoch= 21)\n",
      "CV loss=1.840661968879277(Epoch= 22)\n",
      "CV loss=1.840661968879277(Epoch= 22)\n",
      "train loss=1.8287672367123025(Epoch= 22)\n",
      "train loss=1.8287672367123025(Epoch= 22)\n",
      "CV loss=1.7315332254297193(Epoch= 23)\n",
      "CV loss=1.7315332254297193(Epoch= 23)\n",
      "train loss=1.7197986419514946(Epoch= 23)\n",
      "train loss=1.7197986419514946(Epoch= 23)\n",
      "CV loss=1.6328141194719188(Epoch= 24)\n",
      "CV loss=1.6328141194719188(Epoch= 24)\n",
      "train loss=1.6206351621037722(Epoch= 24)\n",
      "train loss=1.6206351621037722(Epoch= 24)\n",
      "CV loss=1.543711519240966(Epoch= 25)\n",
      "CV loss=1.543711519240966(Epoch= 25)\n",
      "train loss=1.5310512736404276(Epoch= 25)\n",
      "train loss=1.5310512736404276(Epoch= 25)\n",
      "CV loss=1.459154741383079(Epoch= 26)\n",
      "CV loss=1.459154741383079(Epoch= 26)\n",
      "train loss=1.4466154227567096(Epoch= 26)\n",
      "train loss=1.4466154227567096(Epoch= 26)\n",
      "CV loss=1.3823214882217454(Epoch= 27)\n",
      "CV loss=1.3823214882217454(Epoch= 27)\n",
      "train loss=1.3708539662570638(Epoch= 27)\n",
      "train loss=1.3708539662570638(Epoch= 27)\n",
      "CV loss=1.3119286068173133(Epoch= 28)\n",
      "CV loss=1.3119286068173133(Epoch= 28)\n",
      "train loss=1.3004004698785925(Epoch= 28)\n",
      "train loss=1.3004004698785925(Epoch= 28)\n",
      "CV loss=1.247464353808873(Epoch= 29)\n",
      "CV loss=1.247464353808873(Epoch= 29)\n",
      "train loss=1.2352324945784199(Epoch= 29)\n",
      "train loss=1.2352324945784199(Epoch= 29)\n",
      "CV loss=1.1911198460491232(Epoch= 30)\n",
      "CV loss=1.1911198460491232(Epoch= 30)\n",
      "train loss=1.17828250991165(Epoch= 30)\n",
      "train loss=1.17828250991165(Epoch= 30)\n",
      "CV loss=1.1355469357328247(Epoch= 31)\n",
      "CV loss=1.1355469357328247(Epoch= 31)\n",
      "train loss=1.123470338936561(Epoch= 31)\n",
      "train loss=1.123470338936561(Epoch= 31)\n",
      "CV loss=1.0847487693310713(Epoch= 32)\n",
      "CV loss=1.0847487693310713(Epoch= 32)\n",
      "train loss=1.0737129720885(Epoch= 32)\n",
      "train loss=1.0737129720885(Epoch= 32)\n",
      "CV loss=1.0393067845132997(Epoch= 33)\n",
      "CV loss=1.0393067845132997(Epoch= 33)\n",
      "train loss=1.027387786098523(Epoch= 33)\n",
      "train loss=1.027387786098523(Epoch= 33)\n",
      "CV loss=0.9978490063421628(Epoch= 34)\n",
      "CV loss=0.9978490063421628(Epoch= 34)\n",
      "train loss=0.985701370051165(Epoch= 34)\n",
      "train loss=0.985701370051165(Epoch= 34)\n",
      "CV loss=0.9594784402198413(Epoch= 35)\n",
      "CV loss=0.9594784402198413(Epoch= 35)\n",
      "train loss=0.9473629480729008(Epoch= 35)\n",
      "train loss=0.9473629480729008(Epoch= 35)\n",
      "CV loss=0.9254799919667875(Epoch= 36)\n",
      "CV loss=0.9254799919667875(Epoch= 36)\n",
      "train loss=0.9130712418290964(Epoch= 36)\n",
      "train loss=0.9130712418290964(Epoch= 36)\n",
      "CV loss=0.891673405862518(Epoch= 37)\n",
      "CV loss=0.891673405862518(Epoch= 37)\n",
      "train loss=0.8804538303553429(Epoch= 37)\n",
      "train loss=0.8804538303553429(Epoch= 37)\n",
      "CV loss=0.8637260176788022(Epoch= 38)\n",
      "CV loss=0.8637260176788022(Epoch= 38)\n",
      "train loss=0.8507344545234684(Epoch= 38)\n",
      "train loss=0.8507344545234684(Epoch= 38)\n",
      "CV loss=0.8355504174613686(Epoch= 39)\n",
      "CV loss=0.8355504174613686(Epoch= 39)\n",
      "train loss=0.8233137430070325(Epoch= 39)\n",
      "train loss=0.8233137430070325(Epoch= 39)\n",
      "CV loss=0.8097699779829369(Epoch= 40)\n",
      "CV loss=0.8097699779829369(Epoch= 40)\n",
      "train loss=0.7980065677210749(Epoch= 40)\n",
      "train loss=0.7980065677210749(Epoch= 40)\n",
      "CV loss=0.7869659111628569(Epoch= 41)\n",
      "CV loss=0.7869659111628569(Epoch= 41)\n",
      "train loss=0.7751196198700169(Epoch= 41)\n",
      "train loss=0.7751196198700169(Epoch= 41)\n",
      "CV loss=0.7670235913516859(Epoch= 42)\n",
      "CV loss=0.7670235913516859(Epoch= 42)\n",
      "train loss=0.7541064553219021(Epoch= 42)\n",
      "train loss=0.7541064553219021(Epoch= 42)\n",
      "CV loss=0.7488232793014808(Epoch= 43)\n",
      "CV loss=0.7488232793014808(Epoch= 43)\n",
      "train loss=0.7356232530791957(Epoch= 43)\n",
      "train loss=0.7356232530791957(Epoch= 43)\n",
      "CV loss=0.7286662692817821(Epoch= 44)\n",
      "CV loss=0.7286662692817821(Epoch= 44)\n",
      "train loss=0.7161743812711412(Epoch= 44)\n",
      "train loss=0.7161743812711412(Epoch= 44)\n",
      "CV loss=0.7122353914650656(Epoch= 45)\n",
      "CV loss=0.7122353914650656(Epoch= 45)\n",
      "train loss=0.6999989292656498(Epoch= 45)\n",
      "train loss=0.6999989292656498(Epoch= 45)\n",
      "CV loss=0.698035741518822(Epoch= 46)\n",
      "CV loss=0.698035741518822(Epoch= 46)\n",
      "train loss=0.6851329464317085(Epoch= 46)\n",
      "train loss=0.6851329464317085(Epoch= 46)\n",
      "CV loss=0.6834720604460396(Epoch= 47)\n",
      "CV loss=0.6834720604460396(Epoch= 47)\n",
      "train loss=0.6715796731860261(Epoch= 47)\n",
      "train loss=0.6715796731860261(Epoch= 47)\n",
      "CV loss=0.6698911881181658(Epoch= 48)\n",
      "CV loss=0.6698911881181658(Epoch= 48)\n",
      "train loss=0.6578799731826147(Epoch= 48)\n",
      "train loss=0.6578799731826147(Epoch= 48)\n",
      "CV loss=0.6595166146361224(Epoch= 49)\n",
      "CV loss=0.6595166146361224(Epoch= 49)\n",
      "train loss=0.6468402939587571(Epoch= 49)\n",
      "train loss=0.6468402939587571(Epoch= 49)\n",
      "CV loss=0.6485109240344346(Epoch= 50)\n",
      "CV loss=0.6485109240344346(Epoch= 50)\n",
      "train loss=0.6357987367246978(Epoch= 50)\n",
      "train loss=0.6357987367246978(Epoch= 50)\n",
      "CV loss=0.6381364937561714(Epoch= 51)\n",
      "CV loss=0.6381364937561714(Epoch= 51)\n",
      "train loss=0.6257212501676568(Epoch= 51)\n",
      "train loss=0.6257212501676568(Epoch= 51)\n",
      "CV loss=0.6304917650279254(Epoch= 52)\n",
      "CV loss=0.6304917650279254(Epoch= 52)\n",
      "train loss=0.617245827108736(Epoch= 52)\n",
      "train loss=0.617245827108736(Epoch= 52)\n",
      "CV loss=0.6202668929243025(Epoch= 53)\n",
      "CV loss=0.6202668929243025(Epoch= 53)\n",
      "train loss=0.6078504946768922(Epoch= 53)\n",
      "train loss=0.6078504946768922(Epoch= 53)\n",
      "CV loss=0.6128686292938215(Epoch= 54)\n",
      "CV loss=0.6128686292938215(Epoch= 54)\n",
      "train loss=0.6007847407756641(Epoch= 54)\n",
      "train loss=0.6007847407756641(Epoch= 54)\n",
      "CV loss=0.6062380813375641(Epoch= 55)\n",
      "CV loss=0.6062380813375641(Epoch= 55)\n",
      "train loss=0.5930456623160995(Epoch= 55)\n",
      "train loss=0.5930456623160995(Epoch= 55)\n",
      "CV loss=0.598715986402508(Epoch= 56)\n",
      "CV loss=0.598715986402508(Epoch= 56)\n",
      "train loss=0.5861830307465197(Epoch= 56)\n",
      "train loss=0.5861830307465197(Epoch= 56)\n",
      "CV loss=0.5940950220605878(Epoch= 57)\n",
      "CV loss=0.5940950220605878(Epoch= 57)\n",
      "train loss=0.5808762526323759(Epoch= 57)\n",
      "train loss=0.5808762526323759(Epoch= 57)\n",
      "CV loss=0.5912205135933501(Epoch= 58)\n",
      "CV loss=0.5912205135933501(Epoch= 58)\n",
      "train loss=0.5788923673062422(Epoch= 58)\n",
      "train loss=0.5788923673062422(Epoch= 58)\n",
      "CV loss=0.5818942763727157(Epoch= 59)\n",
      "CV loss=0.5818942763727157(Epoch= 59)\n",
      "train loss=0.5692610628847239(Epoch= 59)\n",
      "train loss=0.5692610628847239(Epoch= 59)\n",
      "CV loss=0.5772242941299375(Epoch= 60)\n",
      "CV loss=0.5772242941299375(Epoch= 60)\n",
      "train loss=0.564261225711055(Epoch= 60)\n",
      "train loss=0.564261225711055(Epoch= 60)\n",
      "CV loss=0.5731805857402216(Epoch= 61)\n",
      "CV loss=0.5731805857402216(Epoch= 61)\n",
      "train loss=0.560278504410413(Epoch= 61)\n",
      "train loss=0.560278504410413(Epoch= 61)\n",
      "CV loss=0.5677766884994631(Epoch= 62)\n",
      "CV loss=0.5677766884994631(Epoch= 62)\n",
      "train loss=0.5554271696836268(Epoch= 62)\n",
      "train loss=0.5554271696836268(Epoch= 62)\n",
      "CV loss=0.564907643866811(Epoch= 63)\n",
      "CV loss=0.564907643866811(Epoch= 63)\n",
      "train loss=0.5516367752897792(Epoch= 63)\n",
      "train loss=0.5516367752897792(Epoch= 63)\n",
      "CV loss=0.5649356772587657(Epoch= 64)\n",
      "CV loss=0.5649356772587657(Epoch= 64)\n",
      "train loss=0.551081933437313(Epoch= 64)\n",
      "train loss=0.551081933437313(Epoch= 64)\n",
      "CV loss=0.5580980555490871(Epoch= 65)\n",
      "CV loss=0.5580980555490871(Epoch= 65)\n",
      "train loss=0.5448152501521335(Epoch= 65)\n",
      "train loss=0.5448152501521335(Epoch= 65)\n",
      "CV loss=0.5542962100692737(Epoch= 66)\n",
      "CV loss=0.5542962100692737(Epoch= 66)\n",
      "train loss=0.5417048794974993(Epoch= 66)\n",
      "train loss=0.5417048794974993(Epoch= 66)\n",
      "CV loss=0.5516538231465581(Epoch= 67)\n",
      "CV loss=0.5516538231465581(Epoch= 67)\n",
      "train loss=0.5385012920308552(Epoch= 67)\n",
      "train loss=0.5385012920308552(Epoch= 67)\n",
      "CV loss=0.5497629260336525(Epoch= 68)\n",
      "CV loss=0.5497629260336525(Epoch= 68)\n",
      "train loss=0.5360633310902423(Epoch= 68)\n",
      "train loss=0.5360633310902423(Epoch= 68)\n",
      "CV loss=0.5464488769969478(Epoch= 69)\n",
      "CV loss=0.5464488769969478(Epoch= 69)\n",
      "train loss=0.5339073504947869(Epoch= 69)\n",
      "train loss=0.5339073504947869(Epoch= 69)\n",
      "CV loss=0.5444555400105359(Epoch= 70)\n",
      "CV loss=0.5444555400105359(Epoch= 70)\n",
      "train loss=0.5310599096226295(Epoch= 70)\n",
      "train loss=0.5310599096226295(Epoch= 70)\n",
      "CV loss=0.5422009906422741(Epoch= 71)\n",
      "CV loss=0.5422009906422741(Epoch= 71)\n",
      "train loss=0.5297954882592817(Epoch= 71)\n",
      "train loss=0.5297954882592817(Epoch= 71)\n",
      "CV loss=0.540163461077392(Epoch= 72)\n",
      "CV loss=0.540163461077392(Epoch= 72)\n",
      "train loss=0.5275434183267346(Epoch= 72)\n",
      "train loss=0.5275434183267346(Epoch= 72)\n",
      "CV loss=0.5378221256559819(Epoch= 73)\n",
      "CV loss=0.5378221256559819(Epoch= 73)\n",
      "train loss=0.5245228094721174(Epoch= 73)\n",
      "train loss=0.5245228094721174(Epoch= 73)\n",
      "CV loss=0.5377316673700083(Epoch= 74)\n",
      "CV loss=0.5377316673700083(Epoch= 74)\n",
      "train loss=0.5239120078371282(Epoch= 74)\n",
      "train loss=0.5239120078371282(Epoch= 74)\n",
      "CV loss=0.536591586581163(Epoch= 75)\n",
      "CV loss=0.536591586581163(Epoch= 75)\n",
      "train loss=0.5225993282498596(Epoch= 75)\n",
      "train loss=0.5225993282498596(Epoch= 75)\n",
      "CV loss=0.534073923748958(Epoch= 76)\n",
      "CV loss=0.534073923748958(Epoch= 76)\n",
      "train loss=0.5201223420047016(Epoch= 76)\n",
      "train loss=0.5201223420047016(Epoch= 76)\n",
      "CV loss=0.5324115109078503(Epoch= 77)\n",
      "CV loss=0.5324115109078503(Epoch= 77)\n",
      "train loss=0.5187796268974103(Epoch= 77)\n",
      "train loss=0.5187796268974103(Epoch= 77)\n",
      "CV loss=0.5309944602050741(Epoch= 78)\n",
      "CV loss=0.5309944602050741(Epoch= 78)\n",
      "train loss=0.5173308731260806(Epoch= 78)\n",
      "train loss=0.5173308731260806(Epoch= 78)\n",
      "CV loss=0.5292473060874241(Epoch= 79)\n",
      "CV loss=0.5292473060874241(Epoch= 79)\n",
      "train loss=0.5160845912500944(Epoch= 79)\n",
      "train loss=0.5160845912500944(Epoch= 79)\n",
      "CV loss=0.5293482924534331(Epoch= 80)\n",
      "CV loss=0.5293482924534331(Epoch= 80)\n",
      "train loss=0.515127467690404(Epoch= 80)\n",
      "train loss=0.515127467690404(Epoch= 80)\n",
      "CV loss=0.5274253740520096(Epoch= 81)\n",
      "CV loss=0.5274253740520096(Epoch= 81)\n",
      "train loss=0.5136778499779567(Epoch= 81)\n",
      "train loss=0.5136778499779567(Epoch= 81)\n",
      "CV loss=0.5263371321708638(Epoch= 82)\n",
      "CV loss=0.5263371321708638(Epoch= 82)\n",
      "train loss=0.5125179787003039(Epoch= 82)\n",
      "train loss=0.5125179787003039(Epoch= 82)\n",
      "CV loss=0.524851954330346(Epoch= 83)\n",
      "CV loss=0.524851954330346(Epoch= 83)\n",
      "train loss=0.5117337948276196(Epoch= 83)\n",
      "train loss=0.5117337948276196(Epoch= 83)\n",
      "CV loss=0.5250037823110294(Epoch= 84)\n",
      "CV loss=0.5250037823110294(Epoch= 84)\n",
      "train loss=0.5109942293660421(Epoch= 84)\n",
      "train loss=0.5109942293660421(Epoch= 84)\n",
      "CV loss=0.5227418270771509(Epoch= 85)\n",
      "CV loss=0.5227418270771509(Epoch= 85)\n",
      "train loss=0.5100960584830165(Epoch= 85)\n",
      "train loss=0.5100960584830165(Epoch= 85)\n",
      "CV loss=0.5221668001142067(Epoch= 86)\n",
      "CV loss=0.5221668001142067(Epoch= 86)\n",
      "train loss=0.5085776007637008(Epoch= 86)\n",
      "train loss=0.5085776007637008(Epoch= 86)\n",
      "CV loss=0.5217998932270351(Epoch= 87)\n",
      "CV loss=0.5217998932270351(Epoch= 87)\n",
      "train loss=0.5088905491385399(Epoch= 87)\n",
      "train loss=0.5088905491385399(Epoch= 87)\n",
      "CV loss=0.5204314156896115(Epoch= 88)\n",
      "CV loss=0.5204314156896115(Epoch= 88)\n",
      "train loss=0.506891467047891(Epoch= 88)\n",
      "train loss=0.506891467047891(Epoch= 88)\n",
      "CV loss=0.5202214115305608(Epoch= 89)\n",
      "CV loss=0.5202214115305608(Epoch= 89)\n",
      "train loss=0.5070350220345007(Epoch= 89)\n",
      "train loss=0.5070350220345007(Epoch= 89)\n",
      "CV loss=0.5188860256120625(Epoch= 90)\n",
      "CV loss=0.5188860256120625(Epoch= 90)\n",
      "train loss=0.5052261477772522(Epoch= 90)\n",
      "train loss=0.5052261477772522(Epoch= 90)\n",
      "CV loss=0.5193934801576419(Epoch= 91)\n",
      "CV loss=0.5193934801576419(Epoch= 91)\n",
      "train loss=0.5049471574636435(Epoch= 91)\n",
      "train loss=0.5049471574636435(Epoch= 91)\n",
      "CV loss=0.517547263486904(Epoch= 92)\n",
      "CV loss=0.517547263486904(Epoch= 92)\n",
      "train loss=0.503966525951985(Epoch= 92)\n",
      "train loss=0.503966525951985(Epoch= 92)\n",
      "CV loss=0.5175734958053166(Epoch= 93)\n",
      "CV loss=0.5175734958053166(Epoch= 93)\n",
      "train loss=0.5034773236734947(Epoch= 93)\n",
      "train loss=0.5034773236734947(Epoch= 93)\n",
      "CV loss=0.5161694069238764(Epoch= 94)\n",
      "CV loss=0.5161694069238764(Epoch= 94)\n",
      "train loss=0.5023422739407415(Epoch= 94)\n",
      "train loss=0.5023422739407415(Epoch= 94)\n",
      "CV loss=0.5177808547849763(Epoch= 95)\n",
      "CV loss=0.5177808547849763(Epoch= 95)\n",
      "train loss=0.5036871564115477(Epoch= 95)\n",
      "train loss=0.5036871564115477(Epoch= 95)\n",
      "CV loss=0.514791756430656(Epoch= 96)\n",
      "CV loss=0.514791756430656(Epoch= 96)\n",
      "train loss=0.5012605987901532(Epoch= 96)\n",
      "train loss=0.5012605987901532(Epoch= 96)\n",
      "CV loss=0.5152013931245294(Epoch= 97)\n",
      "CV loss=0.5152013931245294(Epoch= 97)\n",
      "train loss=0.5008832370353639(Epoch= 97)\n",
      "train loss=0.5008832370353639(Epoch= 97)\n",
      "CV loss=0.5140111080929097(Epoch= 98)\n",
      "CV loss=0.5140111080929097(Epoch= 98)\n",
      "train loss=0.5010152051380321(Epoch= 98)\n",
      "train loss=0.5010152051380321(Epoch= 98)\n",
      "CV loss=0.514382588300101(Epoch= 99)\n",
      "CV loss=0.514382588300101(Epoch= 99)\n",
      "train loss=0.5003373815500687(Epoch= 99)\n",
      "train loss=0.5003373815500687(Epoch= 99)\n",
      "CV loss=0.513447831702613(Epoch= 100)\n",
      "CV loss=0.513447831702613(Epoch= 100)\n",
      "train loss=0.49987330783076733(Epoch= 100)\n",
      "train loss=0.49987330783076733(Epoch= 100)\n",
      "CV loss=0.5138625929780547(Epoch= 101)\n",
      "CV loss=0.5138625929780547(Epoch= 101)\n",
      "train loss=0.4992306230668707(Epoch= 101)\n",
      "train loss=0.4992306230668707(Epoch= 101)\n",
      "CV loss=0.5118852778933225(Epoch= 102)\n",
      "CV loss=0.5118852778933225(Epoch= 102)\n",
      "train loss=0.49860997194505297(Epoch= 102)\n",
      "train loss=0.49860997194505297(Epoch= 102)\n",
      "CV loss=0.5123635681565732(Epoch= 103)\n",
      "CV loss=0.5123635681565732(Epoch= 103)\n",
      "train loss=0.4983586878885852(Epoch= 103)\n",
      "train loss=0.4983586878885852(Epoch= 103)\n",
      "CV loss=0.5120092429034427(Epoch= 104)\n",
      "CV loss=0.5120092429034427(Epoch= 104)\n",
      "train loss=0.4978565318676838(Epoch= 104)\n",
      "train loss=0.4978565318676838(Epoch= 104)\n",
      "CV loss=0.511429702768051(Epoch= 105)\n",
      "CV loss=0.511429702768051(Epoch= 105)\n",
      "train loss=0.4975241937867834(Epoch= 105)\n",
      "train loss=0.4975241937867834(Epoch= 105)\n",
      "CV loss=0.5123447016780974(Epoch= 106)\n",
      "CV loss=0.5123447016780974(Epoch= 106)\n",
      "train loss=0.4972754588082861(Epoch= 106)\n",
      "train loss=0.4972754588082861(Epoch= 106)\n",
      "CV loss=0.5106615822325087(Epoch= 107)\n",
      "CV loss=0.5106615822325087(Epoch= 107)\n",
      "train loss=0.49663559397804724(Epoch= 107)\n",
      "train loss=0.49663559397804724(Epoch= 107)\n",
      "CV loss=0.5105737860845327(Epoch= 108)\n",
      "CV loss=0.5105737860845327(Epoch= 108)\n",
      "train loss=0.49633314341957246(Epoch= 108)\n",
      "train loss=0.49633314341957246(Epoch= 108)\n",
      "CV loss=0.5107392902169191(Epoch= 109)\n",
      "CV loss=0.5107392902169191(Epoch= 109)\n",
      "train loss=0.4969470840647837(Epoch= 109)\n",
      "train loss=0.4969470840647837(Epoch= 109)\n",
      "CV loss=0.5097346058274227(Epoch= 110)\n",
      "CV loss=0.5097346058274227(Epoch= 110)\n",
      "train loss=0.49581062913653756(Epoch= 110)\n",
      "train loss=0.49581062913653756(Epoch= 110)\n",
      "CV loss=0.5089915255131512(Epoch= 111)\n",
      "CV loss=0.5089915255131512(Epoch= 111)\n",
      "train loss=0.4949188857427199(Epoch= 111)\n",
      "train loss=0.4949188857427199(Epoch= 111)\n",
      "CV loss=0.5094433289131206(Epoch= 112)\n",
      "CV loss=0.5094433289131206(Epoch= 112)\n",
      "train loss=0.49452347271578967(Epoch= 112)\n",
      "train loss=0.49452347271578967(Epoch= 112)\n",
      "CV loss=0.5094417193985383(Epoch= 113)\n",
      "CV loss=0.5094417193985383(Epoch= 113)\n",
      "train loss=0.4941842849005824(Epoch= 113)\n",
      "train loss=0.4941842849005824(Epoch= 113)\n",
      "CV loss=0.5085875470998408(Epoch= 114)\n",
      "CV loss=0.5085875470998408(Epoch= 114)\n",
      "train loss=0.4945063473343147(Epoch= 114)\n",
      "train loss=0.4945063473343147(Epoch= 114)\n",
      "CV loss=0.5074720008635158(Epoch= 115)\n",
      "CV loss=0.5074720008635158(Epoch= 115)\n",
      "train loss=0.4936078909331115(Epoch= 115)\n",
      "train loss=0.4936078909331115(Epoch= 115)\n",
      "CV loss=0.5072257668998844(Epoch= 116)\n",
      "CV loss=0.5072257668998844(Epoch= 116)\n",
      "train loss=0.4937335097065366(Epoch= 116)\n",
      "train loss=0.4937335097065366(Epoch= 116)\n",
      "CV loss=0.5069015141811698(Epoch= 117)\n",
      "CV loss=0.5069015141811698(Epoch= 117)\n",
      "train loss=0.4925872077843607(Epoch= 117)\n",
      "train loss=0.4925872077843607(Epoch= 117)\n",
      "CV loss=0.5063319847886467(Epoch= 118)\n",
      "CV loss=0.5063319847886467(Epoch= 118)\n",
      "train loss=0.49208582909209836(Epoch= 118)\n",
      "train loss=0.49208582909209836(Epoch= 118)\n",
      "CV loss=0.5063289727328861(Epoch= 119)\n",
      "CV loss=0.5063289727328861(Epoch= 119)\n",
      "train loss=0.4925106720097577(Epoch= 119)\n",
      "train loss=0.4925106720097577(Epoch= 119)\n",
      "CV loss=0.5086075674943032(Epoch= 120)\n",
      "CV loss=0.5086075674943032(Epoch= 120)\n",
      "train loss=0.4932418082204939(Epoch= 120)\n",
      "train loss=0.4932418082204939(Epoch= 120)\n",
      "CV loss=0.505356618691176(Epoch= 121)\n",
      "CV loss=0.505356618691176(Epoch= 121)\n",
      "train loss=0.49142025350697877(Epoch= 121)\n",
      "train loss=0.49142025350697877(Epoch= 121)\n",
      "CV loss=0.5056917163599789(Epoch= 122)\n",
      "CV loss=0.5056917163599789(Epoch= 122)\n",
      "train loss=0.49127601701986234(Epoch= 122)\n",
      "train loss=0.49127601701986234(Epoch= 122)\n",
      "CV loss=0.5057065979369209(Epoch= 123)\n",
      "CV loss=0.5057065979369209(Epoch= 123)\n",
      "train loss=0.49092115669164205(Epoch= 123)\n",
      "train loss=0.49092115669164205(Epoch= 123)\n",
      "CV loss=0.5045516356076721(Epoch= 124)\n",
      "CV loss=0.5045516356076721(Epoch= 124)\n",
      "train loss=0.4903754060395287(Epoch= 124)\n",
      "train loss=0.4903754060395287(Epoch= 124)\n",
      "CV loss=0.5045554927293517(Epoch= 125)\n",
      "CV loss=0.5045554927293517(Epoch= 125)\n",
      "train loss=0.48987410483226573(Epoch= 125)\n",
      "train loss=0.48987410483226573(Epoch= 125)\n",
      "CV loss=0.5047678133199482(Epoch= 126)\n",
      "CV loss=0.5047678133199482(Epoch= 126)\n",
      "train loss=0.4900140863905315(Epoch= 126)\n",
      "train loss=0.4900140863905315(Epoch= 126)\n",
      "CV loss=0.5038975124641947(Epoch= 127)\n",
      "CV loss=0.5038975124641947(Epoch= 127)\n",
      "train loss=0.48948421699068945(Epoch= 127)\n",
      "train loss=0.48948421699068945(Epoch= 127)\n",
      "CV loss=0.5045102191792284(Epoch= 128)\n",
      "CV loss=0.5045102191792284(Epoch= 128)\n",
      "train loss=0.48970043941695557(Epoch= 128)\n",
      "train loss=0.48970043941695557(Epoch= 128)\n",
      "CV loss=0.5029284041952473(Epoch= 129)\n",
      "CV loss=0.5029284041952473(Epoch= 129)\n",
      "train loss=0.4891767643951071(Epoch= 129)\n",
      "train loss=0.4891767643951071(Epoch= 129)\n",
      "CV loss=0.5034527862832402(Epoch= 130)\n",
      "CV loss=0.5034527862832402(Epoch= 130)\n",
      "train loss=0.4886033298174052(Epoch= 130)\n",
      "train loss=0.4886033298174052(Epoch= 130)\n",
      "CV loss=0.5033562455011082(Epoch= 131)\n",
      "CV loss=0.5033562455011082(Epoch= 131)\n",
      "train loss=0.4887358883077021(Epoch= 131)\n",
      "train loss=0.4887358883077021(Epoch= 131)\n",
      "CV loss=0.5024656338460851(Epoch= 132)\n",
      "CV loss=0.5024656338460851(Epoch= 132)\n",
      "train loss=0.4880085855202289(Epoch= 132)\n",
      "train loss=0.4880085855202289(Epoch= 132)\n",
      "CV loss=0.5028414778167518(Epoch= 133)\n",
      "CV loss=0.5028414778167518(Epoch= 133)\n",
      "train loss=0.4877592203060257(Epoch= 133)\n",
      "train loss=0.4877592203060257(Epoch= 133)\n",
      "CV loss=0.5020494734825083(Epoch= 134)\n",
      "CV loss=0.5020494734825083(Epoch= 134)\n",
      "train loss=0.487051161627038(Epoch= 134)\n",
      "train loss=0.487051161627038(Epoch= 134)\n",
      "CV loss=0.5014410889816501(Epoch= 135)\n",
      "CV loss=0.5014410889816501(Epoch= 135)\n",
      "train loss=0.48752956590629404(Epoch= 135)\n",
      "train loss=0.48752956590629404(Epoch= 135)\n",
      "CV loss=0.5015700310684469(Epoch= 136)\n",
      "CV loss=0.5015700310684469(Epoch= 136)\n",
      "train loss=0.48711979640686387(Epoch= 136)\n",
      "train loss=0.48711979640686387(Epoch= 136)\n",
      "CV loss=0.5007562006756321(Epoch= 137)\n",
      "CV loss=0.5007562006756321(Epoch= 137)\n",
      "train loss=0.48642047026021784(Epoch= 137)\n",
      "train loss=0.48642047026021784(Epoch= 137)\n",
      "CV loss=0.5013355531657321(Epoch= 138)\n",
      "CV loss=0.5013355531657321(Epoch= 138)\n",
      "train loss=0.48659356160435363(Epoch= 138)\n",
      "train loss=0.48659356160435363(Epoch= 138)\n",
      "CV loss=0.5008352458732254(Epoch= 139)\n",
      "CV loss=0.5008352458732254(Epoch= 139)\n",
      "train loss=0.4859257223837555(Epoch= 139)\n",
      "train loss=0.4859257223837555(Epoch= 139)\n",
      "CV loss=0.5005504669543772(Epoch= 140)\n",
      "CV loss=0.5005504669543772(Epoch= 140)\n",
      "train loss=0.4858720925144291(Epoch= 140)\n",
      "train loss=0.4858720925144291(Epoch= 140)\n",
      "CV loss=0.5010301258947006(Epoch= 141)\n",
      "CV loss=0.5010301258947006(Epoch= 141)\n",
      "train loss=0.4868197831845015(Epoch= 141)\n",
      "train loss=0.4868197831845015(Epoch= 141)\n",
      "CV loss=0.49980318467424834(Epoch= 142)\n",
      "CV loss=0.49980318467424834(Epoch= 142)\n",
      "train loss=0.48520611785120465(Epoch= 142)\n",
      "train loss=0.48520611785120465(Epoch= 142)\n",
      "CV loss=0.49975036332128797(Epoch= 143)\n",
      "CV loss=0.49975036332128797(Epoch= 143)\n",
      "train loss=0.48519392953961216(Epoch= 143)\n",
      "train loss=0.48519392953961216(Epoch= 143)\n",
      "CV loss=0.4993905176368788(Epoch= 144)\n",
      "CV loss=0.4993905176368788(Epoch= 144)\n",
      "train loss=0.48451918526393417(Epoch= 144)\n",
      "train loss=0.48451918526393417(Epoch= 144)\n",
      "CV loss=0.4991514973766148(Epoch= 145)\n",
      "CV loss=0.4991514973766148(Epoch= 145)\n",
      "train loss=0.48484417177792144(Epoch= 145)\n",
      "train loss=0.48484417177792144(Epoch= 145)\n",
      "CV loss=0.4981550664563489(Epoch= 146)\n",
      "CV loss=0.4981550664563489(Epoch= 146)\n",
      "train loss=0.48414758210822406(Epoch= 146)\n",
      "train loss=0.48414758210822406(Epoch= 146)\n",
      "CV loss=0.5000737408761027(Epoch= 147)\n",
      "CV loss=0.5000737408761027(Epoch= 147)\n",
      "train loss=0.4854393153511215(Epoch= 147)\n",
      "train loss=0.4854393153511215(Epoch= 147)\n",
      "CV loss=0.4986037919060527(Epoch= 148)\n",
      "CV loss=0.4986037919060527(Epoch= 148)\n",
      "train loss=0.4838139626456291(Epoch= 148)\n",
      "train loss=0.4838139626456291(Epoch= 148)\n",
      "CV loss=0.4981136584956346(Epoch= 149)\n",
      "CV loss=0.4981136584956346(Epoch= 149)\n",
      "train loss=0.48386267072541045(Epoch= 149)\n",
      "train loss=0.48386267072541045(Epoch= 149)\n",
      "CV loss=0.49792510802436474(Epoch= 150)\n",
      "CV loss=0.49792510802436474(Epoch= 150)\n",
      "train loss=0.48358653477648705(Epoch= 150)\n",
      "train loss=0.48358653477648705(Epoch= 150)\n",
      "CV loss=0.4994223740253406(Epoch= 151)\n",
      "CV loss=0.4994223740253406(Epoch= 151)\n",
      "train loss=0.48448519629605513(Epoch= 151)\n",
      "train loss=0.48448519629605513(Epoch= 151)\n",
      "CV loss=0.49834268294956835(Epoch= 152)\n",
      "CV loss=0.49834268294956835(Epoch= 152)\n",
      "train loss=0.48395986397397006(Epoch= 152)\n",
      "train loss=0.48395986397397006(Epoch= 152)\n",
      "CV loss=0.49780064530382867(Epoch= 153)\n",
      "CV loss=0.49780064530382867(Epoch= 153)\n",
      "train loss=0.48285901382714264(Epoch= 153)\n",
      "train loss=0.48285901382714264(Epoch= 153)\n",
      "CV loss=0.4975543003321807(Epoch= 154)\n",
      "CV loss=0.4975543003321807(Epoch= 154)\n",
      "train loss=0.4825211020740126(Epoch= 154)\n",
      "train loss=0.4825211020740126(Epoch= 154)\n",
      "CV loss=0.49776844907781903(Epoch= 155)\n",
      "CV loss=0.49776844907781903(Epoch= 155)\n",
      "train loss=0.48322139938153785(Epoch= 155)\n",
      "train loss=0.48322139938153785(Epoch= 155)\n",
      "CV loss=0.49703453756110266(Epoch= 156)\n",
      "CV loss=0.49703453756110266(Epoch= 156)\n",
      "train loss=0.4822362850053009(Epoch= 156)\n",
      "train loss=0.4822362850053009(Epoch= 156)\n",
      "CV loss=0.4969779114427735(Epoch= 157)\n",
      "CV loss=0.4969779114427735(Epoch= 157)\n",
      "train loss=0.4819476103997412(Epoch= 157)\n",
      "train loss=0.4819476103997412(Epoch= 157)\n",
      "CV loss=0.4971331359101147(Epoch= 158)\n",
      "CV loss=0.4971331359101147(Epoch= 158)\n",
      "train loss=0.4825807741136642(Epoch= 158)\n",
      "train loss=0.4825807741136642(Epoch= 158)\n",
      "CV loss=0.49668811003387214(Epoch= 159)\n",
      "CV loss=0.49668811003387214(Epoch= 159)\n",
      "train loss=0.48179992127511506(Epoch= 159)\n",
      "train loss=0.48179992127511506(Epoch= 159)\n",
      "CV loss=0.4964147073415939(Epoch= 160)\n",
      "CV loss=0.4964147073415939(Epoch= 160)\n",
      "train loss=0.48150110874782504(Epoch= 160)\n",
      "train loss=0.48150110874782504(Epoch= 160)\n",
      "CV loss=0.4975469033601917(Epoch= 161)\n",
      "CV loss=0.4975469033601917(Epoch= 161)\n",
      "train loss=0.4826616343256324(Epoch= 161)\n",
      "train loss=0.4826616343256324(Epoch= 161)\n",
      "CV loss=0.4958123382121518(Epoch= 162)\n",
      "CV loss=0.4958123382121518(Epoch= 162)\n",
      "train loss=0.48154337360518495(Epoch= 162)\n",
      "train loss=0.48154337360518495(Epoch= 162)\n",
      "CV loss=0.49644786401930807(Epoch= 163)\n",
      "CV loss=0.49644786401930807(Epoch= 163)\n",
      "train loss=0.4809641331139456(Epoch= 163)\n",
      "train loss=0.4809641331139456(Epoch= 163)\n",
      "CV loss=0.49887201803147035(Epoch= 164)\n",
      "CV loss=0.49887201803147035(Epoch= 164)\n",
      "train loss=0.4825770340405817(Epoch= 164)\n",
      "train loss=0.4825770340405817(Epoch= 164)\n",
      "CV loss=0.4951075951239126(Epoch= 165)\n",
      "CV loss=0.4951075951239126(Epoch= 165)\n",
      "train loss=0.4806174701722528(Epoch= 165)\n",
      "train loss=0.4806174701722528(Epoch= 165)\n",
      "CV loss=0.49501405993803993(Epoch= 166)\n",
      "CV loss=0.49501405993803993(Epoch= 166)\n",
      "train loss=0.48051089958966(Epoch= 166)\n",
      "train loss=0.48051089958966(Epoch= 166)\n",
      "CV loss=0.4958934694015877(Epoch= 167)\n",
      "CV loss=0.4958934694015877(Epoch= 167)\n",
      "train loss=0.48109727706990657(Epoch= 167)\n",
      "train loss=0.48109727706990657(Epoch= 167)\n",
      "CV loss=0.49496974256865034(Epoch= 168)\n",
      "CV loss=0.49496974256865034(Epoch= 168)\n",
      "train loss=0.4804640714538078(Epoch= 168)\n",
      "train loss=0.4804640714538078(Epoch= 168)\n",
      "CV loss=0.4948648472200437(Epoch= 169)\n",
      "CV loss=0.4948648472200437(Epoch= 169)\n",
      "train loss=0.4800603261682127(Epoch= 169)\n",
      "train loss=0.4800603261682127(Epoch= 169)\n",
      "CV loss=0.49456002391129616(Epoch= 170)\n",
      "CV loss=0.49456002391129616(Epoch= 170)\n",
      "train loss=0.48004262450951474(Epoch= 170)\n",
      "train loss=0.48004262450951474(Epoch= 170)\n",
      "CV loss=0.49573799870776863(Epoch= 171)\n",
      "CV loss=0.49573799870776863(Epoch= 171)\n",
      "train loss=0.48086371956158924(Epoch= 171)\n",
      "train loss=0.48086371956158924(Epoch= 171)\n",
      "CV loss=0.4943515218082378(Epoch= 172)\n",
      "CV loss=0.4943515218082378(Epoch= 172)\n",
      "train loss=0.47990298319148417(Epoch= 172)\n",
      "train loss=0.47990298319148417(Epoch= 172)\n",
      "CV loss=0.49430580680120095(Epoch= 173)\n",
      "CV loss=0.49430580680120095(Epoch= 173)\n",
      "train loss=0.47975622925894285(Epoch= 173)\n",
      "train loss=0.47975622925894285(Epoch= 173)\n",
      "CV loss=0.494195653667307(Epoch= 174)\n",
      "CV loss=0.494195653667307(Epoch= 174)\n",
      "train loss=0.47969556311782363(Epoch= 174)\n",
      "train loss=0.47969556311782363(Epoch= 174)\n",
      "CV loss=0.4945136240495683(Epoch= 175)\n",
      "CV loss=0.4945136240495683(Epoch= 175)\n",
      "train loss=0.47966954888776436(Epoch= 175)\n",
      "train loss=0.47966954888776436(Epoch= 175)\n",
      "CV loss=0.49426957801645455(Epoch= 176)\n",
      "CV loss=0.49426957801645455(Epoch= 176)\n",
      "train loss=0.479074619170903(Epoch= 176)\n",
      "train loss=0.479074619170903(Epoch= 176)\n",
      "CV loss=0.4947848254951136(Epoch= 177)\n",
      "CV loss=0.4947848254951136(Epoch= 177)\n",
      "train loss=0.4796674031103455(Epoch= 177)\n",
      "train loss=0.4796674031103455(Epoch= 177)\n",
      "CV loss=0.4944864734952982(Epoch= 178)\n",
      "CV loss=0.4944864734952982(Epoch= 178)\n",
      "train loss=0.4795599869075069(Epoch= 178)\n",
      "train loss=0.4795599869075069(Epoch= 178)\n",
      "CV loss=0.4938132886484689(Epoch= 179)\n",
      "CV loss=0.4938132886484689(Epoch= 179)\n",
      "train loss=0.47918327536416616(Epoch= 179)\n",
      "train loss=0.47918327536416616(Epoch= 179)\n",
      "CV loss=0.4933172944843526(Epoch= 180)\n",
      "CV loss=0.4933172944843526(Epoch= 180)\n",
      "train loss=0.4792090335501289(Epoch= 180)\n",
      "train loss=0.4792090335501289(Epoch= 180)\n",
      "CV loss=0.49373199429085146(Epoch= 181)\n",
      "CV loss=0.49373199429085146(Epoch= 181)\n",
      "train loss=0.47907899486280603(Epoch= 181)\n",
      "train loss=0.47907899486280603(Epoch= 181)\n",
      "CV loss=0.49460510129197766(Epoch= 182)\n",
      "CV loss=0.49460510129197766(Epoch= 182)\n",
      "train loss=0.47991071543170166(Epoch= 182)\n",
      "train loss=0.47991071543170166(Epoch= 182)\n",
      "CV loss=0.4942977049135279(Epoch= 183)\n",
      "CV loss=0.4942977049135279(Epoch= 183)\n",
      "train loss=0.4788524721280999(Epoch= 183)\n",
      "train loss=0.4788524721280999(Epoch= 183)\n",
      "CV loss=0.49386549562633303(Epoch= 184)\n",
      "CV loss=0.49386549562633303(Epoch= 184)\n",
      "train loss=0.4788433544147964(Epoch= 184)\n",
      "train loss=0.4788433544147964(Epoch= 184)\n",
      "CV loss=0.495742953250867(Epoch= 185)\n",
      "CV loss=0.495742953250867(Epoch= 185)\n",
      "train loss=0.4801005695506937(Epoch= 185)\n",
      "train loss=0.4801005695506937(Epoch= 185)\n",
      "CV loss=0.4945527782660142(Epoch= 186)\n",
      "CV loss=0.4945527782660142(Epoch= 186)\n",
      "train loss=0.47902799395685025(Epoch= 186)\n",
      "train loss=0.47902799395685025(Epoch= 186)\n",
      "CV loss=0.4941320333561042(Epoch= 187)\n",
      "CV loss=0.4941320333561042(Epoch= 187)\n",
      "train loss=0.4787546518068547(Epoch= 187)\n",
      "train loss=0.4787546518068547(Epoch= 187)\n",
      "CV loss=0.4926381735968972(Epoch= 188)\n",
      "CV loss=0.4926381735968972(Epoch= 188)\n",
      "train loss=0.47852418232915966(Epoch= 188)\n",
      "train loss=0.47852418232915966(Epoch= 188)\n",
      "CV loss=0.4927477872098659(Epoch= 189)\n",
      "CV loss=0.4927477872098659(Epoch= 189)\n",
      "train loss=0.4781224008770075(Epoch= 189)\n",
      "train loss=0.4781224008770075(Epoch= 189)\n",
      "CV loss=0.49269199032115635(Epoch= 190)\n",
      "CV loss=0.49269199032115635(Epoch= 190)\n",
      "train loss=0.47765933477764844(Epoch= 190)\n",
      "train loss=0.47765933477764844(Epoch= 190)\n",
      "CV loss=0.4929265749140217(Epoch= 191)\n",
      "CV loss=0.4929265749140217(Epoch= 191)\n",
      "train loss=0.47822485734013853(Epoch= 191)\n",
      "train loss=0.47822485734013853(Epoch= 191)\n",
      "CV loss=0.49447797952827394(Epoch= 192)\n",
      "CV loss=0.49447797952827394(Epoch= 192)\n",
      "train loss=0.4789834818820152(Epoch= 192)\n",
      "train loss=0.4789834818820152(Epoch= 192)\n",
      "CV loss=0.4932653451877091(Epoch= 193)\n",
      "CV loss=0.4932653451877091(Epoch= 193)\n",
      "train loss=0.4781637410108152(Epoch= 193)\n",
      "train loss=0.4781637410108152(Epoch= 193)\n",
      "CV loss=0.49308956319314245(Epoch= 194)\n",
      "CV loss=0.49308956319314245(Epoch= 194)\n",
      "train loss=0.47840038083900294(Epoch= 194)\n",
      "train loss=0.47840038083900294(Epoch= 194)\n",
      "CV loss=0.4944017067341967(Epoch= 195)\n",
      "CV loss=0.4944017067341967(Epoch= 195)\n",
      "train loss=0.4786860422863424(Epoch= 195)\n",
      "train loss=0.4786860422863424(Epoch= 195)\n",
      "CV loss=0.4929462898983316(Epoch= 196)\n",
      "CV loss=0.4929462898983316(Epoch= 196)\n",
      "train loss=0.4780223736346815(Epoch= 196)\n",
      "train loss=0.4780223736346815(Epoch= 196)\n",
      "CV loss=0.49199675746811533(Epoch= 197)\n",
      "CV loss=0.49199675746811533(Epoch= 197)\n",
      "train loss=0.47733358589624064(Epoch= 197)\n",
      "train loss=0.47733358589624064(Epoch= 197)\n",
      "CV loss=0.4927892324612838(Epoch= 198)\n",
      "CV loss=0.4927892324612838(Epoch= 198)\n",
      "train loss=0.47764453637368426(Epoch= 198)\n",
      "train loss=0.47764453637368426(Epoch= 198)\n",
      "CV loss=0.49207094150381725(Epoch= 199)\n",
      "CV loss=0.49207094150381725(Epoch= 199)\n",
      "train loss=0.477017324988253(Epoch= 199)\n",
      "train loss=0.477017324988253(Epoch= 199)\n",
      "CV loss=0.49302378851206263(Epoch= 200)\n",
      "CV loss=0.49302378851206263(Epoch= 200)\n",
      "train loss=0.4774403133713371(Epoch= 200)\n",
      "train loss=0.4774403133713371(Epoch= 200)\n",
      "CV loss=0.4927768452237011(Epoch= 201)\n",
      "CV loss=0.4927768452237011(Epoch= 201)\n",
      "train loss=0.477101278668791(Epoch= 201)\n",
      "train loss=0.477101278668791(Epoch= 201)\n",
      "CV loss=0.4925604665287596(Epoch= 202)\n",
      "CV loss=0.4925604665287596(Epoch= 202)\n",
      "train loss=0.4770484689653408(Epoch= 202)\n",
      "train loss=0.4770484689653408(Epoch= 202)\n",
      "CV loss=0.4942134410617169(Epoch= 203)\n",
      "CV loss=0.4942134410617169(Epoch= 203)\n",
      "train loss=0.47812273558426904(Epoch= 203)\n",
      "train loss=0.47812273558426904(Epoch= 203)\n",
      "CV loss=0.4922221955555408(Epoch= 204)\n",
      "CV loss=0.4922221955555408(Epoch= 204)\n",
      "train loss=0.4770847271698376(Epoch= 204)\n",
      "train loss=0.4770847271698376(Epoch= 204)\n",
      "CV loss=0.49178762562828804(Epoch= 205)\n",
      "CV loss=0.49178762562828804(Epoch= 205)\n",
      "train loss=0.47710936834725715(Epoch= 205)\n",
      "train loss=0.47710936834725715(Epoch= 205)\n",
      "CV loss=0.4913002645984349(Epoch= 206)\n",
      "CV loss=0.4913002645984349(Epoch= 206)\n",
      "train loss=0.4763275459848171(Epoch= 206)\n",
      "train loss=0.4763275459848171(Epoch= 206)\n",
      "CV loss=0.49138180892783506(Epoch= 207)\n",
      "CV loss=0.49138180892783506(Epoch= 207)\n",
      "train loss=0.47713680022768373(Epoch= 207)\n",
      "train loss=0.47713680022768373(Epoch= 207)\n",
      "CV loss=0.49126347797590886(Epoch= 208)\n",
      "CV loss=0.49126347797590886(Epoch= 208)\n",
      "train loss=0.47705038593070515(Epoch= 208)\n",
      "train loss=0.47705038593070515(Epoch= 208)\n",
      "CV loss=0.4926728037238648(Epoch= 209)\n",
      "CV loss=0.4926728037238648(Epoch= 209)\n",
      "train loss=0.47666852541761184(Epoch= 209)\n",
      "train loss=0.47666852541761184(Epoch= 209)\n",
      "CV loss=0.4915805097542668(Epoch= 210)\n",
      "CV loss=0.4915805097542668(Epoch= 210)\n",
      "train loss=0.47662591537739935(Epoch= 210)\n",
      "train loss=0.47662591537739935(Epoch= 210)\n",
      "CV loss=0.49113311362061607(Epoch= 211)\n",
      "CV loss=0.49113311362061607(Epoch= 211)\n",
      "train loss=0.4761376260636946(Epoch= 211)\n",
      "train loss=0.4761376260636946(Epoch= 211)\n",
      "CV loss=0.490857681270379(Epoch= 212)\n",
      "CV loss=0.490857681270379(Epoch= 212)\n",
      "train loss=0.47584273759387946(Epoch= 212)\n",
      "train loss=0.47584273759387946(Epoch= 212)\n",
      "CV loss=0.4912554281523376(Epoch= 213)\n",
      "CV loss=0.4912554281523376(Epoch= 213)\n",
      "train loss=0.4759800079344204(Epoch= 213)\n",
      "train loss=0.4759800079344204(Epoch= 213)\n",
      "CV loss=0.4927822599170765(Epoch= 214)\n",
      "CV loss=0.4927822599170765(Epoch= 214)\n",
      "train loss=0.4768984856072994(Epoch= 214)\n",
      "train loss=0.4768984856072994(Epoch= 214)\n",
      "CV loss=0.49055470839018567(Epoch= 215)\n",
      "CV loss=0.49055470839018567(Epoch= 215)\n",
      "train loss=0.47604196862146453(Epoch= 215)\n",
      "train loss=0.47604196862146453(Epoch= 215)\n",
      "CV loss=0.4930025248067754(Epoch= 216)\n",
      "CV loss=0.4930025248067754(Epoch= 216)\n",
      "train loss=0.4766946213092469(Epoch= 216)\n",
      "train loss=0.4766946213092469(Epoch= 216)\n",
      "CV loss=0.49167783185328845(Epoch= 217)\n",
      "CV loss=0.49167783185328845(Epoch= 217)\n",
      "train loss=0.4761732261163387(Epoch= 217)\n",
      "train loss=0.4761732261163387(Epoch= 217)\n",
      "CV loss=0.4929942186542058(Epoch= 218)\n",
      "CV loss=0.4929942186542058(Epoch= 218)\n",
      "train loss=0.4766485329608686(Epoch= 218)\n",
      "train loss=0.4766485329608686(Epoch= 218)\n",
      "CV loss=0.4910480115421372(Epoch= 219)\n",
      "CV loss=0.4910480115421372(Epoch= 219)\n",
      "train loss=0.4756408777763139(Epoch= 219)\n",
      "train loss=0.4756408777763139(Epoch= 219)\n",
      "CV loss=0.49037454908794187(Epoch= 220)\n",
      "CV loss=0.49037454908794187(Epoch= 220)\n",
      "train loss=0.47539812030103323(Epoch= 220)\n",
      "train loss=0.47539812030103323(Epoch= 220)\n",
      "CV loss=0.49099274616316013(Epoch= 221)\n",
      "CV loss=0.49099274616316013(Epoch= 221)\n",
      "train loss=0.47574681464881075(Epoch= 221)\n",
      "train loss=0.47574681464881075(Epoch= 221)\n",
      "CV loss=0.49046231407061813(Epoch= 222)\n",
      "CV loss=0.49046231407061813(Epoch= 222)\n",
      "train loss=0.47587508928608(Epoch= 222)\n",
      "train loss=0.47587508928608(Epoch= 222)\n",
      "CV loss=0.4905763720690559(Epoch= 223)\n",
      "CV loss=0.4905763720690559(Epoch= 223)\n",
      "train loss=0.47553704078125897(Epoch= 223)\n",
      "train loss=0.47553704078125897(Epoch= 223)\n",
      "CV loss=0.4914411859930888(Epoch= 224)\n",
      "CV loss=0.4914411859930888(Epoch= 224)\n",
      "train loss=0.47552621313419685(Epoch= 224)\n",
      "train loss=0.47552621313419685(Epoch= 224)\n",
      "CV loss=0.49050197594769107(Epoch= 225)\n",
      "CV loss=0.49050197594769107(Epoch= 225)\n",
      "train loss=0.47577973285403863(Epoch= 225)\n",
      "train loss=0.47577973285403863(Epoch= 225)\n",
      "CV loss=0.4915800780173704(Epoch= 226)\n",
      "CV loss=0.4915800780173704(Epoch= 226)\n",
      "train loss=0.4758857279368904(Epoch= 226)\n",
      "train loss=0.4758857279368904(Epoch= 226)\n",
      "CV loss=0.4902031327940325(Epoch= 227)\n",
      "CV loss=0.4902031327940325(Epoch= 227)\n",
      "train loss=0.4750422736631326(Epoch= 227)\n",
      "train loss=0.4750422736631326(Epoch= 227)\n",
      "CV loss=0.4904475512509745(Epoch= 228)\n",
      "CV loss=0.4904475512509745(Epoch= 228)\n",
      "train loss=0.4748490986212356(Epoch= 228)\n",
      "train loss=0.4748490986212356(Epoch= 228)\n",
      "CV loss=0.4907270098215466(Epoch= 229)\n",
      "CV loss=0.4907270098215466(Epoch= 229)\n",
      "train loss=0.47571778114136176(Epoch= 229)\n",
      "train loss=0.47571778114136176(Epoch= 229)\n",
      "CV loss=0.49095532615595416(Epoch= 230)\n",
      "CV loss=0.49095532615595416(Epoch= 230)\n",
      "train loss=0.4752773290583302(Epoch= 230)\n",
      "train loss=0.4752773290583302(Epoch= 230)\n",
      "CV loss=0.4901988100107758(Epoch= 231)\n",
      "CV loss=0.4901988100107758(Epoch= 231)\n",
      "train loss=0.4751449226898842(Epoch= 231)\n",
      "train loss=0.4751449226898842(Epoch= 231)\n",
      "CV loss=0.49016873544113093(Epoch= 232)\n",
      "CV loss=0.49016873544113093(Epoch= 232)\n",
      "train loss=0.47460783270421925(Epoch= 232)\n",
      "train loss=0.47460783270421925(Epoch= 232)\n",
      "CV loss=0.48982420046337066(Epoch= 233)\n",
      "CV loss=0.48982420046337066(Epoch= 233)\n",
      "train loss=0.47466902618666573(Epoch= 233)\n",
      "train loss=0.47466902618666573(Epoch= 233)\n",
      "CV loss=0.49030969956336273(Epoch= 234)\n",
      "CV loss=0.49030969956336273(Epoch= 234)\n",
      "train loss=0.4751622163266275(Epoch= 234)\n",
      "train loss=0.4751622163266275(Epoch= 234)\n",
      "CV loss=0.49027534075020685(Epoch= 235)\n",
      "CV loss=0.49027534075020685(Epoch= 235)\n",
      "train loss=0.47492876306763415(Epoch= 235)\n",
      "train loss=0.47492876306763415(Epoch= 235)\n",
      "CV loss=0.48962779015654934(Epoch= 236)\n",
      "CV loss=0.48962779015654934(Epoch= 236)\n",
      "train loss=0.47498928785617056(Epoch= 236)\n",
      "train loss=0.47498928785617056(Epoch= 236)\n",
      "CV loss=0.49211499895402816(Epoch= 237)\n",
      "CV loss=0.49211499895402816(Epoch= 237)\n",
      "train loss=0.4753142179998627(Epoch= 237)\n",
      "train loss=0.4753142179998627(Epoch= 237)\n",
      "CV loss=0.49020452571196566(Epoch= 238)\n",
      "CV loss=0.49020452571196566(Epoch= 238)\n",
      "train loss=0.47477418906009683(Epoch= 238)\n",
      "train loss=0.47477418906009683(Epoch= 238)\n",
      "CV loss=0.4909648413525067(Epoch= 239)\n",
      "CV loss=0.4909648413525067(Epoch= 239)\n",
      "train loss=0.47525542020582473(Epoch= 239)\n",
      "train loss=0.47525542020582473(Epoch= 239)\n",
      "CV loss=0.4911921723381414(Epoch= 240)\n",
      "CV loss=0.4911921723381414(Epoch= 240)\n",
      "train loss=0.4754782711645871(Epoch= 240)\n",
      "train loss=0.4754782711645871(Epoch= 240)\n",
      "CV loss=0.4894731752846986(Epoch= 241)\n",
      "CV loss=0.4894731752846986(Epoch= 241)\n",
      "train loss=0.4744650705799516(Epoch= 241)\n",
      "train loss=0.4744650705799516(Epoch= 241)\n",
      "CV loss=0.4895355391758649(Epoch= 242)\n",
      "CV loss=0.4895355391758649(Epoch= 242)\n",
      "train loss=0.47439652972976676(Epoch= 242)\n",
      "train loss=0.47439652972976676(Epoch= 242)\n",
      "CV loss=0.4898887572609933(Epoch= 243)\n",
      "CV loss=0.4898887572609933(Epoch= 243)\n",
      "train loss=0.4740176317507907(Epoch= 243)\n",
      "train loss=0.4740176317507907(Epoch= 243)\n",
      "CV loss=0.4898638219574153(Epoch= 244)\n",
      "CV loss=0.4898638219574153(Epoch= 244)\n",
      "train loss=0.4745059123442503(Epoch= 244)\n",
      "train loss=0.4745059123442503(Epoch= 244)\n",
      "CV loss=0.48954617131788647(Epoch= 245)\n",
      "CV loss=0.48954617131788647(Epoch= 245)\n",
      "train loss=0.4741875840934895(Epoch= 245)\n",
      "train loss=0.4741875840934895(Epoch= 245)\n",
      "CV loss=0.4902283654534552(Epoch= 246)\n",
      "CV loss=0.4902283654534552(Epoch= 246)\n",
      "train loss=0.4742743334746985(Epoch= 246)\n",
      "train loss=0.4742743334746985(Epoch= 246)\n",
      "CV loss=0.49023498446699176(Epoch= 247)\n",
      "CV loss=0.49023498446699176(Epoch= 247)\n",
      "train loss=0.4751868398691132(Epoch= 247)\n",
      "train loss=0.4751868398691132(Epoch= 247)\n",
      "CV loss=0.4902183503343591(Epoch= 248)\n",
      "CV loss=0.4902183503343591(Epoch= 248)\n",
      "train loss=0.47452240702537457(Epoch= 248)\n",
      "train loss=0.47452240702537457(Epoch= 248)\n",
      "CV loss=0.49003007979495916(Epoch= 249)\n",
      "CV loss=0.49003007979495916(Epoch= 249)\n",
      "train loss=0.4754938073178265(Epoch= 249)\n",
      "train loss=0.4754938073178265(Epoch= 249)\n",
      "CV loss=0.4900923115752213(Epoch= 250)\n",
      "CV loss=0.4900923115752213(Epoch= 250)\n",
      "train loss=0.4752705795243346(Epoch= 250)\n",
      "train loss=0.4752705795243346(Epoch= 250)\n",
      "CV loss=0.48996938061915607(Epoch= 251)\n",
      "CV loss=0.48996938061915607(Epoch= 251)\n",
      "train loss=0.4739158471295866(Epoch= 251)\n",
      "train loss=0.4739158471295866(Epoch= 251)\n",
      "CV loss=0.4887142490339882(Epoch= 252)\n",
      "CV loss=0.4887142490339882(Epoch= 252)\n",
      "train loss=0.47358564245824775(Epoch= 252)\n",
      "train loss=0.47358564245824775(Epoch= 252)\n",
      "CV loss=0.4898415027661139(Epoch= 253)\n",
      "CV loss=0.4898415027661139(Epoch= 253)\n",
      "train loss=0.4737442745321811(Epoch= 253)\n",
      "train loss=0.4737442745321811(Epoch= 253)\n",
      "CV loss=0.48971568478636673(Epoch= 254)\n",
      "CV loss=0.48971568478636673(Epoch= 254)\n",
      "train loss=0.474371693070077(Epoch= 254)\n",
      "train loss=0.474371693070077(Epoch= 254)\n",
      "CV loss=0.4903252213496966(Epoch= 255)\n",
      "CV loss=0.4903252213496966(Epoch= 255)\n",
      "train loss=0.4742195929401466(Epoch= 255)\n",
      "train loss=0.4742195929401466(Epoch= 255)\n",
      "CV loss=0.48883244631691636(Epoch= 256)\n",
      "CV loss=0.48883244631691636(Epoch= 256)\n",
      "train loss=0.47305032697402183(Epoch= 256)\n",
      "train loss=0.47305032697402183(Epoch= 256)\n",
      "CV loss=0.4885945308255952(Epoch= 257)\n",
      "CV loss=0.4885945308255952(Epoch= 257)\n",
      "train loss=0.47391872852303685(Epoch= 257)\n",
      "train loss=0.47391872852303685(Epoch= 257)\n",
      "CV loss=0.48796945604151165(Epoch= 258)\n",
      "CV loss=0.48796945604151165(Epoch= 258)\n",
      "train loss=0.47260862244686336(Epoch= 258)\n",
      "train loss=0.47260862244686336(Epoch= 258)\n",
      "CV loss=0.488684601910616(Epoch= 259)\n",
      "CV loss=0.488684601910616(Epoch= 259)\n",
      "train loss=0.4727657768894622(Epoch= 259)\n",
      "train loss=0.4727657768894622(Epoch= 259)\n",
      "CV loss=0.4880048269436358(Epoch= 260)\n",
      "CV loss=0.4880048269436358(Epoch= 260)\n",
      "train loss=0.4728735818200986(Epoch= 260)\n",
      "train loss=0.4728735818200986(Epoch= 260)\n",
      "CV loss=0.48871931612766284(Epoch= 261)\n",
      "CV loss=0.48871931612766284(Epoch= 261)\n",
      "train loss=0.47306447125055273(Epoch= 261)\n",
      "train loss=0.47306447125055273(Epoch= 261)\n",
      "CV loss=0.48811534276657376(Epoch= 262)\n",
      "CV loss=0.48811534276657376(Epoch= 262)\n",
      "train loss=0.47294600503260026(Epoch= 262)\n",
      "train loss=0.47294600503260026(Epoch= 262)\n",
      "CV loss=0.48880733173604907(Epoch= 263)\n",
      "CV loss=0.48880733173604907(Epoch= 263)\n",
      "train loss=0.4728259216250028(Epoch= 263)\n",
      "train loss=0.4728259216250028(Epoch= 263)\n",
      "CV loss=0.4884453442502217(Epoch= 264)\n",
      "CV loss=0.4884453442502217(Epoch= 264)\n",
      "train loss=0.4723772558827394(Epoch= 264)\n",
      "train loss=0.4723772558827394(Epoch= 264)\n",
      "CV loss=0.48755151437403643(Epoch= 265)\n",
      "CV loss=0.48755151437403643(Epoch= 265)\n",
      "train loss=0.47218238201242835(Epoch= 265)\n",
      "train loss=0.47218238201242835(Epoch= 265)\n",
      "CV loss=0.4879302865075736(Epoch= 266)\n",
      "CV loss=0.4879302865075736(Epoch= 266)\n",
      "train loss=0.4721665676038424(Epoch= 266)\n",
      "train loss=0.4721665676038424(Epoch= 266)\n",
      "CV loss=0.48713781188338245(Epoch= 267)\n",
      "CV loss=0.48713781188338245(Epoch= 267)\n",
      "train loss=0.47199760364496723(Epoch= 267)\n",
      "train loss=0.47199760364496723(Epoch= 267)\n",
      "CV loss=0.48918633954207574(Epoch= 268)\n",
      "CV loss=0.48918633954207574(Epoch= 268)\n",
      "train loss=0.472632690021852(Epoch= 268)\n",
      "train loss=0.472632690021852(Epoch= 268)\n",
      "CV loss=0.48854639906817054(Epoch= 269)\n",
      "CV loss=0.48854639906817054(Epoch= 269)\n",
      "train loss=0.47307574994596757(Epoch= 269)\n",
      "train loss=0.47307574994596757(Epoch= 269)\n",
      "CV loss=0.48699588592238374(Epoch= 270)\n",
      "CV loss=0.48699588592238374(Epoch= 270)\n",
      "train loss=0.4718849448207476(Epoch= 270)\n",
      "train loss=0.4718849448207476(Epoch= 270)\n",
      "CV loss=0.48734091135357666(Epoch= 271)\n",
      "CV loss=0.48734091135357666(Epoch= 271)\n",
      "train loss=0.4718247543310149(Epoch= 271)\n",
      "train loss=0.4718247543310149(Epoch= 271)\n",
      "CV loss=0.48785734517972007(Epoch= 272)\n",
      "CV loss=0.48785734517972007(Epoch= 272)\n",
      "train loss=0.4727748231344206(Epoch= 272)\n",
      "train loss=0.4727748231344206(Epoch= 272)\n",
      "CV loss=0.48744071911501097(Epoch= 273)\n",
      "CV loss=0.48744071911501097(Epoch= 273)\n",
      "train loss=0.47222527111331336(Epoch= 273)\n",
      "train loss=0.47222527111331336(Epoch= 273)\n",
      "CV loss=0.48690688810332217(Epoch= 274)\n",
      "CV loss=0.48690688810332217(Epoch= 274)\n",
      "train loss=0.47140429974920767(Epoch= 274)\n",
      "train loss=0.47140429974920767(Epoch= 274)\n",
      "CV loss=0.487714330371935(Epoch= 275)\n",
      "CV loss=0.487714330371935(Epoch= 275)\n",
      "train loss=0.4725246271420783(Epoch= 275)\n",
      "train loss=0.4725246271420783(Epoch= 275)\n",
      "CV loss=0.48683878622960003(Epoch= 276)\n",
      "CV loss=0.48683878622960003(Epoch= 276)\n",
      "train loss=0.47123768023475043(Epoch= 276)\n",
      "train loss=0.47123768023475043(Epoch= 276)\n",
      "CV loss=0.48710386192996014(Epoch= 277)\n",
      "CV loss=0.48710386192996014(Epoch= 277)\n",
      "train loss=0.47142360866715444(Epoch= 277)\n",
      "train loss=0.47142360866715444(Epoch= 277)\n",
      "CV loss=0.48676545427950485(Epoch= 278)\n",
      "CV loss=0.48676545427950485(Epoch= 278)\n",
      "train loss=0.4709623748867916(Epoch= 278)\n",
      "train loss=0.4709623748867916(Epoch= 278)\n",
      "CV loss=0.48764649331291066(Epoch= 279)\n",
      "CV loss=0.48764649331291066(Epoch= 279)\n",
      "train loss=0.47160196473190985(Epoch= 279)\n",
      "train loss=0.47160196473190985(Epoch= 279)\n",
      "CV loss=0.4873205416651626(Epoch= 280)\n",
      "CV loss=0.4873205416651626(Epoch= 280)\n",
      "train loss=0.4712849828817568(Epoch= 280)\n",
      "train loss=0.4712849828817568(Epoch= 280)\n",
      "CV loss=0.48685835418819146(Epoch= 281)\n",
      "CV loss=0.48685835418819146(Epoch= 281)\n",
      "train loss=0.470707790398334(Epoch= 281)\n",
      "train loss=0.470707790398334(Epoch= 281)\n",
      "CV loss=0.4872347650168687(Epoch= 282)\n",
      "CV loss=0.4872347650168687(Epoch= 282)\n",
      "train loss=0.4715819829726182(Epoch= 282)\n",
      "train loss=0.4715819829726182(Epoch= 282)\n",
      "CV loss=0.48747236426294616(Epoch= 283)\n",
      "CV loss=0.48747236426294616(Epoch= 283)\n",
      "train loss=0.47183621698380296(Epoch= 283)\n",
      "train loss=0.47183621698380296(Epoch= 283)\n",
      "CV loss=0.4889755128291613(Epoch= 284)\n",
      "CV loss=0.4889755128291613(Epoch= 284)\n",
      "train loss=0.47202046413567517(Epoch= 284)\n",
      "train loss=0.47202046413567517(Epoch= 284)\n",
      "CV loss=0.4857850655487324(Epoch= 285)\n",
      "CV loss=0.4857850655487324(Epoch= 285)\n",
      "train loss=0.4703808391479597(Epoch= 285)\n",
      "train loss=0.4703808391479597(Epoch= 285)\n",
      "CV loss=0.4865579441709501(Epoch= 286)\n",
      "CV loss=0.4865579441709501(Epoch= 286)\n",
      "train loss=0.47093480851292424(Epoch= 286)\n",
      "train loss=0.47093480851292424(Epoch= 286)\n",
      "CV loss=0.4868188313106766(Epoch= 287)\n",
      "CV loss=0.4868188313106766(Epoch= 287)\n",
      "train loss=0.4708779880697055(Epoch= 287)\n",
      "train loss=0.4708779880697055(Epoch= 287)\n",
      "CV loss=0.48664954829069385(Epoch= 288)\n",
      "CV loss=0.48664954829069385(Epoch= 288)\n",
      "train loss=0.4705531231359653(Epoch= 288)\n",
      "train loss=0.4705531231359653(Epoch= 288)\n",
      "CV loss=0.4864265408073301(Epoch= 289)\n",
      "CV loss=0.4864265408073301(Epoch= 289)\n",
      "train loss=0.47082853852339607(Epoch= 289)\n",
      "train loss=0.47082853852339607(Epoch= 289)\n",
      "CV loss=0.4863950561910252(Epoch= 290)\n",
      "CV loss=0.4863950561910252(Epoch= 290)\n",
      "train loss=0.47063328801186716(Epoch= 290)\n",
      "train loss=0.47063328801186716(Epoch= 290)\n",
      "CV loss=0.486908506188883(Epoch= 291)\n",
      "CV loss=0.486908506188883(Epoch= 291)\n",
      "train loss=0.47081731848740094(Epoch= 291)\n",
      "train loss=0.47081731848740094(Epoch= 291)\n",
      "CV loss=0.48714689786008303(Epoch= 292)\n",
      "CV loss=0.48714689786008303(Epoch= 292)\n",
      "train loss=0.4709459581641643(Epoch= 292)\n",
      "train loss=0.4709459581641643(Epoch= 292)\n",
      "CV loss=0.4856804092472129(Epoch= 293)\n",
      "CV loss=0.4856804092472129(Epoch= 293)\n",
      "train loss=0.46987526272259794(Epoch= 293)\n",
      "train loss=0.46987526272259794(Epoch= 293)\n",
      "CV loss=0.4855504694378449(Epoch= 294)\n",
      "CV loss=0.4855504694378449(Epoch= 294)\n",
      "train loss=0.470362537230256(Epoch= 294)\n",
      "train loss=0.470362537230256(Epoch= 294)\n",
      "CV loss=0.48603725653430546(Epoch= 295)\n",
      "CV loss=0.48603725653430546(Epoch= 295)\n",
      "train loss=0.46989154682227396(Epoch= 295)\n",
      "train loss=0.46989154682227396(Epoch= 295)\n",
      "CV loss=0.4859650960511913(Epoch= 296)\n",
      "CV loss=0.4859650960511913(Epoch= 296)\n",
      "train loss=0.4701246207209436(Epoch= 296)\n",
      "train loss=0.4701246207209436(Epoch= 296)\n",
      "CV loss=0.4861830362203797(Epoch= 297)\n",
      "CV loss=0.4861830362203797(Epoch= 297)\n",
      "train loss=0.4698672491749206(Epoch= 297)\n",
      "train loss=0.4698672491749206(Epoch= 297)\n",
      "CV loss=0.485701530524833(Epoch= 298)\n",
      "CV loss=0.485701530524833(Epoch= 298)\n",
      "train loss=0.4695779024358049(Epoch= 298)\n",
      "train loss=0.4695779024358049(Epoch= 298)\n",
      "CV loss=0.48614089355985113(Epoch= 299)\n",
      "CV loss=0.48614089355985113(Epoch= 299)\n",
      "train loss=0.47062513010804424(Epoch= 299)\n",
      "train loss=0.47062513010804424(Epoch= 299)\n",
      "CV loss=0.4862017572585876(Epoch= 300)\n",
      "CV loss=0.4862017572585876(Epoch= 300)\n",
      "train loss=0.46938525383700697(Epoch= 300)\n",
      "train loss=0.46938525383700697(Epoch= 300)\n",
      "CV loss=0.48493957046279834(Epoch= 301)\n",
      "CV loss=0.48493957046279834(Epoch= 301)\n",
      "train loss=0.4692823492907156(Epoch= 301)\n",
      "train loss=0.4692823492907156(Epoch= 301)\n",
      "CV loss=0.48602036952541605(Epoch= 302)\n",
      "CV loss=0.48602036952541605(Epoch= 302)\n",
      "train loss=0.46930898901021667(Epoch= 302)\n",
      "train loss=0.46930898901021667(Epoch= 302)\n",
      "CV loss=0.4868647852682256(Epoch= 303)\n",
      "CV loss=0.4868647852682256(Epoch= 303)\n",
      "train loss=0.4703345518910387(Epoch= 303)\n",
      "train loss=0.4703345518910387(Epoch= 303)\n",
      "CV loss=0.4876638602352685(Epoch= 304)\n",
      "CV loss=0.4876638602352685(Epoch= 304)\n",
      "train loss=0.47111885482108945(Epoch= 304)\n",
      "train loss=0.47111885482108945(Epoch= 304)\n",
      "CV loss=0.4852108739707355(Epoch= 305)\n",
      "CV loss=0.4852108739707355(Epoch= 305)\n",
      "train loss=0.4689274648381405(Epoch= 305)\n",
      "train loss=0.4689274648381405(Epoch= 305)\n",
      "CV loss=0.48512876088487944(Epoch= 306)\n",
      "CV loss=0.48512876088487944(Epoch= 306)\n",
      "train loss=0.46961054047458595(Epoch= 306)\n",
      "train loss=0.46961054047458595(Epoch= 306)\n",
      "CV loss=0.4858511732130531(Epoch= 307)\n",
      "CV loss=0.4858511732130531(Epoch= 307)\n",
      "train loss=0.46941743228552896(Epoch= 307)\n",
      "train loss=0.46941743228552896(Epoch= 307)\n",
      "CV loss=0.48592478651234283(Epoch= 308)\n",
      "CV loss=0.48592478651234283(Epoch= 308)\n",
      "train loss=0.47013094180133447(Epoch= 308)\n",
      "train loss=0.47013094180133447(Epoch= 308)\n",
      "CV loss=0.48540448375284573(Epoch= 309)\n",
      "CV loss=0.48540448375284573(Epoch= 309)\n",
      "train loss=0.46899025151164647(Epoch= 309)\n",
      "train loss=0.46899025151164647(Epoch= 309)\n",
      "CV loss=0.48582765339993284(Epoch= 310)\n",
      "CV loss=0.48582765339993284(Epoch= 310)\n",
      "train loss=0.4689883943636364(Epoch= 310)\n",
      "train loss=0.4689883943636364(Epoch= 310)\n",
      "CV loss=0.48489656050103824(Epoch= 311)\n",
      "CV loss=0.48489656050103824(Epoch= 311)\n",
      "train loss=0.46871460642062807(Epoch= 311)\n",
      "train loss=0.46871460642062807(Epoch= 311)\n",
      "CV loss=0.4851306901858601(Epoch= 312)\n",
      "CV loss=0.4851306901858601(Epoch= 312)\n",
      "train loss=0.46986817497561617(Epoch= 312)\n",
      "train loss=0.46986817497561617(Epoch= 312)\n",
      "CV loss=0.48483532972554166(Epoch= 313)\n",
      "CV loss=0.48483532972554166(Epoch= 313)\n",
      "train loss=0.4686664692204764(Epoch= 313)\n",
      "train loss=0.4686664692204764(Epoch= 313)\n",
      "CV loss=0.4841348961450973(Epoch= 314)\n",
      "CV loss=0.4841348961450973(Epoch= 314)\n",
      "train loss=0.4682408367884384(Epoch= 314)\n",
      "train loss=0.4682408367884384(Epoch= 314)\n",
      "CV loss=0.4848511412148508(Epoch= 315)\n",
      "CV loss=0.4848511412148508(Epoch= 315)\n",
      "train loss=0.4688071011749287(Epoch= 315)\n",
      "train loss=0.4688071011749287(Epoch= 315)\n",
      "CV loss=0.4846862509298475(Epoch= 316)\n",
      "CV loss=0.4846862509298475(Epoch= 316)\n",
      "train loss=0.46838925622752847(Epoch= 316)\n",
      "train loss=0.46838925622752847(Epoch= 316)\n",
      "CV loss=0.4852734616623073(Epoch= 317)\n",
      "CV loss=0.4852734616623073(Epoch= 317)\n",
      "train loss=0.46866169902161287(Epoch= 317)\n",
      "train loss=0.46866169902161287(Epoch= 317)\n",
      "CV loss=0.48590302713664585(Epoch= 318)\n",
      "CV loss=0.48590302713664585(Epoch= 318)\n",
      "train loss=0.4694216925346767(Epoch= 318)\n",
      "train loss=0.4694216925346767(Epoch= 318)\n",
      "CV loss=0.4853803282914806(Epoch= 319)\n",
      "CV loss=0.4853803282914806(Epoch= 319)\n",
      "train loss=0.468395483929323(Epoch= 319)\n",
      "train loss=0.468395483929323(Epoch= 319)\n",
      "CV loss=0.4848205753513547(Epoch= 320)\n",
      "CV loss=0.4848205753513547(Epoch= 320)\n",
      "train loss=0.4680832421259868(Epoch= 320)\n",
      "train loss=0.4680832421259868(Epoch= 320)\n",
      "CV loss=0.4857221586578309(Epoch= 321)\n",
      "CV loss=0.4857221586578309(Epoch= 321)\n",
      "train loss=0.46943612035365395(Epoch= 321)\n",
      "train loss=0.46943612035365395(Epoch= 321)\n",
      "CV loss=0.48522663605009764(Epoch= 322)\n",
      "CV loss=0.48522663605009764(Epoch= 322)\n",
      "train loss=0.46864758836379716(Epoch= 322)\n",
      "train loss=0.46864758836379716(Epoch= 322)\n",
      "CV loss=0.4843215624021181(Epoch= 323)\n",
      "CV loss=0.4843215624021181(Epoch= 323)\n",
      "train loss=0.46800275538576197(Epoch= 323)\n",
      "train loss=0.46800275538576197(Epoch= 323)\n",
      "CV loss=0.4852787878605232(Epoch= 324)\n",
      "CV loss=0.4852787878605232(Epoch= 324)\n",
      "train loss=0.4684031498767624(Epoch= 324)\n",
      "train loss=0.4684031498767624(Epoch= 324)\n",
      "CV loss=0.4836687594159961(Epoch= 325)\n",
      "CV loss=0.4836687594159961(Epoch= 325)\n",
      "train loss=0.46791588823119545(Epoch= 325)\n",
      "train loss=0.46791588823119545(Epoch= 325)\n",
      "CV loss=0.4858912085073268(Epoch= 326)\n",
      "CV loss=0.4858912085073268(Epoch= 326)\n",
      "train loss=0.4684582562112459(Epoch= 326)\n",
      "train loss=0.4684582562112459(Epoch= 326)\n",
      "CV loss=0.48365576591804627(Epoch= 327)\n",
      "CV loss=0.48365576591804627(Epoch= 327)\n",
      "train loss=0.46743786043980173(Epoch= 327)\n",
      "train loss=0.46743786043980173(Epoch= 327)\n",
      "CV loss=0.4835897714784958(Epoch= 328)\n",
      "CV loss=0.4835897714784958(Epoch= 328)\n",
      "train loss=0.4676808018789601(Epoch= 328)\n",
      "train loss=0.4676808018789601(Epoch= 328)\n",
      "CV loss=0.4842272013774269(Epoch= 329)\n",
      "CV loss=0.4842272013774269(Epoch= 329)\n",
      "train loss=0.4679124685281142(Epoch= 329)\n",
      "train loss=0.4679124685281142(Epoch= 329)\n",
      "CV loss=0.4838800660278366(Epoch= 330)\n",
      "CV loss=0.4838800660278366(Epoch= 330)\n",
      "train loss=0.46746359030773577(Epoch= 330)\n",
      "train loss=0.46746359030773577(Epoch= 330)\n",
      "CV loss=0.4832627418310027(Epoch= 331)\n",
      "CV loss=0.4832627418310027(Epoch= 331)\n",
      "train loss=0.46711060037400387(Epoch= 331)\n",
      "train loss=0.46711060037400387(Epoch= 331)\n",
      "CV loss=0.4846445775348788(Epoch= 332)\n",
      "CV loss=0.4846445775348788(Epoch= 332)\n",
      "train loss=0.4676973618450888(Epoch= 332)\n",
      "train loss=0.4676973618450888(Epoch= 332)\n",
      "CV loss=0.48347350796243976(Epoch= 333)\n",
      "CV loss=0.48347350796243976(Epoch= 333)\n",
      "train loss=0.4676989939752931(Epoch= 333)\n",
      "train loss=0.4676989939752931(Epoch= 333)\n",
      "CV loss=0.4838003343971166(Epoch= 334)\n",
      "CV loss=0.4838003343971166(Epoch= 334)\n",
      "train loss=0.46755373568677533(Epoch= 334)\n",
      "train loss=0.46755373568677533(Epoch= 334)\n",
      "CV loss=0.4835403504020542(Epoch= 335)\n",
      "CV loss=0.4835403504020542(Epoch= 335)\n",
      "train loss=0.4672742026607221(Epoch= 335)\n",
      "train loss=0.4672742026607221(Epoch= 335)\n",
      "CV loss=0.4838024837275788(Epoch= 336)\n",
      "CV loss=0.4838024837275788(Epoch= 336)\n",
      "train loss=0.4674442565474355(Epoch= 336)\n",
      "train loss=0.4674442565474355(Epoch= 336)\n",
      "CV loss=0.4837819164542023(Epoch= 337)\n",
      "CV loss=0.4837819164542023(Epoch= 337)\n",
      "train loss=0.46782893133901016(Epoch= 337)\n",
      "train loss=0.46782893133901016(Epoch= 337)\n",
      "CV loss=0.4850929944686255(Epoch= 338)\n",
      "CV loss=0.4850929944686255(Epoch= 338)\n",
      "train loss=0.46832550313910976(Epoch= 338)\n",
      "train loss=0.46832550313910976(Epoch= 338)\n",
      "CV loss=0.4833479546968008(Epoch= 339)\n",
      "CV loss=0.4833479546968008(Epoch= 339)\n",
      "train loss=0.467269208429851(Epoch= 339)\n",
      "train loss=0.467269208429851(Epoch= 339)\n",
      "CV loss=0.4838233199858826(Epoch= 340)\n",
      "CV loss=0.4838233199858826(Epoch= 340)\n",
      "train loss=0.46817948903510653(Epoch= 340)\n",
      "train loss=0.46817948903510653(Epoch= 340)\n",
      "CV loss=0.48310053714076906(Epoch= 341)\n",
      "CV loss=0.48310053714076906(Epoch= 341)\n",
      "train loss=0.46732305138143626(Epoch= 341)\n",
      "train loss=0.46732305138143626(Epoch= 341)\n",
      "CV loss=0.4840653564649809(Epoch= 342)\n",
      "CV loss=0.4840653564649809(Epoch= 342)\n",
      "train loss=0.46722192387927275(Epoch= 342)\n",
      "train loss=0.46722192387927275(Epoch= 342)\n",
      "CV loss=0.483127541013326(Epoch= 343)\n",
      "CV loss=0.483127541013326(Epoch= 343)\n",
      "train loss=0.4666226398597295(Epoch= 343)\n",
      "train loss=0.4666226398597295(Epoch= 343)\n",
      "CV loss=0.4833016682915676(Epoch= 344)\n",
      "CV loss=0.4833016682915676(Epoch= 344)\n",
      "train loss=0.46675579945779266(Epoch= 344)\n",
      "train loss=0.46675579945779266(Epoch= 344)\n",
      "CV loss=0.4834008531878359(Epoch= 345)\n",
      "CV loss=0.4834008531878359(Epoch= 345)\n",
      "train loss=0.46741739693080575(Epoch= 345)\n",
      "train loss=0.46741739693080575(Epoch= 345)\n",
      "CV loss=0.4846336409845907(Epoch= 346)\n",
      "CV loss=0.4846336409845907(Epoch= 346)\n",
      "train loss=0.46756128668325075(Epoch= 346)\n",
      "train loss=0.46756128668325075(Epoch= 346)\n",
      "CV loss=0.4828083742163042(Epoch= 347)\n",
      "CV loss=0.4828083742163042(Epoch= 347)\n",
      "train loss=0.46685773818632814(Epoch= 347)\n",
      "train loss=0.46685773818632814(Epoch= 347)\n",
      "CV loss=0.4839709456546813(Epoch= 348)\n",
      "CV loss=0.4839709456546813(Epoch= 348)\n",
      "train loss=0.466756430177926(Epoch= 348)\n",
      "train loss=0.466756430177926(Epoch= 348)\n",
      "CV loss=0.48400675216589306(Epoch= 349)\n",
      "CV loss=0.48400675216589306(Epoch= 349)\n",
      "train loss=0.4673446943736064(Epoch= 349)\n",
      "train loss=0.4673446943736064(Epoch= 349)\n",
      "CV loss=0.48364736030941347(Epoch= 350)\n",
      "CV loss=0.48364736030941347(Epoch= 350)\n",
      "train loss=0.4672081489329556(Epoch= 350)\n",
      "train loss=0.4672081489329556(Epoch= 350)\n",
      "CV loss=0.48406583839694284(Epoch= 351)\n",
      "CV loss=0.48406583839694284(Epoch= 351)\n",
      "train loss=0.46693678508459924(Epoch= 351)\n",
      "train loss=0.46693678508459924(Epoch= 351)\n",
      "CV loss=0.48404326519038154(Epoch= 352)\n",
      "CV loss=0.48404326519038154(Epoch= 352)\n",
      "train loss=0.4670020004897151(Epoch= 352)\n",
      "train loss=0.4670020004897151(Epoch= 352)\n",
      "CV loss=0.48299564963151975(Epoch= 353)\n",
      "CV loss=0.48299564963151975(Epoch= 353)\n",
      "train loss=0.466393857489918(Epoch= 353)\n",
      "train loss=0.466393857489918(Epoch= 353)\n",
      "CV loss=0.4850695132614964(Epoch= 354)\n",
      "CV loss=0.4850695132614964(Epoch= 354)\n",
      "train loss=0.4674668459307865(Epoch= 354)\n",
      "train loss=0.4674668459307865(Epoch= 354)\n",
      "CV loss=0.4833327698618947(Epoch= 355)\n",
      "CV loss=0.4833327698618947(Epoch= 355)\n",
      "train loss=0.46672849188309223(Epoch= 355)\n",
      "train loss=0.46672849188309223(Epoch= 355)\n",
      "CV loss=0.48407015415800536(Epoch= 356)\n",
      "CV loss=0.48407015415800536(Epoch= 356)\n",
      "train loss=0.4672871967632096(Epoch= 356)\n",
      "train loss=0.4672871967632096(Epoch= 356)\n",
      "CV loss=0.4823836429928906(Epoch= 357)\n",
      "CV loss=0.4823836429928906(Epoch= 357)\n",
      "train loss=0.46631724287534215(Epoch= 357)\n",
      "train loss=0.46631724287534215(Epoch= 357)\n",
      "CV loss=0.4829027953906861(Epoch= 358)\n",
      "CV loss=0.4829027953906861(Epoch= 358)\n",
      "train loss=0.46677777828661793(Epoch= 358)\n",
      "train loss=0.46677777828661793(Epoch= 358)\n",
      "CV loss=0.4832667254198669(Epoch= 359)\n",
      "CV loss=0.4832667254198669(Epoch= 359)\n",
      "train loss=0.46632524484538934(Epoch= 359)\n",
      "train loss=0.46632524484538934(Epoch= 359)\n",
      "CV loss=0.4828571929369897(Epoch= 360)\n",
      "CV loss=0.4828571929369897(Epoch= 360)\n",
      "train loss=0.46629600477022254(Epoch= 360)\n",
      "train loss=0.46629600477022254(Epoch= 360)\n",
      "CV loss=0.48434752035917006(Epoch= 361)\n",
      "CV loss=0.48434752035917006(Epoch= 361)\n",
      "train loss=0.4668504552677808(Epoch= 361)\n",
      "train loss=0.4668504552677808(Epoch= 361)\n",
      "CV loss=0.48359289211238676(Epoch= 362)\n",
      "CV loss=0.48359289211238676(Epoch= 362)\n",
      "train loss=0.4664264715029222(Epoch= 362)\n",
      "train loss=0.4664264715029222(Epoch= 362)\n",
      "CV loss=0.4834747064688716(Epoch= 363)\n",
      "CV loss=0.4834747064688716(Epoch= 363)\n",
      "train loss=0.4674069540691832(Epoch= 363)\n",
      "train loss=0.4674069540691832(Epoch= 363)\n",
      "CV loss=0.4827209084464458(Epoch= 364)\n",
      "CV loss=0.4827209084464458(Epoch= 364)\n",
      "train loss=0.4665970224347138(Epoch= 364)\n",
      "train loss=0.4665970224347138(Epoch= 364)\n",
      "CV loss=0.48443884414213134(Epoch= 365)\n",
      "CV loss=0.48443884414213134(Epoch= 365)\n",
      "train loss=0.4673553349377775(Epoch= 365)\n",
      "train loss=0.4673553349377775(Epoch= 365)\n",
      "CV loss=0.4825613960476017(Epoch= 366)\n",
      "CV loss=0.4825613960476017(Epoch= 366)\n",
      "train loss=0.46604503192075003(Epoch= 366)\n",
      "train loss=0.46604503192075003(Epoch= 366)\n",
      "CV loss=0.48212311132427665(Epoch= 367)\n",
      "CV loss=0.48212311132427665(Epoch= 367)\n",
      "train loss=0.4663855101362431(Epoch= 367)\n",
      "train loss=0.4663855101362431(Epoch= 367)\n",
      "CV loss=0.483489470709425(Epoch= 368)\n",
      "CV loss=0.483489470709425(Epoch= 368)\n",
      "train loss=0.4667805869072438(Epoch= 368)\n",
      "train loss=0.4667805869072438(Epoch= 368)\n",
      "CV loss=0.4834423071744892(Epoch= 369)\n",
      "CV loss=0.4834423071744892(Epoch= 369)\n",
      "train loss=0.4662275154274119(Epoch= 369)\n",
      "train loss=0.4662275154274119(Epoch= 369)\n",
      "CV loss=0.4827811456060985(Epoch= 370)\n",
      "CV loss=0.4827811456060985(Epoch= 370)\n",
      "train loss=0.4659809779060189(Epoch= 370)\n",
      "train loss=0.4659809779060189(Epoch= 370)\n",
      "CV loss=0.4840610455217723(Epoch= 371)\n",
      "CV loss=0.4840610455217723(Epoch= 371)\n",
      "train loss=0.4666988579931518(Epoch= 371)\n",
      "train loss=0.4666988579931518(Epoch= 371)\n",
      "CV loss=0.48339857109616424(Epoch= 372)\n",
      "CV loss=0.48339857109616424(Epoch= 372)\n",
      "train loss=0.46626178944715435(Epoch= 372)\n",
      "train loss=0.46626178944715435(Epoch= 372)\n",
      "CV loss=0.4824457840324214(Epoch= 373)\n",
      "CV loss=0.4824457840324214(Epoch= 373)\n",
      "train loss=0.4662356138122562(Epoch= 373)\n",
      "train loss=0.4662356138122562(Epoch= 373)\n",
      "CV loss=0.48364545195227393(Epoch= 374)\n",
      "CV loss=0.48364545195227393(Epoch= 374)\n",
      "train loss=0.4662057280799784(Epoch= 374)\n",
      "train loss=0.4662057280799784(Epoch= 374)\n",
      "CV loss=0.4826029332531235(Epoch= 375)\n",
      "CV loss=0.4826029332531235(Epoch= 375)\n",
      "train loss=0.4658095733504812(Epoch= 375)\n",
      "train loss=0.4658095733504812(Epoch= 375)\n",
      "CV loss=0.48261054518445684(Epoch= 376)\n",
      "CV loss=0.48261054518445684(Epoch= 376)\n",
      "train loss=0.4655914427834777(Epoch= 376)\n",
      "train loss=0.4655914427834777(Epoch= 376)\n",
      "CV loss=0.48304739849074046(Epoch= 377)\n",
      "CV loss=0.48304739849074046(Epoch= 377)\n",
      "train loss=0.46614546934796697(Epoch= 377)\n",
      "train loss=0.46614546934796697(Epoch= 377)\n",
      "CV loss=0.4819584092956929(Epoch= 378)\n",
      "CV loss=0.4819584092956929(Epoch= 378)\n",
      "train loss=0.4656213415722318(Epoch= 378)\n",
      "train loss=0.4656213415722318(Epoch= 378)\n",
      "CV loss=0.48344487895870825(Epoch= 379)\n",
      "CV loss=0.48344487895870825(Epoch= 379)\n",
      "train loss=0.4662838949068071(Epoch= 379)\n",
      "train loss=0.4662838949068071(Epoch= 379)\n",
      "CV loss=0.4829861543581077(Epoch= 380)\n",
      "CV loss=0.4829861543581077(Epoch= 380)\n",
      "train loss=0.4661345823887666(Epoch= 380)\n",
      "train loss=0.4661345823887666(Epoch= 380)\n",
      "CV loss=0.4826794907914091(Epoch= 381)\n",
      "CV loss=0.4826794907914091(Epoch= 381)\n",
      "train loss=0.46591671625945064(Epoch= 381)\n",
      "train loss=0.46591671625945064(Epoch= 381)\n",
      "CV loss=0.4839967481155245(Epoch= 382)\n",
      "CV loss=0.4839967481155245(Epoch= 382)\n",
      "train loss=0.46665270886434196(Epoch= 382)\n",
      "train loss=0.46665270886434196(Epoch= 382)\n",
      "CV loss=0.4821085964619526(Epoch= 383)\n",
      "CV loss=0.4821085964619526(Epoch= 383)\n",
      "train loss=0.4655800821045958(Epoch= 383)\n",
      "train loss=0.4655800821045958(Epoch= 383)\n",
      "CV loss=0.48375249083166216(Epoch= 384)\n",
      "CV loss=0.48375249083166216(Epoch= 384)\n",
      "train loss=0.46673251974153895(Epoch= 384)\n",
      "train loss=0.46673251974153895(Epoch= 384)\n",
      "CV loss=0.4821850621022753(Epoch= 385)\n",
      "CV loss=0.4821850621022753(Epoch= 385)\n",
      "train loss=0.4655425590037948(Epoch= 385)\n",
      "train loss=0.4655425590037948(Epoch= 385)\n",
      "CV loss=0.48218258714203854(Epoch= 386)\n",
      "CV loss=0.48218258714203854(Epoch= 386)\n",
      "train loss=0.4657717907098513(Epoch= 386)\n",
      "train loss=0.4657717907098513(Epoch= 386)\n",
      "CV loss=0.4828816321668933(Epoch= 387)\n",
      "CV loss=0.4828816321668933(Epoch= 387)\n",
      "train loss=0.4664663925039312(Epoch= 387)\n",
      "train loss=0.4664663925039312(Epoch= 387)\n",
      "CV loss=0.4825132250176622(Epoch= 388)\n",
      "CV loss=0.4825132250176622(Epoch= 388)\n",
      "train loss=0.4660876933457062(Epoch= 388)\n",
      "train loss=0.4660876933457062(Epoch= 388)\n",
      "CV loss=0.4827814754773927(Epoch= 389)\n",
      "CV loss=0.4827814754773927(Epoch= 389)\n",
      "train loss=0.46591502563074616(Epoch= 389)\n",
      "train loss=0.46591502563074616(Epoch= 389)\n",
      "CV loss=0.48174759029879244(Epoch= 390)\n",
      "CV loss=0.48174759029879244(Epoch= 390)\n",
      "train loss=0.46530811251868043(Epoch= 390)\n",
      "train loss=0.46530811251868043(Epoch= 390)\n",
      "CV loss=0.48408570084938546(Epoch= 391)\n",
      "CV loss=0.48408570084938546(Epoch= 391)\n",
      "train loss=0.4666378455432868(Epoch= 391)\n",
      "train loss=0.4666378455432868(Epoch= 391)\n",
      "CV loss=0.4823641714356157(Epoch= 392)\n",
      "CV loss=0.4823641714356157(Epoch= 392)\n",
      "train loss=0.46532723791517283(Epoch= 392)\n",
      "train loss=0.46532723791517283(Epoch= 392)\n",
      "CV loss=0.48220715895183464(Epoch= 393)\n",
      "CV loss=0.48220715895183464(Epoch= 393)\n",
      "train loss=0.4654703769032219(Epoch= 393)\n",
      "train loss=0.4654703769032219(Epoch= 393)\n",
      "CV loss=0.48348850267176585(Epoch= 394)\n",
      "CV loss=0.48348850267176585(Epoch= 394)\n",
      "train loss=0.4664510163419605(Epoch= 394)\n",
      "train loss=0.4664510163419605(Epoch= 394)\n",
      "CV loss=0.48233271705150127(Epoch= 395)\n",
      "CV loss=0.48233271705150127(Epoch= 395)\n",
      "train loss=0.4661025302820825(Epoch= 395)\n",
      "train loss=0.4661025302820825(Epoch= 395)\n",
      "CV loss=0.4821528065273411(Epoch= 396)\n",
      "CV loss=0.4821528065273411(Epoch= 396)\n",
      "train loss=0.46586978408352175(Epoch= 396)\n",
      "train loss=0.46586978408352175(Epoch= 396)\n",
      "CV loss=0.48219568773014293(Epoch= 397)\n",
      "CV loss=0.48219568773014293(Epoch= 397)\n",
      "train loss=0.46556760575460765(Epoch= 397)\n",
      "train loss=0.46556760575460765(Epoch= 397)\n",
      "CV loss=0.4822527461102206(Epoch= 398)\n",
      "CV loss=0.4822527461102206(Epoch= 398)\n",
      "train loss=0.4655621107410127(Epoch= 398)\n",
      "train loss=0.4655621107410127(Epoch= 398)\n",
      "CV loss=0.4828792271033594(Epoch= 399)\n",
      "CV loss=0.4828792271033594(Epoch= 399)\n",
      "train loss=0.466648329284561(Epoch= 399)\n",
      "train loss=0.466648329284561(Epoch= 399)\n",
      "CV loss=0.48294494375589114(Epoch= 400)\n",
      "CV loss=0.48294494375589114(Epoch= 400)\n",
      "train loss=0.4654957042856368(Epoch= 400)\n",
      "train loss=0.4654957042856368(Epoch= 400)\n",
      "CV loss=0.48234274668863375(Epoch= 401)\n",
      "CV loss=0.48234274668863375(Epoch= 401)\n",
      "train loss=0.46559657113466035(Epoch= 401)\n",
      "train loss=0.46559657113466035(Epoch= 401)\n",
      "CV loss=0.4833433424309837(Epoch= 402)\n",
      "CV loss=0.4833433424309837(Epoch= 402)\n",
      "train loss=0.4665520875952256(Epoch= 402)\n",
      "train loss=0.4665520875952256(Epoch= 402)\n",
      "CV loss=0.4842104889105696(Epoch= 403)\n",
      "CV loss=0.4842104889105696(Epoch= 403)\n",
      "train loss=0.46646324821692486(Epoch= 403)\n",
      "train loss=0.46646324821692486(Epoch= 403)\n",
      "CV loss=0.4830458639433196(Epoch= 404)\n",
      "CV loss=0.4830458639433196(Epoch= 404)\n",
      "train loss=0.46573015084617875(Epoch= 404)\n",
      "train loss=0.46573015084617875(Epoch= 404)\n",
      "CV loss=0.4854205732758904(Epoch= 405)\n",
      "CV loss=0.4854205732758904(Epoch= 405)\n",
      "train loss=0.46978171546176445(Epoch= 405)\n",
      "train loss=0.46978171546176445(Epoch= 405)\n",
      "CV loss=0.4816650293268535(Epoch= 406)\n",
      "CV loss=0.4816650293268535(Epoch= 406)\n",
      "train loss=0.4654105614694095(Epoch= 406)\n",
      "train loss=0.4654105614694095(Epoch= 406)\n",
      "CV loss=0.482227337548029(Epoch= 407)\n",
      "CV loss=0.482227337548029(Epoch= 407)\n",
      "train loss=0.4653693607496385(Epoch= 407)\n",
      "train loss=0.4653693607496385(Epoch= 407)\n",
      "CV loss=0.4820657276345742(Epoch= 408)\n",
      "CV loss=0.4820657276345742(Epoch= 408)\n",
      "train loss=0.46502179218854867(Epoch= 408)\n",
      "train loss=0.46502179218854867(Epoch= 408)\n",
      "CV loss=0.4824594751829537(Epoch= 409)\n",
      "CV loss=0.4824594751829537(Epoch= 409)\n",
      "train loss=0.46508615459463964(Epoch= 409)\n",
      "train loss=0.46508615459463964(Epoch= 409)\n",
      "CV loss=0.4838631957334212(Epoch= 410)\n",
      "CV loss=0.4838631957334212(Epoch= 410)\n",
      "train loss=0.46807546129796707(Epoch= 410)\n",
      "train loss=0.46807546129796707(Epoch= 410)\n",
      "CV loss=0.48200382087959537(Epoch= 411)\n",
      "CV loss=0.48200382087959537(Epoch= 411)\n",
      "train loss=0.4653186830004815(Epoch= 411)\n",
      "train loss=0.4653186830004815(Epoch= 411)\n",
      "CV loss=0.4822546205796179(Epoch= 412)\n",
      "CV loss=0.4822546205796179(Epoch= 412)\n",
      "train loss=0.46535923882944985(Epoch= 412)\n",
      "train loss=0.46535923882944985(Epoch= 412)\n",
      "CV loss=0.4830807880400402(Epoch= 413)\n",
      "CV loss=0.4830807880400402(Epoch= 413)\n",
      "train loss=0.4657872163324611(Epoch= 413)\n",
      "train loss=0.4657872163324611(Epoch= 413)\n",
      "CV loss=0.481997714666623(Epoch= 414)\n",
      "CV loss=0.481997714666623(Epoch= 414)\n",
      "train loss=0.46510949512743904(Epoch= 414)\n",
      "train loss=0.46510949512743904(Epoch= 414)\n",
      "CV loss=0.4845953936069708(Epoch= 415)\n",
      "CV loss=0.4845953936069708(Epoch= 415)\n",
      "train loss=0.4668756274723634(Epoch= 415)\n",
      "train loss=0.4668756274723634(Epoch= 415)\n",
      "CV loss=0.48152159155103136(Epoch= 416)\n",
      "CV loss=0.48152159155103136(Epoch= 416)\n",
      "train loss=0.46491439101067544(Epoch= 416)\n",
      "train loss=0.46491439101067544(Epoch= 416)\n",
      "CV loss=0.4835953714394809(Epoch= 417)\n",
      "CV loss=0.4835953714394809(Epoch= 417)\n",
      "train loss=0.46590332636519804(Epoch= 417)\n",
      "train loss=0.46590332636519804(Epoch= 417)\n",
      "CV loss=0.4819095072817349(Epoch= 418)\n",
      "CV loss=0.4819095072817349(Epoch= 418)\n",
      "train loss=0.46499643746971075(Epoch= 418)\n",
      "train loss=0.46499643746971075(Epoch= 418)\n",
      "CV loss=0.48179759444217407(Epoch= 419)\n",
      "CV loss=0.48179759444217407(Epoch= 419)\n",
      "train loss=0.4650424319909616(Epoch= 419)\n",
      "train loss=0.4650424319909616(Epoch= 419)\n",
      "CV loss=0.48244483989768006(Epoch= 420)\n",
      "CV loss=0.48244483989768006(Epoch= 420)\n",
      "train loss=0.46542830372497246(Epoch= 420)\n",
      "train loss=0.46542830372497246(Epoch= 420)\n",
      "CV loss=0.4825441169137548(Epoch= 421)\n",
      "CV loss=0.4825441169137548(Epoch= 421)\n",
      "train loss=0.4649439008932079(Epoch= 421)\n",
      "train loss=0.4649439008932079(Epoch= 421)\n",
      "CV loss=0.48255974307620253(Epoch= 422)\n",
      "CV loss=0.48255974307620253(Epoch= 422)\n",
      "train loss=0.46532863609406067(Epoch= 422)\n",
      "train loss=0.46532863609406067(Epoch= 422)\n",
      "CV loss=0.48391525056795626(Epoch= 423)\n",
      "CV loss=0.48391525056795626(Epoch= 423)\n",
      "train loss=0.46667378125542597(Epoch= 423)\n",
      "train loss=0.46667378125542597(Epoch= 423)\n",
      "CV loss=0.48220823354091435(Epoch= 424)\n",
      "CV loss=0.48220823354091435(Epoch= 424)\n",
      "train loss=0.46529950244620566(Epoch= 424)\n",
      "train loss=0.46529950244620566(Epoch= 424)\n",
      "CV loss=0.4819309483587902(Epoch= 425)\n",
      "CV loss=0.4819309483587902(Epoch= 425)\n",
      "train loss=0.4650475747521012(Epoch= 425)\n",
      "train loss=0.4650475747521012(Epoch= 425)\n",
      "CV loss=0.48147865388373956(Epoch= 426)\n",
      "CV loss=0.48147865388373956(Epoch= 426)\n",
      "train loss=0.4647063806688514(Epoch= 426)\n",
      "train loss=0.4647063806688514(Epoch= 426)\n",
      "CV loss=0.4821761421803926(Epoch= 427)\n",
      "CV loss=0.4821761421803926(Epoch= 427)\n",
      "train loss=0.4651067478419165(Epoch= 427)\n",
      "train loss=0.4651067478419165(Epoch= 427)\n",
      "CV loss=0.48308105403425683(Epoch= 428)\n",
      "CV loss=0.48308105403425683(Epoch= 428)\n",
      "train loss=0.4652162082587253(Epoch= 428)\n",
      "train loss=0.4652162082587253(Epoch= 428)\n",
      "CV loss=0.4813640278271099(Epoch= 429)\n",
      "CV loss=0.4813640278271099(Epoch= 429)\n",
      "train loss=0.46485123478713686(Epoch= 429)\n",
      "train loss=0.46485123478713686(Epoch= 429)\n",
      "CV loss=0.4824093191025639(Epoch= 430)\n",
      "CV loss=0.4824093191025639(Epoch= 430)\n",
      "train loss=0.4650485349218103(Epoch= 430)\n",
      "train loss=0.4650485349218103(Epoch= 430)\n",
      "CV loss=0.4823358450184693(Epoch= 431)\n",
      "CV loss=0.4823358450184693(Epoch= 431)\n",
      "train loss=0.46496410259894816(Epoch= 431)\n",
      "train loss=0.46496410259894816(Epoch= 431)\n",
      "CV loss=0.48181534461258435(Epoch= 432)\n",
      "CV loss=0.48181534461258435(Epoch= 432)\n",
      "train loss=0.46513274662343035(Epoch= 432)\n",
      "train loss=0.46513274662343035(Epoch= 432)\n",
      "CV loss=0.4827972709605256(Epoch= 433)\n",
      "CV loss=0.4827972709605256(Epoch= 433)\n",
      "train loss=0.466313991488328(Epoch= 433)\n",
      "train loss=0.466313991488328(Epoch= 433)\n",
      "CV loss=0.48261606151801223(Epoch= 434)\n",
      "CV loss=0.48261606151801223(Epoch= 434)\n",
      "train loss=0.46553214155230427(Epoch= 434)\n",
      "train loss=0.46553214155230427(Epoch= 434)\n",
      "CV loss=0.4828187032319969(Epoch= 435)\n",
      "CV loss=0.4828187032319969(Epoch= 435)\n",
      "train loss=0.4654781275106702(Epoch= 435)\n",
      "train loss=0.4654781275106702(Epoch= 435)\n",
      "CV loss=0.48237454619669407(Epoch= 436)\n",
      "CV loss=0.48237454619669407(Epoch= 436)\n",
      "train loss=0.46474859967707(Epoch= 436)\n",
      "train loss=0.46474859967707(Epoch= 436)\n",
      "CV loss=0.48562308288608547(Epoch= 437)\n",
      "CV loss=0.48562308288608547(Epoch= 437)\n",
      "train loss=0.46740210396894494(Epoch= 437)\n",
      "train loss=0.46740210396894494(Epoch= 437)\n",
      "CV loss=0.48227633816565074(Epoch= 438)\n",
      "CV loss=0.48227633816565074(Epoch= 438)\n",
      "train loss=0.4649980648169918(Epoch= 438)\n",
      "train loss=0.4649980648169918(Epoch= 438)\n",
      "CV loss=0.48226230546128646(Epoch= 439)\n",
      "CV loss=0.48226230546128646(Epoch= 439)\n",
      "train loss=0.4647332689912276(Epoch= 439)\n",
      "train loss=0.4647332689912276(Epoch= 439)\n",
      "CV loss=0.4828857150711652(Epoch= 440)\n",
      "CV loss=0.4828857150711652(Epoch= 440)\n",
      "train loss=0.4656121234803981(Epoch= 440)\n",
      "train loss=0.4656121234803981(Epoch= 440)\n",
      "CV loss=0.48239863000269134(Epoch= 441)\n",
      "CV loss=0.48239863000269134(Epoch= 441)\n",
      "train loss=0.46501367679748673(Epoch= 441)\n",
      "train loss=0.46501367679748673(Epoch= 441)\n",
      "CV loss=0.48123377660469757(Epoch= 442)\n",
      "CV loss=0.48123377660469757(Epoch= 442)\n",
      "train loss=0.46465125723082495(Epoch= 442)\n",
      "train loss=0.46465125723082495(Epoch= 442)\n",
      "CV loss=0.48481098039611636(Epoch= 443)\n",
      "CV loss=0.48481098039611636(Epoch= 443)\n",
      "train loss=0.46781989089829656(Epoch= 443)\n",
      "train loss=0.46781989089829656(Epoch= 443)\n",
      "CV loss=0.4811908003613823(Epoch= 444)\n",
      "CV loss=0.4811908003613823(Epoch= 444)\n",
      "train loss=0.46454748515846334(Epoch= 444)\n",
      "train loss=0.46454748515846334(Epoch= 444)\n",
      "CV loss=0.4816626465950272(Epoch= 445)\n",
      "CV loss=0.4816626465950272(Epoch= 445)\n",
      "train loss=0.4646187483381537(Epoch= 445)\n",
      "train loss=0.4646187483381537(Epoch= 445)\n",
      "CV loss=0.4826381430323507(Epoch= 446)\n",
      "CV loss=0.4826381430323507(Epoch= 446)\n",
      "train loss=0.4657742397725269(Epoch= 446)\n",
      "train loss=0.4657742397725269(Epoch= 446)\n",
      "CV loss=0.48208702920022384(Epoch= 447)\n",
      "CV loss=0.48208702920022384(Epoch= 447)\n",
      "train loss=0.46500083665166947(Epoch= 447)\n",
      "train loss=0.46500083665166947(Epoch= 447)\n",
      "CV loss=0.4825823832516498(Epoch= 448)\n",
      "CV loss=0.4825823832516498(Epoch= 448)\n",
      "train loss=0.4650604145930739(Epoch= 448)\n",
      "train loss=0.4650604145930739(Epoch= 448)\n",
      "CV loss=0.48214406184669006(Epoch= 449)\n",
      "CV loss=0.48214406184669006(Epoch= 449)\n",
      "train loss=0.46455157865087254(Epoch= 449)\n",
      "train loss=0.46455157865087254(Epoch= 449)\n",
      "CV loss=0.48300625389596225(Epoch= 450)\n",
      "CV loss=0.48300625389596225(Epoch= 450)\n",
      "train loss=0.4666378324775383(Epoch= 450)\n",
      "train loss=0.4666378324775383(Epoch= 450)\n",
      "CV loss=0.48209526583284634(Epoch= 451)\n",
      "CV loss=0.48209526583284634(Epoch= 451)\n",
      "train loss=0.4647603347514787(Epoch= 451)\n",
      "train loss=0.4647603347514787(Epoch= 451)\n",
      "CV loss=0.4816751565583042(Epoch= 452)\n",
      "CV loss=0.4816751565583042(Epoch= 452)\n",
      "train loss=0.46451454520304725(Epoch= 452)\n",
      "train loss=0.46451454520304725(Epoch= 452)\n",
      "CV loss=0.48302209450828115(Epoch= 453)\n",
      "CV loss=0.48302209450828115(Epoch= 453)\n",
      "train loss=0.46549857284917284(Epoch= 453)\n",
      "train loss=0.46549857284917284(Epoch= 453)\n",
      "CV loss=0.4816379720698804(Epoch= 454)\n",
      "CV loss=0.4816379720698804(Epoch= 454)\n",
      "train loss=0.4647280694544485(Epoch= 454)\n",
      "train loss=0.4647280694544485(Epoch= 454)\n",
      "CV loss=0.4813571833227164(Epoch= 455)\n",
      "CV loss=0.4813571833227164(Epoch= 455)\n",
      "train loss=0.4645468131472268(Epoch= 455)\n",
      "train loss=0.4645468131472268(Epoch= 455)\n",
      "CV loss=0.4822615117029584(Epoch= 456)\n",
      "CV loss=0.4822615117029584(Epoch= 456)\n",
      "train loss=0.4649285073901469(Epoch= 456)\n",
      "train loss=0.4649285073901469(Epoch= 456)\n",
      "CV loss=0.4835388989061958(Epoch= 457)\n",
      "CV loss=0.4835388989061958(Epoch= 457)\n",
      "train loss=0.46550915001537496(Epoch= 457)\n",
      "train loss=0.46550915001537496(Epoch= 457)\n",
      "CV loss=0.4833484882135596(Epoch= 458)\n",
      "CV loss=0.4833484882135596(Epoch= 458)\n",
      "train loss=0.46713909146233756(Epoch= 458)\n",
      "train loss=0.46713909146233756(Epoch= 458)\n",
      "CV loss=0.48155090033522885(Epoch= 459)\n",
      "CV loss=0.48155090033522885(Epoch= 459)\n",
      "train loss=0.4644566105166269(Epoch= 459)\n",
      "train loss=0.4644566105166269(Epoch= 459)\n",
      "CV loss=0.48127519324369133(Epoch= 460)\n",
      "CV loss=0.48127519324369133(Epoch= 460)\n",
      "train loss=0.4643836296060793(Epoch= 460)\n",
      "train loss=0.4643836296060793(Epoch= 460)\n",
      "CV loss=0.4815133053531012(Epoch= 461)\n",
      "CV loss=0.4815133053531012(Epoch= 461)\n",
      "train loss=0.4642339305041461(Epoch= 461)\n",
      "train loss=0.4642339305041461(Epoch= 461)\n",
      "CV loss=0.482037511698433(Epoch= 462)\n",
      "CV loss=0.482037511698433(Epoch= 462)\n",
      "train loss=0.46526705941112784(Epoch= 462)\n",
      "train loss=0.46526705941112784(Epoch= 462)\n",
      "CV loss=0.48168499213412164(Epoch= 463)\n",
      "CV loss=0.48168499213412164(Epoch= 463)\n",
      "train loss=0.4643257802674302(Epoch= 463)\n",
      "train loss=0.4643257802674302(Epoch= 463)\n",
      "CV loss=0.4812952410668714(Epoch= 464)\n",
      "CV loss=0.4812952410668714(Epoch= 464)\n",
      "train loss=0.4644328482865211(Epoch= 464)\n",
      "train loss=0.4644328482865211(Epoch= 464)\n",
      "CV loss=0.48148521914888187(Epoch= 465)\n",
      "CV loss=0.48148521914888187(Epoch= 465)\n",
      "train loss=0.4643103619843567(Epoch= 465)\n",
      "train loss=0.4643103619843567(Epoch= 465)\n",
      "CV loss=0.4814652159646426(Epoch= 466)\n",
      "CV loss=0.4814652159646426(Epoch= 466)\n",
      "train loss=0.464576840125282(Epoch= 466)\n",
      "train loss=0.464576840125282(Epoch= 466)\n",
      "CV loss=0.4828815792443232(Epoch= 467)\n",
      "CV loss=0.4828815792443232(Epoch= 467)\n",
      "train loss=0.4653527128460934(Epoch= 467)\n",
      "train loss=0.4653527128460934(Epoch= 467)\n",
      "CV loss=0.4812605543029139(Epoch= 468)\n",
      "CV loss=0.4812605543029139(Epoch= 468)\n",
      "train loss=0.464201060229659(Epoch= 468)\n",
      "train loss=0.464201060229659(Epoch= 468)\n",
      "CV loss=0.48101546899947273(Epoch= 469)\n",
      "CV loss=0.48101546899947273(Epoch= 469)\n",
      "train loss=0.46459108533283566(Epoch= 469)\n",
      "train loss=0.46459108533283566(Epoch= 469)\n",
      "CV loss=0.48175490590621695(Epoch= 470)\n",
      "CV loss=0.48175490590621695(Epoch= 470)\n",
      "train loss=0.4648168525164238(Epoch= 470)\n",
      "train loss=0.4648168525164238(Epoch= 470)\n",
      "CV loss=0.4842083475892529(Epoch= 471)\n",
      "CV loss=0.4842083475892529(Epoch= 471)\n",
      "train loss=0.4658904715204344(Epoch= 471)\n",
      "train loss=0.4658904715204344(Epoch= 471)\n",
      "CV loss=0.4814262188327701(Epoch= 472)\n",
      "CV loss=0.4814262188327701(Epoch= 472)\n",
      "train loss=0.4642860067405617(Epoch= 472)\n",
      "train loss=0.4642860067405617(Epoch= 472)\n",
      "CV loss=0.48117779925134685(Epoch= 473)\n",
      "CV loss=0.48117779925134685(Epoch= 473)\n",
      "train loss=0.46450548265498026(Epoch= 473)\n",
      "train loss=0.46450548265498026(Epoch= 473)\n",
      "CV loss=0.48092813778495236(Epoch= 474)\n",
      "CV loss=0.48092813778495236(Epoch= 474)\n",
      "train loss=0.46460198105557193(Epoch= 474)\n",
      "train loss=0.46460198105557193(Epoch= 474)\n",
      "CV loss=0.4819247996571372(Epoch= 475)\n",
      "CV loss=0.4819247996571372(Epoch= 475)\n",
      "train loss=0.46436903378223915(Epoch= 475)\n",
      "train loss=0.46436903378223915(Epoch= 475)\n",
      "CV loss=0.4812585111142499(Epoch= 476)\n",
      "CV loss=0.4812585111142499(Epoch= 476)\n",
      "train loss=0.46444118053470596(Epoch= 476)\n",
      "train loss=0.46444118053470596(Epoch= 476)\n",
      "CV loss=0.4813700676416572(Epoch= 477)\n",
      "CV loss=0.4813700676416572(Epoch= 477)\n",
      "train loss=0.4645678962116154(Epoch= 477)\n",
      "train loss=0.4645678962116154(Epoch= 477)\n",
      "CV loss=0.4828616830020309(Epoch= 478)\n",
      "CV loss=0.4828616830020309(Epoch= 478)\n",
      "train loss=0.46525394613177085(Epoch= 478)\n",
      "train loss=0.46525394613177085(Epoch= 478)\n",
      "CV loss=0.4814102310039842(Epoch= 479)\n",
      "CV loss=0.4814102310039842(Epoch= 479)\n",
      "train loss=0.4645865098630418(Epoch= 479)\n",
      "train loss=0.4645865098630418(Epoch= 479)\n",
      "CV loss=0.4830138819287595(Epoch= 480)\n",
      "CV loss=0.4830138819287595(Epoch= 480)\n",
      "train loss=0.4659416330173549(Epoch= 480)\n",
      "train loss=0.4659416330173549(Epoch= 480)\n",
      "CV loss=0.4817974454042843(Epoch= 481)\n",
      "CV loss=0.4817974454042843(Epoch= 481)\n",
      "train loss=0.46447750431328316(Epoch= 481)\n",
      "train loss=0.46447750431328316(Epoch= 481)\n",
      "CV loss=0.4817465840920051(Epoch= 482)\n",
      "CV loss=0.4817465840920051(Epoch= 482)\n",
      "train loss=0.4642329127717668(Epoch= 482)\n",
      "train loss=0.4642329127717668(Epoch= 482)\n",
      "CV loss=0.48250720652334356(Epoch= 483)\n",
      "CV loss=0.48250720652334356(Epoch= 483)\n",
      "train loss=0.46541883276452817(Epoch= 483)\n",
      "train loss=0.46541883276452817(Epoch= 483)\n",
      "CV loss=0.48097430586880235(Epoch= 484)\n",
      "CV loss=0.48097430586880235(Epoch= 484)\n",
      "train loss=0.4640228664558723(Epoch= 484)\n",
      "train loss=0.4640228664558723(Epoch= 484)\n",
      "CV loss=0.48283565692913244(Epoch= 485)\n",
      "CV loss=0.48283565692913244(Epoch= 485)\n",
      "train loss=0.4648401457875889(Epoch= 485)\n",
      "train loss=0.4648401457875889(Epoch= 485)\n",
      "CV loss=0.4808800570657456(Epoch= 486)\n",
      "CV loss=0.4808800570657456(Epoch= 486)\n",
      "train loss=0.464101288436652(Epoch= 486)\n",
      "train loss=0.464101288436652(Epoch= 486)\n",
      "CV loss=0.4813098502321941(Epoch= 487)\n",
      "CV loss=0.4813098502321941(Epoch= 487)\n",
      "train loss=0.4647664730419456(Epoch= 487)\n",
      "train loss=0.4647664730419456(Epoch= 487)\n",
      "CV loss=0.48178830090609337(Epoch= 488)\n",
      "CV loss=0.48178830090609337(Epoch= 488)\n",
      "train loss=0.46449342251306086(Epoch= 488)\n",
      "train loss=0.46449342251306086(Epoch= 488)\n",
      "CV loss=0.4819729349688328(Epoch= 489)\n",
      "CV loss=0.4819729349688328(Epoch= 489)\n",
      "train loss=0.4644767697542636(Epoch= 489)\n",
      "train loss=0.4644767697542636(Epoch= 489)\n",
      "CV loss=0.4811270968984993(Epoch= 490)\n",
      "CV loss=0.4811270968984993(Epoch= 490)\n",
      "train loss=0.4643228237180629(Epoch= 490)\n",
      "train loss=0.4643228237180629(Epoch= 490)\n",
      "CV loss=0.481690764659114(Epoch= 491)\n",
      "CV loss=0.481690764659114(Epoch= 491)\n",
      "train loss=0.464282191463093(Epoch= 491)\n",
      "train loss=0.464282191463093(Epoch= 491)\n",
      "CV loss=0.48303508958367314(Epoch= 492)\n",
      "CV loss=0.48303508958367314(Epoch= 492)\n",
      "train loss=0.465281893763576(Epoch= 492)\n",
      "train loss=0.465281893763576(Epoch= 492)\n",
      "CV loss=0.48189810051609744(Epoch= 493)\n",
      "CV loss=0.48189810051609744(Epoch= 493)\n",
      "train loss=0.4644948956610002(Epoch= 493)\n",
      "train loss=0.4644948956610002(Epoch= 493)\n",
      "CV loss=0.4809941760514082(Epoch= 494)\n",
      "CV loss=0.4809941760514082(Epoch= 494)\n",
      "train loss=0.4645222469348165(Epoch= 494)\n",
      "train loss=0.4645222469348165(Epoch= 494)\n",
      "CV loss=0.48225888356249824(Epoch= 495)\n",
      "CV loss=0.48225888356249824(Epoch= 495)\n",
      "train loss=0.4650123492453013(Epoch= 495)\n",
      "train loss=0.4650123492453013(Epoch= 495)\n",
      "CV loss=0.48114458405046595(Epoch= 496)\n",
      "CV loss=0.48114458405046595(Epoch= 496)\n",
      "train loss=0.4641389331464221(Epoch= 496)\n",
      "train loss=0.4641389331464221(Epoch= 496)\n",
      "CV loss=0.4825000769103185(Epoch= 497)\n",
      "CV loss=0.4825000769103185(Epoch= 497)\n",
      "train loss=0.4648624568439006(Epoch= 497)\n",
      "train loss=0.4648624568439006(Epoch= 497)\n",
      "CV loss=0.4821912114189888(Epoch= 498)\n",
      "CV loss=0.4821912114189888(Epoch= 498)\n",
      "train loss=0.46450432922977447(Epoch= 498)\n",
      "train loss=0.46450432922977447(Epoch= 498)\n",
      "CV loss=0.4812321169210143(Epoch= 499)\n",
      "CV loss=0.4812321169210143(Epoch= 499)\n",
      "train loss=0.46475970493281904(Epoch= 499)\n",
      "train loss=0.46475970493281904(Epoch= 499)\n",
      "CV loss=0.48106589359122914(Epoch= 500)\n",
      "CV loss=0.48106589359122914(Epoch= 500)\n",
      "train loss=0.4639378823180197(Epoch= 500)\n",
      "train loss=0.4639378823180197(Epoch= 500)\n",
      "CV loss=0.48136070200748987(Epoch= 501)\n",
      "CV loss=0.48136070200748987(Epoch= 501)\n",
      "train loss=0.4638294930912803(Epoch= 501)\n",
      "train loss=0.4638294930912803(Epoch= 501)\n",
      "CV loss=0.48145059527347056(Epoch= 502)\n",
      "CV loss=0.48145059527347056(Epoch= 502)\n",
      "train loss=0.4638627213832348(Epoch= 502)\n",
      "train loss=0.4638627213832348(Epoch= 502)\n",
      "CV loss=0.4844503601483957(Epoch= 503)\n",
      "CV loss=0.4844503601483957(Epoch= 503)\n",
      "train loss=0.46672932025661007(Epoch= 503)\n",
      "train loss=0.46672932025661007(Epoch= 503)\n",
      "CV loss=0.4814159742997398(Epoch= 504)\n",
      "CV loss=0.4814159742997398(Epoch= 504)\n",
      "train loss=0.4638459885681411(Epoch= 504)\n",
      "train loss=0.4638459885681411(Epoch= 504)\n",
      "CV loss=0.48047631071651586(Epoch= 505)\n",
      "CV loss=0.48047631071651586(Epoch= 505)\n",
      "train loss=0.46388557198739344(Epoch= 505)\n",
      "train loss=0.46388557198739344(Epoch= 505)\n",
      "CV loss=0.48214681524859915(Epoch= 506)\n",
      "CV loss=0.48214681524859915(Epoch= 506)\n",
      "train loss=0.46475579503426423(Epoch= 506)\n",
      "train loss=0.46475579503426423(Epoch= 506)\n",
      "CV loss=0.4822044833534429(Epoch= 507)\n",
      "CV loss=0.4822044833534429(Epoch= 507)\n",
      "train loss=0.4643211638123622(Epoch= 507)\n",
      "train loss=0.4643211638123622(Epoch= 507)\n",
      "CV loss=0.48085491648501205(Epoch= 508)\n",
      "CV loss=0.48085491648501205(Epoch= 508)\n",
      "train loss=0.4643215636876693(Epoch= 508)\n",
      "train loss=0.4643215636876693(Epoch= 508)\n",
      "CV loss=0.48299195549061646(Epoch= 509)\n",
      "CV loss=0.48299195549061646(Epoch= 509)\n",
      "train loss=0.4652710256968006(Epoch= 509)\n",
      "train loss=0.4652710256968006(Epoch= 509)\n",
      "CV loss=0.48152836616941086(Epoch= 510)\n",
      "CV loss=0.48152836616941086(Epoch= 510)\n",
      "train loss=0.4647187461388601(Epoch= 510)\n",
      "train loss=0.4647187461388601(Epoch= 510)\n",
      "CV loss=0.48254420124046216(Epoch= 511)\n",
      "CV loss=0.48254420124046216(Epoch= 511)\n",
      "train loss=0.4646602319766413(Epoch= 511)\n",
      "train loss=0.4646602319766413(Epoch= 511)\n",
      "CV loss=0.48106514492487706(Epoch= 512)\n",
      "CV loss=0.48106514492487706(Epoch= 512)\n",
      "train loss=0.46378280792175397(Epoch= 512)\n",
      "train loss=0.46378280792175397(Epoch= 512)\n",
      "CV loss=0.48141300809931986(Epoch= 513)\n",
      "CV loss=0.48141300809931986(Epoch= 513)\n",
      "train loss=0.4647509375714948(Epoch= 513)\n",
      "train loss=0.4647509375714948(Epoch= 513)\n",
      "CV loss=0.48090846496399975(Epoch= 514)\n",
      "CV loss=0.48090846496399975(Epoch= 514)\n",
      "train loss=0.4637532802932944(Epoch= 514)\n",
      "train loss=0.4637532802932944(Epoch= 514)\n",
      "CV loss=0.48153862473122366(Epoch= 515)\n",
      "CV loss=0.48153862473122366(Epoch= 515)\n",
      "train loss=0.46383819634930745(Epoch= 515)\n",
      "train loss=0.46383819634930745(Epoch= 515)\n",
      "CV loss=0.4812093337700879(Epoch= 516)\n",
      "CV loss=0.4812093337700879(Epoch= 516)\n",
      "train loss=0.4644630377434116(Epoch= 516)\n",
      "train loss=0.4644630377434116(Epoch= 516)\n",
      "CV loss=0.4809133956658056(Epoch= 517)\n",
      "CV loss=0.4809133956658056(Epoch= 517)\n",
      "train loss=0.4640864321219491(Epoch= 517)\n",
      "train loss=0.4640864321219491(Epoch= 517)\n",
      "CV loss=0.4817374884967086(Epoch= 518)\n",
      "CV loss=0.4817374884967086(Epoch= 518)\n",
      "train loss=0.4646696354060741(Epoch= 518)\n",
      "train loss=0.4646696354060741(Epoch= 518)\n",
      "CV loss=0.48111903980023124(Epoch= 519)\n",
      "CV loss=0.48111903980023124(Epoch= 519)\n",
      "train loss=0.46385913363206355(Epoch= 519)\n",
      "train loss=0.46385913363206355(Epoch= 519)\n",
      "CV loss=0.48101426038754236(Epoch= 520)\n",
      "CV loss=0.48101426038754236(Epoch= 520)\n",
      "train loss=0.4636138270335126(Epoch= 520)\n",
      "train loss=0.4636138270335126(Epoch= 520)\n",
      "CV loss=0.48152160231617336(Epoch= 521)\n",
      "CV loss=0.48152160231617336(Epoch= 521)\n",
      "train loss=0.46436666538890725(Epoch= 521)\n",
      "train loss=0.46436666538890725(Epoch= 521)\n",
      "CV loss=0.4807671875772309(Epoch= 522)\n",
      "CV loss=0.4807671875772309(Epoch= 522)\n",
      "train loss=0.46420975162658135(Epoch= 522)\n",
      "train loss=0.46420975162658135(Epoch= 522)\n",
      "CV loss=0.4814723598824239(Epoch= 523)\n",
      "CV loss=0.4814723598824239(Epoch= 523)\n",
      "train loss=0.4640406984307428(Epoch= 523)\n",
      "train loss=0.4640406984307428(Epoch= 523)\n",
      "CV loss=0.48290029444071364(Epoch= 524)\n",
      "CV loss=0.48290029444071364(Epoch= 524)\n",
      "train loss=0.4652719150321192(Epoch= 524)\n",
      "train loss=0.4652719150321192(Epoch= 524)\n",
      "CV loss=0.48191098772664326(Epoch= 525)\n",
      "CV loss=0.48191098772664326(Epoch= 525)\n",
      "train loss=0.46453300083383386(Epoch= 525)\n",
      "train loss=0.46453300083383386(Epoch= 525)\n",
      "CV loss=0.4816922452405241(Epoch= 526)\n",
      "CV loss=0.4816922452405241(Epoch= 526)\n",
      "train loss=0.46389465362611193(Epoch= 526)\n",
      "train loss=0.46389465362611193(Epoch= 526)\n",
      "CV loss=0.4809691285272496(Epoch= 527)\n",
      "CV loss=0.4809691285272496(Epoch= 527)\n",
      "train loss=0.4638787561955605(Epoch= 527)\n",
      "train loss=0.4638787561955605(Epoch= 527)\n",
      "CV loss=0.4808906106173054(Epoch= 528)\n",
      "CV loss=0.4808906106173054(Epoch= 528)\n",
      "train loss=0.46383239954341327(Epoch= 528)\n",
      "train loss=0.46383239954341327(Epoch= 528)\n",
      "CV loss=0.48080682458739016(Epoch= 529)\n",
      "CV loss=0.48080682458739016(Epoch= 529)\n",
      "train loss=0.4636711985256752(Epoch= 529)\n",
      "train loss=0.4636711985256752(Epoch= 529)\n",
      "CV loss=0.48195691033138216(Epoch= 530)\n",
      "CV loss=0.48195691033138216(Epoch= 530)\n",
      "train loss=0.4644543235866976(Epoch= 530)\n",
      "train loss=0.4644543235866976(Epoch= 530)\n",
      "CV loss=0.48153772918921256(Epoch= 531)\n",
      "CV loss=0.48153772918921256(Epoch= 531)\n",
      "train loss=0.4637940164834727(Epoch= 531)\n",
      "train loss=0.4637940164834727(Epoch= 531)\n",
      "CV loss=0.4808587521248146(Epoch= 532)\n",
      "CV loss=0.4808587521248146(Epoch= 532)\n",
      "train loss=0.46359127370607345(Epoch= 532)\n",
      "train loss=0.46359127370607345(Epoch= 532)\n",
      "CV loss=0.48171612789559703(Epoch= 533)\n",
      "CV loss=0.48171612789559703(Epoch= 533)\n",
      "train loss=0.46423783520203477(Epoch= 533)\n",
      "train loss=0.46423783520203477(Epoch= 533)\n",
      "CV loss=0.48046675091438334(Epoch= 534)\n",
      "CV loss=0.48046675091438334(Epoch= 534)\n",
      "train loss=0.4635878765636683(Epoch= 534)\n",
      "train loss=0.4635878765636683(Epoch= 534)\n",
      "CV loss=0.4813815495094676(Epoch= 535)\n",
      "CV loss=0.4813815495094676(Epoch= 535)\n",
      "train loss=0.46423250819084916(Epoch= 535)\n",
      "train loss=0.46423250819084916(Epoch= 535)\n",
      "CV loss=0.4814485150968188(Epoch= 536)\n",
      "CV loss=0.4814485150968188(Epoch= 536)\n",
      "train loss=0.46392837868262754(Epoch= 536)\n",
      "train loss=0.46392837868262754(Epoch= 536)\n",
      "CV loss=0.4816402376262636(Epoch= 537)\n",
      "CV loss=0.4816402376262636(Epoch= 537)\n",
      "train loss=0.4642637350101324(Epoch= 537)\n",
      "train loss=0.4642637350101324(Epoch= 537)\n",
      "CV loss=0.48221794402776263(Epoch= 538)\n",
      "CV loss=0.48221794402776263(Epoch= 538)\n",
      "train loss=0.465324385106538(Epoch= 538)\n",
      "train loss=0.465324385106538(Epoch= 538)\n",
      "CV loss=0.4809032720113914(Epoch= 539)\n",
      "CV loss=0.4809032720113914(Epoch= 539)\n",
      "train loss=0.46374288020033017(Epoch= 539)\n",
      "train loss=0.46374288020033017(Epoch= 539)\n",
      "CV loss=0.4807150861015043(Epoch= 540)\n",
      "CV loss=0.4807150861015043(Epoch= 540)\n",
      "train loss=0.46355117797356216(Epoch= 540)\n",
      "train loss=0.46355117797356216(Epoch= 540)\n",
      "CV loss=0.4811946353883835(Epoch= 541)\n",
      "CV loss=0.4811946353883835(Epoch= 541)\n",
      "train loss=0.4638364472633544(Epoch= 541)\n",
      "train loss=0.4638364472633544(Epoch= 541)\n",
      "CV loss=0.4810408173516968(Epoch= 542)\n",
      "CV loss=0.4810408173516968(Epoch= 542)\n",
      "train loss=0.463545363394509(Epoch= 542)\n",
      "train loss=0.463545363394509(Epoch= 542)\n",
      "CV loss=0.482951395970497(Epoch= 543)\n",
      "CV loss=0.482951395970497(Epoch= 543)\n",
      "train loss=0.46590428287587393(Epoch= 543)\n",
      "train loss=0.46590428287587393(Epoch= 543)\n",
      "CV loss=0.4810440683150552(Epoch= 544)\n",
      "CV loss=0.4810440683150552(Epoch= 544)\n",
      "train loss=0.46356808481228606(Epoch= 544)\n",
      "train loss=0.46356808481228606(Epoch= 544)\n",
      "CV loss=0.48067377069904155(Epoch= 545)\n",
      "CV loss=0.48067377069904155(Epoch= 545)\n",
      "train loss=0.46328972527009904(Epoch= 545)\n",
      "train loss=0.46328972527009904(Epoch= 545)\n",
      "CV loss=0.4829322531830353(Epoch= 546)\n",
      "CV loss=0.4829322531830353(Epoch= 546)\n",
      "train loss=0.46455818764698303(Epoch= 546)\n",
      "train loss=0.46455818764698303(Epoch= 546)\n",
      "CV loss=0.48114253489018977(Epoch= 547)\n",
      "CV loss=0.48114253489018977(Epoch= 547)\n",
      "train loss=0.46392271740205565(Epoch= 547)\n",
      "train loss=0.46392271740205565(Epoch= 547)\n",
      "CV loss=0.4812239951593049(Epoch= 548)\n",
      "CV loss=0.4812239951593049(Epoch= 548)\n",
      "train loss=0.46454618827792526(Epoch= 548)\n",
      "train loss=0.46454618827792526(Epoch= 548)\n",
      "CV loss=0.48101081575724186(Epoch= 549)\n",
      "CV loss=0.48101081575724186(Epoch= 549)\n",
      "train loss=0.46357469323558254(Epoch= 549)\n",
      "train loss=0.46357469323558254(Epoch= 549)\n",
      "CV loss=0.48049834971586625(Epoch= 550)\n",
      "CV loss=0.48049834971586625(Epoch= 550)\n",
      "train loss=0.4636005227975688(Epoch= 550)\n",
      "train loss=0.4636005227975688(Epoch= 550)\n",
      "CV loss=0.4812852411973965(Epoch= 551)\n",
      "CV loss=0.4812852411973965(Epoch= 551)\n",
      "train loss=0.4634554280429122(Epoch= 551)\n",
      "train loss=0.4634554280429122(Epoch= 551)\n",
      "CV loss=0.4809098012283998(Epoch= 552)\n",
      "CV loss=0.4809098012283998(Epoch= 552)\n",
      "train loss=0.4641576039129079(Epoch= 552)\n",
      "train loss=0.4641576039129079(Epoch= 552)\n",
      "CV loss=0.4807483193150439(Epoch= 553)\n",
      "CV loss=0.4807483193150439(Epoch= 553)\n",
      "train loss=0.4636008409585499(Epoch= 553)\n",
      "train loss=0.4636008409585499(Epoch= 553)\n",
      "CV loss=0.48127855804711706(Epoch= 554)\n",
      "CV loss=0.48127855804711706(Epoch= 554)\n",
      "train loss=0.4639587588968838(Epoch= 554)\n",
      "train loss=0.4639587588968838(Epoch= 554)\n",
      "CV loss=0.48185282808729035(Epoch= 555)\n",
      "CV loss=0.48185282808729035(Epoch= 555)\n",
      "train loss=0.46412234836154626(Epoch= 555)\n",
      "train loss=0.46412234836154626(Epoch= 555)\n",
      "CV loss=0.480875788159715(Epoch= 556)\n",
      "CV loss=0.480875788159715(Epoch= 556)\n",
      "train loss=0.4635521002360289(Epoch= 556)\n",
      "train loss=0.4635521002360289(Epoch= 556)\n",
      "CV loss=0.4816555206267763(Epoch= 557)\n",
      "CV loss=0.4816555206267763(Epoch= 557)\n",
      "train loss=0.4641099152619639(Epoch= 557)\n",
      "train loss=0.4641099152619639(Epoch= 557)\n",
      "CV loss=0.48061399115105785(Epoch= 558)\n",
      "CV loss=0.48061399115105785(Epoch= 558)\n",
      "train loss=0.4634772614625859(Epoch= 558)\n",
      "train loss=0.4634772614625859(Epoch= 558)\n",
      "CV loss=0.48174475626418267(Epoch= 559)\n",
      "CV loss=0.48174475626418267(Epoch= 559)\n",
      "train loss=0.46383468002331496(Epoch= 559)\n",
      "train loss=0.46383468002331496(Epoch= 559)\n",
      "CV loss=0.4815427438482679(Epoch= 560)\n",
      "CV loss=0.4815427438482679(Epoch= 560)\n",
      "train loss=0.4637474850575812(Epoch= 560)\n",
      "train loss=0.4637474850575812(Epoch= 560)\n",
      "CV loss=0.48218703036814725(Epoch= 561)\n",
      "CV loss=0.48218703036814725(Epoch= 561)\n",
      "train loss=0.46466354981888736(Epoch= 561)\n",
      "train loss=0.46466354981888736(Epoch= 561)\n",
      "CV loss=0.4806213484289832(Epoch= 562)\n",
      "CV loss=0.4806213484289832(Epoch= 562)\n",
      "train loss=0.4635169730043738(Epoch= 562)\n",
      "train loss=0.4635169730043738(Epoch= 562)\n",
      "CV loss=0.48076152674699635(Epoch= 563)\n",
      "CV loss=0.48076152674699635(Epoch= 563)\n",
      "train loss=0.4636208664010313(Epoch= 563)\n",
      "train loss=0.4636208664010313(Epoch= 563)\n",
      "CV loss=0.48048664298079025(Epoch= 564)\n",
      "CV loss=0.48048664298079025(Epoch= 564)\n",
      "train loss=0.46406885430497985(Epoch= 564)\n",
      "early stop\n",
      "train loss=0.46406885430497985(Epoch= 564)\n",
      "early stop\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "batch_size = 64\n",
    "loss = float(\"inf\")\n",
    "epsilon = 0.0001\n",
    "\n",
    "n_input = 784\n",
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 128\n",
    "n_out = 10\n",
    "\n",
    "# early stop logic\n",
    "best_cv_loss = float(\"inf\")\n",
    "patience = 30\n",
    "counter = 0\n",
    "tol = 1e-6 # accounts for floating point noise\n",
    "\n",
    "training_loss_history = []\n",
    "cv_loss_history = []\n",
    "\n",
    "i = 0\n",
    "while i < 2000: # epochs\n",
    "    randomized_indices = np.random.permutation(X_train.shape[0])\n",
    "    \n",
    "    for start_of_batch in range(0, X_train.shape[0], batch_size):\n",
    "        batch_indices = randomized_indices[start_of_batch : start_of_batch + batch_size]\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        loss, cache = forwardPass(X_train=X_batch, y_train=y_batch, weights=weights, lmbda=0.01)\n",
    "        grads = backpropagation(cache=cache, X_train=X_batch, y_train=y_batch, weights=weights, lmbda=0.01)\n",
    "        weights = update(alpha=0.005, cache = cache, weights=weights, grads=grads)\n",
    "\n",
    "        \n",
    "    cv_loss, cache = forwardPass(X_train=X_cv, y_train=y_cv, weights=weights, lmbda=0.01)\n",
    "    print(\"CV loss=\", cv_loss, \"(Epoch= \", i + 1, \")\", sep=\"\")\n",
    "    train_loss, cache = forwardPass(X_train=X_train, y_train=y_train, weights=weights, lmbda=0.01)\n",
    "    print(\"train loss=\", train_loss, \"(Epoch= \", i + 1, \")\", sep=\"\")\n",
    "    training_loss_history.append(train_loss)\n",
    "    cv_loss_history.append(cv_loss)\n",
    "\n",
    "    if cv_loss < best_cv_loss - tol:\n",
    "        best_cv_loss = cv_loss\n",
    "        best_weights = copy.deepcopy(weights)\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"early stop\")\n",
    "        weights = best_weights\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392f703",
   "metadata": {},
   "source": [
    "Plot training and cross validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5d4ffb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_loss_history, label=\"Training Loss, lambda=0.0001\")\n",
    "plt.plot(cv_loss_history, label=\"CV Loss\")\n",
    "\n",
    "# Mark the best CV loss point\n",
    "best_epoch = cv_loss_history.index(min(cv_loss_history))\n",
    "plt.scatter(best_epoch, best_cv_loss, color='red', s=100, zorder=5, label=f'Best CV Loss: {best_cv_loss:.4f} (epoch {best_epoch + 1})')\n",
    "plt.axvline(x=best_epoch, color='red', linestyle='--', alpha=0.3, label='Best epoch')\n",
    "\n",
    "# Mark where training actually stopped\n",
    "plt.axvline(x=len(cv_loss_history) - 1, color='orange', linestyle='--', alpha=0.5, label=f'Stopped at epoch {len(cv_loss_history)}')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Cross-Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ed810c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 96.56964285714285 %\n",
      "CV accuracy: 95.91428571428573 %\n",
      "relative gap between CV and Train loss: 3.5377915418173798 %\n",
      "\n",
      "Training error rate is approximately 3.4303571428571473 %\n",
      "CV error rate is approximately 4.085714285714275 %\n",
      "Bayes/human error rate for mnist datset is approximately 0.2%\n"
     ]
    }
   ],
   "source": [
    "def inference(X, *, weights=weights):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4'] \n",
    "\n",
    "    # First Hidden Layer\n",
    "    z_1 = X @ W1 + b1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    z_2 = a_1 @ W2 + b2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    z_3 = a_2 @ W3 + b3\n",
    "    a_3 = np.maximum(0, z_3)\n",
    "\n",
    "    # Logits before softmax\n",
    "    z_4 = a_3 @ W4 + b4\n",
    "\n",
    "    # Softmax activation applied\n",
    "    shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "    z_4_exp = np.exp(shifted)\n",
    "    p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "\n",
    "    return np.argmax(p, axis=1)\n",
    "\n",
    "train_accuracy = np.mean(inference(X_train, weights=weights) == y_train)\n",
    "print(\"Training accuracy:\", train_accuracy*100, \"%\")\n",
    "\n",
    "cv_accuracy = np.mean(inference(X_cv, weights=weights) == y_cv)\n",
    "print(\"CV accuracy:\", cv_accuracy*100, \"%\")\n",
    "\n",
    "gap = cv_loss - train_loss\n",
    "gap_percentage = (gap / train_loss) * 100\n",
    "print(\"relative gap between CV and Train loss:\", gap_percentage, \"%\")\n",
    "print()\n",
    "\n",
    "print(\"Training error rate is approximately\", 100 - train_accuracy*100, \"%\")\n",
    "print(\"CV error rate is approximately\", 100 - cv_accuracy*100, \"%\")\n",
    "print(\"Bayes/human error rate for mnist datset is approximately 0.2%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c5974d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 56000\n",
      "CV set size: 7000\n",
      "Test set size: 7000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"CV set size:\", X_cv.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9e158f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 1\n",
      "Index = 18652\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(X_train.shape[0])\n",
    "\n",
    "img = X_train[idx].reshape(28, 28)\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label: {y_train[idx]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Predicted Label:\", inference(X_train)[idx])\n",
    "print(\"Index =\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3a2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
