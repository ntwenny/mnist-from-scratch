{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f532c213",
   "metadata": {},
   "source": [
    "This is the notebook for training a neural network from scratch with only numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef183bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906ba58",
   "metadata": {},
   "source": [
    "Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79bb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(name=\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist.data.astype(np.float32)\n",
    "y = mnist.target.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d6fe2",
   "metadata": {},
   "source": [
    "Data exploration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f7ccbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (70000, 784)\n",
      "y.shape: (70000,)\n",
      "X.dtype: float32\n",
      "y.dtype: int64\n",
      "Min pixel value: 0.0\n",
      "Max pixel value: 255.0\n",
      "Unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "Any NaNs in X: False\n",
      "Any NaNs in y: False\n"
     ]
    }
   ],
   "source": [
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "\n",
    "print(\"X.dtype:\", X.dtype)\n",
    "print(\"y.dtype:\", y.dtype)\n",
    "\n",
    "print(\"Min pixel value:\", X.min())\n",
    "print(\"Max pixel value:\", X.max())\n",
    "\n",
    "print(\"Unique labels:\", np.unique(y))\n",
    "\n",
    "print(\"Any NaNs in X:\", np.isnan(X).any())\n",
    "print(\"Any NaNs in y:\", np.isnan(y).any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc8dde",
   "metadata": {},
   "source": [
    "Plotting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a29e85b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting pixel value distribution\n",
    "\n",
    "plt.hist(X.flatten(), bins=50)\n",
    "plt.title(\"Pixel Value Distribution\")\n",
    "plt.xlabel(\"Pixel intensity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting label distribution\n",
    "plt.hist(y, bins=10)\n",
    "plt.title(\"Pixel Value Distribution\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d78c45",
   "metadata": {},
   "source": [
    "Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e681bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = X / 255\n",
    "\n",
    "train_end = int(X.shape[0] * 0.8)\n",
    "cv_end = int(X.shape[0] * 0.9)\n",
    "\n",
    "randomized_indices = np.random.permutation(X.shape[0])\n",
    "\n",
    "train_indices = randomized_indices[0 : train_end]\n",
    "cv_indices = randomized_indices[train_end : cv_end]\n",
    "test_indices = randomized_indices[cv_end:]\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "X_cv = X[cv_indices]\n",
    "y_cv = y[cv_indices]\n",
    "\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print(X_train[np.random.choice(X_train.shape[0], size=10, replace=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32757980",
   "metadata": {},
   "source": [
    "Weight Initialization (Using the He Initialization method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f83f39a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightInit(*, n_input=784, n1=512, n2=256, n3=128, n_out=10, X_train=X_train, y_train=y_train):\n",
    "    W1 = np.random.randn(n_input, n1) * np.sqrt(2.0 / n_input)\n",
    "    b1 = np.zeros(n1)\n",
    "    print(\"W1.shape=\", W1.shape)\n",
    "    print(\"b1.shape=\", b1.shape)\n",
    "    # print(W1[np.random.choice(W1.shape[0], size=10, replace=False)])\n",
    "\n",
    "    W2 = np.random.randn(n1, n2) * np.sqrt(2.0 / n1)\n",
    "    b2 = np.zeros(n2)\n",
    "    print(\"W2.shape=\", W2.shape)\n",
    "    print(\"b2.shape=\", b2.shape)\n",
    "\n",
    "    W3 = np.random.randn(n2, n3) * np.sqrt(2.0 / n2)\n",
    "    b3 = np.random.randn(n3)\n",
    "    print(\"W3.shape=\", W3.shape)\n",
    "    print(\"b3.shape=\", b3.shape)\n",
    "\n",
    "    W4 = np.random.randn(n3, n_out) * np.sqrt(2.0 / n3)\n",
    "    b4 = np.random.randn(n_out)\n",
    "    print(\"W3.shape=\", W3.shape)\n",
    "    print(\"b3.shape=\", b3.shape)\n",
    "\n",
    "    # use smaller set at first to test, comment out when using full dataset\n",
    "    # idx = np.random.choice(X_train.shape[0], size=200, replace=False)\n",
    "    # X_train = X_train[idx]\n",
    "    # y_train = y_train[idx]\n",
    "\n",
    "    return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'W3': W3,\n",
    "        'b3': b3,\n",
    "        'W4': W4,\n",
    "        'b4': b4\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abe5b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape= (784, 512)\n",
      "b1.shape= (512,)\n",
      "W2.shape= (512, 256)\n",
      "b2.shape= (256,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n"
     ]
    }
   ],
   "source": [
    "n_input = 784\n",
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 128\n",
    "n_out = 10\n",
    "\n",
    "# Weights as a dictionary\n",
    "weights = weightInit(n_input=n_input, n1=n1, n2=n2, n3=n3, n_out=n_out, X_train=X_train, y_train=y_train)\n",
    "W1 = weights['W1']\n",
    "b1 = weights['b1']\n",
    "W2 = weights['W2']\n",
    "b2 = weights['b2']\n",
    "W3 = weights['W3']\n",
    "b3 = weights['b3']\n",
    "W4 = weights['W4']\n",
    "b4 = weights['b4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a0953",
   "metadata": {},
   "source": [
    "Now we'll begin programming the forward pass\n",
    "\n",
    "First Hidden Layer (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bee27ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 512)\n"
     ]
    }
   ],
   "source": [
    "z_1 = X_train @ W1 + b1\n",
    "a_1 = np.maximum(0, z_1)\n",
    "print(a_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c8046",
   "metadata": {},
   "source": [
    "Second Hidden Layer (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8072b2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 256)\n"
     ]
    }
   ],
   "source": [
    "z_2 = a_1 @ W2 + b2\n",
    "a_2 = np.maximum(0, z_2)\n",
    "print(a_2.shape)\n",
    "# print(a_2[np.random.choice(a_2.shape[0], size=1, replace=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392327d7",
   "metadata": {},
   "source": [
    "Third Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4de99851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 128)\n"
     ]
    }
   ],
   "source": [
    "z_3 = a_2 @ W3 + b3\n",
    "a_3 = np.maximum(0, z_3)\n",
    "print(a_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17b9a1",
   "metadata": {},
   "source": [
    "The logits before the softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cf56f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_4 = a_3 @ W4 + b4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a58047",
   "metadata": {},
   "source": [
    "Softmax activation function applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ddd303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 10)\n",
      "[0.11577247 0.01580383 0.05529895 0.22177    0.00992068 0.14714474\n",
      " 0.01875554 0.01575198 0.0813076  0.31847422]\n",
      "1.0\n",
      "9\n",
      "vs\n",
      "Correct Image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "z_4_exp = np.exp(shifted)\n",
    "p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "print(p.shape)\n",
    "print(p[0])\n",
    "print(p[0].sum())\n",
    "print(np.argmax(p[0]))\n",
    "print(\"vs\")\n",
    "print(\"Correct Image\")\n",
    "\n",
    "\n",
    "img = X_train[0].reshape(28, 28)\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label: {y_train[0]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf517a",
   "metadata": {},
   "source": [
    "Loss Function for Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e92510a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7622466611385104\n"
     ]
    }
   ],
   "source": [
    "p = np.clip(p, 1e-12, 1.0)\n",
    "loss = -np.mean(np.log(p[np.arange(y_train.shape[0]), y_train]))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60070c1f",
   "metadata": {},
   "source": [
    "Repeatable function for forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10c2c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(*, X_train=X_train, y_train=y_train, weights=weights, lmbda=0):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    # First Hidden Layer\n",
    "    z_1 = X_train @ W1 + b1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    z_2 = a_1 @ W2 + b2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    z_3 = a_2 @ W3 + b3\n",
    "    a_3 = np.maximum(0, z_3)\n",
    "\n",
    "    # Logits before softmax\n",
    "    z_4 = a_3 @ W4 + b4\n",
    "\n",
    "    # Softmax activation applied\n",
    "    shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "    z_4_exp = np.exp(shifted)\n",
    "    p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "\n",
    "    # loss function for softmax\n",
    "    p = np.clip(p, 1e-12, 1.0)\n",
    "    loss = (\n",
    "        -np.mean(np.log(p[np.arange(y_train.shape[0]), y_train])) \n",
    "            + (lmbda/(2)) * ((W1**2).sum() + (W2**2).sum() + (W3**2).sum() + (W4**2).sum())\n",
    "        )\n",
    "    return loss, {\n",
    "        \"a_1\": a_1,\n",
    "        \"a_2\": a_2,\n",
    "        \"a_3\": a_3,\n",
    "        \"p\": p,\n",
    "        \"z_1\": z_1,\n",
    "        \"z_2\": z_2,\n",
    "        \"z_3\": z_3,\n",
    "        \"z_4\": z_4\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738dae8",
   "metadata": {},
   "source": [
    "Repeatable Function for backward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d386147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(*, cache=None, X_train=None, y_train=None, weights=None, lmbda=0):\n",
    "\n",
    "    a_1 = cache[\"a_1\"]\n",
    "    a_2 = cache[\"a_2\"]\n",
    "    a_3 = cache[\"a_3\"]\n",
    "    p = cache[\"p\"]\n",
    "    z_1 = cache[\"z_1\"]\n",
    "    z_2 = cache[\"z_2\"]\n",
    "    z_3 = cache[\"z_3\"]\n",
    "    z_4 = cache[\"z_4\"]\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    y_1he = np.zeros((len(y_train), 10))\n",
    "    y_1he[np.arange(len(y_train)), y_train] = 1\n",
    "\n",
    "    dJ_dz4 = p - y_1he # Gradient of loss for each logit, shape = (200, 10)\n",
    "\n",
    "    dJ_dw4 = (a_3.T @ dJ_dz4) / a_3.shape[0]# Gradients for output layer\n",
    "    dJ_db4 = np.mean(dJ_dz4, axis=0)\n",
    "\n",
    "    dJ_da3 = dJ_dz4 @ W4.T # Gradients for third hidden layer\n",
    "\n",
    "    da3_dz3 = np.where(z_3 > 0, 1, 0)\n",
    "    dJ_dz3 = da3_dz3 * dJ_da3\n",
    "\n",
    "\n",
    "    dJ_dw3 = (a_2.T @ dJ_dz3) / a_2.shape[0]\n",
    "    dJ_db3 = np.mean(dJ_dz3, axis=0)\n",
    "\n",
    "\n",
    "    dJ_da2 = dJ_dz3 @ W3.T # Gradients for second hidden layer\n",
    "\n",
    "    da2_dz2 = np.where(z_2 > 0, 1, 0)\n",
    "    dJ_dz2 = da2_dz2 * dJ_da2\n",
    "\n",
    "    dJ_dw2 = (a_1.T @ dJ_dz2) / a_1.shape[0]\n",
    "    dJ_db2 = np.mean(dJ_dz2, axis=0)\n",
    "\n",
    "    dJ_da1 = dJ_dz2 @ W2.T # Gradients for first hidden layer\n",
    "\n",
    "    da1_dz1 = np.where(z_1 > 0, 1, 0)\n",
    "    dJ_dz1 = da1_dz1 * dJ_da1\n",
    "\n",
    "    dJ_dw1 = (X_train.T @ dJ_dz1) / X_train.shape[0]\n",
    "    dJ_db1 = np.mean(dJ_dz1, axis=0)\n",
    "\n",
    "    # L2 regularization\n",
    "    dJ_dw4 += (lmbda) * W4\n",
    "    dJ_dw3 += (lmbda) * W3\n",
    "    dJ_dw2 += (lmbda) * W2\n",
    "    dJ_dw1 += (lmbda) * W1\n",
    "\n",
    "    return {\"dJ_dw4\": dJ_dw4, \"dJ_db4\": dJ_db4, \"dJ_dw3\": dJ_dw3, \"dJ_db3\": dJ_db3,\n",
    "            \"dJ_dw2\": dJ_dw2, \"dJ_db2\": dJ_db2, \"dJ_dw1\": dJ_dw1, \"dJ_db1\": dJ_db1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70956097",
   "metadata": {},
   "source": [
    "Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8191e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(*, alpha = 0.01, cache = None, weights=None, grads=None):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4']\n",
    "\n",
    "    W4 -= alpha * grads[\"dJ_dw4\"]\n",
    "    b4 -= alpha * grads[\"dJ_db4\"]\n",
    "\n",
    "    W3 -= alpha * grads[\"dJ_dw3\"]\n",
    "    b3 -= alpha * grads[\"dJ_db3\"]\n",
    "\n",
    "    W2 -= alpha * grads[\"dJ_dw2\"]\n",
    "    b2 -= alpha * grads[\"dJ_db2\"]\n",
    "\n",
    "    W1 -= alpha * grads[\"dJ_dw1\"]\n",
    "    b1 -= alpha * grads[\"dJ_db1\"]\n",
    "\n",
    "    return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'W3': W3,\n",
    "        'b3': b3,\n",
    "        'W4': W4,\n",
    "        'b4': b4\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d672345",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baa17d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape= (784, 512)\n",
      "b1.shape= (512,)\n",
      "W2.shape= (512, 256)\n",
      "b2.shape= (256,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n",
      "W3.shape= (256, 128)\n",
      "b3.shape= (128,)\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "weights = weightInit(n_input=n_input, n1=n1, n2=n2, n3=n3, n_out=n_out, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e63e9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV loss=1.7730238601112651(Epoch= 1)\n",
      "train loss=1.7664182591214004(Epoch= 1)\n",
      "CV loss=1.2771976619149654(Epoch= 2)\n",
      "train loss=1.2686492937803628(Epoch= 2)\n",
      "CV loss=1.0663768168384529(Epoch= 3)\n",
      "train loss=1.0559934905334005(Epoch= 3)\n",
      "CV loss=0.963912309574058(Epoch= 4)\n",
      "train loss=0.9531377720528047(Epoch= 4)\n",
      "CV loss=0.9045838009946082(Epoch= 5)\n",
      "train loss=0.8943122514679543(Epoch= 5)\n",
      "CV loss=0.8674787486906592(Epoch= 6)\n",
      "train loss=0.8562756681105423(Epoch= 6)\n",
      "CV loss=0.8399680061317314(Epoch= 7)\n",
      "train loss=0.8287053685508179(Epoch= 7)\n",
      "CV loss=0.8204110545206025(Epoch= 8)\n",
      "train loss=0.8089129030156154(Epoch= 8)\n",
      "CV loss=0.8038589554781359(Epoch= 9)\n",
      "train loss=0.7916371466109349(Epoch= 9)\n",
      "CV loss=0.7901766740583913(Epoch= 10)\n",
      "train loss=0.7778495211384135(Epoch= 10)\n",
      "CV loss=0.7797414215592745(Epoch= 11)\n",
      "train loss=0.7665379913538168(Epoch= 11)\n",
      "CV loss=0.769613951333586(Epoch= 12)\n",
      "train loss=0.7563769931246587(Epoch= 12)\n",
      "CV loss=0.7609195130502202(Epoch= 13)\n",
      "train loss=0.7473258565072453(Epoch= 13)\n",
      "CV loss=0.7539072736374761(Epoch= 14)\n",
      "train loss=0.7393227819763504(Epoch= 14)\n",
      "CV loss=0.746229961276218(Epoch= 15)\n",
      "train loss=0.732074498166063(Epoch= 15)\n",
      "CV loss=0.7397808121791483(Epoch= 16)\n",
      "train loss=0.7250967220711084(Epoch= 16)\n",
      "CV loss=0.7332403507728013(Epoch= 17)\n",
      "train loss=0.7185528539343391(Epoch= 17)\n",
      "CV loss=0.7275134842496076(Epoch= 18)\n",
      "train loss=0.7126956570722116(Epoch= 18)\n",
      "CV loss=0.7230625714731267(Epoch= 19)\n",
      "train loss=0.7074088053278145(Epoch= 19)\n",
      "CV loss=0.7183619094774965(Epoch= 20)\n",
      "train loss=0.7023716831960355(Epoch= 20)\n",
      "CV loss=0.7134066685338736(Epoch= 21)\n",
      "train loss=0.6975758609489933(Epoch= 21)\n",
      "CV loss=0.7081313365830728(Epoch= 22)\n",
      "train loss=0.6923154626472061(Epoch= 22)\n",
      "CV loss=0.7035108168195559(Epoch= 23)\n",
      "train loss=0.6879857446487647(Epoch= 23)\n",
      "CV loss=0.7004390318481807(Epoch= 24)\n",
      "train loss=0.6836768086050511(Epoch= 24)\n",
      "CV loss=0.6959084382768466(Epoch= 25)\n",
      "train loss=0.6799740458516035(Epoch= 25)\n",
      "CV loss=0.6924387839615609(Epoch= 26)\n",
      "train loss=0.6756210140575222(Epoch= 26)\n",
      "CV loss=0.688840658926671(Epoch= 27)\n",
      "train loss=0.671896139775502(Epoch= 27)\n",
      "CV loss=0.6846280358861817(Epoch= 28)\n",
      "train loss=0.6683734550863092(Epoch= 28)\n",
      "CV loss=0.6814740134359936(Epoch= 29)\n",
      "train loss=0.6642701487462366(Epoch= 29)\n",
      "CV loss=0.6783267565853599(Epoch= 30)\n",
      "train loss=0.6609334079714456(Epoch= 30)\n",
      "CV loss=0.6756838172932301(Epoch= 31)\n",
      "train loss=0.6577402390717226(Epoch= 31)\n",
      "CV loss=0.6727361001121106(Epoch= 32)\n",
      "train loss=0.6548084974892571(Epoch= 32)\n",
      "CV loss=0.6689092517681681(Epoch= 33)\n",
      "train loss=0.651358413416906(Epoch= 33)\n",
      "CV loss=0.6662799371785203(Epoch= 34)\n",
      "train loss=0.6479922898963929(Epoch= 34)\n",
      "CV loss=0.6636921364592667(Epoch= 35)\n",
      "train loss=0.6452433727799038(Epoch= 35)\n",
      "CV loss=0.6607718526794234(Epoch= 36)\n",
      "train loss=0.6422299790750389(Epoch= 36)\n",
      "CV loss=0.6584131050006024(Epoch= 37)\n",
      "train loss=0.6394389825761253(Epoch= 37)\n",
      "CV loss=0.6556376857333923(Epoch= 38)\n",
      "train loss=0.6368476027866772(Epoch= 38)\n",
      "CV loss=0.6531259402140419(Epoch= 39)\n",
      "train loss=0.6339182011996931(Epoch= 39)\n",
      "CV loss=0.6499897818378821(Epoch= 40)\n",
      "train loss=0.6310654369511379(Epoch= 40)\n",
      "CV loss=0.647787807174558(Epoch= 41)\n",
      "train loss=0.6286176339632483(Epoch= 41)\n",
      "CV loss=0.6448608804887058(Epoch= 42)\n",
      "train loss=0.6260826764155033(Epoch= 42)\n",
      "CV loss=0.6426416512575763(Epoch= 43)\n",
      "train loss=0.623455015369117(Epoch= 43)\n",
      "CV loss=0.6403116884853564(Epoch= 44)\n",
      "train loss=0.6210732376171041(Epoch= 44)\n",
      "CV loss=0.6385080571990919(Epoch= 45)\n",
      "train loss=0.6188109716207235(Epoch= 45)\n",
      "CV loss=0.6357403537468699(Epoch= 46)\n",
      "train loss=0.6164435966774344(Epoch= 46)\n",
      "CV loss=0.6344228513000689(Epoch= 47)\n",
      "train loss=0.6141808013026817(Epoch= 47)\n",
      "CV loss=0.6323116625890747(Epoch= 48)\n",
      "train loss=0.6120054167444655(Epoch= 48)\n",
      "CV loss=0.6305489147933608(Epoch= 49)\n",
      "train loss=0.6097880565178256(Epoch= 49)\n",
      "CV loss=0.6280679956799725(Epoch= 50)\n",
      "train loss=0.6073744893241181(Epoch= 50)\n",
      "CV loss=0.6257986728041872(Epoch= 51)\n",
      "train loss=0.6052353571297046(Epoch= 51)\n",
      "CV loss=0.6233656969081761(Epoch= 52)\n",
      "train loss=0.60331097516025(Epoch= 52)\n",
      "CV loss=0.6219288930671248(Epoch= 53)\n",
      "train loss=0.6013054028244251(Epoch= 53)\n",
      "CV loss=0.620253315970927(Epoch= 54)\n",
      "train loss=0.5992856895672576(Epoch= 54)\n",
      "CV loss=0.6179959620795227(Epoch= 55)\n",
      "train loss=0.5976264585364759(Epoch= 55)\n",
      "CV loss=0.6163736386814529(Epoch= 56)\n",
      "train loss=0.5952387557222905(Epoch= 56)\n",
      "CV loss=0.6153541337753726(Epoch= 57)\n",
      "train loss=0.5933461802976846(Epoch= 57)\n",
      "CV loss=0.6126885190451625(Epoch= 58)\n",
      "train loss=0.5913193169593944(Epoch= 58)\n",
      "CV loss=0.6115034072858407(Epoch= 59)\n",
      "train loss=0.5896829253410102(Epoch= 59)\n",
      "CV loss=0.609778195179602(Epoch= 60)\n",
      "train loss=0.5878045224006283(Epoch= 60)\n",
      "CV loss=0.6084160664258221(Epoch= 61)\n",
      "train loss=0.5860626507203284(Epoch= 61)\n",
      "CV loss=0.6070931147042558(Epoch= 62)\n",
      "train loss=0.5844697904506879(Epoch= 62)\n",
      "CV loss=0.6047648815545704(Epoch= 63)\n",
      "train loss=0.582593156913868(Epoch= 63)\n",
      "CV loss=0.6034411609639896(Epoch= 64)\n",
      "train loss=0.581070778094519(Epoch= 64)\n",
      "CV loss=0.6016266674581414(Epoch= 65)\n",
      "train loss=0.5790861097273281(Epoch= 65)\n",
      "CV loss=0.6000813732684186(Epoch= 66)\n",
      "train loss=0.5775609615179625(Epoch= 66)\n",
      "CV loss=0.598683471226084(Epoch= 67)\n",
      "train loss=0.5759805681169161(Epoch= 67)\n",
      "CV loss=0.597827671177494(Epoch= 68)\n",
      "train loss=0.5743125205882257(Epoch= 68)\n",
      "CV loss=0.5957405950486014(Epoch= 69)\n",
      "train loss=0.5730385873376318(Epoch= 69)\n",
      "CV loss=0.5949783090624685(Epoch= 70)\n",
      "train loss=0.5711642134357497(Epoch= 70)\n",
      "CV loss=0.5939173558755705(Epoch= 71)\n",
      "train loss=0.5699674764666386(Epoch= 71)\n",
      "CV loss=0.5913645886618659(Epoch= 72)\n",
      "train loss=0.5683657640706401(Epoch= 72)\n",
      "CV loss=0.5913523681341484(Epoch= 73)\n",
      "train loss=0.5670064533336538(Epoch= 73)\n",
      "CV loss=0.5892465786594945(Epoch= 74)\n",
      "train loss=0.5652741750470477(Epoch= 74)\n",
      "CV loss=0.5874589381440762(Epoch= 75)\n",
      "train loss=0.5641140567538867(Epoch= 75)\n",
      "CV loss=0.5869028096755197(Epoch= 76)\n",
      "train loss=0.5624898449317683(Epoch= 76)\n",
      "CV loss=0.585294208232204(Epoch= 77)\n",
      "train loss=0.561077889141974(Epoch= 77)\n",
      "CV loss=0.5843139213859914(Epoch= 78)\n",
      "train loss=0.5595364123702683(Epoch= 78)\n",
      "CV loss=0.5828218659480889(Epoch= 79)\n",
      "train loss=0.5580125742779302(Epoch= 79)\n",
      "CV loss=0.5817053410229431(Epoch= 80)\n",
      "train loss=0.5567767604770272(Epoch= 80)\n",
      "CV loss=0.5812465235164814(Epoch= 81)\n",
      "train loss=0.5555965159270522(Epoch= 81)\n",
      "CV loss=0.5798865069018055(Epoch= 82)\n",
      "train loss=0.554238531823425(Epoch= 82)\n",
      "CV loss=0.5782883287482701(Epoch= 83)\n",
      "train loss=0.5528136231690179(Epoch= 83)\n",
      "CV loss=0.5770320534462192(Epoch= 84)\n",
      "train loss=0.5514508688646035(Epoch= 84)\n",
      "CV loss=0.5759788351734999(Epoch= 85)\n",
      "train loss=0.55018112056077(Epoch= 85)\n",
      "CV loss=0.5749430597028948(Epoch= 86)\n",
      "train loss=0.5491605979796514(Epoch= 86)\n",
      "CV loss=0.573605222513365(Epoch= 87)\n",
      "train loss=0.5476455656923023(Epoch= 87)\n",
      "CV loss=0.5729031928937742(Epoch= 88)\n",
      "train loss=0.5465493450158179(Epoch= 88)\n",
      "CV loss=0.5721169523806166(Epoch= 89)\n",
      "train loss=0.5454065879388301(Epoch= 89)\n",
      "CV loss=0.5706386323500068(Epoch= 90)\n",
      "train loss=0.5441896262626782(Epoch= 90)\n",
      "CV loss=0.5696022123919376(Epoch= 91)\n",
      "train loss=0.5428349374221408(Epoch= 91)\n",
      "CV loss=0.5685829526643537(Epoch= 92)\n",
      "train loss=0.5417013492659777(Epoch= 92)\n",
      "CV loss=0.567671608985419(Epoch= 93)\n",
      "train loss=0.5407495618336253(Epoch= 93)\n",
      "CV loss=0.5664171306424897(Epoch= 94)\n",
      "train loss=0.5393070150171355(Epoch= 94)\n",
      "CV loss=0.5654069526313124(Epoch= 95)\n",
      "train loss=0.538300371654566(Epoch= 95)\n",
      "CV loss=0.5643138752006844(Epoch= 96)\n",
      "train loss=0.5369743190925257(Epoch= 96)\n",
      "CV loss=0.5634764487263315(Epoch= 97)\n",
      "train loss=0.5360825565268122(Epoch= 97)\n",
      "CV loss=0.5636225968725241(Epoch= 98)\n",
      "train loss=0.5353447324210493(Epoch= 98)\n",
      "CV loss=0.5620131636676494(Epoch= 99)\n",
      "train loss=0.5340304324982434(Epoch= 99)\n",
      "CV loss=0.5611762142798646(Epoch= 100)\n",
      "train loss=0.5327268229606753(Epoch= 100)\n",
      "CV loss=0.5593931920144171(Epoch= 101)\n",
      "train loss=0.5317438111120858(Epoch= 101)\n",
      "CV loss=0.5592866421823376(Epoch= 102)\n",
      "train loss=0.5305873997625765(Epoch= 102)\n",
      "CV loss=0.5581369944275603(Epoch= 103)\n",
      "train loss=0.5294029644311788(Epoch= 103)\n",
      "CV loss=0.5581496238672861(Epoch= 104)\n",
      "train loss=0.5287795301836907(Epoch= 104)\n",
      "CV loss=0.5564386489978783(Epoch= 105)\n",
      "train loss=0.5274736602443385(Epoch= 105)\n",
      "CV loss=0.5553692215270485(Epoch= 106)\n",
      "train loss=0.5263896225398647(Epoch= 106)\n",
      "CV loss=0.5543247907875272(Epoch= 107)\n",
      "train loss=0.5253528012570982(Epoch= 107)\n",
      "CV loss=0.5532265483145099(Epoch= 108)\n",
      "train loss=0.5244792233558631(Epoch= 108)\n",
      "CV loss=0.5527044882849683(Epoch= 109)\n",
      "train loss=0.5231316089538709(Epoch= 109)\n",
      "CV loss=0.5517545457383582(Epoch= 110)\n",
      "train loss=0.5223835960949852(Epoch= 110)\n",
      "CV loss=0.5512704718236541(Epoch= 111)\n",
      "train loss=0.5213542607384716(Epoch= 111)\n",
      "CV loss=0.5508274331194596(Epoch= 112)\n",
      "train loss=0.520412856570805(Epoch= 112)\n",
      "CV loss=0.5494083786522528(Epoch= 113)\n",
      "train loss=0.5191927951344106(Epoch= 113)\n",
      "CV loss=0.5493678791045006(Epoch= 114)\n",
      "train loss=0.5186704870079597(Epoch= 114)\n",
      "CV loss=0.548207875088402(Epoch= 115)\n",
      "train loss=0.5174728510777037(Epoch= 115)\n",
      "CV loss=0.5473727877104965(Epoch= 116)\n",
      "train loss=0.5166867615190264(Epoch= 116)\n",
      "CV loss=0.5458980154587947(Epoch= 117)\n",
      "train loss=0.5155050076791755(Epoch= 117)\n",
      "CV loss=0.5458826353117151(Epoch= 118)\n",
      "train loss=0.5146944692376801(Epoch= 118)\n",
      "CV loss=0.5445000326224264(Epoch= 119)\n",
      "train loss=0.5135917682140976(Epoch= 119)\n",
      "CV loss=0.5440608219605474(Epoch= 120)\n",
      "train loss=0.5127803160079972(Epoch= 120)\n",
      "CV loss=0.5430028646505531(Epoch= 121)\n",
      "train loss=0.5119263826079912(Epoch= 121)\n",
      "CV loss=0.5425546446431513(Epoch= 122)\n",
      "train loss=0.5110222399051338(Epoch= 122)\n",
      "CV loss=0.5422355365731826(Epoch= 123)\n",
      "train loss=0.5102710023978456(Epoch= 123)\n",
      "CV loss=0.5414736882477846(Epoch= 124)\n",
      "train loss=0.5094841527936799(Epoch= 124)\n",
      "CV loss=0.5401557158434934(Epoch= 125)\n",
      "train loss=0.5083353893757447(Epoch= 125)\n",
      "CV loss=0.5396724417068218(Epoch= 126)\n",
      "train loss=0.5077294052469989(Epoch= 126)\n",
      "CV loss=0.5390416130396297(Epoch= 127)\n",
      "train loss=0.5067077595864743(Epoch= 127)\n",
      "CV loss=0.5385070562923324(Epoch= 128)\n",
      "train loss=0.5060036654780672(Epoch= 128)\n",
      "CV loss=0.5374263066706686(Epoch= 129)\n",
      "train loss=0.5047484857120652(Epoch= 129)\n",
      "CV loss=0.5371159063341736(Epoch= 130)\n",
      "train loss=0.5040218257746775(Epoch= 130)\n",
      "CV loss=0.5361203177268087(Epoch= 131)\n",
      "train loss=0.5031398606620038(Epoch= 131)\n",
      "CV loss=0.535893181560585(Epoch= 132)\n",
      "train loss=0.5025631293974211(Epoch= 132)\n",
      "CV loss=0.5343116001908289(Epoch= 133)\n",
      "train loss=0.5014819136203779(Epoch= 133)\n",
      "CV loss=0.5338513468563758(Epoch= 134)\n",
      "train loss=0.5007475679565584(Epoch= 134)\n",
      "CV loss=0.5342332752640611(Epoch= 135)\n",
      "train loss=0.5000109458643156(Epoch= 135)\n",
      "CV loss=0.5326177729158796(Epoch= 136)\n",
      "train loss=0.49921309742792996(Epoch= 136)\n",
      "CV loss=0.5325441570890099(Epoch= 137)\n",
      "train loss=0.49821429202114437(Epoch= 137)\n",
      "CV loss=0.5315031067157003(Epoch= 138)\n",
      "train loss=0.4973794534211815(Epoch= 138)\n",
      "CV loss=0.5309077500863781(Epoch= 139)\n",
      "train loss=0.496618144532698(Epoch= 139)\n",
      "CV loss=0.5301292782667238(Epoch= 140)\n",
      "train loss=0.4957750541812328(Epoch= 140)\n",
      "CV loss=0.529678657239999(Epoch= 141)\n",
      "train loss=0.49508470448904696(Epoch= 141)\n",
      "CV loss=0.5288003924937448(Epoch= 142)\n",
      "train loss=0.4942126335055326(Epoch= 142)\n",
      "CV loss=0.5284183656640387(Epoch= 143)\n",
      "train loss=0.4934883942663(Epoch= 143)\n",
      "CV loss=0.5273769074110746(Epoch= 144)\n",
      "train loss=0.49266981878370864(Epoch= 144)\n",
      "CV loss=0.5271514883407581(Epoch= 145)\n",
      "train loss=0.49194478137090825(Epoch= 145)\n",
      "CV loss=0.5262631818650805(Epoch= 146)\n",
      "train loss=0.4911666408132186(Epoch= 146)\n",
      "CV loss=0.5261026050150206(Epoch= 147)\n",
      "train loss=0.4904920641436242(Epoch= 147)\n",
      "CV loss=0.5256066815108682(Epoch= 148)\n",
      "train loss=0.48997508161022774(Epoch= 148)\n",
      "CV loss=0.5250631280989084(Epoch= 149)\n",
      "train loss=0.4891049274589262(Epoch= 149)\n",
      "CV loss=0.5243393407168218(Epoch= 150)\n",
      "train loss=0.4882282402059632(Epoch= 150)\n",
      "CV loss=0.5233282014300175(Epoch= 151)\n",
      "train loss=0.4873717263321965(Epoch= 151)\n",
      "CV loss=0.5233616347013086(Epoch= 152)\n",
      "train loss=0.4869047218938675(Epoch= 152)\n",
      "CV loss=0.5225493420110493(Epoch= 153)\n",
      "train loss=0.48602438309365753(Epoch= 153)\n",
      "CV loss=0.5219237062225592(Epoch= 154)\n",
      "train loss=0.4853473944892645(Epoch= 154)\n",
      "CV loss=0.520687660432481(Epoch= 155)\n",
      "train loss=0.48452439221604193(Epoch= 155)\n",
      "CV loss=0.5203745300353124(Epoch= 156)\n",
      "train loss=0.4838303420664519(Epoch= 156)\n",
      "CV loss=0.5200743785679047(Epoch= 157)\n",
      "train loss=0.48340860815436926(Epoch= 157)\n",
      "CV loss=0.5203660435629649(Epoch= 158)\n",
      "train loss=0.48257120522633296(Epoch= 158)\n",
      "CV loss=0.5185059070624397(Epoch= 159)\n",
      "train loss=0.4819133885040846(Epoch= 159)\n",
      "CV loss=0.5183959913405742(Epoch= 160)\n",
      "train loss=0.48100979153604223(Epoch= 160)\n",
      "CV loss=0.5175782665267075(Epoch= 161)\n",
      "train loss=0.4804513790391007(Epoch= 161)\n",
      "CV loss=0.5178000171108074(Epoch= 162)\n",
      "train loss=0.4799600391869559(Epoch= 162)\n",
      "CV loss=0.5167796546498271(Epoch= 163)\n",
      "train loss=0.4791187178835632(Epoch= 163)\n",
      "CV loss=0.5158004969454644(Epoch= 164)\n",
      "train loss=0.4783145682345961(Epoch= 164)\n",
      "CV loss=0.5152281269944397(Epoch= 165)\n",
      "train loss=0.47760169096050104(Epoch= 165)\n",
      "CV loss=0.5149052420257331(Epoch= 166)\n",
      "train loss=0.47682058757324286(Epoch= 166)\n",
      "CV loss=0.5144343056319702(Epoch= 167)\n",
      "train loss=0.47629841300473125(Epoch= 167)\n",
      "CV loss=0.5143454428389362(Epoch= 168)\n",
      "train loss=0.4759255077108177(Epoch= 168)\n",
      "CV loss=0.513813154969152(Epoch= 169)\n",
      "train loss=0.474976683653101(Epoch= 169)\n",
      "CV loss=0.51321691403976(Epoch= 170)\n",
      "train loss=0.4743776875304243(Epoch= 170)\n",
      "CV loss=0.5123849761026139(Epoch= 171)\n",
      "train loss=0.4736038474989268(Epoch= 171)\n",
      "CV loss=0.5121730786849171(Epoch= 172)\n",
      "train loss=0.47304557469498015(Epoch= 172)\n",
      "CV loss=0.5110766272505773(Epoch= 173)\n",
      "train loss=0.4722931784637256(Epoch= 173)\n",
      "CV loss=0.5107833293299201(Epoch= 174)\n",
      "train loss=0.47180074925207466(Epoch= 174)\n",
      "CV loss=0.510582103769203(Epoch= 175)\n",
      "train loss=0.4710074617061305(Epoch= 175)\n",
      "CV loss=0.509815803688728(Epoch= 176)\n",
      "train loss=0.47039097922621187(Epoch= 176)\n",
      "CV loss=0.5095700447106871(Epoch= 177)\n",
      "train loss=0.4697727322131917(Epoch= 177)\n",
      "CV loss=0.5089696167251236(Epoch= 178)\n",
      "train loss=0.4691587888284323(Epoch= 178)\n",
      "CV loss=0.5085524021442072(Epoch= 179)\n",
      "train loss=0.46843385913582797(Epoch= 179)\n",
      "CV loss=0.5081817007200614(Epoch= 180)\n",
      "train loss=0.467862971277037(Epoch= 180)\n",
      "CV loss=0.5074375055730598(Epoch= 181)\n",
      "train loss=0.4672099724568129(Epoch= 181)\n",
      "CV loss=0.5066880556278831(Epoch= 182)\n",
      "train loss=0.46665798220815624(Epoch= 182)\n",
      "CV loss=0.5060191994117249(Epoch= 183)\n",
      "train loss=0.46595792792738305(Epoch= 183)\n",
      "CV loss=0.5059657975085092(Epoch= 184)\n",
      "train loss=0.46540746043409564(Epoch= 184)\n",
      "CV loss=0.5052795898560244(Epoch= 185)\n",
      "train loss=0.4648764623061262(Epoch= 185)\n",
      "CV loss=0.5053046608152781(Epoch= 186)\n",
      "train loss=0.4641654276850769(Epoch= 186)\n",
      "CV loss=0.5047981816784116(Epoch= 187)\n",
      "train loss=0.4636784441728835(Epoch= 187)\n",
      "CV loss=0.5043339686171514(Epoch= 188)\n",
      "train loss=0.46302107760225203(Epoch= 188)\n",
      "CV loss=0.50404006533878(Epoch= 189)\n",
      "train loss=0.46238022920922567(Epoch= 189)\n",
      "CV loss=0.5027608621119416(Epoch= 190)\n",
      "train loss=0.4617271410252995(Epoch= 190)\n",
      "CV loss=0.5030640343102106(Epoch= 191)\n",
      "train loss=0.4613289011041414(Epoch= 191)\n",
      "CV loss=0.5023246873414184(Epoch= 192)\n",
      "train loss=0.46059649652043333(Epoch= 192)\n",
      "CV loss=0.5017371398893102(Epoch= 193)\n",
      "train loss=0.45992202147377165(Epoch= 193)\n",
      "CV loss=0.5007211609703611(Epoch= 194)\n",
      "train loss=0.4593384367347656(Epoch= 194)\n",
      "CV loss=0.5005868672237882(Epoch= 195)\n",
      "train loss=0.4590539984883444(Epoch= 195)\n",
      "CV loss=0.5010641081221945(Epoch= 196)\n",
      "train loss=0.4584609658656149(Epoch= 196)\n",
      "CV loss=0.4994867593007239(Epoch= 197)\n",
      "train loss=0.45793978591204093(Epoch= 197)\n",
      "CV loss=0.4989466994256709(Epoch= 198)\n",
      "train loss=0.45702312613283996(Epoch= 198)\n",
      "CV loss=0.49859949948537075(Epoch= 199)\n",
      "train loss=0.4564706309864406(Epoch= 199)\n",
      "CV loss=0.49855184928028756(Epoch= 200)\n",
      "train loss=0.4558765374858993(Epoch= 200)\n",
      "CV loss=0.4981740289134493(Epoch= 201)\n",
      "train loss=0.4554167397937372(Epoch= 201)\n",
      "CV loss=0.4976108304896407(Epoch= 202)\n",
      "train loss=0.4547565479893334(Epoch= 202)\n",
      "CV loss=0.49710829189128014(Epoch= 203)\n",
      "train loss=0.4541569696746611(Epoch= 203)\n",
      "CV loss=0.49641327447361433(Epoch= 204)\n",
      "train loss=0.4536534523465125(Epoch= 204)\n",
      "CV loss=0.4963432653882022(Epoch= 205)\n",
      "train loss=0.45312408635451473(Epoch= 205)\n",
      "CV loss=0.4956824521013898(Epoch= 206)\n",
      "train loss=0.4525798160370876(Epoch= 206)\n",
      "CV loss=0.4959520614847705(Epoch= 207)\n",
      "train loss=0.452140965754624(Epoch= 207)\n",
      "CV loss=0.4946168448108259(Epoch= 208)\n",
      "train loss=0.4514358985592805(Epoch= 208)\n",
      "CV loss=0.49435742760301304(Epoch= 209)\n",
      "train loss=0.4509873018972924(Epoch= 209)\n",
      "CV loss=0.4935421592025396(Epoch= 210)\n",
      "train loss=0.450335995460738(Epoch= 210)\n",
      "CV loss=0.4936338286009627(Epoch= 211)\n",
      "train loss=0.44986782600087355(Epoch= 211)\n",
      "CV loss=0.49307431862024453(Epoch= 212)\n",
      "train loss=0.44927702446495993(Epoch= 212)\n",
      "CV loss=0.4930902231986751(Epoch= 213)\n",
      "train loss=0.4490450829687156(Epoch= 213)\n",
      "CV loss=0.49222832470589434(Epoch= 214)\n",
      "train loss=0.4481854957107716(Epoch= 214)\n",
      "CV loss=0.492446451044262(Epoch= 215)\n",
      "train loss=0.44779968171146317(Epoch= 215)\n",
      "CV loss=0.49127493220797197(Epoch= 216)\n",
      "train loss=0.4471091986946803(Epoch= 216)\n",
      "CV loss=0.4907324607920186(Epoch= 217)\n",
      "train loss=0.4465056172768613(Epoch= 217)\n",
      "CV loss=0.49037935592788373(Epoch= 218)\n",
      "train loss=0.4460402294207301(Epoch= 218)\n",
      "CV loss=0.490171263863482(Epoch= 219)\n",
      "train loss=0.44550984283024436(Epoch= 219)\n",
      "CV loss=0.4897182555908433(Epoch= 220)\n",
      "train loss=0.44499275530203913(Epoch= 220)\n",
      "CV loss=0.489523636193379(Epoch= 221)\n",
      "train loss=0.4444687264220741(Epoch= 221)\n",
      "CV loss=0.48880696843227467(Epoch= 222)\n",
      "train loss=0.4439517272407198(Epoch= 222)\n",
      "CV loss=0.48861396572896654(Epoch= 223)\n",
      "train loss=0.4433537385974654(Epoch= 223)\n",
      "CV loss=0.4878442052496488(Epoch= 224)\n",
      "train loss=0.44282606550403264(Epoch= 224)\n",
      "CV loss=0.4878738709637953(Epoch= 225)\n",
      "train loss=0.44242965283842794(Epoch= 225)\n",
      "CV loss=0.4877503153797018(Epoch= 226)\n",
      "train loss=0.4419577340301431(Epoch= 226)\n",
      "CV loss=0.48675489209734113(Epoch= 227)\n",
      "train loss=0.4414246197921781(Epoch= 227)\n",
      "CV loss=0.48640505999741124(Epoch= 228)\n",
      "train loss=0.44092673106392805(Epoch= 228)\n",
      "CV loss=0.4858383757508613(Epoch= 229)\n",
      "train loss=0.44056215672721466(Epoch= 229)\n",
      "CV loss=0.48551395943226144(Epoch= 230)\n",
      "train loss=0.4399244453016052(Epoch= 230)\n",
      "CV loss=0.4856053709199527(Epoch= 231)\n",
      "train loss=0.4394100325100271(Epoch= 231)\n",
      "CV loss=0.48521383718403593(Epoch= 232)\n",
      "train loss=0.4389825159405273(Epoch= 232)\n",
      "CV loss=0.4841747046777214(Epoch= 233)\n",
      "train loss=0.4383216157425835(Epoch= 233)\n",
      "CV loss=0.4839052923179038(Epoch= 234)\n",
      "train loss=0.43780662157441613(Epoch= 234)\n",
      "CV loss=0.48360324196036986(Epoch= 235)\n",
      "train loss=0.43756322467230946(Epoch= 235)\n",
      "CV loss=0.4836224503129207(Epoch= 236)\n",
      "train loss=0.4368865950458879(Epoch= 236)\n",
      "CV loss=0.4829564678192871(Epoch= 237)\n",
      "train loss=0.43634394691046585(Epoch= 237)\n",
      "CV loss=0.48251150717936475(Epoch= 238)\n",
      "train loss=0.43585764454620857(Epoch= 238)\n",
      "CV loss=0.4821242437932362(Epoch= 239)\n",
      "train loss=0.43540233857870775(Epoch= 239)\n",
      "CV loss=0.48146470855313583(Epoch= 240)\n",
      "train loss=0.4348357588075187(Epoch= 240)\n",
      "CV loss=0.4809569216035301(Epoch= 241)\n",
      "train loss=0.4344445942310485(Epoch= 241)\n",
      "CV loss=0.4806892938122795(Epoch= 242)\n",
      "train loss=0.4339248859841258(Epoch= 242)\n",
      "CV loss=0.48057008254004224(Epoch= 243)\n",
      "train loss=0.43352474069622265(Epoch= 243)\n",
      "CV loss=0.4801033633665598(Epoch= 244)\n",
      "train loss=0.43303594849752064(Epoch= 244)\n",
      "CV loss=0.47999213481137326(Epoch= 245)\n",
      "train loss=0.4325148253362994(Epoch= 245)\n",
      "CV loss=0.4793018534942314(Epoch= 246)\n",
      "train loss=0.43204222470855147(Epoch= 246)\n",
      "CV loss=0.4789019623526436(Epoch= 247)\n",
      "train loss=0.43160875086749073(Epoch= 247)\n",
      "CV loss=0.4788307015892429(Epoch= 248)\n",
      "train loss=0.4313592955958072(Epoch= 248)\n",
      "CV loss=0.4780145917966408(Epoch= 249)\n",
      "train loss=0.4307231847346589(Epoch= 249)\n",
      "CV loss=0.47798497739584234(Epoch= 250)\n",
      "train loss=0.4301691962944728(Epoch= 250)\n",
      "CV loss=0.4773536003700556(Epoch= 251)\n",
      "train loss=0.42969291408377375(Epoch= 251)\n",
      "CV loss=0.4772313352120368(Epoch= 252)\n",
      "train loss=0.42920180809354624(Epoch= 252)\n",
      "CV loss=0.47713102093325044(Epoch= 253)\n",
      "train loss=0.42883351205178005(Epoch= 253)\n",
      "CV loss=0.476141773266375(Epoch= 254)\n",
      "train loss=0.42823784218758876(Epoch= 254)\n",
      "CV loss=0.47624870759782856(Epoch= 255)\n",
      "train loss=0.42787699610683916(Epoch= 255)\n",
      "CV loss=0.47537711066881205(Epoch= 256)\n",
      "train loss=0.4274604061698648(Epoch= 256)\n",
      "CV loss=0.4755752512352097(Epoch= 257)\n",
      "train loss=0.42700362336363945(Epoch= 257)\n",
      "CV loss=0.47506555996322125(Epoch= 258)\n",
      "train loss=0.42654010974474277(Epoch= 258)\n",
      "CV loss=0.4744866649113361(Epoch= 259)\n",
      "train loss=0.42597778517359913(Epoch= 259)\n",
      "CV loss=0.4742398481224187(Epoch= 260)\n",
      "train loss=0.4254943043225461(Epoch= 260)\n",
      "CV loss=0.4738250012389097(Epoch= 261)\n",
      "train loss=0.42503167914172985(Epoch= 261)\n",
      "CV loss=0.4733791163824309(Epoch= 262)\n",
      "train loss=0.4245694254062266(Epoch= 262)\n",
      "CV loss=0.47306461658026694(Epoch= 263)\n",
      "train loss=0.4244331596166215(Epoch= 263)\n",
      "CV loss=0.47312216160034776(Epoch= 264)\n",
      "train loss=0.4239660906766125(Epoch= 264)\n",
      "CV loss=0.47290083777641634(Epoch= 265)\n",
      "train loss=0.42338781399781383(Epoch= 265)\n",
      "CV loss=0.4718687951435302(Epoch= 266)\n",
      "train loss=0.4229713987167012(Epoch= 266)\n",
      "CV loss=0.4712974932138193(Epoch= 267)\n",
      "train loss=0.4225213319584739(Epoch= 267)\n",
      "CV loss=0.4710428389265594(Epoch= 268)\n",
      "train loss=0.4219808216420143(Epoch= 268)\n",
      "CV loss=0.47085928878045513(Epoch= 269)\n",
      "train loss=0.42156450766112497(Epoch= 269)\n",
      "CV loss=0.4706271014022909(Epoch= 270)\n",
      "train loss=0.42106643154054985(Epoch= 270)\n",
      "CV loss=0.4703747289348293(Epoch= 271)\n",
      "train loss=0.4207028244372193(Epoch= 271)\n",
      "CV loss=0.4703229830914431(Epoch= 272)\n",
      "train loss=0.42038494937588233(Epoch= 272)\n",
      "CV loss=0.46937993501869746(Epoch= 273)\n",
      "train loss=0.41979517692997104(Epoch= 273)\n",
      "CV loss=0.46905966532330995(Epoch= 274)\n",
      "train loss=0.41936963146674405(Epoch= 274)\n",
      "CV loss=0.46889560583029166(Epoch= 275)\n",
      "train loss=0.41892729821192104(Epoch= 275)\n",
      "CV loss=0.4683033444227016(Epoch= 276)\n",
      "train loss=0.41856598298360204(Epoch= 276)\n",
      "CV loss=0.46800456365735355(Epoch= 277)\n",
      "train loss=0.41805381250912527(Epoch= 277)\n",
      "CV loss=0.46771016017673117(Epoch= 278)\n",
      "train loss=0.41757768144106155(Epoch= 278)\n",
      "CV loss=0.46748525260940743(Epoch= 279)\n",
      "train loss=0.41716590395947717(Epoch= 279)\n",
      "CV loss=0.4673072297326692(Epoch= 280)\n",
      "train loss=0.4167861323212999(Epoch= 280)\n",
      "CV loss=0.46675655106943353(Epoch= 281)\n",
      "train loss=0.4165282200356028(Epoch= 281)\n",
      "CV loss=0.4663784034915785(Epoch= 282)\n",
      "train loss=0.4159401065784499(Epoch= 282)\n",
      "CV loss=0.46610658877413125(Epoch= 283)\n",
      "train loss=0.415533617427132(Epoch= 283)\n",
      "CV loss=0.4656781566827507(Epoch= 284)\n",
      "train loss=0.41507201146512773(Epoch= 284)\n",
      "CV loss=0.46507454628580813(Epoch= 285)\n",
      "train loss=0.41480101872118214(Epoch= 285)\n",
      "CV loss=0.4654193369399723(Epoch= 286)\n",
      "train loss=0.4143214375085421(Epoch= 286)\n",
      "CV loss=0.4649401151250249(Epoch= 287)\n",
      "train loss=0.41390372601851777(Epoch= 287)\n",
      "CV loss=0.46471411085459124(Epoch= 288)\n",
      "train loss=0.41340177636424763(Epoch= 288)\n",
      "CV loss=0.4644493158455334(Epoch= 289)\n",
      "train loss=0.4130917909258019(Epoch= 289)\n",
      "CV loss=0.4636460278737237(Epoch= 290)\n",
      "train loss=0.41269139201974175(Epoch= 290)\n",
      "CV loss=0.4637080615015621(Epoch= 291)\n",
      "train loss=0.41218845036572593(Epoch= 291)\n",
      "CV loss=0.4636173893059786(Epoch= 292)\n",
      "train loss=0.41199904922929803(Epoch= 292)\n",
      "CV loss=0.4627510963599213(Epoch= 293)\n",
      "train loss=0.4113029153990728(Epoch= 293)\n",
      "CV loss=0.46253758444070314(Epoch= 294)\n",
      "train loss=0.41099149248058253(Epoch= 294)\n",
      "CV loss=0.46205126383150386(Epoch= 295)\n",
      "train loss=0.4105849993925257(Epoch= 295)\n",
      "CV loss=0.4617995011188691(Epoch= 296)\n",
      "train loss=0.4102181333857942(Epoch= 296)\n",
      "CV loss=0.4612421664056774(Epoch= 297)\n",
      "train loss=0.4097507754448138(Epoch= 297)\n",
      "CV loss=0.46078111128378607(Epoch= 298)\n",
      "train loss=0.40929616585216105(Epoch= 298)\n",
      "CV loss=0.46052466914137746(Epoch= 299)\n",
      "train loss=0.4089971758951708(Epoch= 299)\n",
      "CV loss=0.4601085194154695(Epoch= 300)\n",
      "train loss=0.40854105410876956(Epoch= 300)\n",
      "CV loss=0.4600300744909376(Epoch= 301)\n",
      "train loss=0.4081091888234725(Epoch= 301)\n",
      "CV loss=0.459969398642899(Epoch= 302)\n",
      "train loss=0.40775002573537733(Epoch= 302)\n",
      "CV loss=0.45950778252753455(Epoch= 303)\n",
      "train loss=0.4073028107899169(Epoch= 303)\n",
      "CV loss=0.4588976529930806(Epoch= 304)\n",
      "train loss=0.40692115728748035(Epoch= 304)\n",
      "CV loss=0.45868885933670883(Epoch= 305)\n",
      "train loss=0.40648924259749286(Epoch= 305)\n",
      "CV loss=0.4584669255181979(Epoch= 306)\n",
      "train loss=0.40612991224626943(Epoch= 306)\n",
      "CV loss=0.4584629004680491(Epoch= 307)\n",
      "train loss=0.4057211970646375(Epoch= 307)\n",
      "CV loss=0.45793649540598397(Epoch= 308)\n",
      "train loss=0.4054178572685523(Epoch= 308)\n",
      "CV loss=0.457341618030171(Epoch= 309)\n",
      "train loss=0.40489907725130975(Epoch= 309)\n",
      "CV loss=0.4570765372273177(Epoch= 310)\n",
      "train loss=0.40446761483084104(Epoch= 310)\n",
      "CV loss=0.4569993420063628(Epoch= 311)\n",
      "train loss=0.4041291189997279(Epoch= 311)\n",
      "CV loss=0.45630732508704075(Epoch= 312)\n",
      "train loss=0.4037960587804441(Epoch= 312)\n",
      "CV loss=0.45599234012295137(Epoch= 313)\n",
      "train loss=0.40339913189589954(Epoch= 313)\n",
      "CV loss=0.45559669168262973(Epoch= 314)\n",
      "train loss=0.402927649877515(Epoch= 314)\n",
      "CV loss=0.4553651477283691(Epoch= 315)\n",
      "train loss=0.4025729201824615(Epoch= 315)\n",
      "CV loss=0.45514254588630265(Epoch= 316)\n",
      "train loss=0.4022553271600414(Epoch= 316)\n",
      "CV loss=0.4546309585282935(Epoch= 317)\n",
      "train loss=0.40182572878440453(Epoch= 317)\n",
      "CV loss=0.4544531400010864(Epoch= 318)\n",
      "train loss=0.4014511657256551(Epoch= 318)\n",
      "CV loss=0.4543046266561687(Epoch= 319)\n",
      "train loss=0.40112062128551085(Epoch= 319)\n",
      "CV loss=0.4541488905262556(Epoch= 320)\n",
      "train loss=0.4007461917063978(Epoch= 320)\n",
      "CV loss=0.4534244443924478(Epoch= 321)\n",
      "train loss=0.40032038326985936(Epoch= 321)\n",
      "CV loss=0.4529901996170027(Epoch= 322)\n",
      "train loss=0.3999120053690837(Epoch= 322)\n",
      "CV loss=0.4529191006714345(Epoch= 323)\n",
      "train loss=0.3994950491637361(Epoch= 323)\n",
      "CV loss=0.452479361746371(Epoch= 324)\n",
      "train loss=0.3992491422762428(Epoch= 324)\n",
      "CV loss=0.4524663059129933(Epoch= 325)\n",
      "train loss=0.39883772459127464(Epoch= 325)\n",
      "CV loss=0.452048086467482(Epoch= 326)\n",
      "train loss=0.39839902716391296(Epoch= 326)\n",
      "CV loss=0.4516317739151776(Epoch= 327)\n",
      "train loss=0.3979923358964571(Epoch= 327)\n",
      "CV loss=0.4513284984454464(Epoch= 328)\n",
      "train loss=0.3976427987828357(Epoch= 328)\n",
      "CV loss=0.45125509971855055(Epoch= 329)\n",
      "train loss=0.39733888081834556(Epoch= 329)\n",
      "CV loss=0.45112433342708125(Epoch= 330)\n",
      "train loss=0.3969609332601271(Epoch= 330)\n",
      "CV loss=0.45062631304982576(Epoch= 331)\n",
      "train loss=0.39662335105499275(Epoch= 331)\n",
      "CV loss=0.45020349247766245(Epoch= 332)\n",
      "train loss=0.3961344927344312(Epoch= 332)\n",
      "CV loss=0.44960955135979347(Epoch= 333)\n",
      "train loss=0.3958656014647036(Epoch= 333)\n",
      "CV loss=0.44979479489042407(Epoch= 334)\n",
      "train loss=0.3954566901895134(Epoch= 334)\n",
      "CV loss=0.44966838279165666(Epoch= 335)\n",
      "train loss=0.39529448187550537(Epoch= 335)\n",
      "CV loss=0.4493496993453432(Epoch= 336)\n",
      "train loss=0.3947481484781007(Epoch= 336)\n",
      "CV loss=0.4485688025518515(Epoch= 337)\n",
      "train loss=0.39433550758044605(Epoch= 337)\n",
      "CV loss=0.4484112388343542(Epoch= 338)\n",
      "train loss=0.39394605511275455(Epoch= 338)\n",
      "CV loss=0.4481933194800378(Epoch= 339)\n",
      "train loss=0.3935878963573426(Epoch= 339)\n",
      "CV loss=0.4477834637276123(Epoch= 340)\n",
      "train loss=0.3932230491486248(Epoch= 340)\n",
      "CV loss=0.4477461145589778(Epoch= 341)\n",
      "train loss=0.3929571863553081(Epoch= 341)\n",
      "CV loss=0.4472330278951096(Epoch= 342)\n",
      "train loss=0.39252066108441125(Epoch= 342)\n",
      "CV loss=0.44713302281477707(Epoch= 343)\n",
      "train loss=0.39217377806470943(Epoch= 343)\n",
      "CV loss=0.44687726261323457(Epoch= 344)\n",
      "train loss=0.39184218679392396(Epoch= 344)\n",
      "CV loss=0.44649477082007316(Epoch= 345)\n",
      "train loss=0.3915001042993025(Epoch= 345)\n",
      "CV loss=0.4461044190991177(Epoch= 346)\n",
      "train loss=0.3910929988562637(Epoch= 346)\n",
      "CV loss=0.44577992759405743(Epoch= 347)\n",
      "train loss=0.39084359127686735(Epoch= 347)\n",
      "CV loss=0.4454089101250176(Epoch= 348)\n",
      "train loss=0.39035747864843173(Epoch= 348)\n",
      "CV loss=0.44524296012417264(Epoch= 349)\n",
      "train loss=0.390030126467174(Epoch= 349)\n",
      "CV loss=0.44496948836898476(Epoch= 350)\n",
      "train loss=0.3896592354275161(Epoch= 350)\n",
      "CV loss=0.4445503292330717(Epoch= 351)\n",
      "train loss=0.3893382421326268(Epoch= 351)\n",
      "CV loss=0.44433530354009465(Epoch= 352)\n",
      "train loss=0.38894303409415376(Epoch= 352)\n",
      "CV loss=0.44370828342025254(Epoch= 353)\n",
      "train loss=0.388647859541907(Epoch= 353)\n",
      "CV loss=0.443738176496076(Epoch= 354)\n",
      "train loss=0.3882397977605955(Epoch= 354)\n",
      "CV loss=0.443419641843126(Epoch= 355)\n",
      "train loss=0.3879398211928374(Epoch= 355)\n",
      "CV loss=0.44315597890228287(Epoch= 356)\n",
      "train loss=0.38755650064559977(Epoch= 356)\n",
      "CV loss=0.44248462728659793(Epoch= 357)\n",
      "train loss=0.38724526290047023(Epoch= 357)\n",
      "CV loss=0.44273964159200835(Epoch= 358)\n",
      "train loss=0.3870167986492728(Epoch= 358)\n",
      "CV loss=0.44228979918044337(Epoch= 359)\n",
      "train loss=0.38651608873506826(Epoch= 359)\n",
      "CV loss=0.4416663161054447(Epoch= 360)\n",
      "train loss=0.3862268721105422(Epoch= 360)\n",
      "CV loss=0.44136584654138916(Epoch= 361)\n",
      "train loss=0.38586540503301164(Epoch= 361)\n",
      "CV loss=0.4412883323875525(Epoch= 362)\n",
      "train loss=0.38549368469795986(Epoch= 362)\n",
      "CV loss=0.44122216245062645(Epoch= 363)\n",
      "train loss=0.3851607646765375(Epoch= 363)\n",
      "CV loss=0.4406428840907932(Epoch= 364)\n",
      "train loss=0.3848355179164407(Epoch= 364)\n",
      "CV loss=0.4406410023687802(Epoch= 365)\n",
      "train loss=0.38447717384125046(Epoch= 365)\n",
      "CV loss=0.4400302946674506(Epoch= 366)\n",
      "train loss=0.3841098291913553(Epoch= 366)\n",
      "CV loss=0.44001553147840144(Epoch= 367)\n",
      "train loss=0.3837990061221135(Epoch= 367)\n",
      "CV loss=0.43931809461069643(Epoch= 368)\n",
      "train loss=0.3835098217481492(Epoch= 368)\n",
      "CV loss=0.43895075647939624(Epoch= 369)\n",
      "train loss=0.38311713061274005(Epoch= 369)\n",
      "CV loss=0.43934689334888366(Epoch= 370)\n",
      "train loss=0.3828677979564748(Epoch= 370)\n",
      "CV loss=0.4388997604192809(Epoch= 371)\n",
      "train loss=0.38244091578367845(Epoch= 371)\n",
      "CV loss=0.43853824176374767(Epoch= 372)\n",
      "train loss=0.3820459730742586(Epoch= 372)\n",
      "CV loss=0.4378123053025649(Epoch= 373)\n",
      "train loss=0.381760868455876(Epoch= 373)\n",
      "CV loss=0.43822410715108145(Epoch= 374)\n",
      "train loss=0.3814717630712701(Epoch= 374)\n",
      "CV loss=0.43728688349003314(Epoch= 375)\n",
      "train loss=0.3811008082206453(Epoch= 375)\n",
      "CV loss=0.4370904180986471(Epoch= 376)\n",
      "train loss=0.38076212159308837(Epoch= 376)\n",
      "CV loss=0.4369855174218171(Epoch= 377)\n",
      "train loss=0.38036912466157496(Epoch= 377)\n",
      "CV loss=0.43651385874321447(Epoch= 378)\n",
      "train loss=0.38010154281944786(Epoch= 378)\n",
      "CV loss=0.4365028170191849(Epoch= 379)\n",
      "train loss=0.3797708521434856(Epoch= 379)\n",
      "CV loss=0.43598594212285136(Epoch= 380)\n",
      "train loss=0.3794454371274772(Epoch= 380)\n",
      "CV loss=0.4357180805103767(Epoch= 381)\n",
      "train loss=0.3790847558135257(Epoch= 381)\n",
      "CV loss=0.43541958422663085(Epoch= 382)\n",
      "train loss=0.3787557305981767(Epoch= 382)\n",
      "CV loss=0.4350937208611(Epoch= 383)\n",
      "train loss=0.3783761039079643(Epoch= 383)\n",
      "CV loss=0.43561691960860943(Epoch= 384)\n",
      "train loss=0.3782750137472545(Epoch= 384)\n",
      "CV loss=0.4349639475792353(Epoch= 385)\n",
      "train loss=0.3777728243278791(Epoch= 385)\n",
      "CV loss=0.4347979373282952(Epoch= 386)\n",
      "train loss=0.3774666028963486(Epoch= 386)\n",
      "CV loss=0.43418937875591845(Epoch= 387)\n",
      "train loss=0.3771029865247974(Epoch= 387)\n",
      "CV loss=0.43359068058347705(Epoch= 388)\n",
      "train loss=0.37688128582186514(Epoch= 388)\n",
      "CV loss=0.4338047363331508(Epoch= 389)\n",
      "train loss=0.37644345499610793(Epoch= 389)\n",
      "CV loss=0.43318127291216985(Epoch= 390)\n",
      "train loss=0.3760943250725051(Epoch= 390)\n",
      "CV loss=0.4328857643306101(Epoch= 391)\n",
      "train loss=0.3759073878663884(Epoch= 391)\n",
      "CV loss=0.43271955845820065(Epoch= 392)\n",
      "train loss=0.3754505503710469(Epoch= 392)\n",
      "CV loss=0.43223760408887424(Epoch= 393)\n",
      "train loss=0.37515330601989505(Epoch= 393)\n",
      "CV loss=0.43219957527392994(Epoch= 394)\n",
      "train loss=0.37482608239114507(Epoch= 394)\n",
      "CV loss=0.4320274929425904(Epoch= 395)\n",
      "train loss=0.3745009436512407(Epoch= 395)\n",
      "CV loss=0.4316742863925789(Epoch= 396)\n",
      "train loss=0.3742171588620275(Epoch= 396)\n",
      "CV loss=0.4314481482005804(Epoch= 397)\n",
      "train loss=0.3738965241163026(Epoch= 397)\n",
      "CV loss=0.4311738305533382(Epoch= 398)\n",
      "train loss=0.3735189203312462(Epoch= 398)\n",
      "CV loss=0.4308489360165857(Epoch= 399)\n",
      "train loss=0.3731738104435535(Epoch= 399)\n",
      "CV loss=0.430355534479184(Epoch= 400)\n",
      "train loss=0.372885109460501(Epoch= 400)\n",
      "CV loss=0.430380193163584(Epoch= 401)\n",
      "train loss=0.3725824142143823(Epoch= 401)\n",
      "CV loss=0.42985213080379864(Epoch= 402)\n",
      "train loss=0.3722237849937973(Epoch= 402)\n",
      "CV loss=0.4295999834268234(Epoch= 403)\n",
      "train loss=0.3719069421966447(Epoch= 403)\n",
      "CV loss=0.4292716982097025(Epoch= 404)\n",
      "train loss=0.3716389858118382(Epoch= 404)\n",
      "CV loss=0.42895358297937386(Epoch= 405)\n",
      "train loss=0.3712804616678976(Epoch= 405)\n",
      "CV loss=0.42871070380946114(Epoch= 406)\n",
      "train loss=0.37099141740715363(Epoch= 406)\n",
      "CV loss=0.42840337321412825(Epoch= 407)\n",
      "train loss=0.3706832810723543(Epoch= 407)\n",
      "CV loss=0.4281735644572537(Epoch= 408)\n",
      "train loss=0.37034911848151353(Epoch= 408)\n",
      "CV loss=0.42794804229223016(Epoch= 409)\n",
      "train loss=0.37000847359355843(Epoch= 409)\n",
      "CV loss=0.4278884644585511(Epoch= 410)\n",
      "train loss=0.3698290127418245(Epoch= 410)\n",
      "CV loss=0.4274173974615281(Epoch= 411)\n",
      "train loss=0.36939299017499955(Epoch= 411)\n",
      "CV loss=0.4270836106096102(Epoch= 412)\n",
      "train loss=0.3690708604966686(Epoch= 412)\n",
      "CV loss=0.4270614396950527(Epoch= 413)\n",
      "train loss=0.3687969726132049(Epoch= 413)\n",
      "CV loss=0.42654664176914275(Epoch= 414)\n",
      "train loss=0.36850199904847897(Epoch= 414)\n",
      "CV loss=0.4265641650782247(Epoch= 415)\n",
      "train loss=0.3681702232692255(Epoch= 415)\n",
      "CV loss=0.42634670888824455(Epoch= 416)\n",
      "train loss=0.3678514530454881(Epoch= 416)\n",
      "CV loss=0.42618211400761785(Epoch= 417)\n",
      "train loss=0.3676209004656856(Epoch= 417)\n",
      "CV loss=0.4255430103942561(Epoch= 418)\n",
      "train loss=0.3672205890639526(Epoch= 418)\n",
      "CV loss=0.4253698746862611(Epoch= 419)\n",
      "train loss=0.36695709811336286(Epoch= 419)\n",
      "CV loss=0.4249888105109378(Epoch= 420)\n",
      "train loss=0.36663477679127776(Epoch= 420)\n",
      "CV loss=0.4251231906775374(Epoch= 421)\n",
      "train loss=0.3663253614951589(Epoch= 421)\n",
      "CV loss=0.42463353151150757(Epoch= 422)\n",
      "train loss=0.3660077917846263(Epoch= 422)\n",
      "CV loss=0.424285426867527(Epoch= 423)\n",
      "train loss=0.3656800128314161(Epoch= 423)\n",
      "CV loss=0.42399254672260034(Epoch= 424)\n",
      "train loss=0.36537047692771235(Epoch= 424)\n",
      "CV loss=0.423577148649947(Epoch= 425)\n",
      "train loss=0.3650880009462665(Epoch= 425)\n",
      "CV loss=0.42342466585083105(Epoch= 426)\n",
      "train loss=0.3647474603962512(Epoch= 426)\n",
      "CV loss=0.4232549482237807(Epoch= 427)\n",
      "train loss=0.3645045698106865(Epoch= 427)\n",
      "CV loss=0.4227737980290981(Epoch= 428)\n",
      "train loss=0.36415741138586905(Epoch= 428)\n",
      "CV loss=0.42259117806616603(Epoch= 429)\n",
      "train loss=0.3638505406977436(Epoch= 429)\n",
      "CV loss=0.422446677036087(Epoch= 430)\n",
      "train loss=0.3635489836818752(Epoch= 430)\n",
      "CV loss=0.4219767836179711(Epoch= 431)\n",
      "train loss=0.36326549132540437(Epoch= 431)\n",
      "CV loss=0.4219776841600792(Epoch= 432)\n",
      "train loss=0.3629631248099644(Epoch= 432)\n",
      "CV loss=0.4215222628525359(Epoch= 433)\n",
      "train loss=0.3626575635850423(Epoch= 433)\n",
      "CV loss=0.42131590076695113(Epoch= 434)\n",
      "train loss=0.3623508767649803(Epoch= 434)\n",
      "CV loss=0.4211821152888605(Epoch= 435)\n",
      "train loss=0.3620423188290272(Epoch= 435)\n",
      "CV loss=0.4205426749306669(Epoch= 436)\n",
      "train loss=0.3617763383269057(Epoch= 436)\n",
      "CV loss=0.420395565793003(Epoch= 437)\n",
      "train loss=0.3614434784595462(Epoch= 437)\n",
      "CV loss=0.420201693640124(Epoch= 438)\n",
      "train loss=0.3611385855480863(Epoch= 438)\n",
      "CV loss=0.41991420600334595(Epoch= 439)\n",
      "train loss=0.36085186411461334(Epoch= 439)\n",
      "CV loss=0.41996398569429977(Epoch= 440)\n",
      "train loss=0.3605610646419504(Epoch= 440)\n",
      "CV loss=0.41930713383742585(Epoch= 441)\n",
      "train loss=0.36026215256470856(Epoch= 441)\n",
      "CV loss=0.41909842411214226(Epoch= 442)\n",
      "train loss=0.3599880186826536(Epoch= 442)\n",
      "CV loss=0.4191365130120812(Epoch= 443)\n",
      "train loss=0.35969406143783045(Epoch= 443)\n",
      "CV loss=0.4187822042001812(Epoch= 444)\n",
      "train loss=0.35938738550721916(Epoch= 444)\n",
      "CV loss=0.4185510842349768(Epoch= 445)\n",
      "train loss=0.359077769948034(Epoch= 445)\n",
      "CV loss=0.41829770908978825(Epoch= 446)\n",
      "train loss=0.3588286285106979(Epoch= 446)\n",
      "CV loss=0.41780803603296574(Epoch= 447)\n",
      "train loss=0.3584611793315916(Epoch= 447)\n",
      "CV loss=0.41741898542822603(Epoch= 448)\n",
      "train loss=0.35820697749747493(Epoch= 448)\n",
      "CV loss=0.4173434672071482(Epoch= 449)\n",
      "train loss=0.35792228363694795(Epoch= 449)\n",
      "CV loss=0.4169352162733557(Epoch= 450)\n",
      "train loss=0.3576114325788323(Epoch= 450)\n",
      "CV loss=0.4171063044879735(Epoch= 451)\n",
      "train loss=0.35732866933843294(Epoch= 451)\n",
      "CV loss=0.4165858264776101(Epoch= 452)\n",
      "train loss=0.35702812719437155(Epoch= 452)\n",
      "CV loss=0.4163712109465897(Epoch= 453)\n",
      "train loss=0.3567276180565433(Epoch= 453)\n",
      "CV loss=0.4159024373054641(Epoch= 454)\n",
      "train loss=0.3564332761814131(Epoch= 454)\n",
      "CV loss=0.41582999868960907(Epoch= 455)\n",
      "train loss=0.35612637521049256(Epoch= 455)\n",
      "CV loss=0.4153649317832415(Epoch= 456)\n",
      "train loss=0.355885596409113(Epoch= 456)\n",
      "CV loss=0.41541542942301873(Epoch= 457)\n",
      "train loss=0.35554263337376873(Epoch= 457)\n",
      "CV loss=0.4149465199174832(Epoch= 458)\n",
      "train loss=0.3552593656324632(Epoch= 458)\n",
      "CV loss=0.4147689163400661(Epoch= 459)\n",
      "train loss=0.3550145877306492(Epoch= 459)\n",
      "CV loss=0.4145644507589109(Epoch= 460)\n",
      "train loss=0.35478309075450937(Epoch= 460)\n",
      "CV loss=0.4143143647798494(Epoch= 461)\n",
      "train loss=0.3544167826533399(Epoch= 461)\n",
      "CV loss=0.41403391146633006(Epoch= 462)\n",
      "train loss=0.35416156566890133(Epoch= 462)\n",
      "CV loss=0.41358646931041754(Epoch= 463)\n",
      "train loss=0.3538271785888196(Epoch= 463)\n",
      "CV loss=0.4133130835627651(Epoch= 464)\n",
      "train loss=0.35356614747422466(Epoch= 464)\n",
      "CV loss=0.41289115515381813(Epoch= 465)\n",
      "train loss=0.3532638379646732(Epoch= 465)\n",
      "CV loss=0.412982273437531(Epoch= 466)\n",
      "train loss=0.3529989170611052(Epoch= 466)\n",
      "CV loss=0.41268850573824767(Epoch= 467)\n",
      "train loss=0.35271403102160104(Epoch= 467)\n",
      "CV loss=0.4129144065852895(Epoch= 468)\n",
      "train loss=0.3524715305107538(Epoch= 468)\n",
      "CV loss=0.4122043209637061(Epoch= 469)\n",
      "train loss=0.3521338735571451(Epoch= 469)\n",
      "CV loss=0.41191278032876527(Epoch= 470)\n",
      "train loss=0.35185136000979045(Epoch= 470)\n",
      "CV loss=0.4116978151173666(Epoch= 471)\n",
      "train loss=0.3515453776383069(Epoch= 471)\n",
      "CV loss=0.41146554726745527(Epoch= 472)\n",
      "train loss=0.35131810113564377(Epoch= 472)\n",
      "CV loss=0.411335475420318(Epoch= 473)\n",
      "train loss=0.3510133308831729(Epoch= 473)\n",
      "CV loss=0.4107589546165493(Epoch= 474)\n",
      "train loss=0.3506997046930904(Epoch= 474)\n",
      "CV loss=0.41086565022081695(Epoch= 475)\n",
      "train loss=0.35043262323690616(Epoch= 475)\n",
      "CV loss=0.4103589822255418(Epoch= 476)\n",
      "train loss=0.3501384958223756(Epoch= 476)\n",
      "CV loss=0.4100812648149361(Epoch= 477)\n",
      "train loss=0.3498794711211455(Epoch= 477)\n",
      "CV loss=0.4098496123068835(Epoch= 478)\n",
      "train loss=0.34956246403602687(Epoch= 478)\n",
      "CV loss=0.40997186318212375(Epoch= 479)\n",
      "train loss=0.3493192686180276(Epoch= 479)\n",
      "CV loss=0.40921076707384674(Epoch= 480)\n",
      "train loss=0.3490286064873836(Epoch= 480)\n",
      "CV loss=0.4093773810721002(Epoch= 481)\n",
      "train loss=0.34874544163550775(Epoch= 481)\n",
      "CV loss=0.40889847806872137(Epoch= 482)\n",
      "train loss=0.34851062259736265(Epoch= 482)\n",
      "CV loss=0.4082704734788265(Epoch= 483)\n",
      "train loss=0.34825843402203926(Epoch= 483)\n",
      "CV loss=0.4086770574174667(Epoch= 484)\n",
      "train loss=0.34794056905818227(Epoch= 484)\n",
      "CV loss=0.4079793822965443(Epoch= 485)\n",
      "train loss=0.3476984550909401(Epoch= 485)\n",
      "CV loss=0.4078829456389372(Epoch= 486)\n",
      "train loss=0.3474109490499691(Epoch= 486)\n",
      "CV loss=0.40761404839069054(Epoch= 487)\n",
      "train loss=0.34709996675459726(Epoch= 487)\n",
      "CV loss=0.4074371718373(Epoch= 488)\n",
      "train loss=0.3468127666192956(Epoch= 488)\n",
      "CV loss=0.40722492977548863(Epoch= 489)\n",
      "train loss=0.3465230831378722(Epoch= 489)\n",
      "CV loss=0.4069128634067972(Epoch= 490)\n",
      "train loss=0.34623245092715976(Epoch= 490)\n",
      "CV loss=0.4064769453829882(Epoch= 491)\n",
      "train loss=0.34600616290219627(Epoch= 491)\n",
      "CV loss=0.40662587200388556(Epoch= 492)\n",
      "train loss=0.3457310000929936(Epoch= 492)\n",
      "CV loss=0.4061563735317645(Epoch= 493)\n",
      "train loss=0.34544214217366215(Epoch= 493)\n",
      "CV loss=0.4058545911919018(Epoch= 494)\n",
      "train loss=0.34518720082192(Epoch= 494)\n",
      "CV loss=0.4056326271105592(Epoch= 495)\n",
      "train loss=0.34485490273239294(Epoch= 495)\n",
      "CV loss=0.4054026458320102(Epoch= 496)\n",
      "train loss=0.34460031255096535(Epoch= 496)\n",
      "CV loss=0.405323815165344(Epoch= 497)\n",
      "train loss=0.34431753308476215(Epoch= 497)\n",
      "CV loss=0.40496529960831074(Epoch= 498)\n",
      "train loss=0.3440986333926503(Epoch= 498)\n",
      "CV loss=0.4046515753213825(Epoch= 499)\n",
      "train loss=0.3437804258828634(Epoch= 499)\n",
      "CV loss=0.40416139939692114(Epoch= 500)\n",
      "train loss=0.34351370628230604(Epoch= 500)\n",
      "CV loss=0.40445040430182244(Epoch= 501)\n",
      "train loss=0.3433028411448064(Epoch= 501)\n",
      "CV loss=0.4041403693135695(Epoch= 502)\n",
      "train loss=0.34298231780424626(Epoch= 502)\n",
      "CV loss=0.40382954949448613(Epoch= 503)\n",
      "train loss=0.34271737094588545(Epoch= 503)\n",
      "CV loss=0.4034003940020439(Epoch= 504)\n",
      "train loss=0.3424816004627453(Epoch= 504)\n",
      "CV loss=0.40337100839179335(Epoch= 505)\n",
      "train loss=0.3421614986069497(Epoch= 505)\n",
      "CV loss=0.4030464053172355(Epoch= 506)\n",
      "train loss=0.34195869385815586(Epoch= 506)\n",
      "CV loss=0.402655519887526(Epoch= 507)\n",
      "train loss=0.34164770117287707(Epoch= 507)\n",
      "CV loss=0.4023910100714817(Epoch= 508)\n",
      "train loss=0.3413475900131113(Epoch= 508)\n",
      "CV loss=0.40225152559206556(Epoch= 509)\n",
      "train loss=0.34109383998169646(Epoch= 509)\n",
      "CV loss=0.4018034232021743(Epoch= 510)\n",
      "train loss=0.3408804419411484(Epoch= 510)\n",
      "CV loss=0.4018450288489892(Epoch= 511)\n",
      "train loss=0.3405755615024399(Epoch= 511)\n",
      "CV loss=0.40150366753115874(Epoch= 512)\n",
      "train loss=0.340318207749302(Epoch= 512)\n",
      "CV loss=0.40114039341888963(Epoch= 513)\n",
      "train loss=0.3400146460639119(Epoch= 513)\n",
      "CV loss=0.4010041246493485(Epoch= 514)\n",
      "train loss=0.3397381277002546(Epoch= 514)\n",
      "CV loss=0.4007998666635826(Epoch= 515)\n",
      "train loss=0.3394831577091758(Epoch= 515)\n",
      "CV loss=0.40052946916733445(Epoch= 516)\n",
      "train loss=0.3392293903125186(Epoch= 516)\n",
      "CV loss=0.40024856515381235(Epoch= 517)\n",
      "train loss=0.33894998735699794(Epoch= 517)\n",
      "CV loss=0.39990368438774726(Epoch= 518)\n",
      "train loss=0.33866770972438753(Epoch= 518)\n",
      "CV loss=0.3998532646507014(Epoch= 519)\n",
      "train loss=0.33843940743063355(Epoch= 519)\n",
      "CV loss=0.3994094228097346(Epoch= 520)\n",
      "train loss=0.33814350337920956(Epoch= 520)\n",
      "CV loss=0.39955289909175884(Epoch= 521)\n",
      "train loss=0.3378991025576216(Epoch= 521)\n",
      "CV loss=0.39912381400866837(Epoch= 522)\n",
      "train loss=0.33769695408577427(Epoch= 522)\n",
      "CV loss=0.3986338076397304(Epoch= 523)\n",
      "train loss=0.33737457815495936(Epoch= 523)\n",
      "CV loss=0.3985746431221618(Epoch= 524)\n",
      "train loss=0.33712576556961316(Epoch= 524)\n",
      "CV loss=0.39832653821239317(Epoch= 525)\n",
      "train loss=0.33685741811321585(Epoch= 525)\n",
      "CV loss=0.39809922962148764(Epoch= 526)\n",
      "train loss=0.3365755418973889(Epoch= 526)\n",
      "CV loss=0.39787078366901996(Epoch= 527)\n",
      "train loss=0.3362930009906352(Epoch= 527)\n",
      "CV loss=0.3977473332038186(Epoch= 528)\n",
      "train loss=0.3360975780058728(Epoch= 528)\n",
      "CV loss=0.3972548104877488(Epoch= 529)\n",
      "train loss=0.3357876855211427(Epoch= 529)\n",
      "CV loss=0.3972992234340405(Epoch= 530)\n",
      "train loss=0.33553216366786814(Epoch= 530)\n",
      "CV loss=0.3967264551425857(Epoch= 531)\n",
      "train loss=0.33530831070635303(Epoch= 531)\n",
      "CV loss=0.3967483272981366(Epoch= 532)\n",
      "train loss=0.335017946820121(Epoch= 532)\n",
      "CV loss=0.3965712594743894(Epoch= 533)\n",
      "train loss=0.33474384982467065(Epoch= 533)\n",
      "CV loss=0.3964875389615289(Epoch= 534)\n",
      "train loss=0.33452093722727694(Epoch= 534)\n",
      "CV loss=0.39583774525675713(Epoch= 535)\n",
      "train loss=0.3342365778427364(Epoch= 535)\n",
      "CV loss=0.3953636324469624(Epoch= 536)\n",
      "train loss=0.3340407742022128(Epoch= 536)\n",
      "CV loss=0.3955806266582749(Epoch= 537)\n",
      "train loss=0.33371363267482373(Epoch= 537)\n",
      "CV loss=0.39513951598733704(Epoch= 538)\n",
      "train loss=0.3334425368097042(Epoch= 538)\n",
      "CV loss=0.39492641036804776(Epoch= 539)\n",
      "train loss=0.3331657483447293(Epoch= 539)\n",
      "CV loss=0.39472343902750057(Epoch= 540)\n",
      "train loss=0.3329161541303834(Epoch= 540)\n",
      "CV loss=0.39480772053847496(Epoch= 541)\n",
      "train loss=0.3326969599171177(Epoch= 541)\n",
      "CV loss=0.3940665644707927(Epoch= 542)\n",
      "train loss=0.33242714058306233(Epoch= 542)\n",
      "CV loss=0.3939152949905702(Epoch= 543)\n",
      "train loss=0.33219655265724635(Epoch= 543)\n",
      "CV loss=0.3939239664257639(Epoch= 544)\n",
      "train loss=0.33192405820114584(Epoch= 544)\n",
      "CV loss=0.3937359694055907(Epoch= 545)\n",
      "train loss=0.3316523993483955(Epoch= 545)\n",
      "CV loss=0.39343396451966284(Epoch= 546)\n",
      "train loss=0.331402043751755(Epoch= 546)\n",
      "CV loss=0.3931127289913888(Epoch= 547)\n",
      "train loss=0.3311233445592885(Epoch= 547)\n",
      "CV loss=0.3931278715539276(Epoch= 548)\n",
      "train loss=0.33091178109659153(Epoch= 548)\n",
      "CV loss=0.3925113795329369(Epoch= 549)\n",
      "train loss=0.330619828970777(Epoch= 549)\n",
      "CV loss=0.39260334702032507(Epoch= 550)\n",
      "train loss=0.3303934505890979(Epoch= 550)\n",
      "CV loss=0.3919027812955637(Epoch= 551)\n",
      "train loss=0.33014451360740593(Epoch= 551)\n",
      "CV loss=0.3917678075747602(Epoch= 552)\n",
      "train loss=0.32984250891622174(Epoch= 552)\n",
      "CV loss=0.39155190646469273(Epoch= 553)\n",
      "train loss=0.3295946799144357(Epoch= 553)\n",
      "CV loss=0.3911632216702491(Epoch= 554)\n",
      "train loss=0.3293904743312714(Epoch= 554)\n",
      "CV loss=0.3912950207656324(Epoch= 555)\n",
      "train loss=0.32909605991164265(Epoch= 555)\n",
      "CV loss=0.39098833637475794(Epoch= 556)\n",
      "train loss=0.32886028559524105(Epoch= 556)\n",
      "CV loss=0.39066791451250327(Epoch= 557)\n",
      "train loss=0.3286143299184242(Epoch= 557)\n",
      "CV loss=0.3907884598310998(Epoch= 558)\n",
      "train loss=0.32837148380963793(Epoch= 558)\n",
      "CV loss=0.3902469994807021(Epoch= 559)\n",
      "train loss=0.3281166128930661(Epoch= 559)\n",
      "CV loss=0.3898846496044304(Epoch= 560)\n",
      "train loss=0.3278408497997426(Epoch= 560)\n",
      "CV loss=0.3896429554908821(Epoch= 561)\n",
      "train loss=0.327587064873321(Epoch= 561)\n",
      "CV loss=0.38946307177344475(Epoch= 562)\n",
      "train loss=0.3273709204777753(Epoch= 562)\n",
      "CV loss=0.389514717515606(Epoch= 563)\n",
      "train loss=0.3271341706076285(Epoch= 563)\n",
      "CV loss=0.3891407888179683(Epoch= 564)\n",
      "train loss=0.3268400094897012(Epoch= 564)\n",
      "CV loss=0.3888437486932947(Epoch= 565)\n",
      "train loss=0.3265977959363324(Epoch= 565)\n",
      "CV loss=0.38861125192836354(Epoch= 566)\n",
      "train loss=0.3263496737795456(Epoch= 566)\n",
      "CV loss=0.38859374887032816(Epoch= 567)\n",
      "train loss=0.32612655240429056(Epoch= 567)\n",
      "CV loss=0.38811632358291803(Epoch= 568)\n",
      "train loss=0.32583973394351035(Epoch= 568)\n",
      "CV loss=0.3878127237111041(Epoch= 569)\n",
      "train loss=0.32558725614458045(Epoch= 569)\n",
      "CV loss=0.3880114817388291(Epoch= 570)\n",
      "train loss=0.3253888200322697(Epoch= 570)\n",
      "CV loss=0.3872046203135451(Epoch= 571)\n",
      "train loss=0.32513893206369665(Epoch= 571)\n",
      "CV loss=0.3874509037339902(Epoch= 572)\n",
      "train loss=0.3248974725570204(Epoch= 572)\n",
      "CV loss=0.387113384686296(Epoch= 573)\n",
      "train loss=0.32460752804271825(Epoch= 573)\n",
      "CV loss=0.3868761052643721(Epoch= 574)\n",
      "train loss=0.32434553207845096(Epoch= 574)\n",
      "CV loss=0.38674699244938965(Epoch= 575)\n",
      "train loss=0.32413101003630246(Epoch= 575)\n",
      "CV loss=0.38647557864235305(Epoch= 576)\n",
      "train loss=0.32390315672139774(Epoch= 576)\n",
      "CV loss=0.3863435930984943(Epoch= 577)\n",
      "train loss=0.3236462905109816(Epoch= 577)\n",
      "CV loss=0.3858845228284835(Epoch= 578)\n",
      "train loss=0.3234442396958652(Epoch= 578)\n",
      "CV loss=0.38575271175621717(Epoch= 579)\n",
      "train loss=0.32314975642865873(Epoch= 579)\n",
      "CV loss=0.3857593539692819(Epoch= 580)\n",
      "train loss=0.3229193999142693(Epoch= 580)\n",
      "CV loss=0.3851689945770679(Epoch= 581)\n",
      "train loss=0.3226499816836881(Epoch= 581)\n",
      "CV loss=0.38521545233627447(Epoch= 582)\n",
      "train loss=0.32240931731479305(Epoch= 582)\n",
      "CV loss=0.38459541469185404(Epoch= 583)\n",
      "train loss=0.3221483318299538(Epoch= 583)\n",
      "CV loss=0.3843739017510459(Epoch= 584)\n",
      "train loss=0.32192356492938495(Epoch= 584)\n",
      "CV loss=0.38432664803427236(Epoch= 585)\n",
      "train loss=0.32165361128800785(Epoch= 585)\n",
      "CV loss=0.3838577070498129(Epoch= 586)\n",
      "train loss=0.32143727165837477(Epoch= 586)\n",
      "CV loss=0.38379356281957105(Epoch= 587)\n",
      "train loss=0.3211726085331741(Epoch= 587)\n",
      "CV loss=0.3834754225664628(Epoch= 588)\n",
      "train loss=0.32092700868006196(Epoch= 588)\n",
      "CV loss=0.38330523283256723(Epoch= 589)\n",
      "train loss=0.320703826142793(Epoch= 589)\n",
      "CV loss=0.38291779820812993(Epoch= 590)\n",
      "train loss=0.3204646586233818(Epoch= 590)\n",
      "CV loss=0.38296770738696645(Epoch= 591)\n",
      "train loss=0.3202043423718124(Epoch= 591)\n",
      "CV loss=0.3824313393998191(Epoch= 592)\n",
      "train loss=0.31996687766825016(Epoch= 592)\n",
      "CV loss=0.382486996674512(Epoch= 593)\n",
      "train loss=0.31972840083462106(Epoch= 593)\n",
      "CV loss=0.3818916252575379(Epoch= 594)\n",
      "train loss=0.31950196420737664(Epoch= 594)\n",
      "CV loss=0.38191723715439285(Epoch= 595)\n",
      "train loss=0.3192207418685073(Epoch= 595)\n",
      "CV loss=0.3816678172383294(Epoch= 596)\n",
      "train loss=0.31900370493259134(Epoch= 596)\n",
      "CV loss=0.38135195521626564(Epoch= 597)\n",
      "train loss=0.31874627165929176(Epoch= 597)\n",
      "CV loss=0.3811086083055344(Epoch= 598)\n",
      "train loss=0.318536245206312(Epoch= 598)\n",
      "CV loss=0.38099384200134057(Epoch= 599)\n",
      "train loss=0.318258364364821(Epoch= 599)\n",
      "CV loss=0.3809063921284338(Epoch= 600)\n",
      "train loss=0.3180236544793358(Epoch= 600)\n",
      "CV loss=0.3805907514955834(Epoch= 601)\n",
      "train loss=0.3177855538020113(Epoch= 601)\n",
      "CV loss=0.38056594893939333(Epoch= 602)\n",
      "train loss=0.3175546875639354(Epoch= 602)\n",
      "CV loss=0.380074045415234(Epoch= 603)\n",
      "train loss=0.317299889612304(Epoch= 603)\n",
      "CV loss=0.38013176928366954(Epoch= 604)\n",
      "train loss=0.31709073906504426(Epoch= 604)\n",
      "CV loss=0.37970226827932607(Epoch= 605)\n",
      "train loss=0.3168382268409575(Epoch= 605)\n",
      "CV loss=0.3794853586358685(Epoch= 606)\n",
      "train loss=0.31658959431737405(Epoch= 606)\n",
      "CV loss=0.37920070078460677(Epoch= 607)\n",
      "train loss=0.3163727941507814(Epoch= 607)\n",
      "CV loss=0.3789633226333473(Epoch= 608)\n",
      "train loss=0.3161068691000011(Epoch= 608)\n",
      "CV loss=0.3788643651789039(Epoch= 609)\n",
      "train loss=0.3158830727837684(Epoch= 609)\n",
      "CV loss=0.3786395041602239(Epoch= 610)\n",
      "train loss=0.315651410274965(Epoch= 610)\n",
      "CV loss=0.378306401434145(Epoch= 611)\n",
      "train loss=0.31540003621492785(Epoch= 611)\n",
      "CV loss=0.37807112763326095(Epoch= 612)\n",
      "train loss=0.31517451554604553(Epoch= 612)\n",
      "CV loss=0.3777714908979972(Epoch= 613)\n",
      "train loss=0.31493703413496504(Epoch= 613)\n",
      "CV loss=0.37767086072679684(Epoch= 614)\n",
      "train loss=0.3147207717515864(Epoch= 614)\n",
      "CV loss=0.3773947160496255(Epoch= 615)\n",
      "train loss=0.3144759742650024(Epoch= 615)\n",
      "CV loss=0.3772579931058272(Epoch= 616)\n",
      "train loss=0.31424562410980805(Epoch= 616)\n",
      "CV loss=0.3771358215515072(Epoch= 617)\n",
      "train loss=0.31399989183462645(Epoch= 617)\n",
      "CV loss=0.37673351107972597(Epoch= 618)\n",
      "train loss=0.31374311344784667(Epoch= 618)\n",
      "CV loss=0.3766360870067146(Epoch= 619)\n",
      "train loss=0.31352939861466117(Epoch= 619)\n",
      "CV loss=0.37657458452536774(Epoch= 620)\n",
      "train loss=0.31332170681133936(Epoch= 620)\n",
      "CV loss=0.3763558852226451(Epoch= 621)\n",
      "train loss=0.3131072660546311(Epoch= 621)\n",
      "CV loss=0.3758967187724831(Epoch= 622)\n",
      "train loss=0.31282908397200204(Epoch= 622)\n",
      "CV loss=0.3759280707428977(Epoch= 623)\n",
      "train loss=0.31260184277064695(Epoch= 623)\n",
      "CV loss=0.375474216244889(Epoch= 624)\n",
      "train loss=0.31233573408162557(Epoch= 624)\n",
      "CV loss=0.3751591973847786(Epoch= 625)\n",
      "train loss=0.31212910120361537(Epoch= 625)\n",
      "CV loss=0.3751827499298459(Epoch= 626)\n",
      "train loss=0.311894440191337(Epoch= 626)\n",
      "CV loss=0.37456807969524(Epoch= 627)\n",
      "train loss=0.31165748189120723(Epoch= 627)\n",
      "CV loss=0.3744743812668233(Epoch= 628)\n",
      "train loss=0.31142509012265485(Epoch= 628)\n",
      "CV loss=0.3745564226948836(Epoch= 629)\n",
      "train loss=0.3112051922389496(Epoch= 629)\n",
      "CV loss=0.3742619079930669(Epoch= 630)\n",
      "train loss=0.3109599404585032(Epoch= 630)\n",
      "CV loss=0.37381726461695386(Epoch= 631)\n",
      "train loss=0.3107445633398224(Epoch= 631)\n",
      "CV loss=0.37354786919193717(Epoch= 632)\n",
      "train loss=0.31050981806685507(Epoch= 632)\n",
      "CV loss=0.3738630435228525(Epoch= 633)\n",
      "train loss=0.31032975719713896(Epoch= 633)\n",
      "CV loss=0.37312809418102494(Epoch= 634)\n",
      "train loss=0.31004727452194725(Epoch= 634)\n",
      "CV loss=0.3733469268019142(Epoch= 635)\n",
      "train loss=0.30984512668819975(Epoch= 635)\n",
      "CV loss=0.37284449462886216(Epoch= 636)\n",
      "train loss=0.30958403109848603(Epoch= 636)\n",
      "CV loss=0.37262472492356735(Epoch= 637)\n",
      "train loss=0.30935818579141633(Epoch= 637)\n",
      "CV loss=0.37238409474694545(Epoch= 638)\n",
      "train loss=0.3091060336035042(Epoch= 638)\n",
      "CV loss=0.3720327466885832(Epoch= 639)\n",
      "train loss=0.30889914582519074(Epoch= 639)\n",
      "CV loss=0.37212292707793304(Epoch= 640)\n",
      "train loss=0.3086885303654752(Epoch= 640)\n",
      "CV loss=0.371606633306349(Epoch= 641)\n",
      "train loss=0.30844459407509567(Epoch= 641)\n",
      "CV loss=0.371889754099317(Epoch= 642)\n",
      "train loss=0.30822463918955967(Epoch= 642)\n",
      "CV loss=0.37107122038045215(Epoch= 643)\n",
      "train loss=0.30796181297565034(Epoch= 643)\n",
      "CV loss=0.37105124401445516(Epoch= 644)\n",
      "train loss=0.30774125217292064(Epoch= 644)\n",
      "CV loss=0.37096031221984554(Epoch= 645)\n",
      "train loss=0.30752088079028633(Epoch= 645)\n",
      "CV loss=0.3705643567739708(Epoch= 646)\n",
      "train loss=0.3072779735807276(Epoch= 646)\n",
      "CV loss=0.37016770662158055(Epoch= 647)\n",
      "train loss=0.3070799863673409(Epoch= 647)\n",
      "CV loss=0.37024979306355804(Epoch= 648)\n",
      "train loss=0.3068413681126247(Epoch= 648)\n",
      "CV loss=0.3700290072379271(Epoch= 649)\n",
      "train loss=0.3065867891551938(Epoch= 649)\n",
      "CV loss=0.3698239289751036(Epoch= 650)\n",
      "train loss=0.30637425971434495(Epoch= 650)\n",
      "CV loss=0.3696376845089009(Epoch= 651)\n",
      "train loss=0.3061560276002542(Epoch= 651)\n",
      "CV loss=0.3693430134596049(Epoch= 652)\n",
      "train loss=0.3059229397733522(Epoch= 652)\n",
      "CV loss=0.3689391785391553(Epoch= 653)\n",
      "train loss=0.3056980866058468(Epoch= 653)\n",
      "CV loss=0.3689517072024088(Epoch= 654)\n",
      "train loss=0.30547210264295527(Epoch= 654)\n",
      "CV loss=0.3684318939333003(Epoch= 655)\n",
      "train loss=0.3052460448440291(Epoch= 655)\n",
      "CV loss=0.3682772436692316(Epoch= 656)\n",
      "train loss=0.30502874607917074(Epoch= 656)\n",
      "CV loss=0.3683372075710418(Epoch= 657)\n",
      "train loss=0.30482452228678303(Epoch= 657)\n",
      "CV loss=0.3678893807891571(Epoch= 658)\n",
      "train loss=0.3045718481875501(Epoch= 658)\n",
      "CV loss=0.36777657137065395(Epoch= 659)\n",
      "train loss=0.3043431651241515(Epoch= 659)\n",
      "CV loss=0.3676585397973623(Epoch= 660)\n",
      "train loss=0.3041158159332587(Epoch= 660)\n",
      "CV loss=0.3672571049702409(Epoch= 661)\n",
      "train loss=0.3039115358139796(Epoch= 661)\n",
      "CV loss=0.36703123183511094(Epoch= 662)\n",
      "train loss=0.30366774702333293(Epoch= 662)\n",
      "CV loss=0.3672048810136379(Epoch= 663)\n",
      "train loss=0.3035086062750591(Epoch= 663)\n",
      "CV loss=0.36658617245780273(Epoch= 664)\n",
      "train loss=0.30328572198699094(Epoch= 664)\n",
      "CV loss=0.3665784119314605(Epoch= 665)\n",
      "train loss=0.30302750682354646(Epoch= 665)\n",
      "CV loss=0.36618411762782266(Epoch= 666)\n",
      "train loss=0.30277686153085676(Epoch= 666)\n",
      "CV loss=0.36618610936031637(Epoch= 667)\n",
      "train loss=0.302591007672101(Epoch= 667)\n",
      "CV loss=0.3658116052248044(Epoch= 668)\n",
      "train loss=0.30235132220156136(Epoch= 668)\n",
      "CV loss=0.3656099142276835(Epoch= 669)\n",
      "train loss=0.3021101457601395(Epoch= 669)\n",
      "CV loss=0.365699765219057(Epoch= 670)\n",
      "train loss=0.3019158449754349(Epoch= 670)\n",
      "CV loss=0.36504172260176876(Epoch= 671)\n",
      "train loss=0.30172357048383236(Epoch= 671)\n",
      "CV loss=0.36495690666711705(Epoch= 672)\n",
      "train loss=0.3014568053047737(Epoch= 672)\n",
      "CV loss=0.3648406102574798(Epoch= 673)\n",
      "train loss=0.3012549747361268(Epoch= 673)\n",
      "CV loss=0.3647967117790499(Epoch= 674)\n",
      "train loss=0.3010178026133623(Epoch= 674)\n",
      "CV loss=0.36440022139484196(Epoch= 675)\n",
      "train loss=0.3007786102123017(Epoch= 675)\n",
      "CV loss=0.3640316274409108(Epoch= 676)\n",
      "train loss=0.3005771120214435(Epoch= 676)\n",
      "CV loss=0.3638462847235792(Epoch= 677)\n",
      "train loss=0.3003476283471513(Epoch= 677)\n",
      "CV loss=0.36376512385166304(Epoch= 678)\n",
      "train loss=0.30013670042068785(Epoch= 678)\n",
      "CV loss=0.36349356268818445(Epoch= 679)\n",
      "train loss=0.2999047029746108(Epoch= 679)\n",
      "CV loss=0.36337641220032524(Epoch= 680)\n",
      "train loss=0.29968416757594407(Epoch= 680)\n",
      "CV loss=0.3633551994636073(Epoch= 681)\n",
      "train loss=0.2994761850599488(Epoch= 681)\n",
      "CV loss=0.36299745640098624(Epoch= 682)\n",
      "train loss=0.29924621617940556(Epoch= 682)\n",
      "CV loss=0.36255546069736133(Epoch= 683)\n",
      "train loss=0.29902825429011193(Epoch= 683)\n",
      "CV loss=0.36267473345517964(Epoch= 684)\n",
      "train loss=0.2988088689041222(Epoch= 684)\n",
      "CV loss=0.36232488111735384(Epoch= 685)\n",
      "train loss=0.29858943142893424(Epoch= 685)\n",
      "CV loss=0.36210087217034065(Epoch= 686)\n",
      "train loss=0.29837721907340387(Epoch= 686)\n",
      "CV loss=0.3619198577476118(Epoch= 687)\n",
      "train loss=0.2981612978891816(Epoch= 687)\n",
      "CV loss=0.3615758121843229(Epoch= 688)\n",
      "train loss=0.2979257575839182(Epoch= 688)\n",
      "CV loss=0.3616544484320138(Epoch= 689)\n",
      "train loss=0.29772680170661775(Epoch= 689)\n",
      "CV loss=0.36112454018302076(Epoch= 690)\n",
      "train loss=0.2974890660058469(Epoch= 690)\n",
      "CV loss=0.361167553270927(Epoch= 691)\n",
      "train loss=0.29728493812054896(Epoch= 691)\n",
      "CV loss=0.3608144223613371(Epoch= 692)\n",
      "train loss=0.2970513018211038(Epoch= 692)\n",
      "CV loss=0.36051301630029187(Epoch= 693)\n",
      "train loss=0.29687942809634776(Epoch= 693)\n",
      "CV loss=0.36020463507409006(Epoch= 694)\n",
      "train loss=0.29664501704500634(Epoch= 694)\n",
      "CV loss=0.36002508334582173(Epoch= 695)\n",
      "train loss=0.2964156494365672(Epoch= 695)\n",
      "CV loss=0.36008049199521186(Epoch= 696)\n",
      "train loss=0.29619243193027306(Epoch= 696)\n",
      "CV loss=0.3598225287346455(Epoch= 697)\n",
      "train loss=0.2959918822840052(Epoch= 697)\n",
      "CV loss=0.359888990613911(Epoch= 698)\n",
      "train loss=0.29579886969595615(Epoch= 698)\n",
      "CV loss=0.35921591238561884(Epoch= 699)\n",
      "train loss=0.29558112276847154(Epoch= 699)\n",
      "CV loss=0.3591694966931163(Epoch= 700)\n",
      "train loss=0.29534306390884046(Epoch= 700)\n",
      "CV loss=0.3589065741494568(Epoch= 701)\n",
      "train loss=0.29512886633467467(Epoch= 701)\n",
      "CV loss=0.3588849627792473(Epoch= 702)\n",
      "train loss=0.29490576320412104(Epoch= 702)\n",
      "CV loss=0.3584197455044993(Epoch= 703)\n",
      "train loss=0.29468015836039346(Epoch= 703)\n",
      "CV loss=0.35820175431475165(Epoch= 704)\n",
      "train loss=0.2944723611958576(Epoch= 704)\n",
      "CV loss=0.3581150174296086(Epoch= 705)\n",
      "train loss=0.29425290976340324(Epoch= 705)\n",
      "CV loss=0.3577133457823253(Epoch= 706)\n",
      "train loss=0.294051829627351(Epoch= 706)\n",
      "CV loss=0.35776337855703044(Epoch= 707)\n",
      "train loss=0.29384774553665005(Epoch= 707)\n",
      "CV loss=0.35748869334619776(Epoch= 708)\n",
      "train loss=0.293608113249112(Epoch= 708)\n",
      "CV loss=0.35720896170809835(Epoch= 709)\n",
      "train loss=0.2934053668248255(Epoch= 709)\n",
      "CV loss=0.3570414645686634(Epoch= 710)\n",
      "train loss=0.29319804900316354(Epoch= 710)\n",
      "CV loss=0.35683757412585604(Epoch= 711)\n",
      "train loss=0.29298059371814184(Epoch= 711)\n",
      "CV loss=0.3566666731994787(Epoch= 712)\n",
      "train loss=0.29276175672951654(Epoch= 712)\n",
      "CV loss=0.3569478682581624(Epoch= 713)\n",
      "train loss=0.2926117260086982(Epoch= 713)\n",
      "CV loss=0.35599081867002147(Epoch= 714)\n",
      "train loss=0.2923446367941466(Epoch= 714)\n",
      "CV loss=0.3559291087823395(Epoch= 715)\n",
      "train loss=0.29213722980642043(Epoch= 715)\n",
      "CV loss=0.35586616628655676(Epoch= 716)\n",
      "train loss=0.2919315394349323(Epoch= 716)\n",
      "CV loss=0.3557208820464299(Epoch= 717)\n",
      "train loss=0.2917016940274601(Epoch= 717)\n",
      "CV loss=0.355439508954745(Epoch= 718)\n",
      "train loss=0.2915055065583963(Epoch= 718)\n",
      "CV loss=0.35521435229350584(Epoch= 719)\n",
      "train loss=0.2913010044875257(Epoch= 719)\n",
      "CV loss=0.35500707513569385(Epoch= 720)\n",
      "train loss=0.2910672251257311(Epoch= 720)\n",
      "CV loss=0.3548466363936086(Epoch= 721)\n",
      "train loss=0.290856148880846(Epoch= 721)\n",
      "CV loss=0.3546684062242613(Epoch= 722)\n",
      "train loss=0.2906630726638502(Epoch= 722)\n",
      "CV loss=0.3544126949512988(Epoch= 723)\n",
      "train loss=0.2904535734328811(Epoch= 723)\n",
      "CV loss=0.35439002524617597(Epoch= 724)\n",
      "train loss=0.29023723680171437(Epoch= 724)\n",
      "CV loss=0.35413251139067853(Epoch= 725)\n",
      "train loss=0.29002412952586476(Epoch= 725)\n",
      "CV loss=0.3536233794444212(Epoch= 726)\n",
      "train loss=0.28980695329316714(Epoch= 726)\n",
      "CV loss=0.35395727360634394(Epoch= 727)\n",
      "train loss=0.28966931203857493(Epoch= 727)\n",
      "CV loss=0.35363047858114505(Epoch= 728)\n",
      "train loss=0.28939795529061596(Epoch= 728)\n",
      "CV loss=0.35329058767103744(Epoch= 729)\n",
      "train loss=0.2891962644569925(Epoch= 729)\n",
      "CV loss=0.3529296346964659(Epoch= 730)\n",
      "train loss=0.2889758166165974(Epoch= 730)\n",
      "CV loss=0.35268660204488617(Epoch= 731)\n",
      "train loss=0.2887751508528565(Epoch= 731)\n",
      "CV loss=0.3527322534647704(Epoch= 732)\n",
      "train loss=0.28856319723831997(Epoch= 732)\n",
      "CV loss=0.35221237245660125(Epoch= 733)\n",
      "train loss=0.2883551196652294(Epoch= 733)\n",
      "CV loss=0.35204386299793416(Epoch= 734)\n",
      "train loss=0.2881344382503596(Epoch= 734)\n",
      "CV loss=0.352203611990289(Epoch= 735)\n",
      "train loss=0.28794848926171257(Epoch= 735)\n",
      "CV loss=0.35189641083094053(Epoch= 736)\n",
      "train loss=0.2877318784907174(Epoch= 736)\n",
      "CV loss=0.3513746283519324(Epoch= 737)\n",
      "train loss=0.28752136659860017(Epoch= 737)\n",
      "CV loss=0.3513143645720586(Epoch= 738)\n",
      "train loss=0.2873069617481531(Epoch= 738)\n",
      "CV loss=0.3511823347192564(Epoch= 739)\n",
      "train loss=0.28710590302018374(Epoch= 739)\n",
      "CV loss=0.35102848213735866(Epoch= 740)\n",
      "train loss=0.2868938479918176(Epoch= 740)\n",
      "CV loss=0.3508892005603877(Epoch= 741)\n",
      "train loss=0.28670161552848084(Epoch= 741)\n",
      "CV loss=0.3505888138165446(Epoch= 742)\n",
      "train loss=0.2864985462943534(Epoch= 742)\n",
      "CV loss=0.3502849809607028(Epoch= 743)\n",
      "train loss=0.2862814211711917(Epoch= 743)\n",
      "CV loss=0.35016141021464386(Epoch= 744)\n",
      "train loss=0.2860704120941032(Epoch= 744)\n",
      "CV loss=0.3498088129995155(Epoch= 745)\n",
      "train loss=0.2858793142626348(Epoch= 745)\n",
      "CV loss=0.3496003300952529(Epoch= 746)\n",
      "train loss=0.285700513125447(Epoch= 746)\n",
      "CV loss=0.3493100575785062(Epoch= 747)\n",
      "train loss=0.2854689566880277(Epoch= 747)\n",
      "CV loss=0.34940822462440896(Epoch= 748)\n",
      "train loss=0.2852574516236124(Epoch= 748)\n",
      "CV loss=0.3490786234587235(Epoch= 749)\n",
      "train loss=0.2850522994313392(Epoch= 749)\n",
      "CV loss=0.3486707641165665(Epoch= 750)\n",
      "train loss=0.28485795606217834(Epoch= 750)\n",
      "CV loss=0.3487243142054684(Epoch= 751)\n",
      "train loss=0.2846464006530655(Epoch= 751)\n",
      "CV loss=0.34844563003365864(Epoch= 752)\n",
      "train loss=0.2844200897626396(Epoch= 752)\n",
      "CV loss=0.34812389213082806(Epoch= 753)\n",
      "train loss=0.2842385618018003(Epoch= 753)\n",
      "CV loss=0.3482092893378417(Epoch= 754)\n",
      "train loss=0.28403875180211496(Epoch= 754)\n",
      "CV loss=0.34793960835033344(Epoch= 755)\n",
      "train loss=0.2838240957604296(Epoch= 755)\n",
      "CV loss=0.3475952675160784(Epoch= 756)\n",
      "train loss=0.28365339535425205(Epoch= 756)\n",
      "CV loss=0.34767831784796166(Epoch= 757)\n",
      "train loss=0.2834373704551349(Epoch= 757)\n",
      "CV loss=0.3472230714148302(Epoch= 758)\n",
      "train loss=0.2832128724494354(Epoch= 758)\n",
      "CV loss=0.34701826300905286(Epoch= 759)\n",
      "train loss=0.28302551676059157(Epoch= 759)\n",
      "CV loss=0.34696154935563683(Epoch= 760)\n",
      "train loss=0.2827921037601921(Epoch= 760)\n",
      "CV loss=0.3466441701952372(Epoch= 761)\n",
      "train loss=0.28259694735026236(Epoch= 761)\n",
      "CV loss=0.3464892987098809(Epoch= 762)\n",
      "train loss=0.2823976784849593(Epoch= 762)\n",
      "CV loss=0.3463402581779226(Epoch= 763)\n",
      "train loss=0.2821870665607823(Epoch= 763)\n",
      "CV loss=0.3462837660077009(Epoch= 764)\n",
      "train loss=0.28203337265493017(Epoch= 764)\n",
      "CV loss=0.34599139749499774(Epoch= 765)\n",
      "train loss=0.2817998030773651(Epoch= 765)\n",
      "CV loss=0.34572142924442467(Epoch= 766)\n",
      "train loss=0.2815891890576445(Epoch= 766)\n",
      "CV loss=0.34567830236543806(Epoch= 767)\n",
      "train loss=0.2813996466586354(Epoch= 767)\n",
      "CV loss=0.34541553862476204(Epoch= 768)\n",
      "train loss=0.2812198453261268(Epoch= 768)\n",
      "CV loss=0.34520988739950226(Epoch= 769)\n",
      "train loss=0.28097988382048544(Epoch= 769)\n",
      "CV loss=0.3451775970739206(Epoch= 770)\n",
      "train loss=0.28081188581490146(Epoch= 770)\n",
      "CV loss=0.34486962406855814(Epoch= 771)\n",
      "train loss=0.2805794970671231(Epoch= 771)\n",
      "CV loss=0.34463452290367114(Epoch= 772)\n",
      "train loss=0.28038003738135087(Epoch= 772)\n",
      "CV loss=0.3442839567630392(Epoch= 773)\n",
      "train loss=0.28020705152868636(Epoch= 773)\n",
      "CV loss=0.34409096614843915(Epoch= 774)\n",
      "train loss=0.2799918104303599(Epoch= 774)\n",
      "CV loss=0.3440664235482649(Epoch= 775)\n",
      "train loss=0.2797863487947768(Epoch= 775)\n",
      "CV loss=0.34389141828181313(Epoch= 776)\n",
      "train loss=0.27958680514983086(Epoch= 776)\n",
      "CV loss=0.3437675317518787(Epoch= 777)\n",
      "train loss=0.2793906541845456(Epoch= 777)\n",
      "CV loss=0.3434497813675805(Epoch= 778)\n",
      "train loss=0.279186997782333(Epoch= 778)\n",
      "CV loss=0.34314621684066016(Epoch= 779)\n",
      "train loss=0.27901969930824116(Epoch= 779)\n",
      "CV loss=0.343094167530733(Epoch= 780)\n",
      "train loss=0.2787898379418046(Epoch= 780)\n",
      "CV loss=0.3430095630547566(Epoch= 781)\n",
      "train loss=0.27858878153410427(Epoch= 781)\n",
      "CV loss=0.3425395581459481(Epoch= 782)\n",
      "train loss=0.2784031523923838(Epoch= 782)\n",
      "CV loss=0.3425190378521457(Epoch= 783)\n",
      "train loss=0.2781998737010058(Epoch= 783)\n",
      "CV loss=0.3426652442874726(Epoch= 784)\n",
      "train loss=0.27803486473715827(Epoch= 784)\n",
      "CV loss=0.3419381248945999(Epoch= 785)\n",
      "train loss=0.2777935625074725(Epoch= 785)\n",
      "CV loss=0.34183571228297355(Epoch= 786)\n",
      "train loss=0.27759147680913077(Epoch= 786)\n",
      "CV loss=0.3416842389282254(Epoch= 787)\n",
      "train loss=0.2774022752100157(Epoch= 787)\n",
      "CV loss=0.34142261534338175(Epoch= 788)\n",
      "train loss=0.2772007434172004(Epoch= 788)\n",
      "CV loss=0.3413178734848389(Epoch= 789)\n",
      "train loss=0.2770034524515212(Epoch= 789)\n",
      "CV loss=0.3408638764655062(Epoch= 790)\n",
      "train loss=0.2768097482219255(Epoch= 790)\n",
      "CV loss=0.3406405660168338(Epoch= 791)\n",
      "train loss=0.27662898204889275(Epoch= 791)\n",
      "CV loss=0.3407165069097719(Epoch= 792)\n",
      "train loss=0.27640687035377876(Epoch= 792)\n",
      "CV loss=0.3404890627853818(Epoch= 793)\n",
      "train loss=0.27623465170039546(Epoch= 793)\n",
      "CV loss=0.3404079932854096(Epoch= 794)\n",
      "train loss=0.2760518388393807(Epoch= 794)\n",
      "CV loss=0.33989009533311043(Epoch= 795)\n",
      "train loss=0.2758558326257069(Epoch= 795)\n",
      "CV loss=0.33991303924711663(Epoch= 796)\n",
      "train loss=0.2756371902989331(Epoch= 796)\n",
      "CV loss=0.33960526533033986(Epoch= 797)\n",
      "train loss=0.27543628848782176(Epoch= 797)\n",
      "CV loss=0.3395390994956372(Epoch= 798)\n",
      "train loss=0.2752384524465565(Epoch= 798)\n",
      "CV loss=0.3392065217203854(Epoch= 799)\n",
      "train loss=0.2750444368983885(Epoch= 799)\n",
      "CV loss=0.33911364983471537(Epoch= 800)\n",
      "train loss=0.274840727482965(Epoch= 800)\n",
      "CV loss=0.3389261805178754(Epoch= 801)\n",
      "train loss=0.2746487811862699(Epoch= 801)\n",
      "CV loss=0.3387438599736195(Epoch= 802)\n",
      "train loss=0.27446783802712166(Epoch= 802)\n",
      "CV loss=0.33848794321806036(Epoch= 803)\n",
      "train loss=0.2742667232663657(Epoch= 803)\n",
      "CV loss=0.33828783397604445(Epoch= 804)\n",
      "train loss=0.2740637549605649(Epoch= 804)\n",
      "CV loss=0.3382809327475358(Epoch= 805)\n",
      "train loss=0.2738834747089527(Epoch= 805)\n",
      "CV loss=0.3378797836237925(Epoch= 806)\n",
      "train loss=0.273675239734634(Epoch= 806)\n",
      "CV loss=0.33791287959888594(Epoch= 807)\n",
      "train loss=0.2734907229760146(Epoch= 807)\n",
      "CV loss=0.33744855870860563(Epoch= 808)\n",
      "train loss=0.2732850615293664(Epoch= 808)\n",
      "CV loss=0.337410092722087(Epoch= 809)\n",
      "train loss=0.2730960595980787(Epoch= 809)\n",
      "CV loss=0.3372094489682205(Epoch= 810)\n",
      "train loss=0.2729085057411255(Epoch= 810)\n",
      "CV loss=0.336909939253362(Epoch= 811)\n",
      "train loss=0.2727109333535011(Epoch= 811)\n",
      "CV loss=0.3367594661992744(Epoch= 812)\n",
      "train loss=0.27251461536536625(Epoch= 812)\n",
      "CV loss=0.3363155796344886(Epoch= 813)\n",
      "train loss=0.27234142610663237(Epoch= 813)\n",
      "CV loss=0.3365239734796877(Epoch= 814)\n",
      "train loss=0.27213485517327884(Epoch= 814)\n",
      "CV loss=0.3365016456061478(Epoch= 815)\n",
      "train loss=0.27196488727428597(Epoch= 815)\n",
      "CV loss=0.33595185871898475(Epoch= 816)\n",
      "train loss=0.2717543731230561(Epoch= 816)\n",
      "CV loss=0.3357201544262595(Epoch= 817)\n",
      "train loss=0.2715684795801158(Epoch= 817)\n",
      "CV loss=0.3355756270453375(Epoch= 818)\n",
      "train loss=0.27136343656912965(Epoch= 818)\n",
      "CV loss=0.3357091140175042(Epoch= 819)\n",
      "train loss=0.27119011640510066(Epoch= 819)\n",
      "CV loss=0.33522291566257295(Epoch= 820)\n",
      "train loss=0.27097620085774954(Epoch= 820)\n",
      "CV loss=0.33504371842283087(Epoch= 821)\n",
      "train loss=0.2707936131078811(Epoch= 821)\n",
      "CV loss=0.33496024481185077(Epoch= 822)\n",
      "train loss=0.27059998488825276(Epoch= 822)\n",
      "CV loss=0.3348195271221312(Epoch= 823)\n",
      "train loss=0.27040951163277505(Epoch= 823)\n",
      "CV loss=0.33456709210605834(Epoch= 824)\n",
      "train loss=0.2702244619515931(Epoch= 824)\n",
      "CV loss=0.33428884094224376(Epoch= 825)\n",
      "train loss=0.27002944034576365(Epoch= 825)\n",
      "CV loss=0.33421193635508145(Epoch= 826)\n",
      "train loss=0.26983934231068807(Epoch= 826)\n",
      "CV loss=0.333888964836597(Epoch= 827)\n",
      "train loss=0.26966274698283765(Epoch= 827)\n",
      "CV loss=0.33373977147909495(Epoch= 828)\n",
      "train loss=0.26945270763013823(Epoch= 828)\n",
      "CV loss=0.3335373725127118(Epoch= 829)\n",
      "train loss=0.269274569057863(Epoch= 829)\n",
      "CV loss=0.33340468008073737(Epoch= 830)\n",
      "train loss=0.2691076081318268(Epoch= 830)\n",
      "CV loss=0.33307340463427737(Epoch= 831)\n",
      "train loss=0.2689024933983572(Epoch= 831)\n",
      "CV loss=0.33313010653114683(Epoch= 832)\n",
      "train loss=0.26871312782748813(Epoch= 832)\n",
      "CV loss=0.3328235928347255(Epoch= 833)\n",
      "train loss=0.2685022071629989(Epoch= 833)\n",
      "CV loss=0.3325151076900891(Epoch= 834)\n",
      "train loss=0.26831285148562417(Epoch= 834)\n",
      "CV loss=0.33254907557728347(Epoch= 835)\n",
      "train loss=0.2681311012288309(Epoch= 835)\n",
      "CV loss=0.3321994391967472(Epoch= 836)\n",
      "train loss=0.26794162537292676(Epoch= 836)\n",
      "CV loss=0.33201495694889516(Epoch= 837)\n",
      "train loss=0.2677456869859142(Epoch= 837)\n",
      "CV loss=0.33190391942920505(Epoch= 838)\n",
      "train loss=0.2675729795132134(Epoch= 838)\n",
      "CV loss=0.3318165665082159(Epoch= 839)\n",
      "train loss=0.2673818485829623(Epoch= 839)\n",
      "CV loss=0.3314790538129069(Epoch= 840)\n",
      "train loss=0.26719144937742767(Epoch= 840)\n",
      "CV loss=0.33116677065981165(Epoch= 841)\n",
      "train loss=0.2670007302029468(Epoch= 841)\n",
      "CV loss=0.33118909411337183(Epoch= 842)\n",
      "train loss=0.26682248004323644(Epoch= 842)\n",
      "CV loss=0.331095425116313(Epoch= 843)\n",
      "train loss=0.2666429477596134(Epoch= 843)\n",
      "CV loss=0.3307994497046177(Epoch= 844)\n",
      "train loss=0.26643252430934233(Epoch= 844)\n",
      "CV loss=0.3307669718960046(Epoch= 845)\n",
      "train loss=0.26626315102568376(Epoch= 845)\n",
      "CV loss=0.33056248472712874(Epoch= 846)\n",
      "train loss=0.26606293207333054(Epoch= 846)\n",
      "CV loss=0.33009697982207736(Epoch= 847)\n",
      "train loss=0.2658802060979119(Epoch= 847)\n",
      "CV loss=0.3300204404735509(Epoch= 848)\n",
      "train loss=0.2656979306503728(Epoch= 848)\n",
      "CV loss=0.3297828213998633(Epoch= 849)\n",
      "train loss=0.26549835966932256(Epoch= 849)\n",
      "CV loss=0.329714973770595(Epoch= 850)\n",
      "train loss=0.26534398708814677(Epoch= 850)\n",
      "CV loss=0.32937276907151075(Epoch= 851)\n",
      "train loss=0.2651521133381051(Epoch= 851)\n",
      "CV loss=0.32930973059276586(Epoch= 852)\n",
      "train loss=0.2649463265764171(Epoch= 852)\n",
      "CV loss=0.32933251924758333(Epoch= 853)\n",
      "train loss=0.26477825400883526(Epoch= 853)\n",
      "CV loss=0.32896727391501696(Epoch= 854)\n",
      "train loss=0.26457318729204465(Epoch= 854)\n",
      "CV loss=0.3291409763998502(Epoch= 855)\n",
      "train loss=0.26443625508348073(Epoch= 855)\n",
      "CV loss=0.3286769636009441(Epoch= 856)\n",
      "train loss=0.2642140976187715(Epoch= 856)\n",
      "CV loss=0.3283227319027701(Epoch= 857)\n",
      "train loss=0.26403483310934445(Epoch= 857)\n",
      "CV loss=0.32818217127319926(Epoch= 858)\n",
      "train loss=0.2638697129317067(Epoch= 858)\n",
      "CV loss=0.32811994472300154(Epoch= 859)\n",
      "train loss=0.263646866596995(Epoch= 859)\n",
      "CV loss=0.32757799192234166(Epoch= 860)\n",
      "train loss=0.26350931455230825(Epoch= 860)\n",
      "CV loss=0.3275072402311149(Epoch= 861)\n",
      "train loss=0.26328342675455807(Epoch= 861)\n",
      "CV loss=0.327378852024925(Epoch= 862)\n",
      "train loss=0.2631115953937043(Epoch= 862)\n",
      "CV loss=0.3271049765501429(Epoch= 863)\n",
      "train loss=0.26290861228406565(Epoch= 863)\n",
      "CV loss=0.3269973078296222(Epoch= 864)\n",
      "train loss=0.26274795492819036(Epoch= 864)\n",
      "CV loss=0.3270157452773303(Epoch= 865)\n",
      "train loss=0.2625448122990596(Epoch= 865)\n",
      "CV loss=0.3266684484752919(Epoch= 866)\n",
      "train loss=0.26235846473250035(Epoch= 866)\n",
      "CV loss=0.3263697188791276(Epoch= 867)\n",
      "train loss=0.2621756134453694(Epoch= 867)\n",
      "CV loss=0.3264551492231589(Epoch= 868)\n",
      "train loss=0.2620123363321627(Epoch= 868)\n",
      "CV loss=0.3263151748062558(Epoch= 869)\n",
      "train loss=0.2618373304696519(Epoch= 869)\n",
      "CV loss=0.3262789706254998(Epoch= 870)\n",
      "train loss=0.26166666825514606(Epoch= 870)\n",
      "CV loss=0.3258272878540543(Epoch= 871)\n",
      "train loss=0.2614612765259993(Epoch= 871)\n",
      "CV loss=0.32542718722864333(Epoch= 872)\n",
      "train loss=0.2612979697421223(Epoch= 872)\n",
      "CV loss=0.32545377099148676(Epoch= 873)\n",
      "train loss=0.2610787794122578(Epoch= 873)\n",
      "CV loss=0.3252684747299175(Epoch= 874)\n",
      "train loss=0.26090003215586427(Epoch= 874)\n",
      "CV loss=0.3251185469666781(Epoch= 875)\n",
      "train loss=0.26071239705656(Epoch= 875)\n",
      "CV loss=0.3248891986495847(Epoch= 876)\n",
      "train loss=0.26054511959594756(Epoch= 876)\n",
      "CV loss=0.32488472352978287(Epoch= 877)\n",
      "train loss=0.26035908944667274(Epoch= 877)\n",
      "CV loss=0.3245898528022886(Epoch= 878)\n",
      "train loss=0.26016644906233416(Epoch= 878)\n",
      "CV loss=0.3241233609960863(Epoch= 879)\n",
      "train loss=0.2599961193732276(Epoch= 879)\n",
      "CV loss=0.32423455532736856(Epoch= 880)\n",
      "train loss=0.2598214222534039(Epoch= 880)\n",
      "CV loss=0.3239233712594014(Epoch= 881)\n",
      "train loss=0.2596395704270071(Epoch= 881)\n",
      "CV loss=0.3240263610133681(Epoch= 882)\n",
      "train loss=0.25945450856112223(Epoch= 882)\n",
      "CV loss=0.3236459209621201(Epoch= 883)\n",
      "train loss=0.2592833818609163(Epoch= 883)\n",
      "CV loss=0.32349874117709415(Epoch= 884)\n",
      "train loss=0.2590855532566589(Epoch= 884)\n",
      "CV loss=0.3231058941569067(Epoch= 885)\n",
      "train loss=0.2589091531128119(Epoch= 885)\n",
      "CV loss=0.3232115605388537(Epoch= 886)\n",
      "train loss=0.25873216370364116(Epoch= 886)\n",
      "CV loss=0.32288060939784374(Epoch= 887)\n",
      "train loss=0.258544731799825(Epoch= 887)\n",
      "CV loss=0.3227850975778988(Epoch= 888)\n",
      "train loss=0.2583877454364167(Epoch= 888)\n",
      "CV loss=0.3226723760130228(Epoch= 889)\n",
      "train loss=0.25818293022190525(Epoch= 889)\n",
      "CV loss=0.32256005417336114(Epoch= 890)\n",
      "train loss=0.25800730884588424(Epoch= 890)\n",
      "CV loss=0.32228216618476546(Epoch= 891)\n",
      "train loss=0.2578696574895164(Epoch= 891)\n",
      "CV loss=0.32208752512311495(Epoch= 892)\n",
      "train loss=0.2576435090960159(Epoch= 892)\n",
      "CV loss=0.3218621766414615(Epoch= 893)\n",
      "train loss=0.25747668614852354(Epoch= 893)\n",
      "CV loss=0.32173172237533765(Epoch= 894)\n",
      "train loss=0.25728838539142784(Epoch= 894)\n",
      "CV loss=0.3214380788908335(Epoch= 895)\n",
      "train loss=0.2571166926608319(Epoch= 895)\n",
      "CV loss=0.3211239490525645(Epoch= 896)\n",
      "train loss=0.25694174459375474(Epoch= 896)\n",
      "CV loss=0.32126058310784317(Epoch= 897)\n",
      "train loss=0.25675431015643024(Epoch= 897)\n",
      "CV loss=0.3211667022341602(Epoch= 898)\n",
      "train loss=0.2565834916569965(Epoch= 898)\n",
      "CV loss=0.3209184137447205(Epoch= 899)\n",
      "train loss=0.25640718281180813(Epoch= 899)\n",
      "CV loss=0.3206375166555252(Epoch= 900)\n",
      "train loss=0.2562126889952645(Epoch= 900)\n",
      "CV loss=0.3204515116005635(Epoch= 901)\n",
      "train loss=0.2560586510295301(Epoch= 901)\n",
      "CV loss=0.32032739228857854(Epoch= 902)\n",
      "train loss=0.25586382287226644(Epoch= 902)\n",
      "CV loss=0.320010333019321(Epoch= 903)\n",
      "train loss=0.25569841311277425(Epoch= 903)\n",
      "CV loss=0.31975509934348845(Epoch= 904)\n",
      "train loss=0.2555081577153184(Epoch= 904)\n",
      "CV loss=0.31978643315345195(Epoch= 905)\n",
      "train loss=0.2553372248973126(Epoch= 905)\n",
      "CV loss=0.31972588761071574(Epoch= 906)\n",
      "train loss=0.25518435960594515(Epoch= 906)\n",
      "CV loss=0.3195121382810775(Epoch= 907)\n",
      "train loss=0.25499165231042614(Epoch= 907)\n",
      "CV loss=0.3193373116341548(Epoch= 908)\n",
      "train loss=0.25485773829821845(Epoch= 908)\n",
      "CV loss=0.31931571486876914(Epoch= 909)\n",
      "train loss=0.2546420588396626(Epoch= 909)\n",
      "CV loss=0.3188195029217613(Epoch= 910)\n",
      "train loss=0.254453913218773(Epoch= 910)\n",
      "CV loss=0.3186710151822873(Epoch= 911)\n",
      "train loss=0.25427055994411746(Epoch= 911)\n",
      "CV loss=0.31845053293631487(Epoch= 912)\n",
      "train loss=0.25410770305759434(Epoch= 912)\n",
      "CV loss=0.31828571449978516(Epoch= 913)\n",
      "train loss=0.2539191013014002(Epoch= 913)\n",
      "CV loss=0.31804873163520053(Epoch= 914)\n",
      "train loss=0.25374238117693987(Epoch= 914)\n",
      "CV loss=0.31794023342423205(Epoch= 915)\n",
      "train loss=0.25356748965838694(Epoch= 915)\n",
      "CV loss=0.3180056612188361(Epoch= 916)\n",
      "train loss=0.25340599925166785(Epoch= 916)\n",
      "CV loss=0.31759470686318364(Epoch= 917)\n",
      "train loss=0.25321907438622615(Epoch= 917)\n",
      "CV loss=0.3175430937304179(Epoch= 918)\n",
      "train loss=0.25305369819178525(Epoch= 918)\n",
      "CV loss=0.3173391097488208(Epoch= 919)\n",
      "train loss=0.2528767686919223(Epoch= 919)\n",
      "CV loss=0.31716513226525644(Epoch= 920)\n",
      "train loss=0.2527074627618161(Epoch= 920)\n",
      "CV loss=0.31696142078833256(Epoch= 921)\n",
      "train loss=0.2525252797559151(Epoch= 921)\n",
      "CV loss=0.3168267661529212(Epoch= 922)\n",
      "train loss=0.2523488820125969(Epoch= 922)\n",
      "CV loss=0.3166128065184797(Epoch= 923)\n",
      "train loss=0.2521797335867503(Epoch= 923)\n",
      "CV loss=0.316381943918089(Epoch= 924)\n",
      "train loss=0.2520133401891928(Epoch= 924)\n",
      "CV loss=0.316135473228553(Epoch= 925)\n",
      "train loss=0.2518359147769876(Epoch= 925)\n",
      "CV loss=0.31598899739373915(Epoch= 926)\n",
      "train loss=0.25165332120826284(Epoch= 926)\n",
      "CV loss=0.3159393554814014(Epoch= 927)\n",
      "train loss=0.251479867411021(Epoch= 927)\n",
      "CV loss=0.3156745710004905(Epoch= 928)\n",
      "train loss=0.25131920835119353(Epoch= 928)\n",
      "CV loss=0.31565948508331043(Epoch= 929)\n",
      "train loss=0.25114052684216376(Epoch= 929)\n",
      "CV loss=0.3155061134123624(Epoch= 930)\n",
      "train loss=0.2509908985999143(Epoch= 930)\n",
      "CV loss=0.31519009076181914(Epoch= 931)\n",
      "train loss=0.25078400059450484(Epoch= 931)\n",
      "CV loss=0.3148581046713598(Epoch= 932)\n",
      "train loss=0.250615748031673(Epoch= 932)\n",
      "CV loss=0.31489122311249096(Epoch= 933)\n",
      "train loss=0.2504543152011399(Epoch= 933)\n",
      "CV loss=0.3145843547978929(Epoch= 934)\n",
      "train loss=0.25026658075023334(Epoch= 934)\n",
      "CV loss=0.3145686880574676(Epoch= 935)\n",
      "train loss=0.25009781079893084(Epoch= 935)\n",
      "CV loss=0.3142401431594623(Epoch= 936)\n",
      "train loss=0.24993673671785233(Epoch= 936)\n",
      "CV loss=0.31390756103078904(Epoch= 937)\n",
      "train loss=0.24976451550695708(Epoch= 937)\n",
      "CV loss=0.31402629277471583(Epoch= 938)\n",
      "train loss=0.2495830314961537(Epoch= 938)\n",
      "CV loss=0.3136828178987357(Epoch= 939)\n",
      "train loss=0.24942164087160879(Epoch= 939)\n",
      "CV loss=0.3137127555465647(Epoch= 940)\n",
      "train loss=0.2492509385226012(Epoch= 940)\n",
      "CV loss=0.313525317475575(Epoch= 941)\n",
      "train loss=0.24908696912587525(Epoch= 941)\n",
      "CV loss=0.31346100425456414(Epoch= 942)\n",
      "train loss=0.24891502965734721(Epoch= 942)\n",
      "CV loss=0.3129873941845597(Epoch= 943)\n",
      "train loss=0.24874014600437408(Epoch= 943)\n",
      "CV loss=0.3130664736788804(Epoch= 944)\n",
      "train loss=0.24855964681142867(Epoch= 944)\n",
      "CV loss=0.31272732844646906(Epoch= 945)\n",
      "train loss=0.24841388651905993(Epoch= 945)\n",
      "CV loss=0.31261713948859854(Epoch= 946)\n",
      "train loss=0.24820749062127395(Epoch= 946)\n",
      "CV loss=0.3125061697064174(Epoch= 947)\n",
      "train loss=0.2480608003464995(Epoch= 947)\n",
      "CV loss=0.31218425757583806(Epoch= 948)\n",
      "train loss=0.247898767307949(Epoch= 948)\n",
      "CV loss=0.31195112519065593(Epoch= 949)\n",
      "train loss=0.2477120440286478(Epoch= 949)\n",
      "CV loss=0.31186249963494755(Epoch= 950)\n",
      "train loss=0.2475370404289501(Epoch= 950)\n",
      "CV loss=0.3116231271522324(Epoch= 951)\n",
      "train loss=0.24737681829476163(Epoch= 951)\n",
      "CV loss=0.3113272688549593(Epoch= 952)\n",
      "train loss=0.24722316599831534(Epoch= 952)\n",
      "CV loss=0.3112524999278427(Epoch= 953)\n",
      "train loss=0.24701995399003102(Epoch= 953)\n",
      "CV loss=0.31129193444516506(Epoch= 954)\n",
      "train loss=0.24688600094986293(Epoch= 954)\n",
      "CV loss=0.3113049886725461(Epoch= 955)\n",
      "train loss=0.24670014121278774(Epoch= 955)\n",
      "CV loss=0.3110756557094693(Epoch= 956)\n",
      "train loss=0.24654220700300336(Epoch= 956)\n",
      "CV loss=0.31071973950189535(Epoch= 957)\n",
      "train loss=0.24638258606403446(Epoch= 957)\n",
      "CV loss=0.3104869100846527(Epoch= 958)\n",
      "train loss=0.24617802212956108(Epoch= 958)\n",
      "CV loss=0.310497473542051(Epoch= 959)\n",
      "train loss=0.2460222230707859(Epoch= 959)\n",
      "CV loss=0.31001261896726995(Epoch= 960)\n",
      "train loss=0.24586694201779513(Epoch= 960)\n",
      "CV loss=0.3100022900742328(Epoch= 961)\n",
      "train loss=0.24567737340938708(Epoch= 961)\n",
      "CV loss=0.3097920777440355(Epoch= 962)\n",
      "train loss=0.2455094308570308(Epoch= 962)\n",
      "CV loss=0.3097350019048509(Epoch= 963)\n",
      "train loss=0.24534358606629741(Epoch= 963)\n",
      "CV loss=0.30977534565849385(Epoch= 964)\n",
      "train loss=0.24517871172465985(Epoch= 964)\n",
      "CV loss=0.309415052649317(Epoch= 965)\n",
      "train loss=0.24501396223050137(Epoch= 965)\n",
      "CV loss=0.3093124281326697(Epoch= 966)\n",
      "train loss=0.24483782989081324(Epoch= 966)\n",
      "CV loss=0.3088601860619085(Epoch= 967)\n",
      "train loss=0.24469280017267703(Epoch= 967)\n",
      "CV loss=0.30906756932643026(Epoch= 968)\n",
      "train loss=0.24451383141300667(Epoch= 968)\n",
      "CV loss=0.3086831343452427(Epoch= 969)\n",
      "train loss=0.24433223726708925(Epoch= 969)\n",
      "CV loss=0.30835626557991047(Epoch= 970)\n",
      "train loss=0.24416911586220685(Epoch= 970)\n",
      "CV loss=0.30838234684228716(Epoch= 971)\n",
      "train loss=0.2440031725562323(Epoch= 971)\n",
      "CV loss=0.3082186171962531(Epoch= 972)\n",
      "train loss=0.24383035841721157(Epoch= 972)\n",
      "CV loss=0.3080773264764951(Epoch= 973)\n",
      "train loss=0.24366856527039366(Epoch= 973)\n",
      "CV loss=0.3078574053576312(Epoch= 974)\n",
      "train loss=0.24349809024012137(Epoch= 974)\n",
      "CV loss=0.3076850562372805(Epoch= 975)\n",
      "train loss=0.2433358109308831(Epoch= 975)\n",
      "CV loss=0.30736312365443935(Epoch= 976)\n",
      "train loss=0.2431706946281475(Epoch= 976)\n",
      "CV loss=0.3076191950833116(Epoch= 977)\n",
      "train loss=0.24301853128973533(Epoch= 977)\n",
      "CV loss=0.30746061536603886(Epoch= 978)\n",
      "train loss=0.24285060673002853(Epoch= 978)\n",
      "CV loss=0.3070076563533293(Epoch= 979)\n",
      "train loss=0.2426734931216922(Epoch= 979)\n",
      "CV loss=0.30677577392081445(Epoch= 980)\n",
      "train loss=0.2425051688631074(Epoch= 980)\n",
      "CV loss=0.3065431648222532(Epoch= 981)\n",
      "train loss=0.2423424080836005(Epoch= 981)\n",
      "CV loss=0.30651575699631245(Epoch= 982)\n",
      "train loss=0.24217496928881366(Epoch= 982)\n",
      "CV loss=0.3061984242702205(Epoch= 983)\n",
      "train loss=0.24200479458422167(Epoch= 983)\n",
      "CV loss=0.3061307596066074(Epoch= 984)\n",
      "train loss=0.2418449154622614(Epoch= 984)\n",
      "CV loss=0.30600815206074405(Epoch= 985)\n",
      "train loss=0.2416801431309764(Epoch= 985)\n",
      "CV loss=0.305962299356777(Epoch= 986)\n",
      "train loss=0.2415279859874933(Epoch= 986)\n",
      "CV loss=0.3054212015898639(Epoch= 987)\n",
      "train loss=0.24136043310314276(Epoch= 987)\n",
      "CV loss=0.3056306677626891(Epoch= 988)\n",
      "train loss=0.24119913219817854(Epoch= 988)\n",
      "CV loss=0.30541107214162017(Epoch= 989)\n",
      "train loss=0.24102802509769725(Epoch= 989)\n",
      "CV loss=0.30509395960173774(Epoch= 990)\n",
      "train loss=0.24085392122259608(Epoch= 990)\n",
      "CV loss=0.30499875296530016(Epoch= 991)\n",
      "train loss=0.24068946776033626(Epoch= 991)\n",
      "CV loss=0.3048781420716411(Epoch= 992)\n",
      "train loss=0.2405322607341447(Epoch= 992)\n",
      "CV loss=0.30453048747753314(Epoch= 993)\n",
      "train loss=0.24037921476561216(Epoch= 993)\n",
      "CV loss=0.3048650042294044(Epoch= 994)\n",
      "train loss=0.24024673845694955(Epoch= 994)\n",
      "CV loss=0.3043021044643281(Epoch= 995)\n",
      "train loss=0.2400404939414928(Epoch= 995)\n",
      "CV loss=0.3041583444478788(Epoch= 996)\n",
      "train loss=0.23988072957322698(Epoch= 996)\n",
      "CV loss=0.30411306160561413(Epoch= 997)\n",
      "train loss=0.23972526099066896(Epoch= 997)\n",
      "CV loss=0.3039638835016923(Epoch= 998)\n",
      "train loss=0.2395623411272641(Epoch= 998)\n",
      "CV loss=0.3038030171799515(Epoch= 999)\n",
      "train loss=0.23939963642110804(Epoch= 999)\n",
      "CV loss=0.30348175029076613(Epoch= 1000)\n",
      "train loss=0.23923865713450296(Epoch= 1000)\n",
      "CV loss=0.3032937479131244(Epoch= 1001)\n",
      "train loss=0.23906235408236687(Epoch= 1001)\n",
      "CV loss=0.3031239372080436(Epoch= 1002)\n",
      "train loss=0.2388971779792055(Epoch= 1002)\n",
      "CV loss=0.3030879949053714(Epoch= 1003)\n",
      "train loss=0.23873651342876628(Epoch= 1003)\n",
      "CV loss=0.3030144234264709(Epoch= 1004)\n",
      "train loss=0.2385986856523497(Epoch= 1004)\n",
      "CV loss=0.3025840680066143(Epoch= 1005)\n",
      "train loss=0.23844604401816008(Epoch= 1005)\n",
      "CV loss=0.30258090816712424(Epoch= 1006)\n",
      "train loss=0.23825589949481696(Epoch= 1006)\n",
      "CV loss=0.30219333146797467(Epoch= 1007)\n",
      "train loss=0.23810293232280552(Epoch= 1007)\n",
      "CV loss=0.30218859676469656(Epoch= 1008)\n",
      "train loss=0.23793436849761268(Epoch= 1008)\n",
      "CV loss=0.3020160939128932(Epoch= 1009)\n",
      "train loss=0.23776651496474055(Epoch= 1009)\n",
      "CV loss=0.3016868259194199(Epoch= 1010)\n",
      "train loss=0.2376168992776174(Epoch= 1010)\n",
      "CV loss=0.3018140539528932(Epoch= 1011)\n",
      "train loss=0.23745581314002362(Epoch= 1011)\n",
      "CV loss=0.30146588241022143(Epoch= 1012)\n",
      "train loss=0.23729469981126067(Epoch= 1012)\n",
      "CV loss=0.301447548172466(Epoch= 1013)\n",
      "train loss=0.23712626409734291(Epoch= 1013)\n",
      "CV loss=0.3011503916687975(Epoch= 1014)\n",
      "train loss=0.23697382654092564(Epoch= 1014)\n",
      "CV loss=0.3011580050107947(Epoch= 1015)\n",
      "train loss=0.23681413486653996(Epoch= 1015)\n",
      "CV loss=0.3010260998974449(Epoch= 1016)\n",
      "train loss=0.23664690058560167(Epoch= 1016)\n",
      "CV loss=0.300688477898937(Epoch= 1017)\n",
      "train loss=0.2364836373692131(Epoch= 1017)\n",
      "CV loss=0.3005621329321164(Epoch= 1018)\n",
      "train loss=0.2363298230077297(Epoch= 1018)\n",
      "CV loss=0.30028460487790626(Epoch= 1019)\n",
      "train loss=0.2361681910765059(Epoch= 1019)\n",
      "CV loss=0.30019765334825765(Epoch= 1020)\n",
      "train loss=0.2360086596263604(Epoch= 1020)\n",
      "CV loss=0.29997645902506986(Epoch= 1021)\n",
      "train loss=0.23585752013941946(Epoch= 1021)\n",
      "CV loss=0.30008024246933707(Epoch= 1022)\n",
      "train loss=0.23569884788270473(Epoch= 1022)\n",
      "CV loss=0.29958927644300853(Epoch= 1023)\n",
      "train loss=0.2355306523825672(Epoch= 1023)\n",
      "CV loss=0.2996358660341517(Epoch= 1024)\n",
      "train loss=0.23537169631232735(Epoch= 1024)\n",
      "CV loss=0.29945104227139896(Epoch= 1025)\n",
      "train loss=0.23521158995811536(Epoch= 1025)\n",
      "CV loss=0.2993934782968806(Epoch= 1026)\n",
      "train loss=0.23505188945875824(Epoch= 1026)\n",
      "CV loss=0.29928273589597126(Epoch= 1027)\n",
      "train loss=0.23490577329099907(Epoch= 1027)\n",
      "CV loss=0.2989728285869271(Epoch= 1028)\n",
      "train loss=0.2347364069409702(Epoch= 1028)\n",
      "CV loss=0.299095502709962(Epoch= 1029)\n",
      "train loss=0.23459024659176303(Epoch= 1029)\n",
      "CV loss=0.29865773098642706(Epoch= 1030)\n",
      "train loss=0.23441585279689287(Epoch= 1030)\n",
      "CV loss=0.2983522111304174(Epoch= 1031)\n",
      "train loss=0.23426623574183167(Epoch= 1031)\n",
      "CV loss=0.2982752420599621(Epoch= 1032)\n",
      "train loss=0.23409833687450604(Epoch= 1032)\n",
      "CV loss=0.29825516575271516(Epoch= 1033)\n",
      "train loss=0.23395010241013495(Epoch= 1033)\n",
      "CV loss=0.29812940400400456(Epoch= 1034)\n",
      "train loss=0.2337872320906028(Epoch= 1034)\n",
      "CV loss=0.2980024432809323(Epoch= 1035)\n",
      "train loss=0.23363221552845936(Epoch= 1035)\n",
      "CV loss=0.29753936425489236(Epoch= 1036)\n",
      "train loss=0.2334784328853147(Epoch= 1036)\n",
      "CV loss=0.29737358059124863(Epoch= 1037)\n",
      "train loss=0.23331550721123642(Epoch= 1037)\n",
      "CV loss=0.29764041440598105(Epoch= 1038)\n",
      "train loss=0.2331749783723934(Epoch= 1038)\n",
      "CV loss=0.2971871514621085(Epoch= 1039)\n",
      "train loss=0.2330023872638208(Epoch= 1039)\n",
      "CV loss=0.2969496698416759(Epoch= 1040)\n",
      "train loss=0.23285120172658685(Epoch= 1040)\n",
      "CV loss=0.2968802950064626(Epoch= 1041)\n",
      "train loss=0.2327022609038871(Epoch= 1041)\n",
      "CV loss=0.29654223565282944(Epoch= 1042)\n",
      "train loss=0.2325450525565339(Epoch= 1042)\n",
      "CV loss=0.296677813562601(Epoch= 1043)\n",
      "train loss=0.23238105014883165(Epoch= 1043)\n",
      "CV loss=0.2965219358279875(Epoch= 1044)\n",
      "train loss=0.23223020663945182(Epoch= 1044)\n",
      "CV loss=0.2961979381554344(Epoch= 1045)\n",
      "train loss=0.23206382311921872(Epoch= 1045)\n",
      "CV loss=0.2961983209092054(Epoch= 1046)\n",
      "train loss=0.23190717128270075(Epoch= 1046)\n",
      "CV loss=0.29580567339956015(Epoch= 1047)\n",
      "train loss=0.23174888541609345(Epoch= 1047)\n",
      "CV loss=0.2957189164567913(Epoch= 1048)\n",
      "train loss=0.23159229056396477(Epoch= 1048)\n",
      "CV loss=0.2954662424917673(Epoch= 1049)\n",
      "train loss=0.23144240720559736(Epoch= 1049)\n",
      "CV loss=0.29548098135350614(Epoch= 1050)\n",
      "train loss=0.23128195807144428(Epoch= 1050)\n",
      "CV loss=0.2954562278509076(Epoch= 1051)\n",
      "train loss=0.23113702187601246(Epoch= 1051)\n",
      "CV loss=0.2950207322470219(Epoch= 1052)\n",
      "train loss=0.2309775920368947(Epoch= 1052)\n",
      "CV loss=0.294864677249163(Epoch= 1053)\n",
      "train loss=0.23081851174531326(Epoch= 1053)\n",
      "CV loss=0.29501894084690483(Epoch= 1054)\n",
      "train loss=0.23067020662870555(Epoch= 1054)\n",
      "CV loss=0.29471239524471204(Epoch= 1055)\n",
      "train loss=0.2305060366683684(Epoch= 1055)\n",
      "CV loss=0.29455993616092585(Epoch= 1056)\n",
      "train loss=0.23036255076661522(Epoch= 1056)\n",
      "CV loss=0.2942180618429894(Epoch= 1057)\n",
      "train loss=0.2302067679788733(Epoch= 1057)\n",
      "CV loss=0.2943108496114068(Epoch= 1058)\n",
      "train loss=0.23005113271119326(Epoch= 1058)\n",
      "CV loss=0.29392115140500596(Epoch= 1059)\n",
      "train loss=0.2298945906468845(Epoch= 1059)\n",
      "CV loss=0.293787328929245(Epoch= 1060)\n",
      "train loss=0.22974720265701762(Epoch= 1060)\n",
      "CV loss=0.29367699062934705(Epoch= 1061)\n",
      "train loss=0.2295841339438105(Epoch= 1061)\n",
      "CV loss=0.2935658457148068(Epoch= 1062)\n",
      "train loss=0.2294253611959088(Epoch= 1062)\n",
      "CV loss=0.29323268512549766(Epoch= 1063)\n",
      "train loss=0.22928974729363774(Epoch= 1063)\n",
      "CV loss=0.2935669384088513(Epoch= 1064)\n",
      "train loss=0.2291426070677372(Epoch= 1064)\n",
      "CV loss=0.2932138150694079(Epoch= 1065)\n",
      "train loss=0.22897209171526345(Epoch= 1065)\n",
      "CV loss=0.2931912048204638(Epoch= 1066)\n",
      "train loss=0.22883440561158913(Epoch= 1066)\n",
      "CV loss=0.29271831484984384(Epoch= 1067)\n",
      "train loss=0.22867598926999202(Epoch= 1067)\n",
      "CV loss=0.29268766371006155(Epoch= 1068)\n",
      "train loss=0.22850635904545216(Epoch= 1068)\n",
      "CV loss=0.29252423880770007(Epoch= 1069)\n",
      "train loss=0.22836829500948894(Epoch= 1069)\n",
      "CV loss=0.2922820859973613(Epoch= 1070)\n",
      "train loss=0.2282037151886897(Epoch= 1070)\n",
      "CV loss=0.29211851804546274(Epoch= 1071)\n",
      "train loss=0.22805601954287433(Epoch= 1071)\n",
      "CV loss=0.2920671527950877(Epoch= 1072)\n",
      "train loss=0.22789653432268964(Epoch= 1072)\n",
      "CV loss=0.291752837414267(Epoch= 1073)\n",
      "train loss=0.2277649953333949(Epoch= 1073)\n",
      "CV loss=0.29170002457368804(Epoch= 1074)\n",
      "train loss=0.22758894962028733(Epoch= 1074)\n",
      "CV loss=0.2914555908267981(Epoch= 1075)\n",
      "train loss=0.2274432652174196(Epoch= 1075)\n",
      "CV loss=0.29120017768597206(Epoch= 1076)\n",
      "train loss=0.2273009470421145(Epoch= 1076)\n",
      "CV loss=0.29146689202928056(Epoch= 1077)\n",
      "train loss=0.2271479561950873(Epoch= 1077)\n",
      "CV loss=0.29118195966191457(Epoch= 1078)\n",
      "train loss=0.226989770228215(Epoch= 1078)\n",
      "CV loss=0.29102261504804267(Epoch= 1079)\n",
      "train loss=0.22683886574432197(Epoch= 1079)\n",
      "CV loss=0.2906860148097862(Epoch= 1080)\n",
      "train loss=0.22669234074731398(Epoch= 1080)\n",
      "CV loss=0.29077403757496534(Epoch= 1081)\n",
      "train loss=0.22653594311873526(Epoch= 1081)\n",
      "CV loss=0.2903816959903763(Epoch= 1082)\n",
      "train loss=0.22639234064369612(Epoch= 1082)\n",
      "CV loss=0.29032236145900275(Epoch= 1083)\n",
      "train loss=0.22623502393931344(Epoch= 1083)\n",
      "CV loss=0.29018252791474763(Epoch= 1084)\n",
      "train loss=0.22607568649291224(Epoch= 1084)\n",
      "CV loss=0.2899663604711239(Epoch= 1085)\n",
      "train loss=0.22594043546343515(Epoch= 1085)\n",
      "CV loss=0.2897891435363469(Epoch= 1086)\n",
      "train loss=0.22577753232402106(Epoch= 1086)\n",
      "CV loss=0.2897126851946131(Epoch= 1087)\n",
      "train loss=0.22563132381163004(Epoch= 1087)\n",
      "CV loss=0.28949364124398036(Epoch= 1088)\n",
      "train loss=0.22548486007026844(Epoch= 1088)\n",
      "CV loss=0.2895646003296274(Epoch= 1089)\n",
      "train loss=0.2253414047573493(Epoch= 1089)\n",
      "CV loss=0.2890588617297644(Epoch= 1090)\n",
      "train loss=0.22518573580661205(Epoch= 1090)\n",
      "CV loss=0.28924299905042333(Epoch= 1091)\n",
      "train loss=0.22503031723866782(Epoch= 1091)\n",
      "CV loss=0.2889503814162485(Epoch= 1092)\n",
      "train loss=0.22488113724481676(Epoch= 1092)\n",
      "CV loss=0.28886735365134325(Epoch= 1093)\n",
      "train loss=0.2247264196343974(Epoch= 1093)\n",
      "CV loss=0.28862618922328276(Epoch= 1094)\n",
      "train loss=0.22458054035607905(Epoch= 1094)\n",
      "CV loss=0.28864201004291123(Epoch= 1095)\n",
      "train loss=0.2244487895149318(Epoch= 1095)\n",
      "CV loss=0.28825941658018545(Epoch= 1096)\n",
      "train loss=0.22428661448374318(Epoch= 1096)\n",
      "CV loss=0.28817663777295466(Epoch= 1097)\n",
      "train loss=0.22414405761351736(Epoch= 1097)\n",
      "CV loss=0.2879247051177481(Epoch= 1098)\n",
      "train loss=0.2239842799507257(Epoch= 1098)\n",
      "CV loss=0.287886634629377(Epoch= 1099)\n",
      "train loss=0.22383575300604913(Epoch= 1099)\n",
      "CV loss=0.28776608792698544(Epoch= 1100)\n",
      "train loss=0.2236891505727292(Epoch= 1100)\n",
      "CV loss=0.2876663114098537(Epoch= 1101)\n",
      "train loss=0.22354679008935854(Epoch= 1101)\n",
      "CV loss=0.2874101382035646(Epoch= 1102)\n",
      "train loss=0.2233904430029141(Epoch= 1102)\n",
      "CV loss=0.28735547056727323(Epoch= 1103)\n",
      "train loss=0.22323578849278444(Epoch= 1103)\n",
      "CV loss=0.2870985436437081(Epoch= 1104)\n",
      "train loss=0.22309978654513238(Epoch= 1104)\n",
      "CV loss=0.2870139044490482(Epoch= 1105)\n",
      "train loss=0.22296932980442413(Epoch= 1105)\n",
      "CV loss=0.28707707772629343(Epoch= 1106)\n",
      "train loss=0.22282950871565843(Epoch= 1106)\n",
      "CV loss=0.2866449423582298(Epoch= 1107)\n",
      "train loss=0.22264904842904112(Epoch= 1107)\n",
      "CV loss=0.28647206492711474(Epoch= 1108)\n",
      "train loss=0.2225059800799168(Epoch= 1108)\n",
      "CV loss=0.2865819191464429(Epoch= 1109)\n",
      "train loss=0.22237111765137277(Epoch= 1109)\n",
      "CV loss=0.2864570964990047(Epoch= 1110)\n",
      "train loss=0.22224466987934977(Epoch= 1110)\n",
      "CV loss=0.2859230437799335(Epoch= 1111)\n",
      "train loss=0.22206592665596295(Epoch= 1111)\n",
      "CV loss=0.2858970119524219(Epoch= 1112)\n",
      "train loss=0.22191867555440312(Epoch= 1112)\n",
      "CV loss=0.28586425510419833(Epoch= 1113)\n",
      "train loss=0.2217642852397303(Epoch= 1113)\n",
      "CV loss=0.28559086920883847(Epoch= 1114)\n",
      "train loss=0.221620270040847(Epoch= 1114)\n",
      "CV loss=0.2857090915014495(Epoch= 1115)\n",
      "train loss=0.2215006162509689(Epoch= 1115)\n",
      "CV loss=0.285387844891681(Epoch= 1116)\n",
      "train loss=0.22132412244215557(Epoch= 1116)\n",
      "CV loss=0.2852793718026713(Epoch= 1117)\n",
      "train loss=0.22118170811036608(Epoch= 1117)\n",
      "CV loss=0.28500264162056316(Epoch= 1118)\n",
      "train loss=0.22103828408445683(Epoch= 1118)\n",
      "CV loss=0.2850091611808624(Epoch= 1119)\n",
      "train loss=0.2208869129293318(Epoch= 1119)\n",
      "CV loss=0.28489330530105905(Epoch= 1120)\n",
      "train loss=0.2207657364097368(Epoch= 1120)\n",
      "CV loss=0.2846232246953759(Epoch= 1121)\n",
      "train loss=0.2206038431710261(Epoch= 1121)\n",
      "CV loss=0.2844370614464009(Epoch= 1122)\n",
      "train loss=0.22045000115809454(Epoch= 1122)\n",
      "CV loss=0.28442027922758106(Epoch= 1123)\n",
      "train loss=0.2203075521121745(Epoch= 1123)\n",
      "CV loss=0.28396810800424177(Epoch= 1124)\n",
      "train loss=0.22015765255422498(Epoch= 1124)\n",
      "CV loss=0.2838155722637439(Epoch= 1125)\n",
      "train loss=0.22002566680595836(Epoch= 1125)\n",
      "CV loss=0.28396952356714583(Epoch= 1126)\n",
      "train loss=0.21987308105871217(Epoch= 1126)\n",
      "CV loss=0.2836892834722317(Epoch= 1127)\n",
      "train loss=0.2197239970284771(Epoch= 1127)\n",
      "CV loss=0.28361340501563415(Epoch= 1128)\n",
      "train loss=0.21957669885956196(Epoch= 1128)\n",
      "CV loss=0.28348206552079847(Epoch= 1129)\n",
      "train loss=0.21943249143424595(Epoch= 1129)\n",
      "CV loss=0.28322120503872283(Epoch= 1130)\n",
      "train loss=0.2192873329668116(Epoch= 1130)\n",
      "CV loss=0.28321785256666476(Epoch= 1131)\n",
      "train loss=0.21914672853189898(Epoch= 1131)\n",
      "CV loss=0.28313219234065146(Epoch= 1132)\n",
      "train loss=0.2190050747373179(Epoch= 1132)\n",
      "CV loss=0.2827550051574582(Epoch= 1133)\n",
      "train loss=0.21885448958695708(Epoch= 1133)\n",
      "CV loss=0.28271958826902627(Epoch= 1134)\n",
      "train loss=0.21871086567728545(Epoch= 1134)\n",
      "CV loss=0.2824981596161534(Epoch= 1135)\n",
      "train loss=0.2185673531661148(Epoch= 1135)\n",
      "CV loss=0.28224888733058534(Epoch= 1136)\n",
      "train loss=0.21841973664647904(Epoch= 1136)\n",
      "CV loss=0.2820691553918473(Epoch= 1137)\n",
      "train loss=0.2182862049819032(Epoch= 1137)\n",
      "CV loss=0.2820417937193358(Epoch= 1138)\n",
      "train loss=0.2181378050930155(Epoch= 1138)\n",
      "CV loss=0.2819643343007423(Epoch= 1139)\n",
      "train loss=0.2179922213723982(Epoch= 1139)\n",
      "CV loss=0.28187418637872425(Epoch= 1140)\n",
      "train loss=0.21784671625283208(Epoch= 1140)\n",
      "CV loss=0.28160552195726496(Epoch= 1141)\n",
      "train loss=0.2177095716409818(Epoch= 1141)\n",
      "CV loss=0.28139829469812994(Epoch= 1142)\n",
      "train loss=0.21755976063906937(Epoch= 1142)\n",
      "CV loss=0.28138209958046556(Epoch= 1143)\n",
      "train loss=0.2174177637116068(Epoch= 1143)\n",
      "CV loss=0.2814731534934242(Epoch= 1144)\n",
      "train loss=0.2172808641936411(Epoch= 1144)\n",
      "CV loss=0.281077170713826(Epoch= 1145)\n",
      "train loss=0.21712960391178382(Epoch= 1145)\n",
      "CV loss=0.28086056888645383(Epoch= 1146)\n",
      "train loss=0.21700852108920893(Epoch= 1146)\n",
      "CV loss=0.28085573394736685(Epoch= 1147)\n",
      "train loss=0.2168488329035076(Epoch= 1147)\n",
      "CV loss=0.2808745514305919(Epoch= 1148)\n",
      "train loss=0.2167112783466481(Epoch= 1148)\n",
      "CV loss=0.28068833084603473(Epoch= 1149)\n",
      "train loss=0.2165766326795306(Epoch= 1149)\n",
      "CV loss=0.28039236206413115(Epoch= 1150)\n",
      "train loss=0.2164305747721148(Epoch= 1150)\n",
      "CV loss=0.2802424692735258(Epoch= 1151)\n",
      "train loss=0.216276921419444(Epoch= 1151)\n",
      "CV loss=0.27996193972523137(Epoch= 1152)\n",
      "train loss=0.21615176039575085(Epoch= 1152)\n",
      "CV loss=0.2799462340933256(Epoch= 1153)\n",
      "train loss=0.21598866958638138(Epoch= 1153)\n",
      "CV loss=0.27969091991438316(Epoch= 1154)\n",
      "train loss=0.21585942332047267(Epoch= 1154)\n",
      "CV loss=0.27961929199891616(Epoch= 1155)\n",
      "train loss=0.2157055566641036(Epoch= 1155)\n",
      "CV loss=0.27950374857048427(Epoch= 1156)\n",
      "train loss=0.21556685485127808(Epoch= 1156)\n",
      "CV loss=0.2794176365738611(Epoch= 1157)\n",
      "train loss=0.21542255773684602(Epoch= 1157)\n",
      "CV loss=0.2793224709517765(Epoch= 1158)\n",
      "train loss=0.21529464414197055(Epoch= 1158)\n",
      "CV loss=0.2791690447938523(Epoch= 1159)\n",
      "train loss=0.21514731937793385(Epoch= 1159)\n",
      "CV loss=0.27910649367466184(Epoch= 1160)\n",
      "train loss=0.21501914082880366(Epoch= 1160)\n",
      "CV loss=0.2786270065197036(Epoch= 1161)\n",
      "train loss=0.2148722259778831(Epoch= 1161)\n",
      "CV loss=0.2787369055871499(Epoch= 1162)\n",
      "train loss=0.21472190172122713(Epoch= 1162)\n",
      "CV loss=0.27836483176036797(Epoch= 1163)\n",
      "train loss=0.2145938370283545(Epoch= 1163)\n",
      "CV loss=0.2781352541502719(Epoch= 1164)\n",
      "train loss=0.21444267754266122(Epoch= 1164)\n",
      "CV loss=0.27813485021152656(Epoch= 1165)\n",
      "train loss=0.21429600662488832(Epoch= 1165)\n",
      "CV loss=0.2782038901996045(Epoch= 1166)\n",
      "train loss=0.21416156893405808(Epoch= 1166)\n",
      "CV loss=0.2778144033649682(Epoch= 1167)\n",
      "train loss=0.21401871086206672(Epoch= 1167)\n",
      "CV loss=0.2778019909097227(Epoch= 1168)\n",
      "train loss=0.2138805481586794(Epoch= 1168)\n",
      "CV loss=0.2777036279274913(Epoch= 1169)\n",
      "train loss=0.21375712685082404(Epoch= 1169)\n",
      "CV loss=0.2774701327957836(Epoch= 1170)\n",
      "train loss=0.2135962769756445(Epoch= 1170)\n",
      "CV loss=0.2771924196896697(Epoch= 1171)\n",
      "train loss=0.21347135523060204(Epoch= 1171)\n",
      "CV loss=0.27708741944491067(Epoch= 1172)\n",
      "train loss=0.21332308547399922(Epoch= 1172)\n",
      "CV loss=0.27706680473600875(Epoch= 1173)\n",
      "train loss=0.21317924718177675(Epoch= 1173)\n",
      "CV loss=0.2769362666245526(Epoch= 1174)\n",
      "train loss=0.21304985863619497(Epoch= 1174)\n",
      "CV loss=0.27687617959142696(Epoch= 1175)\n",
      "train loss=0.2129110327545613(Epoch= 1175)\n",
      "CV loss=0.27670790421696667(Epoch= 1176)\n",
      "train loss=0.212762358301506(Epoch= 1176)\n",
      "CV loss=0.27657443558250805(Epoch= 1177)\n",
      "train loss=0.212625394962608(Epoch= 1177)\n",
      "CV loss=0.2762792655204761(Epoch= 1178)\n",
      "train loss=0.21248678536995114(Epoch= 1178)\n",
      "CV loss=0.2762105386277911(Epoch= 1179)\n",
      "train loss=0.21234097618626366(Epoch= 1179)\n",
      "CV loss=0.27599195922843583(Epoch= 1180)\n",
      "train loss=0.21221503780359252(Epoch= 1180)\n",
      "CV loss=0.27594298443511794(Epoch= 1181)\n",
      "train loss=0.21206739492286988(Epoch= 1181)\n",
      "CV loss=0.2756997199165872(Epoch= 1182)\n",
      "train loss=0.21194556473045845(Epoch= 1182)\n",
      "CV loss=0.2756140676696069(Epoch= 1183)\n",
      "train loss=0.2117964490803429(Epoch= 1183)\n",
      "CV loss=0.27553611461094296(Epoch= 1184)\n",
      "train loss=0.2116610177456166(Epoch= 1184)\n",
      "CV loss=0.27555341021753604(Epoch= 1185)\n",
      "train loss=0.2115317720391476(Epoch= 1185)\n",
      "CV loss=0.27511251682231225(Epoch= 1186)\n",
      "train loss=0.2113802841345136(Epoch= 1186)\n",
      "CV loss=0.27517679335998635(Epoch= 1187)\n",
      "train loss=0.21123780719001195(Epoch= 1187)\n",
      "CV loss=0.27526980202731444(Epoch= 1188)\n",
      "train loss=0.21112953728823625(Epoch= 1188)\n",
      "CV loss=0.27484503064694205(Epoch= 1189)\n",
      "train loss=0.2109648871428617(Epoch= 1189)\n",
      "CV loss=0.2748340519024898(Epoch= 1190)\n",
      "train loss=0.21084240475253255(Epoch= 1190)\n",
      "CV loss=0.27431699765125417(Epoch= 1191)\n",
      "train loss=0.21069903127902018(Epoch= 1191)\n",
      "CV loss=0.2745678570915676(Epoch= 1192)\n",
      "train loss=0.2105700654587731(Epoch= 1192)\n",
      "CV loss=0.2741678119687512(Epoch= 1193)\n",
      "train loss=0.21041625414224052(Epoch= 1193)\n",
      "CV loss=0.274115575504616(Epoch= 1194)\n",
      "train loss=0.21029057133441068(Epoch= 1194)\n",
      "CV loss=0.2739784064286736(Epoch= 1195)\n",
      "train loss=0.21015228880269238(Epoch= 1195)\n",
      "CV loss=0.2737110959390967(Epoch= 1196)\n",
      "train loss=0.21000828136967528(Epoch= 1196)\n",
      "CV loss=0.27361942691900953(Epoch= 1197)\n",
      "train loss=0.20987778293545345(Epoch= 1197)\n",
      "CV loss=0.27343136202580154(Epoch= 1198)\n",
      "train loss=0.20974069300643022(Epoch= 1198)\n",
      "CV loss=0.2734130491249674(Epoch= 1199)\n",
      "train loss=0.20961314361566485(Epoch= 1199)\n",
      "CV loss=0.27308197567666376(Epoch= 1200)\n",
      "train loss=0.20948407778729797(Epoch= 1200)\n",
      "CV loss=0.2732240030638122(Epoch= 1201)\n",
      "train loss=0.20932642102863946(Epoch= 1201)\n",
      "CV loss=0.2729482078110726(Epoch= 1202)\n",
      "train loss=0.20919311428362275(Epoch= 1202)\n",
      "CV loss=0.2728031234205104(Epoch= 1203)\n",
      "train loss=0.2090588551302373(Epoch= 1203)\n",
      "CV loss=0.2726045422433122(Epoch= 1204)\n",
      "train loss=0.2089231877880006(Epoch= 1204)\n",
      "CV loss=0.2726110990216193(Epoch= 1205)\n",
      "train loss=0.20878119219595542(Epoch= 1205)\n",
      "CV loss=0.27227095357071307(Epoch= 1206)\n",
      "train loss=0.20867216733003444(Epoch= 1206)\n",
      "CV loss=0.27230624113037494(Epoch= 1207)\n",
      "train loss=0.20852494312419317(Epoch= 1207)\n",
      "CV loss=0.2722428510151374(Epoch= 1208)\n",
      "train loss=0.20838058103639265(Epoch= 1208)\n",
      "CV loss=0.2718514773556069(Epoch= 1209)\n",
      "train loss=0.20824909944729036(Epoch= 1209)\n",
      "CV loss=0.2718811555271833(Epoch= 1210)\n",
      "train loss=0.20810813718074253(Epoch= 1210)\n",
      "CV loss=0.27181702467443986(Epoch= 1211)\n",
      "train loss=0.20797436134247463(Epoch= 1211)\n",
      "CV loss=0.27156130614300084(Epoch= 1212)\n",
      "train loss=0.20783529436625964(Epoch= 1212)\n",
      "CV loss=0.2714409950272942(Epoch= 1213)\n",
      "train loss=0.20770110777060996(Epoch= 1213)\n",
      "CV loss=0.2711523155195722(Epoch= 1214)\n",
      "train loss=0.2075690232697602(Epoch= 1214)\n",
      "CV loss=0.27103603728663356(Epoch= 1215)\n",
      "train loss=0.20743903792084661(Epoch= 1215)\n",
      "CV loss=0.2709982995204649(Epoch= 1216)\n",
      "train loss=0.20730913235476356(Epoch= 1216)\n",
      "CV loss=0.27090056905343596(Epoch= 1217)\n",
      "train loss=0.207167674918975(Epoch= 1217)\n",
      "CV loss=0.2708123738453795(Epoch= 1218)\n",
      "train loss=0.20703416799492597(Epoch= 1218)\n",
      "CV loss=0.2705849187885663(Epoch= 1219)\n",
      "train loss=0.20690421713124466(Epoch= 1219)\n",
      "CV loss=0.2702891890725263(Epoch= 1220)\n",
      "train loss=0.20676998240318145(Epoch= 1220)\n",
      "CV loss=0.2702828338049019(Epoch= 1221)\n",
      "train loss=0.2066289336889877(Epoch= 1221)\n",
      "CV loss=0.27021980082247593(Epoch= 1222)\n",
      "train loss=0.20650011990163075(Epoch= 1222)\n",
      "CV loss=0.27017404609578377(Epoch= 1223)\n",
      "train loss=0.20636356427403757(Epoch= 1223)\n",
      "CV loss=0.2699864423874683(Epoch= 1224)\n",
      "train loss=0.20622996796939747(Epoch= 1224)\n",
      "CV loss=0.2698397952064355(Epoch= 1225)\n",
      "train loss=0.2061119247047896(Epoch= 1225)\n",
      "CV loss=0.2696850556591892(Epoch= 1226)\n",
      "train loss=0.2059651856135231(Epoch= 1226)\n",
      "CV loss=0.2694785735354883(Epoch= 1227)\n",
      "train loss=0.2058309708583845(Epoch= 1227)\n",
      "CV loss=0.26941791162722595(Epoch= 1228)\n",
      "train loss=0.2056965253218681(Epoch= 1228)\n",
      "CV loss=0.2693796353540626(Epoch= 1229)\n",
      "train loss=0.20557473604485166(Epoch= 1229)\n",
      "CV loss=0.26916299556494844(Epoch= 1230)\n",
      "train loss=0.20543414279310127(Epoch= 1230)\n",
      "CV loss=0.26914027218521824(Epoch= 1231)\n",
      "train loss=0.2053051288232596(Epoch= 1231)\n",
      "CV loss=0.2690419874927375(Epoch= 1232)\n",
      "train loss=0.2051796205651319(Epoch= 1232)\n",
      "CV loss=0.26863137411604743(Epoch= 1233)\n",
      "train loss=0.20503605910697364(Epoch= 1233)\n",
      "CV loss=0.26868607961320756(Epoch= 1234)\n",
      "train loss=0.20491880213248737(Epoch= 1234)\n",
      "CV loss=0.2684059397833591(Epoch= 1235)\n",
      "train loss=0.2047700257880309(Epoch= 1235)\n",
      "CV loss=0.2683917673294921(Epoch= 1236)\n",
      "train loss=0.20465885725467564(Epoch= 1236)\n",
      "CV loss=0.26804821526950584(Epoch= 1237)\n",
      "train loss=0.20450811148963458(Epoch= 1237)\n",
      "CV loss=0.2679597879281626(Epoch= 1238)\n",
      "train loss=0.20439559633483806(Epoch= 1238)\n",
      "CV loss=0.267969098126629(Epoch= 1239)\n",
      "train loss=0.2042433760172936(Epoch= 1239)\n",
      "CV loss=0.26772504334097447(Epoch= 1240)\n",
      "train loss=0.2041163633175851(Epoch= 1240)\n",
      "CV loss=0.26751803877408264(Epoch= 1241)\n",
      "train loss=0.20398508200538715(Epoch= 1241)\n",
      "CV loss=0.26757448519991467(Epoch= 1242)\n",
      "train loss=0.203851980676226(Epoch= 1242)\n",
      "CV loss=0.26766297080612467(Epoch= 1243)\n",
      "train loss=0.20373424971779436(Epoch= 1243)\n",
      "CV loss=0.2670740968611828(Epoch= 1244)\n",
      "train loss=0.2035995582548723(Epoch= 1244)\n",
      "CV loss=0.26703928828873513(Epoch= 1245)\n",
      "train loss=0.20346057420235325(Epoch= 1245)\n",
      "CV loss=0.2669254431433552(Epoch= 1246)\n",
      "train loss=0.20334096886931666(Epoch= 1246)\n",
      "CV loss=0.2669296365594527(Epoch= 1247)\n",
      "train loss=0.20320386343317565(Epoch= 1247)\n",
      "CV loss=0.26672581544651486(Epoch= 1248)\n",
      "train loss=0.20306329368276993(Epoch= 1248)\n",
      "CV loss=0.26641454845003554(Epoch= 1249)\n",
      "train loss=0.20293779954408764(Epoch= 1249)\n",
      "CV loss=0.26657663469900444(Epoch= 1250)\n",
      "train loss=0.20282302515722622(Epoch= 1250)\n",
      "CV loss=0.26627496081889007(Epoch= 1251)\n",
      "train loss=0.20267937796668853(Epoch= 1251)\n",
      "CV loss=0.2660491387942778(Epoch= 1252)\n",
      "train loss=0.20255165987671042(Epoch= 1252)\n",
      "CV loss=0.2661274190715144(Epoch= 1253)\n",
      "train loss=0.20242460417463776(Epoch= 1253)\n",
      "CV loss=0.2657717137244267(Epoch= 1254)\n",
      "train loss=0.20229255793789158(Epoch= 1254)\n",
      "CV loss=0.2658177918017864(Epoch= 1255)\n",
      "train loss=0.20216186798105756(Epoch= 1255)\n",
      "CV loss=0.2657656646563175(Epoch= 1256)\n",
      "train loss=0.20203136663862187(Epoch= 1256)\n",
      "CV loss=0.2654109128721732(Epoch= 1257)\n",
      "train loss=0.2018995153868162(Epoch= 1257)\n",
      "CV loss=0.2654309644711089(Epoch= 1258)\n",
      "train loss=0.20176899241369453(Epoch= 1258)\n",
      "CV loss=0.2651994618142911(Epoch= 1259)\n",
      "train loss=0.2016396523967107(Epoch= 1259)\n",
      "CV loss=0.2650193493667449(Epoch= 1260)\n",
      "train loss=0.20151578748166044(Epoch= 1260)\n",
      "CV loss=0.2648009299512658(Epoch= 1261)\n",
      "train loss=0.2013824171923313(Epoch= 1261)\n",
      "CV loss=0.26486311110105737(Epoch= 1262)\n",
      "train loss=0.2012517914409657(Epoch= 1262)\n",
      "CV loss=0.26475738161983525(Epoch= 1263)\n",
      "train loss=0.2011198559409451(Epoch= 1263)\n",
      "CV loss=0.26475844688031297(Epoch= 1264)\n",
      "train loss=0.20100443691161524(Epoch= 1264)\n",
      "CV loss=0.26459558883060913(Epoch= 1265)\n",
      "train loss=0.2008829915893736(Epoch= 1265)\n",
      "CV loss=0.264322283401801(Epoch= 1266)\n",
      "train loss=0.20073338624709539(Epoch= 1266)\n",
      "CV loss=0.2641579473083489(Epoch= 1267)\n",
      "train loss=0.2006112330028656(Epoch= 1267)\n",
      "CV loss=0.26421061166740933(Epoch= 1268)\n",
      "train loss=0.20048779377141618(Epoch= 1268)\n",
      "CV loss=0.26388440110991396(Epoch= 1269)\n",
      "train loss=0.20035915031321466(Epoch= 1269)\n",
      "CV loss=0.2638371626283383(Epoch= 1270)\n",
      "train loss=0.20023264505225863(Epoch= 1270)\n",
      "CV loss=0.263766583647102(Epoch= 1271)\n",
      "train loss=0.20010098829745104(Epoch= 1271)\n",
      "CV loss=0.2633577517161343(Epoch= 1272)\n",
      "train loss=0.19998126800003774(Epoch= 1272)\n",
      "CV loss=0.2633326143848017(Epoch= 1273)\n",
      "train loss=0.19984302845132437(Epoch= 1273)\n",
      "CV loss=0.26320905814336043(Epoch= 1274)\n",
      "train loss=0.1997139536793031(Epoch= 1274)\n",
      "CV loss=0.26311287061034705(Epoch= 1275)\n",
      "train loss=0.19959277882118637(Epoch= 1275)\n",
      "CV loss=0.26316122795037067(Epoch= 1276)\n",
      "train loss=0.19946045369575224(Epoch= 1276)\n",
      "CV loss=0.2630636826370549(Epoch= 1277)\n",
      "train loss=0.1993369906385979(Epoch= 1277)\n",
      "CV loss=0.2626035119440714(Epoch= 1278)\n",
      "train loss=0.19921485569871153(Epoch= 1278)\n",
      "CV loss=0.2627036772443446(Epoch= 1279)\n",
      "train loss=0.19907822756279067(Epoch= 1279)\n",
      "CV loss=0.26248974247591506(Epoch= 1280)\n",
      "train loss=0.19895556490535699(Epoch= 1280)\n",
      "CV loss=0.26243840746500746(Epoch= 1281)\n",
      "train loss=0.19883190918720406(Epoch= 1281)\n",
      "CV loss=0.2623556125185462(Epoch= 1282)\n",
      "train loss=0.19869616434894075(Epoch= 1282)\n",
      "CV loss=0.26210994809751076(Epoch= 1283)\n",
      "train loss=0.1985776727306697(Epoch= 1283)\n",
      "CV loss=0.26191717450221125(Epoch= 1284)\n",
      "train loss=0.19844818575096101(Epoch= 1284)\n",
      "CV loss=0.2619468149868558(Epoch= 1285)\n",
      "train loss=0.1983206802096844(Epoch= 1285)\n",
      "CV loss=0.26189137158624237(Epoch= 1286)\n",
      "train loss=0.1981950310942886(Epoch= 1286)\n",
      "CV loss=0.2616145979768881(Epoch= 1287)\n",
      "train loss=0.19806578176521267(Epoch= 1287)\n",
      "CV loss=0.26148517313132963(Epoch= 1288)\n",
      "train loss=0.19795103581235207(Epoch= 1288)\n",
      "CV loss=0.2612757915337923(Epoch= 1289)\n",
      "train loss=0.1978288068743558(Epoch= 1289)\n",
      "CV loss=0.2613354645985858(Epoch= 1290)\n",
      "train loss=0.1976996666665877(Epoch= 1290)\n",
      "CV loss=0.2610117083584619(Epoch= 1291)\n",
      "train loss=0.19757560175213656(Epoch= 1291)\n",
      "CV loss=0.2609838673146774(Epoch= 1292)\n",
      "train loss=0.19744103467597832(Epoch= 1292)\n",
      "CV loss=0.26063694886300126(Epoch= 1293)\n",
      "train loss=0.19732877682606886(Epoch= 1293)\n",
      "CV loss=0.26093269943212627(Epoch= 1294)\n",
      "train loss=0.19720375475456542(Epoch= 1294)\n",
      "CV loss=0.26061294928892487(Epoch= 1295)\n",
      "train loss=0.19705995259151726(Epoch= 1295)\n",
      "CV loss=0.2604432861170532(Epoch= 1296)\n",
      "train loss=0.19694185559554622(Epoch= 1296)\n",
      "CV loss=0.26034126996081364(Epoch= 1297)\n",
      "train loss=0.19681305142275948(Epoch= 1297)\n",
      "CV loss=0.26003338770286377(Epoch= 1298)\n",
      "train loss=0.19668818579666728(Epoch= 1298)\n",
      "CV loss=0.25998284494591356(Epoch= 1299)\n",
      "train loss=0.19655994670427498(Epoch= 1299)\n",
      "CV loss=0.25989279827002015(Epoch= 1300)\n",
      "train loss=0.19643757234597767(Epoch= 1300)\n",
      "CV loss=0.2600497889225051(Epoch= 1301)\n",
      "train loss=0.19632383805591067(Epoch= 1301)\n",
      "CV loss=0.25965729548872707(Epoch= 1302)\n",
      "train loss=0.19618401027407995(Epoch= 1302)\n",
      "CV loss=0.2594079905173705(Epoch= 1303)\n",
      "train loss=0.1960689803118627(Epoch= 1303)\n",
      "CV loss=0.2593243378232045(Epoch= 1304)\n",
      "train loss=0.1959402039865838(Epoch= 1304)\n",
      "CV loss=0.2591595211348232(Epoch= 1305)\n",
      "train loss=0.19582229452355207(Epoch= 1305)\n",
      "CV loss=0.2590690246206323(Epoch= 1306)\n",
      "train loss=0.19570209065412947(Epoch= 1306)\n",
      "CV loss=0.2590284539574915(Epoch= 1307)\n",
      "train loss=0.19556997434344725(Epoch= 1307)\n",
      "CV loss=0.25895644772532955(Epoch= 1308)\n",
      "train loss=0.1954425654746104(Epoch= 1308)\n",
      "CV loss=0.2588917083750363(Epoch= 1309)\n",
      "train loss=0.1953260317366628(Epoch= 1309)\n",
      "CV loss=0.25862257005916944(Epoch= 1310)\n",
      "train loss=0.19520791608532118(Epoch= 1310)\n",
      "CV loss=0.2584156198840256(Epoch= 1311)\n",
      "train loss=0.19508390175155368(Epoch= 1311)\n",
      "CV loss=0.25831877950323984(Epoch= 1312)\n",
      "train loss=0.1949683207498795(Epoch= 1312)\n",
      "CV loss=0.25819915650491787(Epoch= 1313)\n",
      "train loss=0.19482774946661274(Epoch= 1313)\n",
      "CV loss=0.2580383251853537(Epoch= 1314)\n",
      "train loss=0.19471444481878586(Epoch= 1314)\n",
      "CV loss=0.2580611035175639(Epoch= 1315)\n",
      "train loss=0.19458365638814798(Epoch= 1315)\n",
      "CV loss=0.25783662690301534(Epoch= 1316)\n",
      "train loss=0.19448168841407185(Epoch= 1316)\n",
      "CV loss=0.2580909647888949(Epoch= 1317)\n",
      "train loss=0.1943639513914676(Epoch= 1317)\n",
      "CV loss=0.2576704730565083(Epoch= 1318)\n",
      "train loss=0.19421285124475166(Epoch= 1318)\n",
      "CV loss=0.2575208533239309(Epoch= 1319)\n",
      "train loss=0.1940850779582786(Epoch= 1319)\n",
      "CV loss=0.25726252371655134(Epoch= 1320)\n",
      "train loss=0.19396954963501414(Epoch= 1320)\n",
      "CV loss=0.25726308546185406(Epoch= 1321)\n",
      "train loss=0.19384705244878925(Epoch= 1321)\n",
      "CV loss=0.2571647529987951(Epoch= 1322)\n",
      "train loss=0.19372106475599934(Epoch= 1322)\n",
      "CV loss=0.2571212240908056(Epoch= 1323)\n",
      "train loss=0.19360196545191782(Epoch= 1323)\n",
      "CV loss=0.25686366171960495(Epoch= 1324)\n",
      "train loss=0.19347841401802457(Epoch= 1324)\n",
      "CV loss=0.25683367456735506(Epoch= 1325)\n",
      "train loss=0.19336126253327415(Epoch= 1325)\n",
      "CV loss=0.25672682877300423(Epoch= 1326)\n",
      "train loss=0.19323875651140476(Epoch= 1326)\n",
      "CV loss=0.2565705537298078(Epoch= 1327)\n",
      "train loss=0.19311616586673236(Epoch= 1327)\n",
      "CV loss=0.2563388521113625(Epoch= 1328)\n",
      "train loss=0.19299405871641645(Epoch= 1328)\n",
      "CV loss=0.256252445900869(Epoch= 1329)\n",
      "train loss=0.192865929947023(Epoch= 1329)\n",
      "CV loss=0.2561407347428562(Epoch= 1330)\n",
      "train loss=0.1927456096058376(Epoch= 1330)\n",
      "CV loss=0.25591988991286957(Epoch= 1331)\n",
      "train loss=0.19263213234513143(Epoch= 1331)\n",
      "CV loss=0.2560442849195193(Epoch= 1332)\n",
      "train loss=0.1925067252450312(Epoch= 1332)\n",
      "CV loss=0.25576638685088304(Epoch= 1333)\n",
      "train loss=0.1923943363352316(Epoch= 1333)\n",
      "CV loss=0.2557440019614709(Epoch= 1334)\n",
      "train loss=0.19226936939585418(Epoch= 1334)\n",
      "CV loss=0.25553552620723063(Epoch= 1335)\n",
      "train loss=0.19215001368468806(Epoch= 1335)\n",
      "CV loss=0.25564309547684777(Epoch= 1336)\n",
      "train loss=0.1920370424168022(Epoch= 1336)\n",
      "CV loss=0.25525594323668455(Epoch= 1337)\n",
      "train loss=0.191899944982433(Epoch= 1337)\n",
      "CV loss=0.25514059497560415(Epoch= 1338)\n",
      "train loss=0.19177535887810807(Epoch= 1338)\n",
      "CV loss=0.254921485024118(Epoch= 1339)\n",
      "train loss=0.19166011310761036(Epoch= 1339)\n",
      "CV loss=0.25480129264215656(Epoch= 1340)\n",
      "train loss=0.1915353935506571(Epoch= 1340)\n",
      "CV loss=0.2547165388307257(Epoch= 1341)\n",
      "train loss=0.1914223565607416(Epoch= 1341)\n",
      "CV loss=0.25464646206757047(Epoch= 1342)\n",
      "train loss=0.1912951685780857(Epoch= 1342)\n",
      "CV loss=0.25466153409389325(Epoch= 1343)\n",
      "train loss=0.19119218722445425(Epoch= 1343)\n",
      "CV loss=0.2544400623026436(Epoch= 1344)\n",
      "train loss=0.1910845618239824(Epoch= 1344)\n",
      "CV loss=0.254205430222229(Epoch= 1345)\n",
      "train loss=0.19094650727497753(Epoch= 1345)\n",
      "CV loss=0.2541525825155093(Epoch= 1346)\n",
      "train loss=0.19082344241773694(Epoch= 1346)\n",
      "CV loss=0.254140596369838(Epoch= 1347)\n",
      "train loss=0.1906947385626703(Epoch= 1347)\n",
      "CV loss=0.2538693794923015(Epoch= 1348)\n",
      "train loss=0.19057928394328205(Epoch= 1348)\n",
      "CV loss=0.2538045229175573(Epoch= 1349)\n",
      "train loss=0.19046509089266342(Epoch= 1349)\n",
      "CV loss=0.2537482927667969(Epoch= 1350)\n",
      "train loss=0.19033715458341355(Epoch= 1350)\n",
      "CV loss=0.25354855126364095(Epoch= 1351)\n",
      "train loss=0.19023112769264644(Epoch= 1351)\n",
      "CV loss=0.2536155662123643(Epoch= 1352)\n",
      "train loss=0.19010825386581132(Epoch= 1352)\n",
      "CV loss=0.25318805623816065(Epoch= 1353)\n",
      "train loss=0.18998536667803995(Epoch= 1353)\n",
      "CV loss=0.25322911220936706(Epoch= 1354)\n",
      "train loss=0.1898727313833916(Epoch= 1354)\n",
      "CV loss=0.2529940632530129(Epoch= 1355)\n",
      "train loss=0.18975124867993387(Epoch= 1355)\n",
      "CV loss=0.2528457622576781(Epoch= 1356)\n",
      "train loss=0.18962692847687862(Epoch= 1356)\n",
      "CV loss=0.252751243842735(Epoch= 1357)\n",
      "train loss=0.18953855600266734(Epoch= 1357)\n",
      "CV loss=0.25285333397248133(Epoch= 1358)\n",
      "train loss=0.1893953155697469(Epoch= 1358)\n",
      "CV loss=0.25282918609078675(Epoch= 1359)\n",
      "train loss=0.18927708474132313(Epoch= 1359)\n",
      "CV loss=0.25239639913322764(Epoch= 1360)\n",
      "train loss=0.18915141171967342(Epoch= 1360)\n",
      "CV loss=0.2525340367294944(Epoch= 1361)\n",
      "train loss=0.18905080419992656(Epoch= 1361)\n",
      "CV loss=0.25211607587234386(Epoch= 1362)\n",
      "train loss=0.18891802349859887(Epoch= 1362)\n",
      "CV loss=0.25213821977402506(Epoch= 1363)\n",
      "train loss=0.18880207221325904(Epoch= 1363)\n",
      "CV loss=0.2521465262621091(Epoch= 1364)\n",
      "train loss=0.18868320633354269(Epoch= 1364)\n",
      "CV loss=0.25185657094092945(Epoch= 1365)\n",
      "train loss=0.1885609113397056(Epoch= 1365)\n",
      "CV loss=0.2517883766799698(Epoch= 1366)\n",
      "train loss=0.1884645175477042(Epoch= 1366)\n",
      "CV loss=0.25169741506504717(Epoch= 1367)\n",
      "train loss=0.18833352416869048(Epoch= 1367)\n",
      "CV loss=0.2515119846477252(Epoch= 1368)\n",
      "train loss=0.1882080641691964(Epoch= 1368)\n",
      "CV loss=0.251431359593236(Epoch= 1369)\n",
      "train loss=0.188091796773509(Epoch= 1369)\n",
      "CV loss=0.25113210814017384(Epoch= 1370)\n",
      "train loss=0.1879770307509223(Epoch= 1370)\n",
      "CV loss=0.25112307680777074(Epoch= 1371)\n",
      "train loss=0.18786158154500146(Epoch= 1371)\n",
      "CV loss=0.25098690855307365(Epoch= 1372)\n",
      "train loss=0.1877418753932149(Epoch= 1372)\n",
      "CV loss=0.2508705700618409(Epoch= 1373)\n",
      "train loss=0.1876251402843888(Epoch= 1373)\n",
      "CV loss=0.25077582889422423(Epoch= 1374)\n",
      "train loss=0.18751067808242672(Epoch= 1374)\n",
      "CV loss=0.2505668675729187(Epoch= 1375)\n",
      "train loss=0.18740560971851833(Epoch= 1375)\n",
      "CV loss=0.25068613802453327(Epoch= 1376)\n",
      "train loss=0.18727733632600058(Epoch= 1376)\n",
      "CV loss=0.25050271856772915(Epoch= 1377)\n",
      "train loss=0.1871632146713003(Epoch= 1377)\n",
      "CV loss=0.25033107232800855(Epoch= 1378)\n",
      "train loss=0.18704628157717215(Epoch= 1378)\n",
      "CV loss=0.25016639706359817(Epoch= 1379)\n",
      "train loss=0.18692334483199505(Epoch= 1379)\n",
      "CV loss=0.25009187115771947(Epoch= 1380)\n",
      "train loss=0.18680736524719632(Epoch= 1380)\n",
      "CV loss=0.24984199351999065(Epoch= 1381)\n",
      "train loss=0.18670117258416113(Epoch= 1381)\n",
      "CV loss=0.25008990483549265(Epoch= 1382)\n",
      "train loss=0.1865817169854448(Epoch= 1382)\n",
      "CV loss=0.2497192206264876(Epoch= 1383)\n",
      "train loss=0.18645722203085316(Epoch= 1383)\n",
      "CV loss=0.24962985114692676(Epoch= 1384)\n",
      "train loss=0.18634166615887604(Epoch= 1384)\n",
      "CV loss=0.2495656980812015(Epoch= 1385)\n",
      "train loss=0.1862303740614062(Epoch= 1385)\n",
      "CV loss=0.24941984071500573(Epoch= 1386)\n",
      "train loss=0.1861121276126908(Epoch= 1386)\n",
      "CV loss=0.2493617557595703(Epoch= 1387)\n",
      "train loss=0.18600330068904286(Epoch= 1387)\n",
      "CV loss=0.249239827971768(Epoch= 1388)\n",
      "train loss=0.18588039416488628(Epoch= 1388)\n",
      "CV loss=0.2489763329114172(Epoch= 1389)\n",
      "train loss=0.18576255202410125(Epoch= 1389)\n",
      "CV loss=0.2490297046486791(Epoch= 1390)\n",
      "train loss=0.18565047771691587(Epoch= 1390)\n",
      "CV loss=0.2488653122832457(Epoch= 1391)\n",
      "train loss=0.18553221721424606(Epoch= 1391)\n",
      "CV loss=0.24872434176146357(Epoch= 1392)\n",
      "train loss=0.18542034029426196(Epoch= 1392)\n",
      "CV loss=0.24860080978145815(Epoch= 1393)\n",
      "train loss=0.185307883933794(Epoch= 1393)\n",
      "CV loss=0.2486137545106058(Epoch= 1394)\n",
      "train loss=0.18519058103997518(Epoch= 1394)\n",
      "CV loss=0.24832883824758895(Epoch= 1395)\n",
      "train loss=0.18508152375275919(Epoch= 1395)\n",
      "CV loss=0.2481656617794873(Epoch= 1396)\n",
      "train loss=0.18497959620143978(Epoch= 1396)\n",
      "CV loss=0.2481933148384718(Epoch= 1397)\n",
      "train loss=0.1848624196111192(Epoch= 1397)\n",
      "CV loss=0.247988000588755(Epoch= 1398)\n",
      "train loss=0.18473246999592738(Epoch= 1398)\n",
      "CV loss=0.24791261087147665(Epoch= 1399)\n",
      "train loss=0.1846302373116748(Epoch= 1399)\n",
      "CV loss=0.24770971621816956(Epoch= 1400)\n",
      "train loss=0.18451151605935995(Epoch= 1400)\n",
      "CV loss=0.24778287798799958(Epoch= 1401)\n",
      "train loss=0.18439615874283263(Epoch= 1401)\n",
      "CV loss=0.24756892536728806(Epoch= 1402)\n",
      "train loss=0.18427069653918432(Epoch= 1402)\n",
      "CV loss=0.24733132819554499(Epoch= 1403)\n",
      "train loss=0.1841549991848028(Epoch= 1403)\n",
      "CV loss=0.2472757856711234(Epoch= 1404)\n",
      "train loss=0.1840470289590397(Epoch= 1404)\n",
      "CV loss=0.2470244384155037(Epoch= 1405)\n",
      "train loss=0.18393556543289136(Epoch= 1405)\n",
      "CV loss=0.24702418824982614(Epoch= 1406)\n",
      "train loss=0.18382766980600404(Epoch= 1406)\n",
      "CV loss=0.24689593453474873(Epoch= 1407)\n",
      "train loss=0.1837020444263097(Epoch= 1407)\n",
      "CV loss=0.2468911242255496(Epoch= 1408)\n",
      "train loss=0.18359755395588784(Epoch= 1408)\n",
      "CV loss=0.24674336003941172(Epoch= 1409)\n",
      "train loss=0.1834761895978672(Epoch= 1409)\n",
      "CV loss=0.2464688964069133(Epoch= 1410)\n",
      "train loss=0.18339176078952177(Epoch= 1410)\n",
      "CV loss=0.24641364747133007(Epoch= 1411)\n",
      "train loss=0.18326820680310277(Epoch= 1411)\n",
      "CV loss=0.24626167829417422(Epoch= 1412)\n",
      "train loss=0.18314801553853172(Epoch= 1412)\n",
      "CV loss=0.2462889581991792(Epoch= 1413)\n",
      "train loss=0.18302468915856185(Epoch= 1413)\n",
      "CV loss=0.24618834007067403(Epoch= 1414)\n",
      "train loss=0.18290991935786396(Epoch= 1414)\n",
      "CV loss=0.2460223304968644(Epoch= 1415)\n",
      "train loss=0.18280569805555305(Epoch= 1415)\n",
      "CV loss=0.24593100341541857(Epoch= 1416)\n",
      "train loss=0.18270203256531548(Epoch= 1416)\n",
      "CV loss=0.24572858422842364(Epoch= 1417)\n",
      "train loss=0.18258385755290993(Epoch= 1417)\n",
      "CV loss=0.24573128510332906(Epoch= 1418)\n",
      "train loss=0.18247255783578134(Epoch= 1418)\n",
      "CV loss=0.2454083367699125(Epoch= 1419)\n",
      "train loss=0.182359186441893(Epoch= 1419)\n",
      "CV loss=0.2454099993552985(Epoch= 1420)\n",
      "train loss=0.18224516791638093(Epoch= 1420)\n",
      "CV loss=0.24539716010007456(Epoch= 1421)\n",
      "train loss=0.18212840612809614(Epoch= 1421)\n",
      "CV loss=0.24515105799824694(Epoch= 1422)\n",
      "train loss=0.18201063225990316(Epoch= 1422)\n",
      "CV loss=0.2450203412916144(Epoch= 1423)\n",
      "train loss=0.18189886635424066(Epoch= 1423)\n",
      "CV loss=0.24480044841174398(Epoch= 1424)\n",
      "train loss=0.1817990438564803(Epoch= 1424)\n",
      "CV loss=0.24499774505131378(Epoch= 1425)\n",
      "train loss=0.1816834239296895(Epoch= 1425)\n",
      "CV loss=0.24478540031836196(Epoch= 1426)\n",
      "train loss=0.18157139217574944(Epoch= 1426)\n",
      "CV loss=0.24451495592076092(Epoch= 1427)\n",
      "train loss=0.18145404366073029(Epoch= 1427)\n",
      "CV loss=0.24452530672100944(Epoch= 1428)\n",
      "train loss=0.1813429852791615(Epoch= 1428)\n",
      "CV loss=0.24445730536253973(Epoch= 1429)\n",
      "train loss=0.18122910130218267(Epoch= 1429)\n",
      "CV loss=0.24428297741275062(Epoch= 1430)\n",
      "train loss=0.18111726664111955(Epoch= 1430)\n",
      "CV loss=0.24419105975601757(Epoch= 1431)\n",
      "train loss=0.18100685263477798(Epoch= 1431)\n",
      "CV loss=0.24398033448098538(Epoch= 1432)\n",
      "train loss=0.18089804385290995(Epoch= 1432)\n",
      "CV loss=0.24405867355351535(Epoch= 1433)\n",
      "train loss=0.18079499314277128(Epoch= 1433)\n",
      "CV loss=0.24398502134816322(Epoch= 1434)\n",
      "train loss=0.1806785605970541(Epoch= 1434)\n",
      "CV loss=0.2438853439791992(Epoch= 1435)\n",
      "train loss=0.18057341459507237(Epoch= 1435)\n",
      "CV loss=0.243637778200197(Epoch= 1436)\n",
      "train loss=0.1804575793416439(Epoch= 1436)\n",
      "CV loss=0.24345532372201165(Epoch= 1437)\n",
      "train loss=0.1803433171007634(Epoch= 1437)\n",
      "CV loss=0.24351414234110696(Epoch= 1438)\n",
      "train loss=0.1802311043821871(Epoch= 1438)\n",
      "CV loss=0.2434236350261103(Epoch= 1439)\n",
      "train loss=0.1801246664972999(Epoch= 1439)\n",
      "CV loss=0.24340529268257646(Epoch= 1440)\n",
      "train loss=0.1800231303463226(Epoch= 1440)\n",
      "CV loss=0.2429784203354982(Epoch= 1441)\n",
      "train loss=0.17989968687161012(Epoch= 1441)\n",
      "CV loss=0.24275633703697325(Epoch= 1442)\n",
      "train loss=0.17979893363568347(Epoch= 1442)\n",
      "CV loss=0.24281287923960843(Epoch= 1443)\n",
      "train loss=0.17968639375646472(Epoch= 1443)\n",
      "CV loss=0.2426914282134613(Epoch= 1444)\n",
      "train loss=0.17956838549454596(Epoch= 1444)\n",
      "CV loss=0.24261478234946254(Epoch= 1445)\n",
      "train loss=0.1794696910918859(Epoch= 1445)\n",
      "CV loss=0.2425434126596934(Epoch= 1446)\n",
      "train loss=0.1793674980318603(Epoch= 1446)\n",
      "CV loss=0.24231897728677992(Epoch= 1447)\n",
      "train loss=0.17924886867459258(Epoch= 1447)\n",
      "CV loss=0.2422321372306369(Epoch= 1448)\n",
      "train loss=0.17913197003100348(Epoch= 1448)\n",
      "CV loss=0.2422657279016983(Epoch= 1449)\n",
      "train loss=0.1790244125776926(Epoch= 1449)\n",
      "CV loss=0.24217502146299352(Epoch= 1450)\n",
      "train loss=0.17891559459582088(Epoch= 1450)\n",
      "CV loss=0.24196564051494784(Epoch= 1451)\n",
      "train loss=0.17880379053618758(Epoch= 1451)\n",
      "CV loss=0.24212756157545473(Epoch= 1452)\n",
      "train loss=0.17871115710879884(Epoch= 1452)\n",
      "CV loss=0.24160705331677418(Epoch= 1453)\n",
      "train loss=0.1785944060393783(Epoch= 1453)\n",
      "CV loss=0.24146493621474302(Epoch= 1454)\n",
      "train loss=0.17849621078405292(Epoch= 1454)\n",
      "CV loss=0.24152791251062483(Epoch= 1455)\n",
      "train loss=0.1783625147726042(Epoch= 1455)\n",
      "CV loss=0.241312694371907(Epoch= 1456)\n",
      "train loss=0.17825516225184254(Epoch= 1456)\n",
      "CV loss=0.24128526213343465(Epoch= 1457)\n",
      "train loss=0.1781501190965113(Epoch= 1457)\n",
      "CV loss=0.24114273835986982(Epoch= 1458)\n",
      "train loss=0.1780444941280061(Epoch= 1458)\n",
      "CV loss=0.2409686174962437(Epoch= 1459)\n",
      "train loss=0.17793919538162734(Epoch= 1459)\n",
      "CV loss=0.24091696827860798(Epoch= 1460)\n",
      "train loss=0.17783460163612086(Epoch= 1460)\n",
      "CV loss=0.240861701939855(Epoch= 1461)\n",
      "train loss=0.17771751007813164(Epoch= 1461)\n",
      "CV loss=0.24077195612209915(Epoch= 1462)\n",
      "train loss=0.1776074232930548(Epoch= 1462)\n",
      "CV loss=0.2405372859494318(Epoch= 1463)\n",
      "train loss=0.17749379515400765(Epoch= 1463)\n",
      "CV loss=0.24056936850225696(Epoch= 1464)\n",
      "train loss=0.17738816430772283(Epoch= 1464)\n",
      "CV loss=0.24035853989773864(Epoch= 1465)\n",
      "train loss=0.1772802698079084(Epoch= 1465)\n",
      "CV loss=0.24015784508693788(Epoch= 1466)\n",
      "train loss=0.17717860647551595(Epoch= 1466)\n",
      "CV loss=0.2401172011146865(Epoch= 1467)\n",
      "train loss=0.17706712276550524(Epoch= 1467)\n",
      "CV loss=0.24003539105665317(Epoch= 1468)\n",
      "train loss=0.17696895160219522(Epoch= 1468)\n",
      "CV loss=0.2399873812318384(Epoch= 1469)\n",
      "train loss=0.17685541385367815(Epoch= 1469)\n",
      "CV loss=0.2396741661232282(Epoch= 1470)\n",
      "train loss=0.17674977844960182(Epoch= 1470)\n",
      "CV loss=0.23967594775211232(Epoch= 1471)\n",
      "train loss=0.1766331510852556(Epoch= 1471)\n",
      "CV loss=0.2397796865458102(Epoch= 1472)\n",
      "train loss=0.17654195221560073(Epoch= 1472)\n",
      "CV loss=0.23964557667452702(Epoch= 1473)\n",
      "train loss=0.17643201175076403(Epoch= 1473)\n",
      "CV loss=0.23918946862716495(Epoch= 1474)\n",
      "train loss=0.17633431960320406(Epoch= 1474)\n",
      "CV loss=0.23952514071849684(Epoch= 1475)\n",
      "train loss=0.17621236295727583(Epoch= 1475)\n",
      "CV loss=0.23923601932527094(Epoch= 1476)\n",
      "train loss=0.17610400443389837(Epoch= 1476)\n",
      "CV loss=0.239078754347804(Epoch= 1477)\n",
      "train loss=0.1759972019371128(Epoch= 1477)\n",
      "CV loss=0.23903394261577682(Epoch= 1478)\n",
      "train loss=0.1758830093849499(Epoch= 1478)\n",
      "CV loss=0.23892673427583508(Epoch= 1479)\n",
      "train loss=0.17579935542543587(Epoch= 1479)\n",
      "CV loss=0.23875145528023073(Epoch= 1480)\n",
      "train loss=0.17567155677696922(Epoch= 1480)\n",
      "CV loss=0.2388005338889323(Epoch= 1481)\n",
      "train loss=0.17557518275537404(Epoch= 1481)\n",
      "CV loss=0.23841272037050643(Epoch= 1482)\n",
      "train loss=0.1754614670189653(Epoch= 1482)\n",
      "CV loss=0.2383006217569576(Epoch= 1483)\n",
      "train loss=0.1753624837505132(Epoch= 1483)\n",
      "CV loss=0.23834694356405162(Epoch= 1484)\n",
      "train loss=0.17524432621451197(Epoch= 1484)\n",
      "CV loss=0.23817072566152137(Epoch= 1485)\n",
      "train loss=0.17516197985486734(Epoch= 1485)\n",
      "CV loss=0.23808465550382063(Epoch= 1486)\n",
      "train loss=0.17503800838858075(Epoch= 1486)\n",
      "CV loss=0.23790357182844907(Epoch= 1487)\n",
      "train loss=0.17493708218000262(Epoch= 1487)\n",
      "CV loss=0.23809404465624084(Epoch= 1488)\n",
      "train loss=0.17483029474734174(Epoch= 1488)\n",
      "CV loss=0.2378414395364275(Epoch= 1489)\n",
      "train loss=0.17472023670070147(Epoch= 1489)\n",
      "CV loss=0.23773541217539743(Epoch= 1490)\n",
      "train loss=0.1746149201938983(Epoch= 1490)\n",
      "CV loss=0.2377764256767274(Epoch= 1491)\n",
      "train loss=0.17451079733534094(Epoch= 1491)\n",
      "CV loss=0.23745595064423014(Epoch= 1492)\n",
      "train loss=0.17441006385871777(Epoch= 1492)\n",
      "CV loss=0.2373745775551258(Epoch= 1493)\n",
      "train loss=0.1742928523788081(Epoch= 1493)\n",
      "CV loss=0.23727582138189093(Epoch= 1494)\n",
      "train loss=0.17418417043438866(Epoch= 1494)\n",
      "CV loss=0.23709915567037848(Epoch= 1495)\n",
      "train loss=0.17408910860995042(Epoch= 1495)\n",
      "CV loss=0.23695680221223597(Epoch= 1496)\n",
      "train loss=0.17397919543892548(Epoch= 1496)\n",
      "CV loss=0.23698430750421579(Epoch= 1497)\n",
      "train loss=0.1738749105883214(Epoch= 1497)\n",
      "CV loss=0.23676388267481682(Epoch= 1498)\n",
      "train loss=0.17377535677278153(Epoch= 1498)\n",
      "CV loss=0.236710613632572(Epoch= 1499)\n",
      "train loss=0.17367243600623766(Epoch= 1499)\n",
      "CV loss=0.2366186022928683(Epoch= 1500)\n",
      "train loss=0.17358086813703325(Epoch= 1500)\n",
      "CV loss=0.2365136607082604(Epoch= 1501)\n",
      "train loss=0.17345248493198331(Epoch= 1501)\n",
      "CV loss=0.23650363200074243(Epoch= 1502)\n",
      "train loss=0.17335982349159282(Epoch= 1502)\n",
      "CV loss=0.23617183325711955(Epoch= 1503)\n",
      "train loss=0.1732553771881046(Epoch= 1503)\n",
      "CV loss=0.23615611316175045(Epoch= 1504)\n",
      "train loss=0.17314076032211556(Epoch= 1504)\n",
      "CV loss=0.23609959693741106(Epoch= 1505)\n",
      "train loss=0.17304541659819142(Epoch= 1505)\n",
      "CV loss=0.23603929421435366(Epoch= 1506)\n",
      "train loss=0.17293893466182894(Epoch= 1506)\n",
      "CV loss=0.2358211194827764(Epoch= 1507)\n",
      "train loss=0.17283050541576436(Epoch= 1507)\n",
      "CV loss=0.23562887286828976(Epoch= 1508)\n",
      "train loss=0.1727248150949338(Epoch= 1508)\n",
      "CV loss=0.2355946570003279(Epoch= 1509)\n",
      "train loss=0.17262412063159874(Epoch= 1509)\n",
      "CV loss=0.23551820144335278(Epoch= 1510)\n",
      "train loss=0.17253452137544936(Epoch= 1510)\n",
      "CV loss=0.23551605673443093(Epoch= 1511)\n",
      "train loss=0.17241685727797526(Epoch= 1511)\n",
      "CV loss=0.23525813257369357(Epoch= 1512)\n",
      "train loss=0.1723069361733422(Epoch= 1512)\n",
      "CV loss=0.2351688427505276(Epoch= 1513)\n",
      "train loss=0.17220655716028915(Epoch= 1513)\n",
      "CV loss=0.2351755168488276(Epoch= 1514)\n",
      "train loss=0.1720968123914555(Epoch= 1514)\n",
      "CV loss=0.23488324391094895(Epoch= 1515)\n",
      "train loss=0.1720086559563608(Epoch= 1515)\n",
      "CV loss=0.23497952347342133(Epoch= 1516)\n",
      "train loss=0.1718950190984252(Epoch= 1516)\n",
      "CV loss=0.23483987216352592(Epoch= 1517)\n",
      "train loss=0.17179320326106878(Epoch= 1517)\n",
      "CV loss=0.23480552352630585(Epoch= 1518)\n",
      "train loss=0.171691567156524(Epoch= 1518)\n",
      "CV loss=0.2346520356632849(Epoch= 1519)\n",
      "train loss=0.17158632921331166(Epoch= 1519)\n",
      "CV loss=0.23458480545318783(Epoch= 1520)\n",
      "train loss=0.1714804099940248(Epoch= 1520)\n",
      "CV loss=0.23435179551068855(Epoch= 1521)\n",
      "train loss=0.17137494516423185(Epoch= 1521)\n",
      "CV loss=0.23425290935816026(Epoch= 1522)\n",
      "train loss=0.17127521388947625(Epoch= 1522)\n",
      "CV loss=0.23421519269084856(Epoch= 1523)\n",
      "train loss=0.1711903140709806(Epoch= 1523)\n",
      "CV loss=0.23398060241748503(Epoch= 1524)\n",
      "train loss=0.17107591757878252(Epoch= 1524)\n",
      "CV loss=0.2340129055058513(Epoch= 1525)\n",
      "train loss=0.1709700556170368(Epoch= 1525)\n",
      "CV loss=0.2337507430701435(Epoch= 1526)\n",
      "train loss=0.1708751226412342(Epoch= 1526)\n",
      "CV loss=0.23371345247971506(Epoch= 1527)\n",
      "train loss=0.170764734672891(Epoch= 1527)\n",
      "CV loss=0.23373254216330264(Epoch= 1528)\n",
      "train loss=0.1706648190483868(Epoch= 1528)\n",
      "CV loss=0.23363022793517288(Epoch= 1529)\n",
      "train loss=0.17056578970026076(Epoch= 1529)\n",
      "CV loss=0.23337867971937143(Epoch= 1530)\n",
      "train loss=0.17047140331961708(Epoch= 1530)\n",
      "CV loss=0.23322779340117628(Epoch= 1531)\n",
      "train loss=0.17036320883831282(Epoch= 1531)\n",
      "CV loss=0.23311266580277812(Epoch= 1532)\n",
      "train loss=0.17025444036320372(Epoch= 1532)\n",
      "CV loss=0.23324041627523273(Epoch= 1533)\n",
      "train loss=0.1701600942282094(Epoch= 1533)\n",
      "CV loss=0.23302384147906186(Epoch= 1534)\n",
      "train loss=0.17004954720319898(Epoch= 1534)\n",
      "CV loss=0.2328388621951052(Epoch= 1535)\n",
      "train loss=0.1699527627242962(Epoch= 1535)\n",
      "CV loss=0.23283114585352438(Epoch= 1536)\n",
      "train loss=0.1698592070849743(Epoch= 1536)\n",
      "CV loss=0.23270767060978043(Epoch= 1537)\n",
      "train loss=0.16975088073279987(Epoch= 1537)\n",
      "CV loss=0.23268123745061883(Epoch= 1538)\n",
      "train loss=0.1696721179498254(Epoch= 1538)\n",
      "CV loss=0.23268611680312745(Epoch= 1539)\n",
      "train loss=0.16954805277943602(Epoch= 1539)\n",
      "CV loss=0.23240137959859042(Epoch= 1540)\n",
      "train loss=0.16944180589648467(Epoch= 1540)\n",
      "CV loss=0.23222668774379096(Epoch= 1541)\n",
      "train loss=0.16935119998443995(Epoch= 1541)\n",
      "CV loss=0.23226307521021383(Epoch= 1542)\n",
      "train loss=0.16925752972117117(Epoch= 1542)\n",
      "CV loss=0.23210816231605547(Epoch= 1543)\n",
      "train loss=0.16913867537341776(Epoch= 1543)\n",
      "CV loss=0.23203749488414294(Epoch= 1544)\n",
      "train loss=0.1690354700491713(Epoch= 1544)\n",
      "CV loss=0.2318586617014598(Epoch= 1545)\n",
      "train loss=0.1689363640740702(Epoch= 1545)\n",
      "CV loss=0.23152304932726447(Epoch= 1546)\n",
      "train loss=0.16884033457830236(Epoch= 1546)\n",
      "CV loss=0.23153436707919411(Epoch= 1547)\n",
      "train loss=0.168733873024823(Epoch= 1547)\n",
      "CV loss=0.23159685716274983(Epoch= 1548)\n",
      "train loss=0.1686345514908979(Epoch= 1548)\n",
      "CV loss=0.23149056048199784(Epoch= 1549)\n",
      "train loss=0.1685396923792131(Epoch= 1549)\n",
      "CV loss=0.23126067199145206(Epoch= 1550)\n",
      "train loss=0.1684351976182765(Epoch= 1550)\n",
      "CV loss=0.23135026358732325(Epoch= 1551)\n",
      "train loss=0.16833977942811984(Epoch= 1551)\n",
      "CV loss=0.23129427282547044(Epoch= 1552)\n",
      "train loss=0.1682323543856107(Epoch= 1552)\n",
      "CV loss=0.2309234980782759(Epoch= 1553)\n",
      "train loss=0.16814162842767827(Epoch= 1553)\n",
      "CV loss=0.2310999050780243(Epoch= 1554)\n",
      "train loss=0.168039492542079(Epoch= 1554)\n",
      "CV loss=0.23079439386555814(Epoch= 1555)\n",
      "train loss=0.1679322570534594(Epoch= 1555)\n",
      "CV loss=0.2306567159865577(Epoch= 1556)\n",
      "train loss=0.16784010388242387(Epoch= 1556)\n",
      "CV loss=0.2306641068391486(Epoch= 1557)\n",
      "train loss=0.1677367600426511(Epoch= 1557)\n",
      "CV loss=0.23050138473164117(Epoch= 1558)\n",
      "train loss=0.16763155622847273(Epoch= 1558)\n",
      "CV loss=0.230476376227492(Epoch= 1559)\n",
      "train loss=0.16753355381718502(Epoch= 1559)\n",
      "CV loss=0.23029149704253968(Epoch= 1560)\n",
      "train loss=0.16743333148246373(Epoch= 1560)\n",
      "CV loss=0.2301384748867027(Epoch= 1561)\n",
      "train loss=0.16734903555210287(Epoch= 1561)\n",
      "CV loss=0.23005824087086246(Epoch= 1562)\n",
      "train loss=0.16723498954449315(Epoch= 1562)\n",
      "CV loss=0.23002692827440258(Epoch= 1563)\n",
      "train loss=0.16713341717077787(Epoch= 1563)\n",
      "CV loss=0.22988447909133755(Epoch= 1564)\n",
      "train loss=0.16704107982331518(Epoch= 1564)\n",
      "CV loss=0.22991754773211218(Epoch= 1565)\n",
      "train loss=0.16693940010883498(Epoch= 1565)\n",
      "CV loss=0.22958639899337122(Epoch= 1566)\n",
      "train loss=0.16684338832047646(Epoch= 1566)\n",
      "CV loss=0.2295108689968199(Epoch= 1567)\n",
      "train loss=0.16674893849285197(Epoch= 1567)\n",
      "CV loss=0.22943602311355005(Epoch= 1568)\n",
      "train loss=0.16665920502802714(Epoch= 1568)\n",
      "CV loss=0.22941107660835836(Epoch= 1569)\n",
      "train loss=0.16654701922340245(Epoch= 1569)\n",
      "CV loss=0.22925980279779534(Epoch= 1570)\n",
      "train loss=0.16644726384358016(Epoch= 1570)\n",
      "CV loss=0.22923832121042903(Epoch= 1571)\n",
      "train loss=0.1663508876692237(Epoch= 1571)\n",
      "CV loss=0.22899646307849814(Epoch= 1572)\n",
      "train loss=0.1662437263347866(Epoch= 1572)\n",
      "CV loss=0.22896776726036433(Epoch= 1573)\n",
      "train loss=0.16614429807456654(Epoch= 1573)\n",
      "CV loss=0.22891517893057634(Epoch= 1574)\n",
      "train loss=0.1660515439760543(Epoch= 1574)\n",
      "CV loss=0.22880563678484128(Epoch= 1575)\n",
      "train loss=0.16594893780509698(Epoch= 1575)\n",
      "CV loss=0.22862543380485129(Epoch= 1576)\n",
      "train loss=0.16586070481965026(Epoch= 1576)\n",
      "CV loss=0.22848594088789254(Epoch= 1577)\n",
      "train loss=0.16575301549421464(Epoch= 1577)\n",
      "CV loss=0.2285305392870041(Epoch= 1578)\n",
      "train loss=0.16566178238066573(Epoch= 1578)\n",
      "CV loss=0.22848821174614667(Epoch= 1579)\n",
      "train loss=0.16556140053830698(Epoch= 1579)\n",
      "CV loss=0.2283257118579457(Epoch= 1580)\n",
      "train loss=0.16546427412288317(Epoch= 1580)\n",
      "CV loss=0.2284572887411226(Epoch= 1581)\n",
      "train loss=0.16538555933351876(Epoch= 1581)\n",
      "CV loss=0.22812933570778882(Epoch= 1582)\n",
      "train loss=0.16526560777011934(Epoch= 1582)\n",
      "CV loss=0.2277820241248323(Epoch= 1583)\n",
      "train loss=0.1651800360024406(Epoch= 1583)\n",
      "CV loss=0.22788892132615607(Epoch= 1584)\n",
      "train loss=0.1650720559835717(Epoch= 1584)\n",
      "CV loss=0.22773270735447182(Epoch= 1585)\n",
      "train loss=0.16497249067706501(Epoch= 1585)\n",
      "CV loss=0.22750455468616076(Epoch= 1586)\n",
      "train loss=0.16488692508371366(Epoch= 1586)\n",
      "CV loss=0.22762603739689174(Epoch= 1587)\n",
      "train loss=0.16478140126258453(Epoch= 1587)\n",
      "CV loss=0.227238598137752(Epoch= 1588)\n",
      "train loss=0.16469290306583348(Epoch= 1588)\n",
      "CV loss=0.22728967086666074(Epoch= 1589)\n",
      "train loss=0.1645975359013866(Epoch= 1589)\n",
      "CV loss=0.227342240255548(Epoch= 1590)\n",
      "train loss=0.16449581615619036(Epoch= 1590)\n",
      "CV loss=0.22746370365734248(Epoch= 1591)\n",
      "train loss=0.16440711218590567(Epoch= 1591)\n",
      "CV loss=0.22719991105137669(Epoch= 1592)\n",
      "train loss=0.16429557907424797(Epoch= 1592)\n",
      "CV loss=0.22715179060753804(Epoch= 1593)\n",
      "train loss=0.16420837358721746(Epoch= 1593)\n",
      "CV loss=0.22672305292148598(Epoch= 1594)\n",
      "train loss=0.164116845832448(Epoch= 1594)\n",
      "CV loss=0.22678723045400678(Epoch= 1595)\n",
      "train loss=0.16400118730118535(Epoch= 1595)\n",
      "CV loss=0.22668620324644634(Epoch= 1596)\n",
      "train loss=0.16391648317698604(Epoch= 1596)\n",
      "CV loss=0.22657145025092046(Epoch= 1597)\n",
      "train loss=0.16381276860303007(Epoch= 1597)\n",
      "CV loss=0.22663023030841742(Epoch= 1598)\n",
      "train loss=0.1637210967325918(Epoch= 1598)\n",
      "CV loss=0.22618653690096702(Epoch= 1599)\n",
      "train loss=0.16362657275860065(Epoch= 1599)\n",
      "CV loss=0.22639620103738917(Epoch= 1600)\n",
      "train loss=0.16352530982182487(Epoch= 1600)\n",
      "CV loss=0.22610681259346532(Epoch= 1601)\n",
      "train loss=0.16343536310909515(Epoch= 1601)\n",
      "CV loss=0.22605393852112676(Epoch= 1602)\n",
      "train loss=0.16333202501374566(Epoch= 1602)\n",
      "CV loss=0.22599819351896627(Epoch= 1603)\n",
      "train loss=0.16323533693780026(Epoch= 1603)\n",
      "CV loss=0.22598877281778906(Epoch= 1604)\n",
      "train loss=0.1631525616972542(Epoch= 1604)\n",
      "CV loss=0.2257574541753013(Epoch= 1605)\n",
      "train loss=0.16304069336748145(Epoch= 1605)\n",
      "CV loss=0.22567237770408916(Epoch= 1606)\n",
      "train loss=0.16295120382474107(Epoch= 1606)\n",
      "CV loss=0.22598366122185448(Epoch= 1607)\n",
      "train loss=0.16288303040368887(Epoch= 1607)\n",
      "CV loss=0.2254068565181523(Epoch= 1608)\n",
      "train loss=0.16275625698341556(Epoch= 1608)\n",
      "CV loss=0.22538181435285085(Epoch= 1609)\n",
      "train loss=0.16265874672068797(Epoch= 1609)\n",
      "CV loss=0.22529698048591695(Epoch= 1610)\n",
      "train loss=0.1625659497557631(Epoch= 1610)\n",
      "CV loss=0.22527179303116773(Epoch= 1611)\n",
      "train loss=0.1624732976719951(Epoch= 1611)\n",
      "CV loss=0.22513396800425697(Epoch= 1612)\n",
      "train loss=0.16237673412626386(Epoch= 1612)\n",
      "CV loss=0.2251185414530765(Epoch= 1613)\n",
      "train loss=0.1622802953991764(Epoch= 1613)\n",
      "CV loss=0.22468608602159765(Epoch= 1614)\n",
      "train loss=0.16219977763839116(Epoch= 1614)\n",
      "CV loss=0.22476583866945654(Epoch= 1615)\n",
      "train loss=0.16209099639567096(Epoch= 1615)\n",
      "CV loss=0.22470017368651118(Epoch= 1616)\n",
      "train loss=0.16199442942184497(Epoch= 1616)\n",
      "CV loss=0.2245431997574205(Epoch= 1617)\n",
      "train loss=0.16190618518325675(Epoch= 1617)\n",
      "CV loss=0.224417515134654(Epoch= 1618)\n",
      "train loss=0.16181488266351388(Epoch= 1618)\n",
      "CV loss=0.22451645311531143(Epoch= 1619)\n",
      "train loss=0.16171842961122013(Epoch= 1619)\n",
      "CV loss=0.2242434496140799(Epoch= 1620)\n",
      "train loss=0.1616164772475564(Epoch= 1620)\n",
      "CV loss=0.22411623762845767(Epoch= 1621)\n",
      "train loss=0.16151957548195284(Epoch= 1621)\n",
      "CV loss=0.22403511275776788(Epoch= 1622)\n",
      "train loss=0.16143194819165746(Epoch= 1622)\n",
      "CV loss=0.22394329628566162(Epoch= 1623)\n",
      "train loss=0.16133724467646932(Epoch= 1623)\n",
      "CV loss=0.2238943160822699(Epoch= 1624)\n",
      "train loss=0.16124635247694136(Epoch= 1624)\n",
      "CV loss=0.22388810020353372(Epoch= 1625)\n",
      "train loss=0.16116386387243134(Epoch= 1625)\n",
      "CV loss=0.22361967622601017(Epoch= 1626)\n",
      "train loss=0.1610554130986329(Epoch= 1626)\n",
      "CV loss=0.2236359786637241(Epoch= 1627)\n",
      "train loss=0.16095775868075224(Epoch= 1627)\n",
      "CV loss=0.2236823404607624(Epoch= 1628)\n",
      "train loss=0.16087100972220436(Epoch= 1628)\n",
      "CV loss=0.2234348170612358(Epoch= 1629)\n",
      "train loss=0.16077759246170606(Epoch= 1629)\n",
      "CV loss=0.22329024732266206(Epoch= 1630)\n",
      "train loss=0.1606794766695354(Epoch= 1630)\n",
      "CV loss=0.22316279522084964(Epoch= 1631)\n",
      "train loss=0.16058263785035357(Epoch= 1631)\n",
      "CV loss=0.2230915133668206(Epoch= 1632)\n",
      "train loss=0.16048724424440813(Epoch= 1632)\n",
      "CV loss=0.22317760297588696(Epoch= 1633)\n",
      "train loss=0.16040118361167122(Epoch= 1633)\n",
      "CV loss=0.22292805000184424(Epoch= 1634)\n",
      "train loss=0.16030880451381208(Epoch= 1634)\n",
      "CV loss=0.2227230231846734(Epoch= 1635)\n",
      "train loss=0.16021767330638836(Epoch= 1635)\n",
      "CV loss=0.22290715905559208(Epoch= 1636)\n",
      "train loss=0.1601301185341893(Epoch= 1636)\n",
      "CV loss=0.2228093381088298(Epoch= 1637)\n",
      "train loss=0.16002747565464956(Epoch= 1637)\n",
      "CV loss=0.22245628968879494(Epoch= 1638)\n",
      "train loss=0.1599302923198061(Epoch= 1638)\n",
      "CV loss=0.22250011740616893(Epoch= 1639)\n",
      "train loss=0.1598383880278119(Epoch= 1639)\n",
      "CV loss=0.22245566631128438(Epoch= 1640)\n",
      "train loss=0.15974842166116468(Epoch= 1640)\n",
      "CV loss=0.22219975187447666(Epoch= 1641)\n",
      "train loss=0.15965255276801588(Epoch= 1641)\n",
      "CV loss=0.22209444213437676(Epoch= 1642)\n",
      "train loss=0.1595569459883787(Epoch= 1642)\n",
      "CV loss=0.2220455146389888(Epoch= 1643)\n",
      "train loss=0.15946896087399784(Epoch= 1643)\n",
      "CV loss=0.22194775010800574(Epoch= 1644)\n",
      "train loss=0.15937232101236753(Epoch= 1644)\n",
      "CV loss=0.22193096025740378(Epoch= 1645)\n",
      "train loss=0.15928270287119645(Epoch= 1645)\n",
      "CV loss=0.22178848569185738(Epoch= 1646)\n",
      "train loss=0.15919333408349742(Epoch= 1646)\n",
      "CV loss=0.22178748550880667(Epoch= 1647)\n",
      "train loss=0.15910406442946365(Epoch= 1647)\n",
      "CV loss=0.2216028319061335(Epoch= 1648)\n",
      "train loss=0.15900411006319487(Epoch= 1648)\n",
      "CV loss=0.22146666315167635(Epoch= 1649)\n",
      "train loss=0.15891410447923737(Epoch= 1649)\n",
      "CV loss=0.22149136298116093(Epoch= 1650)\n",
      "train loss=0.15881889799892515(Epoch= 1650)\n",
      "CV loss=0.22133249168263086(Epoch= 1651)\n",
      "train loss=0.15872732657126293(Epoch= 1651)\n",
      "CV loss=0.22115598368208883(Epoch= 1652)\n",
      "train loss=0.15863741169066423(Epoch= 1652)\n",
      "CV loss=0.22122747027553982(Epoch= 1653)\n",
      "train loss=0.1585440320096147(Epoch= 1653)\n",
      "CV loss=0.22096643817763884(Epoch= 1654)\n",
      "train loss=0.15845664855272226(Epoch= 1654)\n",
      "CV loss=0.22100755468613292(Epoch= 1655)\n",
      "train loss=0.15836228726068027(Epoch= 1655)\n",
      "CV loss=0.2208008429370807(Epoch= 1656)\n",
      "train loss=0.15827628539400582(Epoch= 1656)\n",
      "CV loss=0.22067424088115328(Epoch= 1657)\n",
      "train loss=0.15818817937083765(Epoch= 1657)\n",
      "CV loss=0.2206284330942177(Epoch= 1658)\n",
      "train loss=0.15808421652721902(Epoch= 1658)\n",
      "CV loss=0.2205777681469564(Epoch= 1659)\n",
      "train loss=0.15800200156834943(Epoch= 1659)\n",
      "CV loss=0.22041887903605895(Epoch= 1660)\n",
      "train loss=0.15792149447117348(Epoch= 1660)\n",
      "CV loss=0.22034139747917203(Epoch= 1661)\n",
      "train loss=0.15781304275520286(Epoch= 1661)\n",
      "CV loss=0.22014802577239517(Epoch= 1662)\n",
      "train loss=0.15772753852801868(Epoch= 1662)\n",
      "CV loss=0.22030741708643692(Epoch= 1663)\n",
      "train loss=0.15763195798460167(Epoch= 1663)\n",
      "CV loss=0.22026842233260313(Epoch= 1664)\n",
      "train loss=0.15755189334372321(Epoch= 1664)\n",
      "CV loss=0.21984256553787596(Epoch= 1665)\n",
      "train loss=0.15745297763280025(Epoch= 1665)\n",
      "CV loss=0.22003441865885787(Epoch= 1666)\n",
      "train loss=0.15736650246387243(Epoch= 1666)\n",
      "CV loss=0.2199368205087051(Epoch= 1667)\n",
      "train loss=0.15729409212418471(Epoch= 1667)\n",
      "CV loss=0.21954143150546157(Epoch= 1668)\n",
      "train loss=0.15718234556525984(Epoch= 1668)\n",
      "CV loss=0.21953236669915896(Epoch= 1669)\n",
      "train loss=0.15708599301246692(Epoch= 1669)\n",
      "CV loss=0.21955885988213236(Epoch= 1670)\n",
      "train loss=0.15699720534692516(Epoch= 1670)\n",
      "CV loss=0.21964435727980636(Epoch= 1671)\n",
      "train loss=0.15691926943950088(Epoch= 1671)\n",
      "CV loss=0.2192803413241476(Epoch= 1672)\n",
      "train loss=0.15681744377476373(Epoch= 1672)\n",
      "CV loss=0.2192075209690001(Epoch= 1673)\n",
      "train loss=0.15673331199048737(Epoch= 1673)\n",
      "CV loss=0.2190395497699157(Epoch= 1674)\n",
      "train loss=0.15664096842858852(Epoch= 1674)\n",
      "CV loss=0.21908518170577954(Epoch= 1675)\n",
      "train loss=0.15654313244241244(Epoch= 1675)\n",
      "CV loss=0.21895250193898835(Epoch= 1676)\n",
      "train loss=0.15645712303287637(Epoch= 1676)\n",
      "CV loss=0.21871169768036455(Epoch= 1677)\n",
      "train loss=0.15638287165573933(Epoch= 1677)\n",
      "CV loss=0.21893686283133865(Epoch= 1678)\n",
      "train loss=0.1562828926709464(Epoch= 1678)\n",
      "CV loss=0.2187741081395999(Epoch= 1679)\n",
      "train loss=0.15619826999969746(Epoch= 1679)\n",
      "CV loss=0.2186608881196881(Epoch= 1680)\n",
      "train loss=0.15611225892742794(Epoch= 1680)\n",
      "CV loss=0.2184147155711218(Epoch= 1681)\n",
      "train loss=0.15600803477660155(Epoch= 1681)\n",
      "CV loss=0.21844182885717467(Epoch= 1682)\n",
      "train loss=0.15591917036636696(Epoch= 1682)\n",
      "CV loss=0.21823460300745218(Epoch= 1683)\n",
      "train loss=0.15582621784520606(Epoch= 1683)\n",
      "CV loss=0.21820358700399659(Epoch= 1684)\n",
      "train loss=0.15574030018459936(Epoch= 1684)\n",
      "CV loss=0.21827439711784719(Epoch= 1685)\n",
      "train loss=0.15564937634799084(Epoch= 1685)\n",
      "CV loss=0.2183503156470611(Epoch= 1686)\n",
      "train loss=0.1555795532577461(Epoch= 1686)\n",
      "CV loss=0.21787542656405373(Epoch= 1687)\n",
      "train loss=0.15547027285820483(Epoch= 1687)\n",
      "CV loss=0.2177464509205804(Epoch= 1688)\n",
      "train loss=0.15538369790689222(Epoch= 1688)\n",
      "CV loss=0.21759497849718928(Epoch= 1689)\n",
      "train loss=0.15529851510752643(Epoch= 1689)\n",
      "CV loss=0.21762403254092938(Epoch= 1690)\n",
      "train loss=0.15520078641703908(Epoch= 1690)\n",
      "CV loss=0.21762053184486035(Epoch= 1691)\n",
      "train loss=0.15511413177181704(Epoch= 1691)\n",
      "CV loss=0.21755163740035907(Epoch= 1692)\n",
      "train loss=0.15502601525584134(Epoch= 1692)\n",
      "CV loss=0.21734236545936741(Epoch= 1693)\n",
      "train loss=0.1549338634177106(Epoch= 1693)\n",
      "CV loss=0.2173634970368255(Epoch= 1694)\n",
      "train loss=0.1548503339277865(Epoch= 1694)\n",
      "CV loss=0.21722500442405757(Epoch= 1695)\n",
      "train loss=0.15475858429000766(Epoch= 1695)\n",
      "CV loss=0.2171232361985689(Epoch= 1696)\n",
      "train loss=0.15467707448521278(Epoch= 1696)\n",
      "CV loss=0.2171433435567212(Epoch= 1697)\n",
      "train loss=0.15458702766954763(Epoch= 1697)\n",
      "CV loss=0.21693229242099143(Epoch= 1698)\n",
      "train loss=0.15449602806544444(Epoch= 1698)\n",
      "CV loss=0.21680155304822668(Epoch= 1699)\n",
      "train loss=0.154407689521507(Epoch= 1699)\n",
      "CV loss=0.21679409958377183(Epoch= 1700)\n",
      "train loss=0.15433551222754507(Epoch= 1700)\n",
      "CV loss=0.21665638672792042(Epoch= 1701)\n",
      "train loss=0.15423661174743933(Epoch= 1701)\n",
      "CV loss=0.2165098982798258(Epoch= 1702)\n",
      "train loss=0.15414211872617128(Epoch= 1702)\n",
      "CV loss=0.2163859808650732(Epoch= 1703)\n",
      "train loss=0.15405615920711008(Epoch= 1703)\n",
      "CV loss=0.21643457171578445(Epoch= 1704)\n",
      "train loss=0.153966697817168(Epoch= 1704)\n",
      "CV loss=0.21636785615681514(Epoch= 1705)\n",
      "train loss=0.15389068101455947(Epoch= 1705)\n",
      "CV loss=0.21623977794288365(Epoch= 1706)\n",
      "train loss=0.1537971475964382(Epoch= 1706)\n",
      "CV loss=0.21621442026148097(Epoch= 1707)\n",
      "train loss=0.15370820723567505(Epoch= 1707)\n",
      "CV loss=0.21609063432082223(Epoch= 1708)\n",
      "train loss=0.1536132718252315(Epoch= 1708)\n",
      "CV loss=0.21589761850633205(Epoch= 1709)\n",
      "train loss=0.15353239723688789(Epoch= 1709)\n",
      "CV loss=0.21572603977397284(Epoch= 1710)\n",
      "train loss=0.15344521566885663(Epoch= 1710)\n",
      "CV loss=0.21584983355064946(Epoch= 1711)\n",
      "train loss=0.15335435056879104(Epoch= 1711)\n",
      "CV loss=0.2157066646277968(Epoch= 1712)\n",
      "train loss=0.15327358373691088(Epoch= 1712)\n",
      "CV loss=0.21557830807478456(Epoch= 1713)\n",
      "train loss=0.15318623610042476(Epoch= 1713)\n",
      "CV loss=0.21570274088865066(Epoch= 1714)\n",
      "train loss=0.1530991206601106(Epoch= 1714)\n",
      "CV loss=0.2153935176750487(Epoch= 1715)\n",
      "train loss=0.15300212897857976(Epoch= 1715)\n",
      "CV loss=0.2155060618680827(Epoch= 1716)\n",
      "train loss=0.15292791613221968(Epoch= 1716)\n",
      "CV loss=0.21532962213794243(Epoch= 1717)\n",
      "train loss=0.1528318046245334(Epoch= 1717)\n",
      "CV loss=0.21520471285606793(Epoch= 1718)\n",
      "train loss=0.152742360010431(Epoch= 1718)\n",
      "CV loss=0.21514023721749823(Epoch= 1719)\n",
      "train loss=0.15267127912919176(Epoch= 1719)\n",
      "CV loss=0.21486020686464113(Epoch= 1720)\n",
      "train loss=0.15257413814900733(Epoch= 1720)\n",
      "CV loss=0.21491431608045164(Epoch= 1721)\n",
      "train loss=0.15249196519661978(Epoch= 1721)\n",
      "CV loss=0.21471457274132366(Epoch= 1722)\n",
      "train loss=0.15239873040682006(Epoch= 1722)\n",
      "CV loss=0.21486669815937134(Epoch= 1723)\n",
      "train loss=0.15232693162049785(Epoch= 1723)\n",
      "CV loss=0.214719710681666(Epoch= 1724)\n",
      "train loss=0.15222593884407265(Epoch= 1724)\n",
      "CV loss=0.21464646147250566(Epoch= 1725)\n",
      "train loss=0.15214327763933394(Epoch= 1725)\n",
      "CV loss=0.21433640105064838(Epoch= 1726)\n",
      "train loss=0.15205527027113971(Epoch= 1726)\n",
      "CV loss=0.21437657808995642(Epoch= 1727)\n",
      "train loss=0.15196832063078733(Epoch= 1727)\n",
      "CV loss=0.21425114960989938(Epoch= 1728)\n",
      "train loss=0.15188184510213143(Epoch= 1728)\n",
      "CV loss=0.21412411806782028(Epoch= 1729)\n",
      "train loss=0.15179212954270238(Epoch= 1729)\n",
      "CV loss=0.21412049354127685(Epoch= 1730)\n",
      "train loss=0.15171740480870333(Epoch= 1730)\n",
      "CV loss=0.21400378351207508(Epoch= 1731)\n",
      "train loss=0.15162621123953152(Epoch= 1731)\n",
      "CV loss=0.2139583674634108(Epoch= 1732)\n",
      "train loss=0.15153429519742478(Epoch= 1732)\n",
      "CV loss=0.21388702789768627(Epoch= 1733)\n",
      "train loss=0.15145801075855495(Epoch= 1733)\n",
      "CV loss=0.21369311591668333(Epoch= 1734)\n",
      "train loss=0.15136302352694117(Epoch= 1734)\n",
      "CV loss=0.21381449202831648(Epoch= 1735)\n",
      "train loss=0.1512830831221425(Epoch= 1735)\n",
      "CV loss=0.21347486126396026(Epoch= 1736)\n",
      "train loss=0.15120610525474004(Epoch= 1736)\n",
      "CV loss=0.2134372261106042(Epoch= 1737)\n",
      "train loss=0.1511103016609178(Epoch= 1737)\n",
      "CV loss=0.21343570815389856(Epoch= 1738)\n",
      "train loss=0.1510218910372389(Epoch= 1738)\n",
      "CV loss=0.21336804581986474(Epoch= 1739)\n",
      "train loss=0.15094115138778072(Epoch= 1739)\n",
      "CV loss=0.2133340515340445(Epoch= 1740)\n",
      "train loss=0.1508574359872135(Epoch= 1740)\n",
      "CV loss=0.21306771124804852(Epoch= 1741)\n",
      "train loss=0.1507715404205256(Epoch= 1741)\n",
      "CV loss=0.2128623291120177(Epoch= 1742)\n",
      "train loss=0.15068163427752404(Epoch= 1742)\n",
      "CV loss=0.2128694851050259(Epoch= 1743)\n",
      "train loss=0.15060180316374897(Epoch= 1743)\n",
      "CV loss=0.21282463127565232(Epoch= 1744)\n",
      "train loss=0.1505098378825653(Epoch= 1744)\n",
      "CV loss=0.21267029194379733(Epoch= 1745)\n",
      "train loss=0.1504248336413205(Epoch= 1745)\n",
      "CV loss=0.21269675655197354(Epoch= 1746)\n",
      "train loss=0.15034359448527582(Epoch= 1746)\n",
      "CV loss=0.2125857733008461(Epoch= 1747)\n",
      "train loss=0.15025900242038365(Epoch= 1747)\n",
      "CV loss=0.21230344768309173(Epoch= 1748)\n",
      "train loss=0.15018064795338448(Epoch= 1748)\n",
      "CV loss=0.21242083907517983(Epoch= 1749)\n",
      "train loss=0.15009016908662987(Epoch= 1749)\n",
      "CV loss=0.21237346707980564(Epoch= 1750)\n",
      "train loss=0.15001305205031876(Epoch= 1750)\n",
      "CV loss=0.21228753167127767(Epoch= 1751)\n",
      "train loss=0.14991766069906484(Epoch= 1751)\n",
      "CV loss=0.21201981173993617(Epoch= 1752)\n",
      "train loss=0.14984891634969602(Epoch= 1752)\n",
      "CV loss=0.21208521685459708(Epoch= 1753)\n",
      "train loss=0.149754465766131(Epoch= 1753)\n",
      "CV loss=0.2118111317423335(Epoch= 1754)\n",
      "train loss=0.1496834371938829(Epoch= 1754)\n",
      "CV loss=0.21172616675627287(Epoch= 1755)\n",
      "train loss=0.14958732954015533(Epoch= 1755)\n",
      "CV loss=0.21185614869129332(Epoch= 1756)\n",
      "train loss=0.14949819875653808(Epoch= 1756)\n",
      "CV loss=0.2117166795342066(Epoch= 1757)\n",
      "train loss=0.14941989257843613(Epoch= 1757)\n",
      "CV loss=0.2117119765044483(Epoch= 1758)\n",
      "train loss=0.14933547034773728(Epoch= 1758)\n",
      "CV loss=0.2113305489724606(Epoch= 1759)\n",
      "train loss=0.1492523279943914(Epoch= 1759)\n",
      "CV loss=0.21164920341768106(Epoch= 1760)\n",
      "train loss=0.14917101622754134(Epoch= 1760)\n",
      "CV loss=0.21140659528042588(Epoch= 1761)\n",
      "train loss=0.14908588595309313(Epoch= 1761)\n",
      "CV loss=0.21129081457502707(Epoch= 1762)\n",
      "train loss=0.14899640509454692(Epoch= 1762)\n",
      "CV loss=0.21127535236933107(Epoch= 1763)\n",
      "train loss=0.1489242998441578(Epoch= 1763)\n",
      "CV loss=0.21099542526817067(Epoch= 1764)\n",
      "train loss=0.1488274259717501(Epoch= 1764)\n",
      "CV loss=0.21087798824748233(Epoch= 1765)\n",
      "train loss=0.1487567253821535(Epoch= 1765)\n",
      "CV loss=0.21090201404211778(Epoch= 1766)\n",
      "train loss=0.1486744169583314(Epoch= 1766)\n",
      "CV loss=0.21099498507364955(Epoch= 1767)\n",
      "train loss=0.14859551328695514(Epoch= 1767)\n",
      "CV loss=0.21091205016777148(Epoch= 1768)\n",
      "train loss=0.14850841972710016(Epoch= 1768)\n",
      "CV loss=0.21051248413090928(Epoch= 1769)\n",
      "train loss=0.14844834933419068(Epoch= 1769)\n",
      "CV loss=0.21064377570700982(Epoch= 1770)\n",
      "train loss=0.14832713404532347(Epoch= 1770)\n",
      "CV loss=0.2104815897516401(Epoch= 1771)\n",
      "train loss=0.14824776658818564(Epoch= 1771)\n",
      "CV loss=0.21031293544809632(Epoch= 1772)\n",
      "train loss=0.1481652632813045(Epoch= 1772)\n",
      "CV loss=0.21034020111561935(Epoch= 1773)\n",
      "train loss=0.14808107036727652(Epoch= 1773)\n",
      "CV loss=0.21025422369476(Epoch= 1774)\n",
      "train loss=0.1480082471477533(Epoch= 1774)\n",
      "CV loss=0.21012787468034272(Epoch= 1775)\n",
      "train loss=0.1479152102491031(Epoch= 1775)\n",
      "CV loss=0.210039083488399(Epoch= 1776)\n",
      "train loss=0.14783578454909724(Epoch= 1776)\n",
      "CV loss=0.21014885015074064(Epoch= 1777)\n",
      "train loss=0.14775529417871539(Epoch= 1777)\n",
      "CV loss=0.20999040998110455(Epoch= 1778)\n",
      "train loss=0.14767304606384207(Epoch= 1778)\n",
      "CV loss=0.20962015759706293(Epoch= 1779)\n",
      "train loss=0.14759146764964617(Epoch= 1779)\n",
      "CV loss=0.20987051762180425(Epoch= 1780)\n",
      "train loss=0.14750619911801452(Epoch= 1780)\n",
      "CV loss=0.20972399850242468(Epoch= 1781)\n",
      "train loss=0.14743163542678236(Epoch= 1781)\n",
      "CV loss=0.20960715818330633(Epoch= 1782)\n",
      "train loss=0.14733583726748664(Epoch= 1782)\n",
      "CV loss=0.20942558627531277(Epoch= 1783)\n",
      "train loss=0.14725886714120248(Epoch= 1783)\n",
      "CV loss=0.2094125360991334(Epoch= 1784)\n",
      "train loss=0.14717939813208772(Epoch= 1784)\n",
      "CV loss=0.2093861410014335(Epoch= 1785)\n",
      "train loss=0.14709553967080885(Epoch= 1785)\n",
      "CV loss=0.209278729669529(Epoch= 1786)\n",
      "train loss=0.14701398636224458(Epoch= 1786)\n",
      "CV loss=0.20902060245117698(Epoch= 1787)\n",
      "train loss=0.14694159963986153(Epoch= 1787)\n",
      "CV loss=0.2089691334145168(Epoch= 1788)\n",
      "train loss=0.1468498175984817(Epoch= 1788)\n",
      "CV loss=0.20917951649249172(Epoch= 1789)\n",
      "train loss=0.14677938926575831(Epoch= 1789)\n",
      "CV loss=0.208847978732829(Epoch= 1790)\n",
      "train loss=0.14669131318997491(Epoch= 1790)\n",
      "CV loss=0.20890693047357958(Epoch= 1791)\n",
      "train loss=0.14661473351045087(Epoch= 1791)\n",
      "CV loss=0.2086658459163087(Epoch= 1792)\n",
      "train loss=0.14654946092232088(Epoch= 1792)\n",
      "CV loss=0.20865781897478858(Epoch= 1793)\n",
      "train loss=0.1464453844815029(Epoch= 1793)\n",
      "CV loss=0.20873070255289564(Epoch= 1794)\n",
      "train loss=0.1463619018030235(Epoch= 1794)\n",
      "CV loss=0.2084089511525134(Epoch= 1795)\n",
      "train loss=0.14627459536885454(Epoch= 1795)\n",
      "CV loss=0.2085276848372588(Epoch= 1796)\n",
      "train loss=0.14620617029526894(Epoch= 1796)\n",
      "CV loss=0.20815782718441098(Epoch= 1797)\n",
      "train loss=0.146115296909929(Epoch= 1797)\n",
      "CV loss=0.2080781605205127(Epoch= 1798)\n",
      "train loss=0.14603846221543237(Epoch= 1798)\n",
      "CV loss=0.2082029103963746(Epoch= 1799)\n",
      "train loss=0.14595921758087727(Epoch= 1799)\n",
      "CV loss=0.20799280859958214(Epoch= 1800)\n",
      "train loss=0.1458783684842909(Epoch= 1800)\n",
      "CV loss=0.2078534248744267(Epoch= 1801)\n",
      "train loss=0.14579860411098067(Epoch= 1801)\n",
      "CV loss=0.20790634059340957(Epoch= 1802)\n",
      "train loss=0.1457197551981866(Epoch= 1802)\n",
      "CV loss=0.2078768600876157(Epoch= 1803)\n",
      "train loss=0.14563661836404074(Epoch= 1803)\n",
      "CV loss=0.207686687788648(Epoch= 1804)\n",
      "train loss=0.14555874887716946(Epoch= 1804)\n",
      "CV loss=0.20761692292223405(Epoch= 1805)\n",
      "train loss=0.14548258626392724(Epoch= 1805)\n",
      "CV loss=0.20744447174160535(Epoch= 1806)\n",
      "train loss=0.14539611420636592(Epoch= 1806)\n",
      "CV loss=0.20748389001936202(Epoch= 1807)\n",
      "train loss=0.14531367324123856(Epoch= 1807)\n",
      "CV loss=0.20730221571648055(Epoch= 1808)\n",
      "train loss=0.14522786208354033(Epoch= 1808)\n",
      "CV loss=0.20722162387347354(Epoch= 1809)\n",
      "train loss=0.14515292913356478(Epoch= 1809)\n",
      "CV loss=0.20710948584273295(Epoch= 1810)\n",
      "train loss=0.14507644201069833(Epoch= 1810)\n",
      "CV loss=0.20727003885572914(Epoch= 1811)\n",
      "train loss=0.14499080059357247(Epoch= 1811)\n",
      "CV loss=0.2068929986226642(Epoch= 1812)\n",
      "train loss=0.14491537619389128(Epoch= 1812)\n",
      "CV loss=0.20686867677156517(Epoch= 1813)\n",
      "train loss=0.14482930927177315(Epoch= 1813)\n",
      "CV loss=0.20681239334410925(Epoch= 1814)\n",
      "train loss=0.14474931296770413(Epoch= 1814)\n",
      "CV loss=0.206736654686464(Epoch= 1815)\n",
      "train loss=0.14466725161516575(Epoch= 1815)\n",
      "CV loss=0.20686255856837665(Epoch= 1816)\n",
      "train loss=0.14459501801245989(Epoch= 1816)\n",
      "CV loss=0.20659212402935168(Epoch= 1817)\n",
      "train loss=0.14451160669799543(Epoch= 1817)\n",
      "CV loss=0.20662770835112976(Epoch= 1818)\n",
      "train loss=0.14442981747177885(Epoch= 1818)\n",
      "CV loss=0.20645561186230588(Epoch= 1819)\n",
      "train loss=0.14434848479415396(Epoch= 1819)\n",
      "CV loss=0.20640328889694748(Epoch= 1820)\n",
      "train loss=0.14426900835212889(Epoch= 1820)\n",
      "CV loss=0.20621598147541748(Epoch= 1821)\n",
      "train loss=0.14419304688575624(Epoch= 1821)\n",
      "CV loss=0.20623129700439305(Epoch= 1822)\n",
      "train loss=0.1441210180119892(Epoch= 1822)\n",
      "CV loss=0.20628056530369732(Epoch= 1823)\n",
      "train loss=0.14403460579156543(Epoch= 1823)\n",
      "CV loss=0.20604079431985925(Epoch= 1824)\n",
      "train loss=0.14395499934161346(Epoch= 1824)\n",
      "CV loss=0.2061452807953345(Epoch= 1825)\n",
      "train loss=0.14387944003087128(Epoch= 1825)\n",
      "CV loss=0.2058672174297047(Epoch= 1826)\n",
      "train loss=0.14379097118307502(Epoch= 1826)\n",
      "CV loss=0.20575429781549107(Epoch= 1827)\n",
      "train loss=0.1437270120590998(Epoch= 1827)\n",
      "CV loss=0.20571120349471467(Epoch= 1828)\n",
      "train loss=0.14363815430071214(Epoch= 1828)\n",
      "CV loss=0.20567099149506196(Epoch= 1829)\n",
      "train loss=0.1435608666911703(Epoch= 1829)\n",
      "CV loss=0.20584249530383564(Epoch= 1830)\n",
      "train loss=0.14349245289124887(Epoch= 1830)\n",
      "CV loss=0.20557702047948656(Epoch= 1831)\n",
      "train loss=0.14340297797849755(Epoch= 1831)\n",
      "CV loss=0.205167234927358(Epoch= 1832)\n",
      "train loss=0.14332742921115962(Epoch= 1832)\n",
      "CV loss=0.20545693357590655(Epoch= 1833)\n",
      "train loss=0.14325182528992758(Epoch= 1833)\n",
      "CV loss=0.20522453682432598(Epoch= 1834)\n",
      "train loss=0.14316045038757683(Epoch= 1834)\n",
      "CV loss=0.20514604022601374(Epoch= 1835)\n",
      "train loss=0.14308104410585382(Epoch= 1835)\n",
      "CV loss=0.20497099619112397(Epoch= 1836)\n",
      "train loss=0.14301191348527278(Epoch= 1836)\n",
      "CV loss=0.2048703071329717(Epoch= 1837)\n",
      "train loss=0.14293094486109492(Epoch= 1837)\n",
      "CV loss=0.20493547268204737(Epoch= 1838)\n",
      "train loss=0.1428520501701936(Epoch= 1838)\n",
      "CV loss=0.20497237625289183(Epoch= 1839)\n",
      "train loss=0.1427779478489049(Epoch= 1839)\n",
      "CV loss=0.20466918127920225(Epoch= 1840)\n",
      "train loss=0.14269800178399702(Epoch= 1840)\n",
      "CV loss=0.20466749586125252(Epoch= 1841)\n",
      "train loss=0.1426220405470472(Epoch= 1841)\n",
      "CV loss=0.20455080120774455(Epoch= 1842)\n",
      "train loss=0.14253703292999914(Epoch= 1842)\n",
      "CV loss=0.20450057793882453(Epoch= 1843)\n",
      "train loss=0.14245557220228364(Epoch= 1843)\n",
      "CV loss=0.20435230301490007(Epoch= 1844)\n",
      "train loss=0.14238098809386315(Epoch= 1844)\n",
      "CV loss=0.20427687778046139(Epoch= 1845)\n",
      "train loss=0.14230151416261866(Epoch= 1845)\n",
      "CV loss=0.20407806845895493(Epoch= 1846)\n",
      "train loss=0.14222728952612015(Epoch= 1846)\n",
      "CV loss=0.20399813204365014(Epoch= 1847)\n",
      "train loss=0.14215234821669387(Epoch= 1847)\n",
      "CV loss=0.20396348702391953(Epoch= 1848)\n",
      "train loss=0.14208090826957206(Epoch= 1848)\n",
      "CV loss=0.20400237394983056(Epoch= 1849)\n",
      "train loss=0.14199390742700718(Epoch= 1849)\n",
      "CV loss=0.2038715951712779(Epoch= 1850)\n",
      "train loss=0.14191694124832605(Epoch= 1850)\n",
      "CV loss=0.2036643666304934(Epoch= 1851)\n",
      "train loss=0.1418404445556054(Epoch= 1851)\n",
      "CV loss=0.2035856089553439(Epoch= 1852)\n",
      "train loss=0.14177359733096787(Epoch= 1852)\n",
      "CV loss=0.20371522150724192(Epoch= 1853)\n",
      "train loss=0.1416901588001001(Epoch= 1853)\n",
      "CV loss=0.20344542040295838(Epoch= 1854)\n",
      "train loss=0.14161101442061086(Epoch= 1854)\n",
      "CV loss=0.20346610789218783(Epoch= 1855)\n",
      "train loss=0.1415421024709758(Epoch= 1855)\n",
      "CV loss=0.20343589670704076(Epoch= 1856)\n",
      "train loss=0.14145220773896486(Epoch= 1856)\n",
      "CV loss=0.20316339906930042(Epoch= 1857)\n",
      "train loss=0.14138157723131983(Epoch= 1857)\n",
      "CV loss=0.20321614291692136(Epoch= 1858)\n",
      "train loss=0.14129783015703964(Epoch= 1858)\n",
      "CV loss=0.2032609019898062(Epoch= 1859)\n",
      "train loss=0.14122017029644743(Epoch= 1859)\n",
      "CV loss=0.20310032953846283(Epoch= 1860)\n",
      "train loss=0.1411435846830093(Epoch= 1860)\n",
      "CV loss=0.2030497601819352(Epoch= 1861)\n",
      "train loss=0.14107491102535058(Epoch= 1861)\n",
      "CV loss=0.2030014653457452(Epoch= 1862)\n",
      "train loss=0.14099661452807508(Epoch= 1862)\n",
      "CV loss=0.2029184935546436(Epoch= 1863)\n",
      "train loss=0.14091798923813859(Epoch= 1863)\n",
      "CV loss=0.20277151417066622(Epoch= 1864)\n",
      "train loss=0.14083501595851788(Epoch= 1864)\n",
      "CV loss=0.20282218890853426(Epoch= 1865)\n",
      "train loss=0.14076195623570176(Epoch= 1865)\n",
      "CV loss=0.20263993665964353(Epoch= 1866)\n",
      "train loss=0.1406889359132882(Epoch= 1866)\n",
      "CV loss=0.20246348838975287(Epoch= 1867)\n",
      "train loss=0.14062266258597866(Epoch= 1867)\n",
      "CV loss=0.20271591500674202(Epoch= 1868)\n",
      "train loss=0.1405397271616892(Epoch= 1868)\n",
      "CV loss=0.20232813506040676(Epoch= 1869)\n",
      "train loss=0.14045314698446276(Epoch= 1869)\n",
      "CV loss=0.2022899194321271(Epoch= 1870)\n",
      "train loss=0.14038394454874975(Epoch= 1870)\n",
      "CV loss=0.20242615761843166(Epoch= 1871)\n",
      "train loss=0.14031579954000165(Epoch= 1871)\n",
      "CV loss=0.2021377295854922(Epoch= 1872)\n",
      "train loss=0.14023015421336166(Epoch= 1872)\n",
      "CV loss=0.20219390858305414(Epoch= 1873)\n",
      "train loss=0.14016354483395985(Epoch= 1873)\n",
      "CV loss=0.20214444767539252(Epoch= 1874)\n",
      "train loss=0.140078048154478(Epoch= 1874)\n",
      "CV loss=0.2020483427364539(Epoch= 1875)\n",
      "train loss=0.1400001301421978(Epoch= 1875)\n",
      "CV loss=0.2018365161573996(Epoch= 1876)\n",
      "train loss=0.13992374404213623(Epoch= 1876)\n",
      "CV loss=0.20183049740781894(Epoch= 1877)\n",
      "train loss=0.13984990530977576(Epoch= 1877)\n",
      "CV loss=0.20171395261069208(Epoch= 1878)\n",
      "train loss=0.13977339532086375(Epoch= 1878)\n",
      "CV loss=0.2014445517810694(Epoch= 1879)\n",
      "train loss=0.13970232103809663(Epoch= 1879)\n",
      "CV loss=0.20151136221463645(Epoch= 1880)\n",
      "train loss=0.13962178996033064(Epoch= 1880)\n",
      "CV loss=0.20138055415628886(Epoch= 1881)\n",
      "train loss=0.1395466898522207(Epoch= 1881)\n",
      "CV loss=0.20118235628582193(Epoch= 1882)\n",
      "train loss=0.1394883593856127(Epoch= 1882)\n",
      "CV loss=0.2013235534170448(Epoch= 1883)\n",
      "train loss=0.1393935534502315(Epoch= 1883)\n",
      "CV loss=0.20125705556334308(Epoch= 1884)\n",
      "train loss=0.13931667684862217(Epoch= 1884)\n",
      "CV loss=0.20112191705765836(Epoch= 1885)\n",
      "train loss=0.13923991598663651(Epoch= 1885)\n",
      "CV loss=0.2011338526172366(Epoch= 1886)\n",
      "train loss=0.13916814608635844(Epoch= 1886)\n",
      "CV loss=0.20107225598951922(Epoch= 1887)\n",
      "train loss=0.13909325820689128(Epoch= 1887)\n",
      "CV loss=0.2010091470607686(Epoch= 1888)\n",
      "train loss=0.13901972477130753(Epoch= 1888)\n",
      "CV loss=0.20086811283686523(Epoch= 1889)\n",
      "train loss=0.13894160439562267(Epoch= 1889)\n",
      "CV loss=0.20080982117161802(Epoch= 1890)\n",
      "train loss=0.138878632809302(Epoch= 1890)\n",
      "CV loss=0.20070596541320476(Epoch= 1891)\n",
      "train loss=0.1387950353511211(Epoch= 1891)\n",
      "CV loss=0.20057082430876721(Epoch= 1892)\n",
      "train loss=0.13872134350802004(Epoch= 1892)\n",
      "CV loss=0.20035778279433253(Epoch= 1893)\n",
      "train loss=0.13864805617713516(Epoch= 1893)\n",
      "CV loss=0.20049810933926862(Epoch= 1894)\n",
      "train loss=0.13857436781345198(Epoch= 1894)\n",
      "CV loss=0.2003806697877914(Epoch= 1895)\n",
      "train loss=0.13849328056070623(Epoch= 1895)\n",
      "CV loss=0.20019652981339384(Epoch= 1896)\n",
      "train loss=0.1384308266647565(Epoch= 1896)\n",
      "CV loss=0.20028670154816958(Epoch= 1897)\n",
      "train loss=0.1383501167765296(Epoch= 1897)\n",
      "CV loss=0.20023625033965026(Epoch= 1898)\n",
      "train loss=0.13828078513694864(Epoch= 1898)\n",
      "CV loss=0.20003536036332745(Epoch= 1899)\n",
      "train loss=0.13819607881614052(Epoch= 1899)\n",
      "CV loss=0.20009409976996814(Epoch= 1900)\n",
      "train loss=0.1381316700050556(Epoch= 1900)\n",
      "CV loss=0.19999952403339272(Epoch= 1901)\n",
      "train loss=0.13804766996939322(Epoch= 1901)\n",
      "CV loss=0.19994482861222157(Epoch= 1902)\n",
      "train loss=0.13797430077470219(Epoch= 1902)\n",
      "CV loss=0.19968671743131022(Epoch= 1903)\n",
      "train loss=0.13790063639345507(Epoch= 1903)\n",
      "CV loss=0.19973625122690536(Epoch= 1904)\n",
      "train loss=0.13782728673645855(Epoch= 1904)\n",
      "CV loss=0.19946568460823377(Epoch= 1905)\n",
      "train loss=0.13775467912908815(Epoch= 1905)\n",
      "CV loss=0.19964034955265825(Epoch= 1906)\n",
      "train loss=0.1376814311825328(Epoch= 1906)\n",
      "CV loss=0.1995607965560978(Epoch= 1907)\n",
      "train loss=0.13760641379654465(Epoch= 1907)\n",
      "CV loss=0.19934216473815583(Epoch= 1908)\n",
      "train loss=0.13753312949122(Epoch= 1908)\n",
      "CV loss=0.19929208238472015(Epoch= 1909)\n",
      "train loss=0.13745893400854922(Epoch= 1909)\n",
      "CV loss=0.1992379246458767(Epoch= 1910)\n",
      "train loss=0.13738188917980906(Epoch= 1910)\n",
      "CV loss=0.19917105522608472(Epoch= 1911)\n",
      "train loss=0.13731005951264388(Epoch= 1911)\n",
      "CV loss=0.19909730748422694(Epoch= 1912)\n",
      "train loss=0.13723586553944375(Epoch= 1912)\n",
      "CV loss=0.19887656110335805(Epoch= 1913)\n",
      "train loss=0.13716251744900507(Epoch= 1913)\n",
      "CV loss=0.19892753079985695(Epoch= 1914)\n",
      "train loss=0.13709025997578667(Epoch= 1914)\n",
      "CV loss=0.19872032151265448(Epoch= 1915)\n",
      "train loss=0.1370221098895374(Epoch= 1915)\n",
      "CV loss=0.19871164242037748(Epoch= 1916)\n",
      "train loss=0.13695265766210205(Epoch= 1916)\n",
      "CV loss=0.19858117224095867(Epoch= 1917)\n",
      "train loss=0.13687261894158353(Epoch= 1917)\n",
      "CV loss=0.19858598925144522(Epoch= 1918)\n",
      "train loss=0.13679697790031844(Epoch= 1918)\n",
      "CV loss=0.19855139459805854(Epoch= 1919)\n",
      "train loss=0.13673676224078138(Epoch= 1919)\n",
      "CV loss=0.19847163073831026(Epoch= 1920)\n",
      "train loss=0.1366485774561696(Epoch= 1920)\n",
      "CV loss=0.1985227814233205(Epoch= 1921)\n",
      "train loss=0.13658426709369756(Epoch= 1921)\n",
      "CV loss=0.19837927452305676(Epoch= 1922)\n",
      "train loss=0.13651201624896714(Epoch= 1922)\n",
      "CV loss=0.19822721909630694(Epoch= 1923)\n",
      "train loss=0.13643149939245042(Epoch= 1923)\n",
      "CV loss=0.19832616496041927(Epoch= 1924)\n",
      "train loss=0.13636534407977724(Epoch= 1924)\n",
      "CV loss=0.1982313698839443(Epoch= 1925)\n",
      "train loss=0.13628697953361443(Epoch= 1925)\n",
      "CV loss=0.19803143413003765(Epoch= 1926)\n",
      "train loss=0.13622546108915162(Epoch= 1926)\n",
      "CV loss=0.19781160904105463(Epoch= 1927)\n",
      "train loss=0.1361423255748854(Epoch= 1927)\n",
      "CV loss=0.19777824143986047(Epoch= 1928)\n",
      "train loss=0.13607215357446356(Epoch= 1928)\n",
      "CV loss=0.19785674414926602(Epoch= 1929)\n",
      "train loss=0.13600643441467958(Epoch= 1929)\n",
      "CV loss=0.19776265402573467(Epoch= 1930)\n",
      "train loss=0.13592702001275647(Epoch= 1930)\n",
      "CV loss=0.19764228978614773(Epoch= 1931)\n",
      "train loss=0.13585838919480653(Epoch= 1931)\n",
      "CV loss=0.19752166166347837(Epoch= 1932)\n",
      "train loss=0.1357811123974141(Epoch= 1932)\n",
      "CV loss=0.19743778846951174(Epoch= 1933)\n",
      "train loss=0.1357228612862768(Epoch= 1933)\n",
      "CV loss=0.19732878989678895(Epoch= 1934)\n",
      "train loss=0.13563417822536417(Epoch= 1934)\n",
      "CV loss=0.19730055899895094(Epoch= 1935)\n",
      "train loss=0.13556345428122107(Epoch= 1935)\n",
      "CV loss=0.19715434572379315(Epoch= 1936)\n",
      "train loss=0.13552679098679118(Epoch= 1936)\n",
      "CV loss=0.19728218371517786(Epoch= 1937)\n",
      "train loss=0.13542704519882437(Epoch= 1937)\n",
      "CV loss=0.19706553478546934(Epoch= 1938)\n",
      "train loss=0.13535112194058813(Epoch= 1938)\n",
      "CV loss=0.1970117031283071(Epoch= 1939)\n",
      "train loss=0.13528609913452258(Epoch= 1939)\n",
      "CV loss=0.19707832239481538(Epoch= 1940)\n",
      "train loss=0.13520889580425768(Epoch= 1940)\n",
      "CV loss=0.1968529050142333(Epoch= 1941)\n",
      "train loss=0.13513151229675405(Epoch= 1941)\n",
      "CV loss=0.1968911979312264(Epoch= 1942)\n",
      "train loss=0.13507167387898456(Epoch= 1942)\n",
      "CV loss=0.19678397404814862(Epoch= 1943)\n",
      "train loss=0.13499134885449984(Epoch= 1943)\n",
      "CV loss=0.1966034720239555(Epoch= 1944)\n",
      "train loss=0.1349228299040033(Epoch= 1944)\n",
      "CV loss=0.19650506949897448(Epoch= 1945)\n",
      "train loss=0.13485497454459727(Epoch= 1945)\n",
      "CV loss=0.19654727693004329(Epoch= 1946)\n",
      "train loss=0.13477860370997996(Epoch= 1946)\n",
      "CV loss=0.19636447474245525(Epoch= 1947)\n",
      "train loss=0.13470179919381461(Epoch= 1947)\n",
      "CV loss=0.19628546699068372(Epoch= 1948)\n",
      "train loss=0.13463195731827862(Epoch= 1948)\n",
      "CV loss=0.19638612404089606(Epoch= 1949)\n",
      "train loss=0.1345750490471798(Epoch= 1949)\n",
      "CV loss=0.19615169922608997(Epoch= 1950)\n",
      "train loss=0.13448830601761957(Epoch= 1950)\n",
      "CV loss=0.19618846472231022(Epoch= 1951)\n",
      "train loss=0.13442345558667657(Epoch= 1951)\n",
      "CV loss=0.19607656771749243(Epoch= 1952)\n",
      "train loss=0.13435978576095156(Epoch= 1952)\n",
      "CV loss=0.1961552284399698(Epoch= 1953)\n",
      "train loss=0.13428987353785987(Epoch= 1953)\n",
      "CV loss=0.19597419738520877(Epoch= 1954)\n",
      "train loss=0.1342207833305119(Epoch= 1954)\n",
      "CV loss=0.19586542570677534(Epoch= 1955)\n",
      "train loss=0.1341428733816421(Epoch= 1955)\n",
      "CV loss=0.1958078808309229(Epoch= 1956)\n",
      "train loss=0.13407072174040735(Epoch= 1956)\n",
      "CV loss=0.19575993244054507(Epoch= 1957)\n",
      "train loss=0.1340033284872397(Epoch= 1957)\n",
      "CV loss=0.19557025266843225(Epoch= 1958)\n",
      "train loss=0.13392475210170326(Epoch= 1958)\n",
      "CV loss=0.19577438144425904(Epoch= 1959)\n",
      "train loss=0.13386521455340208(Epoch= 1959)\n",
      "CV loss=0.19537324420092964(Epoch= 1960)\n",
      "train loss=0.1337877135653155(Epoch= 1960)\n",
      "CV loss=0.19547345271063093(Epoch= 1961)\n",
      "train loss=0.13372029760027948(Epoch= 1961)\n",
      "CV loss=0.19527508023125473(Epoch= 1962)\n",
      "train loss=0.13364370384669363(Epoch= 1962)\n",
      "CV loss=0.19519478463339218(Epoch= 1963)\n",
      "train loss=0.13356971977326765(Epoch= 1963)\n",
      "CV loss=0.1952147020746654(Epoch= 1964)\n",
      "train loss=0.13350420138317318(Epoch= 1964)\n",
      "CV loss=0.1952844408062518(Epoch= 1965)\n",
      "train loss=0.13344087602295984(Epoch= 1965)\n",
      "CV loss=0.19504571739057888(Epoch= 1966)\n",
      "train loss=0.1333607413300248(Epoch= 1966)\n",
      "CV loss=0.1949862131329235(Epoch= 1967)\n",
      "train loss=0.13329267932893438(Epoch= 1967)\n",
      "CV loss=0.1949141040122678(Epoch= 1968)\n",
      "train loss=0.1332206160350033(Epoch= 1968)\n",
      "CV loss=0.1947799401394148(Epoch= 1969)\n",
      "train loss=0.13316156022691059(Epoch= 1969)\n",
      "CV loss=0.19471378979768006(Epoch= 1970)\n",
      "train loss=0.1330811386616786(Epoch= 1970)\n",
      "CV loss=0.19464359779327103(Epoch= 1971)\n",
      "train loss=0.13302351570931006(Epoch= 1971)\n",
      "CV loss=0.19453654202116877(Epoch= 1972)\n",
      "train loss=0.13294845553955364(Epoch= 1972)\n",
      "CV loss=0.19452327923923882(Epoch= 1973)\n",
      "train loss=0.13287821565861824(Epoch= 1973)\n",
      "CV loss=0.1945222257584519(Epoch= 1974)\n",
      "train loss=0.1328026955611429(Epoch= 1974)\n",
      "CV loss=0.1943265235816351(Epoch= 1975)\n",
      "train loss=0.13273546368697278(Epoch= 1975)\n",
      "CV loss=0.1942656547527311(Epoch= 1976)\n",
      "train loss=0.13266568773252183(Epoch= 1976)\n",
      "CV loss=0.19462686528846063(Epoch= 1977)\n",
      "train loss=0.13263513279709083(Epoch= 1977)\n",
      "CV loss=0.19419438899097258(Epoch= 1978)\n",
      "train loss=0.13252537917827806(Epoch= 1978)\n",
      "CV loss=0.19413664048359025(Epoch= 1979)\n",
      "train loss=0.13246808629601425(Epoch= 1979)\n",
      "CV loss=0.19405106366389113(Epoch= 1980)\n",
      "train loss=0.132384034230694(Epoch= 1980)\n",
      "CV loss=0.19402428404646255(Epoch= 1981)\n",
      "train loss=0.13232168963922997(Epoch= 1981)\n",
      "CV loss=0.1938600341047792(Epoch= 1982)\n",
      "train loss=0.1322462289473105(Epoch= 1982)\n",
      "CV loss=0.19369567300436163(Epoch= 1983)\n",
      "train loss=0.13217679856946876(Epoch= 1983)\n",
      "CV loss=0.19372105365383224(Epoch= 1984)\n",
      "train loss=0.1321114810725488(Epoch= 1984)\n",
      "CV loss=0.19374470484053374(Epoch= 1985)\n",
      "train loss=0.13204269385028347(Epoch= 1985)\n",
      "CV loss=0.19350798710810885(Epoch= 1986)\n",
      "train loss=0.13197507993075788(Epoch= 1986)\n",
      "CV loss=0.1936345241300389(Epoch= 1987)\n",
      "train loss=0.1319057929294166(Epoch= 1987)\n",
      "CV loss=0.19358015876135154(Epoch= 1988)\n",
      "train loss=0.13183738714512092(Epoch= 1988)\n",
      "CV loss=0.19342463581989905(Epoch= 1989)\n",
      "train loss=0.13176537564701557(Epoch= 1989)\n",
      "CV loss=0.19329741688348173(Epoch= 1990)\n",
      "train loss=0.131695192074534(Epoch= 1990)\n",
      "CV loss=0.19332983446043903(Epoch= 1991)\n",
      "train loss=0.13162996492195897(Epoch= 1991)\n",
      "CV loss=0.1932067015588791(Epoch= 1992)\n",
      "train loss=0.1315585441543252(Epoch= 1992)\n",
      "CV loss=0.1931010786883254(Epoch= 1993)\n",
      "train loss=0.13149531028433822(Epoch= 1993)\n",
      "CV loss=0.19306217553944688(Epoch= 1994)\n",
      "train loss=0.13142057001514873(Epoch= 1994)\n",
      "CV loss=0.19299847508742707(Epoch= 1995)\n",
      "train loss=0.13135370340882427(Epoch= 1995)\n",
      "CV loss=0.19276659125404122(Epoch= 1996)\n",
      "train loss=0.13128769105287996(Epoch= 1996)\n",
      "CV loss=0.19284352747552858(Epoch= 1997)\n",
      "train loss=0.13122963713986782(Epoch= 1997)\n",
      "CV loss=0.1927357392087358(Epoch= 1998)\n",
      "train loss=0.13114997119339664(Epoch= 1998)\n",
      "CV loss=0.19268953353589807(Epoch= 1999)\n",
      "train loss=0.13107945857364853(Epoch= 1999)\n",
      "CV loss=0.1927565950957253(Epoch= 2000)\n",
      "train loss=0.1310377219895195(Epoch= 2000)\n",
      "CV loss=0.1924728462649884(Epoch= 2001)\n",
      "train loss=0.13094465990861529(Epoch= 2001)\n",
      "CV loss=0.1924124359463495(Epoch= 2002)\n",
      "train loss=0.13088404333075326(Epoch= 2002)\n",
      "CV loss=0.19235578317744584(Epoch= 2003)\n",
      "train loss=0.1308075961794648(Epoch= 2003)\n",
      "CV loss=0.19211532391007685(Epoch= 2004)\n",
      "train loss=0.13074973028489728(Epoch= 2004)\n",
      "CV loss=0.19207853834458066(Epoch= 2005)\n",
      "train loss=0.13067724929831614(Epoch= 2005)\n",
      "CV loss=0.19207406610756392(Epoch= 2006)\n",
      "train loss=0.1306064657581562(Epoch= 2006)\n",
      "CV loss=0.19214256624335535(Epoch= 2007)\n",
      "train loss=0.13054573164031058(Epoch= 2007)\n",
      "CV loss=0.19195008853798268(Epoch= 2008)\n",
      "train loss=0.13047441639495205(Epoch= 2008)\n",
      "CV loss=0.19190842839509153(Epoch= 2009)\n",
      "train loss=0.13039944720493277(Epoch= 2009)\n",
      "CV loss=0.19196239504158247(Epoch= 2010)\n",
      "train loss=0.13033859870878742(Epoch= 2010)\n",
      "CV loss=0.19189225933021908(Epoch= 2011)\n",
      "train loss=0.13027180251884313(Epoch= 2011)\n",
      "CV loss=0.19167083909805718(Epoch= 2012)\n",
      "train loss=0.13020048243634386(Epoch= 2012)\n",
      "CV loss=0.19159189946978433(Epoch= 2013)\n",
      "train loss=0.13013290471313022(Epoch= 2013)\n",
      "CV loss=0.1914838391060757(Epoch= 2014)\n",
      "train loss=0.13007825912169682(Epoch= 2014)\n",
      "CV loss=0.1916291351167672(Epoch= 2015)\n",
      "train loss=0.13000879993715125(Epoch= 2015)\n",
      "CV loss=0.19148956960946328(Epoch= 2016)\n",
      "train loss=0.1299336670283438(Epoch= 2016)\n",
      "CV loss=0.19153070434747715(Epoch= 2017)\n",
      "train loss=0.12987148319123237(Epoch= 2017)\n",
      "CV loss=0.19152508001858531(Epoch= 2018)\n",
      "train loss=0.129823363502232(Epoch= 2018)\n",
      "CV loss=0.1912422546770509(Epoch= 2019)\n",
      "train loss=0.12973327022728953(Epoch= 2019)\n",
      "CV loss=0.1910698210433033(Epoch= 2020)\n",
      "train loss=0.12966327995650287(Epoch= 2020)\n",
      "CV loss=0.19097733239874284(Epoch= 2021)\n",
      "train loss=0.1295999590423413(Epoch= 2021)\n",
      "CV loss=0.19127817232325306(Epoch= 2022)\n",
      "train loss=0.12953614208225808(Epoch= 2022)\n",
      "CV loss=0.1911747889710362(Epoch= 2023)\n",
      "train loss=0.12947910841151802(Epoch= 2023)\n",
      "CV loss=0.1909018369704174(Epoch= 2024)\n",
      "train loss=0.12939773783478822(Epoch= 2024)\n",
      "CV loss=0.19088755000472124(Epoch= 2025)\n",
      "train loss=0.12933536179645647(Epoch= 2025)\n",
      "CV loss=0.19070505876640442(Epoch= 2026)\n",
      "train loss=0.129270189984494(Epoch= 2026)\n",
      "CV loss=0.1907538355723816(Epoch= 2027)\n",
      "train loss=0.12919924967841684(Epoch= 2027)\n",
      "CV loss=0.19074675448263326(Epoch= 2028)\n",
      "train loss=0.12913577660208758(Epoch= 2028)\n",
      "CV loss=0.19056930469896546(Epoch= 2029)\n",
      "train loss=0.12906012660493738(Epoch= 2029)\n",
      "CV loss=0.1903377044882435(Epoch= 2030)\n",
      "train loss=0.12899865164645222(Epoch= 2030)\n",
      "CV loss=0.1906861378438759(Epoch= 2031)\n",
      "train loss=0.12894969870631076(Epoch= 2031)\n",
      "CV loss=0.19038572550682176(Epoch= 2032)\n",
      "train loss=0.128861539777275(Epoch= 2032)\n",
      "CV loss=0.19030130069158802(Epoch= 2033)\n",
      "train loss=0.1287998133706609(Epoch= 2033)\n",
      "CV loss=0.190126945704517(Epoch= 2034)\n",
      "train loss=0.12874013659306788(Epoch= 2034)\n",
      "CV loss=0.1902184805983146(Epoch= 2035)\n",
      "train loss=0.1286640890216989(Epoch= 2035)\n",
      "CV loss=0.18997091657053672(Epoch= 2036)\n",
      "train loss=0.1286111190547048(Epoch= 2036)\n",
      "CV loss=0.1900428995024914(Epoch= 2037)\n",
      "train loss=0.1285300329812973(Epoch= 2037)\n",
      "CV loss=0.1899511066483517(Epoch= 2038)\n",
      "train loss=0.1284671150402632(Epoch= 2038)\n",
      "CV loss=0.18992487192437757(Epoch= 2039)\n",
      "train loss=0.1284013098936241(Epoch= 2039)\n",
      "CV loss=0.18977559353055118(Epoch= 2040)\n",
      "train loss=0.1283417251488615(Epoch= 2040)\n",
      "CV loss=0.18980730475487023(Epoch= 2041)\n",
      "train loss=0.1282796293534324(Epoch= 2041)\n",
      "CV loss=0.189758584103486(Epoch= 2042)\n",
      "train loss=0.1282061446519423(Epoch= 2042)\n",
      "CV loss=0.18966159786973716(Epoch= 2043)\n",
      "train loss=0.12814006056108743(Epoch= 2043)\n",
      "CV loss=0.18941748020823196(Epoch= 2044)\n",
      "train loss=0.12807545951756538(Epoch= 2044)\n",
      "CV loss=0.18939409136006513(Epoch= 2045)\n",
      "train loss=0.1280209985098032(Epoch= 2045)\n",
      "CV loss=0.18940966232342973(Epoch= 2046)\n",
      "train loss=0.12794195605868072(Epoch= 2046)\n",
      "CV loss=0.1893921384727011(Epoch= 2047)\n",
      "train loss=0.12787563222694626(Epoch= 2047)\n",
      "CV loss=0.18916096181210423(Epoch= 2048)\n",
      "train loss=0.12781352595668244(Epoch= 2048)\n",
      "CV loss=0.1893514387300231(Epoch= 2049)\n",
      "train loss=0.12775068058839265(Epoch= 2049)\n",
      "CV loss=0.1892267627557681(Epoch= 2050)\n",
      "train loss=0.12768247998241503(Epoch= 2050)\n",
      "CV loss=0.18894278900403264(Epoch= 2051)\n",
      "train loss=0.12761870752516075(Epoch= 2051)\n",
      "CV loss=0.1890974583547488(Epoch= 2052)\n",
      "train loss=0.12755954286631005(Epoch= 2052)\n",
      "CV loss=0.18897297692463988(Epoch= 2053)\n",
      "train loss=0.1274914316522431(Epoch= 2053)\n",
      "CV loss=0.18882867114547697(Epoch= 2054)\n",
      "train loss=0.12742038081462168(Epoch= 2054)\n",
      "CV loss=0.1886296835261699(Epoch= 2055)\n",
      "train loss=0.12736448741546524(Epoch= 2055)\n",
      "CV loss=0.18873526776626526(Epoch= 2056)\n",
      "train loss=0.12728882684124182(Epoch= 2056)\n",
      "CV loss=0.1884473589789111(Epoch= 2057)\n",
      "train loss=0.12723801929101497(Epoch= 2057)\n",
      "CV loss=0.18879041654432163(Epoch= 2058)\n",
      "train loss=0.1271728828280138(Epoch= 2058)\n",
      "CV loss=0.18847804285090924(Epoch= 2059)\n",
      "train loss=0.12710099322380589(Epoch= 2059)\n",
      "CV loss=0.1884796184529704(Epoch= 2060)\n",
      "train loss=0.1270283999103699(Epoch= 2060)\n",
      "CV loss=0.18839275120300875(Epoch= 2061)\n",
      "train loss=0.12696815303361264(Epoch= 2061)\n",
      "CV loss=0.1884066810113458(Epoch= 2062)\n",
      "train loss=0.12690583601240224(Epoch= 2062)\n",
      "CV loss=0.18819245568232795(Epoch= 2063)\n",
      "train loss=0.12685038987795844(Epoch= 2063)\n",
      "CV loss=0.18826509706870437(Epoch= 2064)\n",
      "train loss=0.1267740407315176(Epoch= 2064)\n",
      "CV loss=0.18805952333251813(Epoch= 2065)\n",
      "train loss=0.12671877163340273(Epoch= 2065)\n",
      "CV loss=0.18800238086972354(Epoch= 2066)\n",
      "train loss=0.12664108461293383(Epoch= 2066)\n",
      "CV loss=0.18796655362325132(Epoch= 2067)\n",
      "train loss=0.12658053715931353(Epoch= 2067)\n",
      "CV loss=0.18779132459142395(Epoch= 2068)\n",
      "train loss=0.1265225712864885(Epoch= 2068)\n",
      "CV loss=0.18773902687026994(Epoch= 2069)\n",
      "train loss=0.1264513626078235(Epoch= 2069)\n",
      "CV loss=0.1880515004599666(Epoch= 2070)\n",
      "train loss=0.12642217045502185(Epoch= 2070)\n",
      "CV loss=0.18778342156888705(Epoch= 2071)\n",
      "train loss=0.12632527636573443(Epoch= 2071)\n",
      "CV loss=0.1875894576268965(Epoch= 2072)\n",
      "train loss=0.1262553098598428(Epoch= 2072)\n",
      "CV loss=0.1875784165636637(Epoch= 2073)\n",
      "train loss=0.12619400154987978(Epoch= 2073)\n",
      "CV loss=0.1875171187994441(Epoch= 2074)\n",
      "train loss=0.1261347836337729(Epoch= 2074)\n",
      "CV loss=0.18734741695438434(Epoch= 2075)\n",
      "train loss=0.12607917606740685(Epoch= 2075)\n",
      "CV loss=0.18718744144626043(Epoch= 2076)\n",
      "train loss=0.12600916317897853(Epoch= 2076)\n",
      "CV loss=0.18739780474103773(Epoch= 2077)\n",
      "train loss=0.1259493956984687(Epoch= 2077)\n",
      "CV loss=0.18721417205041951(Epoch= 2078)\n",
      "train loss=0.12587433543674748(Epoch= 2078)\n",
      "CV loss=0.18710530584059382(Epoch= 2079)\n",
      "train loss=0.12581142670700848(Epoch= 2079)\n",
      "CV loss=0.1870316689831864(Epoch= 2080)\n",
      "train loss=0.12575009112000007(Epoch= 2080)\n",
      "CV loss=0.18699201984261882(Epoch= 2081)\n",
      "train loss=0.12568174956012787(Epoch= 2081)\n",
      "CV loss=0.18712177957677623(Epoch= 2082)\n",
      "train loss=0.1256461516288023(Epoch= 2082)\n",
      "CV loss=0.18707853331547644(Epoch= 2083)\n",
      "train loss=0.12557917570278207(Epoch= 2083)\n",
      "CV loss=0.18678942354698197(Epoch= 2084)\n",
      "train loss=0.12549516841025(Epoch= 2084)\n",
      "CV loss=0.18670313061929517(Epoch= 2085)\n",
      "train loss=0.12543274139321056(Epoch= 2085)\n",
      "CV loss=0.186652737083439(Epoch= 2086)\n",
      "train loss=0.1253692193306964(Epoch= 2086)\n",
      "CV loss=0.18660233064665807(Epoch= 2087)\n",
      "train loss=0.12530849451831916(Epoch= 2087)\n",
      "CV loss=0.18672377814370178(Epoch= 2088)\n",
      "train loss=0.12524321118895448(Epoch= 2088)\n",
      "CV loss=0.18664548341733905(Epoch= 2089)\n",
      "train loss=0.1251814835550429(Epoch= 2089)\n",
      "CV loss=0.18646563724366672(Epoch= 2090)\n",
      "train loss=0.12511147148591828(Epoch= 2090)\n",
      "CV loss=0.18643357901276086(Epoch= 2091)\n",
      "train loss=0.125056336609312(Epoch= 2091)\n",
      "CV loss=0.18637219193704202(Epoch= 2092)\n",
      "train loss=0.12499657825255434(Epoch= 2092)\n",
      "CV loss=0.18620110739394324(Epoch= 2093)\n",
      "train loss=0.12492394215486671(Epoch= 2093)\n",
      "CV loss=0.1862539806070742(Epoch= 2094)\n",
      "train loss=0.12486441084879238(Epoch= 2094)\n",
      "CV loss=0.18619440701981232(Epoch= 2095)\n",
      "train loss=0.12479981705617677(Epoch= 2095)\n",
      "CV loss=0.18585558468187685(Epoch= 2096)\n",
      "train loss=0.12474567739039479(Epoch= 2096)\n",
      "CV loss=0.1860878010496701(Epoch= 2097)\n",
      "train loss=0.1246843102997795(Epoch= 2097)\n",
      "CV loss=0.18589121311892642(Epoch= 2098)\n",
      "train loss=0.12461905082200417(Epoch= 2098)\n",
      "CV loss=0.18596164709537613(Epoch= 2099)\n",
      "train loss=0.12455566872452392(Epoch= 2099)\n",
      "CV loss=0.18573678530088883(Epoch= 2100)\n",
      "train loss=0.12449270151173254(Epoch= 2100)\n",
      "CV loss=0.185694691977918(Epoch= 2101)\n",
      "train loss=0.12442896100622736(Epoch= 2101)\n",
      "CV loss=0.18557532094991125(Epoch= 2102)\n",
      "train loss=0.12436021735705133(Epoch= 2102)\n",
      "CV loss=0.18556191321335067(Epoch= 2103)\n",
      "train loss=0.1243015316722508(Epoch= 2103)\n",
      "CV loss=0.18561876993266652(Epoch= 2104)\n",
      "train loss=0.12425140900284246(Epoch= 2104)\n",
      "CV loss=0.18535393781759327(Epoch= 2105)\n",
      "train loss=0.12417566048248323(Epoch= 2105)\n",
      "CV loss=0.18536072682622745(Epoch= 2106)\n",
      "train loss=0.12411788806095997(Epoch= 2106)\n",
      "CV loss=0.18530270825758388(Epoch= 2107)\n",
      "train loss=0.12404883793940101(Epoch= 2107)\n",
      "CV loss=0.18523790936640616(Epoch= 2108)\n",
      "train loss=0.12398642105356257(Epoch= 2108)\n",
      "CV loss=0.18511578648859517(Epoch= 2109)\n",
      "train loss=0.1239229400848037(Epoch= 2109)\n",
      "CV loss=0.18511242503859976(Epoch= 2110)\n",
      "train loss=0.12386722353830985(Epoch= 2110)\n",
      "CV loss=0.18513685844162786(Epoch= 2111)\n",
      "train loss=0.12380117430557638(Epoch= 2111)\n",
      "CV loss=0.18494653574540054(Epoch= 2112)\n",
      "train loss=0.12373683446077573(Epoch= 2112)\n",
      "CV loss=0.18497743746403844(Epoch= 2113)\n",
      "train loss=0.12367847257118333(Epoch= 2113)\n",
      "CV loss=0.18493499333720465(Epoch= 2114)\n",
      "train loss=0.12361587585098115(Epoch= 2114)\n",
      "CV loss=0.18481022707923345(Epoch= 2115)\n",
      "train loss=0.12355117795942436(Epoch= 2115)\n",
      "CV loss=0.1848128537074113(Epoch= 2116)\n",
      "train loss=0.12349126824631934(Epoch= 2116)\n",
      "CV loss=0.18483031182224025(Epoch= 2117)\n",
      "train loss=0.12344323713977141(Epoch= 2117)\n",
      "CV loss=0.18466110239532502(Epoch= 2118)\n",
      "train loss=0.12336808080002862(Epoch= 2118)\n",
      "CV loss=0.18466419721990734(Epoch= 2119)\n",
      "train loss=0.12331474638818814(Epoch= 2119)\n",
      "CV loss=0.18448963095168808(Epoch= 2120)\n",
      "train loss=0.12324813495407885(Epoch= 2120)\n",
      "CV loss=0.18442086753478637(Epoch= 2121)\n",
      "train loss=0.12318367896356577(Epoch= 2121)\n",
      "CV loss=0.1843968387558117(Epoch= 2122)\n",
      "train loss=0.12312056487981013(Epoch= 2122)\n",
      "CV loss=0.18426457262286028(Epoch= 2123)\n",
      "train loss=0.12307680056592402(Epoch= 2123)\n",
      "CV loss=0.18422023884941327(Epoch= 2124)\n",
      "train loss=0.12300166211573466(Epoch= 2124)\n",
      "CV loss=0.18419080995231596(Epoch= 2125)\n",
      "train loss=0.1229463199353307(Epoch= 2125)\n",
      "CV loss=0.18431911639919896(Epoch= 2126)\n",
      "train loss=0.1228970391719891(Epoch= 2126)\n",
      "CV loss=0.18405842246499682(Epoch= 2127)\n",
      "train loss=0.12281550371475208(Epoch= 2127)\n",
      "CV loss=0.18395547285647956(Epoch= 2128)\n",
      "train loss=0.12276110369782225(Epoch= 2128)\n",
      "CV loss=0.1839261887089953(Epoch= 2129)\n",
      "train loss=0.12270557519397665(Epoch= 2129)\n",
      "CV loss=0.18386255493767492(Epoch= 2130)\n",
      "train loss=0.12263993105613065(Epoch= 2130)\n",
      "CV loss=0.18387476403348788(Epoch= 2131)\n",
      "train loss=0.12258689039885966(Epoch= 2131)\n",
      "CV loss=0.18375216592079213(Epoch= 2132)\n",
      "train loss=0.12251346318503602(Epoch= 2132)\n",
      "CV loss=0.18379803450728216(Epoch= 2133)\n",
      "train loss=0.12245314346633572(Epoch= 2133)\n",
      "CV loss=0.18370728771971556(Epoch= 2134)\n",
      "train loss=0.12239119175181898(Epoch= 2134)\n",
      "CV loss=0.1835221883237802(Epoch= 2135)\n",
      "train loss=0.12233030289496302(Epoch= 2135)\n",
      "CV loss=0.18353601597772298(Epoch= 2136)\n",
      "train loss=0.12227369936881739(Epoch= 2136)\n",
      "CV loss=0.18344913978579347(Epoch= 2137)\n",
      "train loss=0.122210593318007(Epoch= 2137)\n",
      "CV loss=0.1833888982841465(Epoch= 2138)\n",
      "train loss=0.1221488968849382(Epoch= 2138)\n",
      "CV loss=0.18330548630770765(Epoch= 2139)\n",
      "train loss=0.1220883886803997(Epoch= 2139)\n",
      "CV loss=0.18318512725648928(Epoch= 2140)\n",
      "train loss=0.12203475934718915(Epoch= 2140)\n",
      "CV loss=0.18305992036800228(Epoch= 2141)\n",
      "train loss=0.12197055376365691(Epoch= 2141)\n",
      "CV loss=0.1832286604845853(Epoch= 2142)\n",
      "train loss=0.12190701996044151(Epoch= 2142)\n",
      "CV loss=0.1830925275667133(Epoch= 2143)\n",
      "train loss=0.12184666243291614(Epoch= 2143)\n",
      "CV loss=0.18304859169744997(Epoch= 2144)\n",
      "train loss=0.12179518259624961(Epoch= 2144)\n",
      "CV loss=0.18288389432596633(Epoch= 2145)\n",
      "train loss=0.12173687403649652(Epoch= 2145)\n",
      "CV loss=0.1829545737582705(Epoch= 2146)\n",
      "train loss=0.12166421242242145(Epoch= 2146)\n",
      "CV loss=0.18265886413118215(Epoch= 2147)\n",
      "train loss=0.12160987792183252(Epoch= 2147)\n",
      "CV loss=0.18288482610937035(Epoch= 2148)\n",
      "train loss=0.12154952393665028(Epoch= 2148)\n",
      "CV loss=0.18261174888059228(Epoch= 2149)\n",
      "train loss=0.12148358902756073(Epoch= 2149)\n",
      "CV loss=0.1827453835794775(Epoch= 2150)\n",
      "train loss=0.12143563709885931(Epoch= 2150)\n",
      "CV loss=0.1825295139402126(Epoch= 2151)\n",
      "train loss=0.12136761332987016(Epoch= 2151)\n",
      "CV loss=0.18253440944248955(Epoch= 2152)\n",
      "train loss=0.12130238402940288(Epoch= 2152)\n",
      "CV loss=0.18231838275868145(Epoch= 2153)\n",
      "train loss=0.1212578554993021(Epoch= 2153)\n",
      "CV loss=0.18243953385664313(Epoch= 2154)\n",
      "train loss=0.12118370150794738(Epoch= 2154)\n",
      "CV loss=0.1822751307021968(Epoch= 2155)\n",
      "train loss=0.12112658842637532(Epoch= 2155)\n",
      "CV loss=0.1821962815549332(Epoch= 2156)\n",
      "train loss=0.12107373137846983(Epoch= 2156)\n",
      "CV loss=0.18229583836552898(Epoch= 2157)\n",
      "train loss=0.1210130680005606(Epoch= 2157)\n",
      "CV loss=0.1819433204080889(Epoch= 2158)\n",
      "train loss=0.12095445355764736(Epoch= 2158)\n",
      "CV loss=0.1820231120975364(Epoch= 2159)\n",
      "train loss=0.12088494701970867(Epoch= 2159)\n",
      "CV loss=0.1818636993808257(Epoch= 2160)\n",
      "train loss=0.12083050918882263(Epoch= 2160)\n",
      "CV loss=0.18204922142232233(Epoch= 2161)\n",
      "train loss=0.12077341370580141(Epoch= 2161)\n",
      "CV loss=0.1817800355935553(Epoch= 2162)\n",
      "train loss=0.12070983831468603(Epoch= 2162)\n",
      "CV loss=0.1816676818290492(Epoch= 2163)\n",
      "train loss=0.12065160115491953(Epoch= 2163)\n",
      "CV loss=0.1816305491718444(Epoch= 2164)\n",
      "train loss=0.12059696801023921(Epoch= 2164)\n",
      "CV loss=0.18186139387454897(Epoch= 2165)\n",
      "train loss=0.12053904214058954(Epoch= 2165)\n",
      "CV loss=0.18149103715840198(Epoch= 2166)\n",
      "train loss=0.12049108713462714(Epoch= 2166)\n",
      "CV loss=0.1813773294410116(Epoch= 2167)\n",
      "train loss=0.12043174338647683(Epoch= 2167)\n",
      "CV loss=0.18145389598152634(Epoch= 2168)\n",
      "train loss=0.12035494522039565(Epoch= 2168)\n",
      "CV loss=0.18135003368387814(Epoch= 2169)\n",
      "train loss=0.12029792782201211(Epoch= 2169)\n",
      "CV loss=0.1814869927597699(Epoch= 2170)\n",
      "train loss=0.12023709326784174(Epoch= 2170)\n",
      "CV loss=0.18132408520389107(Epoch= 2171)\n",
      "train loss=0.12017724620032419(Epoch= 2171)\n",
      "CV loss=0.18113396801209652(Epoch= 2172)\n",
      "train loss=0.12012150081123585(Epoch= 2172)\n",
      "CV loss=0.1811973466748703(Epoch= 2173)\n",
      "train loss=0.1200616446037084(Epoch= 2173)\n",
      "CV loss=0.18113746736929465(Epoch= 2174)\n",
      "train loss=0.12000085032852655(Epoch= 2174)\n",
      "CV loss=0.18115451573203586(Epoch= 2175)\n",
      "train loss=0.1199464240863338(Epoch= 2175)\n",
      "CV loss=0.18099428333836826(Epoch= 2176)\n",
      "train loss=0.11988738468809099(Epoch= 2176)\n",
      "CV loss=0.18100533258324575(Epoch= 2177)\n",
      "train loss=0.1198347595957295(Epoch= 2177)\n",
      "CV loss=0.18077976493618014(Epoch= 2178)\n",
      "train loss=0.11976549882808193(Epoch= 2178)\n",
      "CV loss=0.18081757157958164(Epoch= 2179)\n",
      "train loss=0.11971252901069727(Epoch= 2179)\n",
      "CV loss=0.180784811858496(Epoch= 2180)\n",
      "train loss=0.1196479906894246(Epoch= 2180)\n",
      "CV loss=0.1807444984037398(Epoch= 2181)\n",
      "train loss=0.11959483161629597(Epoch= 2181)\n",
      "CV loss=0.18072936904976022(Epoch= 2182)\n",
      "train loss=0.1195341277658516(Epoch= 2182)\n",
      "CV loss=0.18071658037794364(Epoch= 2183)\n",
      "train loss=0.11950051381872395(Epoch= 2183)\n",
      "CV loss=0.18053417428651486(Epoch= 2184)\n",
      "train loss=0.11942284117731773(Epoch= 2184)\n",
      "CV loss=0.18052215492427984(Epoch= 2185)\n",
      "train loss=0.11935787750006516(Epoch= 2185)\n",
      "CV loss=0.18034569375151888(Epoch= 2186)\n",
      "train loss=0.11930574494767023(Epoch= 2186)\n",
      "CV loss=0.180414069140312(Epoch= 2187)\n",
      "train loss=0.11924174730548705(Epoch= 2187)\n",
      "CV loss=0.1802592218949739(Epoch= 2188)\n",
      "train loss=0.11918270174703573(Epoch= 2188)\n",
      "CV loss=0.18024926661558735(Epoch= 2189)\n",
      "train loss=0.11913301306473385(Epoch= 2189)\n",
      "CV loss=0.18017127708685532(Epoch= 2190)\n",
      "train loss=0.11907498119810696(Epoch= 2190)\n",
      "CV loss=0.17993292068937614(Epoch= 2191)\n",
      "train loss=0.11902115784385531(Epoch= 2191)\n",
      "CV loss=0.18012922801585723(Epoch= 2192)\n",
      "train loss=0.11895542284673682(Epoch= 2192)\n",
      "CV loss=0.17986465489521125(Epoch= 2193)\n",
      "train loss=0.11889332839495496(Epoch= 2193)\n",
      "CV loss=0.17978995134047948(Epoch= 2194)\n",
      "train loss=0.11884228545098151(Epoch= 2194)\n",
      "CV loss=0.17974695083103254(Epoch= 2195)\n",
      "train loss=0.11878061483753094(Epoch= 2195)\n",
      "CV loss=0.17973652345127572(Epoch= 2196)\n",
      "train loss=0.11872098463965697(Epoch= 2196)\n",
      "CV loss=0.179575634751927(Epoch= 2197)\n",
      "train loss=0.11867467936453943(Epoch= 2197)\n",
      "CV loss=0.17974035083026246(Epoch= 2198)\n",
      "train loss=0.11861058854101438(Epoch= 2198)\n",
      "CV loss=0.1797653525382829(Epoch= 2199)\n",
      "train loss=0.11855251577429059(Epoch= 2199)\n",
      "CV loss=0.17969606897694446(Epoch= 2200)\n",
      "train loss=0.11849896328697221(Epoch= 2200)\n",
      "CV loss=0.17952085333602633(Epoch= 2201)\n",
      "train loss=0.11843641963155288(Epoch= 2201)\n",
      "CV loss=0.17953465443640354(Epoch= 2202)\n",
      "train loss=0.11837923190189645(Epoch= 2202)\n",
      "CV loss=0.17938995238013794(Epoch= 2203)\n",
      "train loss=0.11832138850286911(Epoch= 2203)\n",
      "CV loss=0.17929344529456742(Epoch= 2204)\n",
      "train loss=0.11827104082781487(Epoch= 2204)\n",
      "CV loss=0.17908632403756958(Epoch= 2205)\n",
      "train loss=0.11821822465693098(Epoch= 2205)\n",
      "CV loss=0.17917206252714366(Epoch= 2206)\n",
      "train loss=0.11815642472791921(Epoch= 2206)\n",
      "CV loss=0.1789928322492534(Epoch= 2207)\n",
      "train loss=0.11809410887397526(Epoch= 2207)\n",
      "CV loss=0.17913035466448712(Epoch= 2208)\n",
      "train loss=0.1180354731589716(Epoch= 2208)\n",
      "CV loss=0.17906227012917536(Epoch= 2209)\n",
      "train loss=0.11797633707837782(Epoch= 2209)\n",
      "CV loss=0.1789306155909488(Epoch= 2210)\n",
      "train loss=0.11792291700260056(Epoch= 2210)\n",
      "CV loss=0.17900902477430203(Epoch= 2211)\n",
      "train loss=0.11786307149969943(Epoch= 2211)\n",
      "CV loss=0.17891003830038665(Epoch= 2212)\n",
      "train loss=0.11780562852656884(Epoch= 2212)\n",
      "CV loss=0.1789292182970426(Epoch= 2213)\n",
      "train loss=0.1177538758082839(Epoch= 2213)\n",
      "CV loss=0.17861324737031875(Epoch= 2214)\n",
      "train loss=0.11769600457663104(Epoch= 2214)\n",
      "CV loss=0.17874481443826187(Epoch= 2215)\n",
      "train loss=0.1176398192916535(Epoch= 2215)\n",
      "CV loss=0.17880083465967106(Epoch= 2216)\n",
      "train loss=0.11758710196340592(Epoch= 2216)\n",
      "CV loss=0.17847216629471852(Epoch= 2217)\n",
      "train loss=0.11751956392108635(Epoch= 2217)\n",
      "CV loss=0.1783928942349921(Epoch= 2218)\n",
      "train loss=0.11747053932539438(Epoch= 2218)\n",
      "CV loss=0.17835399428790522(Epoch= 2219)\n",
      "train loss=0.11740986179171212(Epoch= 2219)\n",
      "CV loss=0.1782031382468368(Epoch= 2220)\n",
      "train loss=0.1173524940837883(Epoch= 2220)\n",
      "CV loss=0.17837869331818068(Epoch= 2221)\n",
      "train loss=0.1173029444423274(Epoch= 2221)\n",
      "CV loss=0.17814459435699945(Epoch= 2222)\n",
      "train loss=0.11723821248241817(Epoch= 2222)\n",
      "CV loss=0.17837592998174162(Epoch= 2223)\n",
      "train loss=0.1172022733235429(Epoch= 2223)\n",
      "CV loss=0.1781343649060317(Epoch= 2224)\n",
      "train loss=0.11712879450972322(Epoch= 2224)\n",
      "CV loss=0.1780000327718617(Epoch= 2225)\n",
      "train loss=0.11706976272045279(Epoch= 2225)\n",
      "CV loss=0.17785132742844337(Epoch= 2226)\n",
      "train loss=0.1170307535808304(Epoch= 2226)\n",
      "CV loss=0.17789072246114473(Epoch= 2227)\n",
      "train loss=0.11696563345460896(Epoch= 2227)\n",
      "CV loss=0.1777341826787387(Epoch= 2228)\n",
      "train loss=0.1169052379651277(Epoch= 2228)\n",
      "CV loss=0.17792723708701236(Epoch= 2229)\n",
      "train loss=0.11684952642766393(Epoch= 2229)\n",
      "CV loss=0.17773885604478146(Epoch= 2230)\n",
      "train loss=0.11680774131408778(Epoch= 2230)\n",
      "CV loss=0.17788931616515102(Epoch= 2231)\n",
      "train loss=0.11674752683589498(Epoch= 2231)\n",
      "CV loss=0.17771277068402458(Epoch= 2232)\n",
      "train loss=0.11668378898304392(Epoch= 2232)\n",
      "CV loss=0.17763019968030846(Epoch= 2233)\n",
      "train loss=0.11662867301979113(Epoch= 2233)\n",
      "CV loss=0.17749295659209338(Epoch= 2234)\n",
      "train loss=0.11657171495278998(Epoch= 2234)\n",
      "CV loss=0.177426738200805(Epoch= 2235)\n",
      "train loss=0.1165166538922116(Epoch= 2235)\n",
      "CV loss=0.17740264464288974(Epoch= 2236)\n",
      "train loss=0.11646308527905734(Epoch= 2236)\n",
      "CV loss=0.17726580518754537(Epoch= 2237)\n",
      "train loss=0.1163994655822559(Epoch= 2237)\n",
      "CV loss=0.1773945614040101(Epoch= 2238)\n",
      "train loss=0.11634322964426119(Epoch= 2238)\n",
      "CV loss=0.17725700306725967(Epoch= 2239)\n",
      "train loss=0.11629523969577456(Epoch= 2239)\n",
      "CV loss=0.17722368770001007(Epoch= 2240)\n",
      "train loss=0.11623305666714889(Epoch= 2240)\n",
      "CV loss=0.17699950665995468(Epoch= 2241)\n",
      "train loss=0.11619133768242965(Epoch= 2241)\n",
      "CV loss=0.17707170586479537(Epoch= 2242)\n",
      "train loss=0.11612878966571333(Epoch= 2242)\n",
      "CV loss=0.17697599944915338(Epoch= 2243)\n",
      "train loss=0.11606936017623047(Epoch= 2243)\n",
      "CV loss=0.17698673617180308(Epoch= 2244)\n",
      "train loss=0.11601077961496331(Epoch= 2244)\n",
      "CV loss=0.17697307389175018(Epoch= 2245)\n",
      "train loss=0.11595946516695019(Epoch= 2245)\n",
      "CV loss=0.17681964495356237(Epoch= 2246)\n",
      "train loss=0.11589832134288866(Epoch= 2246)\n",
      "CV loss=0.17678861476629007(Epoch= 2247)\n",
      "train loss=0.11584865701568572(Epoch= 2247)\n",
      "CV loss=0.17682125165991408(Epoch= 2248)\n",
      "train loss=0.11579349991958611(Epoch= 2248)\n",
      "CV loss=0.17665888856652046(Epoch= 2249)\n",
      "train loss=0.115733159942565(Epoch= 2249)\n",
      "CV loss=0.17673673795151126(Epoch= 2250)\n",
      "train loss=0.11568819649691174(Epoch= 2250)\n",
      "CV loss=0.17641941749893558(Epoch= 2251)\n",
      "train loss=0.11563095477557833(Epoch= 2251)\n",
      "CV loss=0.1765042463677578(Epoch= 2252)\n",
      "train loss=0.11556845245151062(Epoch= 2252)\n",
      "CV loss=0.17640676303688774(Epoch= 2253)\n",
      "train loss=0.11551926981564448(Epoch= 2253)\n",
      "CV loss=0.17633810087950574(Epoch= 2254)\n",
      "train loss=0.11546977016617868(Epoch= 2254)\n",
      "CV loss=0.17609504215284344(Epoch= 2255)\n",
      "train loss=0.11542272570586179(Epoch= 2255)\n",
      "CV loss=0.17614643390030615(Epoch= 2256)\n",
      "train loss=0.11535168568058372(Epoch= 2256)\n",
      "CV loss=0.1761673231996477(Epoch= 2257)\n",
      "train loss=0.11529610893532809(Epoch= 2257)\n",
      "CV loss=0.17605220298501273(Epoch= 2258)\n",
      "train loss=0.11524801592829292(Epoch= 2258)\n",
      "CV loss=0.1761774876658046(Epoch= 2259)\n",
      "train loss=0.11519795930030265(Epoch= 2259)\n",
      "CV loss=0.17588124274310699(Epoch= 2260)\n",
      "train loss=0.11513893138712017(Epoch= 2260)\n",
      "CV loss=0.17584975474488043(Epoch= 2261)\n",
      "train loss=0.11507806614039942(Epoch= 2261)\n",
      "CV loss=0.17579590787620958(Epoch= 2262)\n",
      "train loss=0.1150241013830148(Epoch= 2262)\n",
      "CV loss=0.1757213013602185(Epoch= 2263)\n",
      "train loss=0.11496928248673029(Epoch= 2263)\n",
      "CV loss=0.17571266340903496(Epoch= 2264)\n",
      "train loss=0.1149210990443375(Epoch= 2264)\n",
      "CV loss=0.17567170371455312(Epoch= 2265)\n",
      "train loss=0.11486168914299044(Epoch= 2265)\n",
      "CV loss=0.17556534643784863(Epoch= 2266)\n",
      "train loss=0.11483244581329803(Epoch= 2266)\n",
      "CV loss=0.17551274451112422(Epoch= 2267)\n",
      "train loss=0.11475026047830143(Epoch= 2267)\n",
      "CV loss=0.17555155304162723(Epoch= 2268)\n",
      "train loss=0.11470742836536106(Epoch= 2268)\n",
      "CV loss=0.1754538972843141(Epoch= 2269)\n",
      "train loss=0.1146441771810477(Epoch= 2269)\n",
      "CV loss=0.1754410322188439(Epoch= 2270)\n",
      "train loss=0.11460240320803136(Epoch= 2270)\n",
      "CV loss=0.17538943104192234(Epoch= 2271)\n",
      "train loss=0.1145334869952466(Epoch= 2271)\n",
      "CV loss=0.1754684355979731(Epoch= 2272)\n",
      "train loss=0.11448588333871847(Epoch= 2272)\n",
      "CV loss=0.17511462846139506(Epoch= 2273)\n",
      "train loss=0.1144407955479747(Epoch= 2273)\n",
      "CV loss=0.17529439677271363(Epoch= 2274)\n",
      "train loss=0.11437458790077287(Epoch= 2274)\n",
      "CV loss=0.17531623222043047(Epoch= 2275)\n",
      "train loss=0.11433264005237863(Epoch= 2275)\n",
      "CV loss=0.17535339381828324(Epoch= 2276)\n",
      "train loss=0.11427603636068119(Epoch= 2276)\n",
      "CV loss=0.17498236301755365(Epoch= 2277)\n",
      "train loss=0.11422183211638809(Epoch= 2277)\n",
      "CV loss=0.17494195144312(Epoch= 2278)\n",
      "train loss=0.1141698577520382(Epoch= 2278)\n",
      "CV loss=0.17501874694301744(Epoch= 2279)\n",
      "train loss=0.11411463639375805(Epoch= 2279)\n",
      "CV loss=0.17484058585780643(Epoch= 2280)\n",
      "train loss=0.11405019458485968(Epoch= 2280)\n",
      "CV loss=0.17479488043809302(Epoch= 2281)\n",
      "train loss=0.11400172952978942(Epoch= 2281)\n",
      "CV loss=0.17481110787372228(Epoch= 2282)\n",
      "train loss=0.11394250247249749(Epoch= 2282)\n",
      "CV loss=0.17468663591138506(Epoch= 2283)\n",
      "train loss=0.11389835028394565(Epoch= 2283)\n",
      "CV loss=0.17468577881876318(Epoch= 2284)\n",
      "train loss=0.11383999761920731(Epoch= 2284)\n",
      "CV loss=0.17457135524623146(Epoch= 2285)\n",
      "train loss=0.11379467347261647(Epoch= 2285)\n",
      "CV loss=0.17450913420207298(Epoch= 2286)\n",
      "train loss=0.11372999829296797(Epoch= 2286)\n",
      "CV loss=0.17436038701699846(Epoch= 2287)\n",
      "train loss=0.11368097676653788(Epoch= 2287)\n",
      "CV loss=0.17446256443095112(Epoch= 2288)\n",
      "train loss=0.11364829102086063(Epoch= 2288)\n",
      "CV loss=0.17432092605183874(Epoch= 2289)\n",
      "train loss=0.11357394362155777(Epoch= 2289)\n",
      "CV loss=0.17435264132714776(Epoch= 2290)\n",
      "train loss=0.1135166722880187(Epoch= 2290)\n",
      "CV loss=0.17436916036838515(Epoch= 2291)\n",
      "train loss=0.1134660411333209(Epoch= 2291)\n",
      "CV loss=0.1741683999810868(Epoch= 2292)\n",
      "train loss=0.1134155249082007(Epoch= 2292)\n",
      "CV loss=0.1741518908732793(Epoch= 2293)\n",
      "train loss=0.11337375421633825(Epoch= 2293)\n",
      "CV loss=0.17402127040217247(Epoch= 2294)\n",
      "train loss=0.1133153991796046(Epoch= 2294)\n",
      "CV loss=0.17412511727354402(Epoch= 2295)\n",
      "train loss=0.11325142577038025(Epoch= 2295)\n",
      "CV loss=0.1739941104923104(Epoch= 2296)\n",
      "train loss=0.1132012553022893(Epoch= 2296)\n",
      "CV loss=0.17387270502355875(Epoch= 2297)\n",
      "train loss=0.11314909992064379(Epoch= 2297)\n",
      "CV loss=0.17388222719918614(Epoch= 2298)\n",
      "train loss=0.11309424621944711(Epoch= 2298)\n",
      "CV loss=0.17371763135056104(Epoch= 2299)\n",
      "train loss=0.11305055770512001(Epoch= 2299)\n",
      "CV loss=0.17391935227478045(Epoch= 2300)\n",
      "train loss=0.11299927772815321(Epoch= 2300)\n",
      "CV loss=0.17374248741318232(Epoch= 2301)\n",
      "train loss=0.1129396011576045(Epoch= 2301)\n",
      "CV loss=0.17366267755948578(Epoch= 2302)\n",
      "train loss=0.11288346593597015(Epoch= 2302)\n",
      "CV loss=0.1736408672343122(Epoch= 2303)\n",
      "train loss=0.11283446574432349(Epoch= 2303)\n",
      "CV loss=0.17354333445740566(Epoch= 2304)\n",
      "train loss=0.11277453981261323(Epoch= 2304)\n",
      "CV loss=0.17357002298332203(Epoch= 2305)\n",
      "train loss=0.11273266293980021(Epoch= 2305)\n",
      "CV loss=0.17334817136249536(Epoch= 2306)\n",
      "train loss=0.11267648592840765(Epoch= 2306)\n",
      "CV loss=0.17338169840936696(Epoch= 2307)\n",
      "train loss=0.11261964388133351(Epoch= 2307)\n",
      "CV loss=0.17344000811290627(Epoch= 2308)\n",
      "train loss=0.11257215235626078(Epoch= 2308)\n",
      "CV loss=0.17329544471910996(Epoch= 2309)\n",
      "train loss=0.11251342283174633(Epoch= 2309)\n",
      "CV loss=0.1732522584975903(Epoch= 2310)\n",
      "train loss=0.1124764664377424(Epoch= 2310)\n",
      "CV loss=0.17308342094957507(Epoch= 2311)\n",
      "train loss=0.1124088400877712(Epoch= 2311)\n",
      "CV loss=0.1733494925445161(Epoch= 2312)\n",
      "train loss=0.1123697204337278(Epoch= 2312)\n",
      "CV loss=0.1731440150619893(Epoch= 2313)\n",
      "train loss=0.11230820201552959(Epoch= 2313)\n",
      "CV loss=0.17300763579256295(Epoch= 2314)\n",
      "train loss=0.11225758164956998(Epoch= 2314)\n",
      "CV loss=0.17295277937708264(Epoch= 2315)\n",
      "train loss=0.11219829286228036(Epoch= 2315)\n",
      "CV loss=0.17289358578321146(Epoch= 2316)\n",
      "train loss=0.11215248936448914(Epoch= 2316)\n",
      "CV loss=0.17280210734101664(Epoch= 2317)\n",
      "train loss=0.11210247198859742(Epoch= 2317)\n",
      "CV loss=0.1727530258183992(Epoch= 2318)\n",
      "train loss=0.11204643216851291(Epoch= 2318)\n",
      "CV loss=0.1726555195679939(Epoch= 2319)\n",
      "train loss=0.11200131728935205(Epoch= 2319)\n",
      "CV loss=0.1726406265333304(Epoch= 2320)\n",
      "train loss=0.11193950907089345(Epoch= 2320)\n",
      "CV loss=0.17280835010441029(Epoch= 2321)\n",
      "train loss=0.11189836414339956(Epoch= 2321)\n",
      "CV loss=0.1727400348158891(Epoch= 2322)\n",
      "train loss=0.11184306763257393(Epoch= 2322)\n",
      "CV loss=0.1726294635897785(Epoch= 2323)\n",
      "train loss=0.11178629568154586(Epoch= 2323)\n",
      "CV loss=0.17249417473754552(Epoch= 2324)\n",
      "train loss=0.11173265139992844(Epoch= 2324)\n",
      "CV loss=0.1724937558070668(Epoch= 2325)\n",
      "train loss=0.11168091513985065(Epoch= 2325)\n",
      "CV loss=0.17228001449693248(Epoch= 2326)\n",
      "train loss=0.11162915671743052(Epoch= 2326)\n",
      "CV loss=0.17213400446541674(Epoch= 2327)\n",
      "train loss=0.11159596673933189(Epoch= 2327)\n",
      "CV loss=0.17231974287861412(Epoch= 2328)\n",
      "train loss=0.11152628141916952(Epoch= 2328)\n",
      "CV loss=0.1721071792716784(Epoch= 2329)\n",
      "train loss=0.11147680421934822(Epoch= 2329)\n",
      "CV loss=0.1722943839915677(Epoch= 2330)\n",
      "train loss=0.11142977056600717(Epoch= 2330)\n",
      "CV loss=0.17204106465310656(Epoch= 2331)\n",
      "train loss=0.11137722738178962(Epoch= 2331)\n",
      "CV loss=0.17205742793230222(Epoch= 2332)\n",
      "train loss=0.11132504978365014(Epoch= 2332)\n",
      "CV loss=0.1719866392890493(Epoch= 2333)\n",
      "train loss=0.11126894355306356(Epoch= 2333)\n",
      "CV loss=0.1718475109402452(Epoch= 2334)\n",
      "train loss=0.11121904352185571(Epoch= 2334)\n",
      "CV loss=0.17201332507471745(Epoch= 2335)\n",
      "train loss=0.11117345467585538(Epoch= 2335)\n",
      "CV loss=0.17173979598552644(Epoch= 2336)\n",
      "train loss=0.11111795118961415(Epoch= 2336)\n",
      "CV loss=0.17178187690237584(Epoch= 2337)\n",
      "train loss=0.11106415687224729(Epoch= 2337)\n",
      "CV loss=0.17186428653562397(Epoch= 2338)\n",
      "train loss=0.11102077207495449(Epoch= 2338)\n",
      "CV loss=0.17193999665780266(Epoch= 2339)\n",
      "train loss=0.11097496154797448(Epoch= 2339)\n",
      "CV loss=0.17181331189553883(Epoch= 2340)\n",
      "train loss=0.11092021711616562(Epoch= 2340)\n",
      "CV loss=0.17167820799583722(Epoch= 2341)\n",
      "train loss=0.11086107372333073(Epoch= 2341)\n",
      "CV loss=0.17142561758184055(Epoch= 2342)\n",
      "train loss=0.1108109189812703(Epoch= 2342)\n",
      "CV loss=0.17139606098604426(Epoch= 2343)\n",
      "train loss=0.11078098394573396(Epoch= 2343)\n",
      "CV loss=0.17155015739625(Epoch= 2344)\n",
      "train loss=0.11072322501941471(Epoch= 2344)\n",
      "CV loss=0.17131877491473996(Epoch= 2345)\n",
      "train loss=0.11065719807632658(Epoch= 2345)\n",
      "CV loss=0.17122504367669356(Epoch= 2346)\n",
      "train loss=0.11062263033580036(Epoch= 2346)\n",
      "CV loss=0.17123921579849916(Epoch= 2347)\n",
      "train loss=0.11055902558529689(Epoch= 2347)\n",
      "CV loss=0.17127895657271594(Epoch= 2348)\n",
      "train loss=0.11050945882642307(Epoch= 2348)\n",
      "CV loss=0.1711787235997194(Epoch= 2349)\n",
      "train loss=0.11045832376893576(Epoch= 2349)\n",
      "CV loss=0.17105838579880978(Epoch= 2350)\n",
      "train loss=0.1104086961978667(Epoch= 2350)\n",
      "CV loss=0.1710320500607317(Epoch= 2351)\n",
      "train loss=0.11035794681922524(Epoch= 2351)\n",
      "CV loss=0.1711158739572057(Epoch= 2352)\n",
      "train loss=0.11030661455893044(Epoch= 2352)\n",
      "CV loss=0.17085292971671281(Epoch= 2353)\n",
      "train loss=0.11025977803465035(Epoch= 2353)\n",
      "CV loss=0.17079310018969907(Epoch= 2354)\n",
      "train loss=0.11020059331060199(Epoch= 2354)\n",
      "CV loss=0.17088732172184512(Epoch= 2355)\n",
      "train loss=0.11015515762457333(Epoch= 2355)\n",
      "CV loss=0.17072276645373174(Epoch= 2356)\n",
      "train loss=0.11010306080139123(Epoch= 2356)\n",
      "CV loss=0.17069793554796564(Epoch= 2357)\n",
      "train loss=0.11005939110012722(Epoch= 2357)\n",
      "CV loss=0.17082220080078(Epoch= 2358)\n",
      "train loss=0.11000458067275533(Epoch= 2358)\n",
      "CV loss=0.17070970159843268(Epoch= 2359)\n",
      "train loss=0.10995036209321092(Epoch= 2359)\n",
      "CV loss=0.17048396919308392(Epoch= 2360)\n",
      "train loss=0.10990272182384413(Epoch= 2360)\n",
      "CV loss=0.17051847818416754(Epoch= 2361)\n",
      "train loss=0.109857673296313(Epoch= 2361)\n",
      "CV loss=0.17061179323289336(Epoch= 2362)\n",
      "train loss=0.10981318199070457(Epoch= 2362)\n",
      "CV loss=0.17045991003047745(Epoch= 2363)\n",
      "train loss=0.10974788051269833(Epoch= 2363)\n",
      "CV loss=0.17023083421577634(Epoch= 2364)\n",
      "train loss=0.1097025453682297(Epoch= 2364)\n",
      "CV loss=0.1703694084663318(Epoch= 2365)\n",
      "train loss=0.10965153311363417(Epoch= 2365)\n",
      "CV loss=0.17041485251313299(Epoch= 2366)\n",
      "train loss=0.10960016159890651(Epoch= 2366)\n",
      "CV loss=0.17017816983340367(Epoch= 2367)\n",
      "train loss=0.1095498058612053(Epoch= 2367)\n",
      "CV loss=0.16999190313131035(Epoch= 2368)\n",
      "train loss=0.10950745333330755(Epoch= 2368)\n",
      "CV loss=0.1700155766978237(Epoch= 2369)\n",
      "train loss=0.10945655716838595(Epoch= 2369)\n",
      "CV loss=0.1700094792453079(Epoch= 2370)\n",
      "train loss=0.10940376837264561(Epoch= 2370)\n",
      "CV loss=0.17015661586278075(Epoch= 2371)\n",
      "train loss=0.10936370566637417(Epoch= 2371)\n",
      "CV loss=0.16994459798414463(Epoch= 2372)\n",
      "train loss=0.10929916652959476(Epoch= 2372)\n",
      "CV loss=0.1698750885136705(Epoch= 2373)\n",
      "train loss=0.10925333314768816(Epoch= 2373)\n",
      "CV loss=0.16988609138583466(Epoch= 2374)\n",
      "train loss=0.10920103057065998(Epoch= 2374)\n",
      "CV loss=0.16970936690122385(Epoch= 2375)\n",
      "train loss=0.10915666260517151(Epoch= 2375)\n",
      "CV loss=0.1696426964689321(Epoch= 2376)\n",
      "train loss=0.10910898242757336(Epoch= 2376)\n",
      "CV loss=0.16963994006123162(Epoch= 2377)\n",
      "train loss=0.10905396032662526(Epoch= 2377)\n",
      "CV loss=0.16968556636615634(Epoch= 2378)\n",
      "train loss=0.10900248375272305(Epoch= 2378)\n",
      "CV loss=0.1696270008147327(Epoch= 2379)\n",
      "train loss=0.10895923853741384(Epoch= 2379)\n",
      "CV loss=0.16945930485352922(Epoch= 2380)\n",
      "train loss=0.10890970435290911(Epoch= 2380)\n",
      "CV loss=0.1695119120120303(Epoch= 2381)\n",
      "train loss=0.10885876758020878(Epoch= 2381)\n",
      "CV loss=0.16946954209005752(Epoch= 2382)\n",
      "train loss=0.10880662403563397(Epoch= 2382)\n",
      "CV loss=0.16922650961379254(Epoch= 2383)\n",
      "train loss=0.10875891812243453(Epoch= 2383)\n",
      "CV loss=0.1693621098464485(Epoch= 2384)\n",
      "train loss=0.10871174002103887(Epoch= 2384)\n",
      "CV loss=0.16922373750131958(Epoch= 2385)\n",
      "train loss=0.10865944743373011(Epoch= 2385)\n",
      "CV loss=0.1693218588536135(Epoch= 2386)\n",
      "train loss=0.10861140897900767(Epoch= 2386)\n",
      "CV loss=0.169269190004201(Epoch= 2387)\n",
      "train loss=0.10857239559527831(Epoch= 2387)\n",
      "CV loss=0.1689861806556296(Epoch= 2388)\n",
      "train loss=0.1085187772483101(Epoch= 2388)\n",
      "CV loss=0.16918553061566824(Epoch= 2389)\n",
      "train loss=0.10846235729439414(Epoch= 2389)\n",
      "CV loss=0.1689152008830379(Epoch= 2390)\n",
      "train loss=0.1084136243073969(Epoch= 2390)\n",
      "CV loss=0.16889446281868595(Epoch= 2391)\n",
      "train loss=0.10836860702608854(Epoch= 2391)\n",
      "CV loss=0.16898629942511673(Epoch= 2392)\n",
      "train loss=0.10831430460970393(Epoch= 2392)\n",
      "CV loss=0.16887434381072125(Epoch= 2393)\n",
      "train loss=0.10827037086951659(Epoch= 2393)\n",
      "CV loss=0.16901557353066043(Epoch= 2394)\n",
      "train loss=0.10822867070336165(Epoch= 2394)\n",
      "CV loss=0.16881363774279406(Epoch= 2395)\n",
      "train loss=0.10817143769043105(Epoch= 2395)\n",
      "CV loss=0.16876155599524675(Epoch= 2396)\n",
      "train loss=0.10813294332744919(Epoch= 2396)\n",
      "CV loss=0.1686348235013457(Epoch= 2397)\n",
      "train loss=0.10807120459558689(Epoch= 2397)\n",
      "CV loss=0.16865082267952547(Epoch= 2398)\n",
      "train loss=0.10802854615626753(Epoch= 2398)\n",
      "CV loss=0.1685291128315039(Epoch= 2399)\n",
      "train loss=0.10798276097457964(Epoch= 2399)\n",
      "CV loss=0.16840718389694137(Epoch= 2400)\n",
      "train loss=0.10792962031571107(Epoch= 2400)\n",
      "CV loss=0.16832349578776726(Epoch= 2401)\n",
      "train loss=0.10788457988851609(Epoch= 2401)\n",
      "CV loss=0.1682419050559555(Epoch= 2402)\n",
      "train loss=0.10784672807853608(Epoch= 2402)\n",
      "CV loss=0.16842558274656733(Epoch= 2403)\n",
      "train loss=0.10778357464363691(Epoch= 2403)\n",
      "CV loss=0.16833686434645628(Epoch= 2404)\n",
      "train loss=0.10773730621028044(Epoch= 2404)\n",
      "CV loss=0.16820316543881378(Epoch= 2405)\n",
      "train loss=0.10768564907599801(Epoch= 2405)\n",
      "CV loss=0.16824185634999295(Epoch= 2406)\n",
      "train loss=0.1076420193590136(Epoch= 2406)\n",
      "CV loss=0.16822766983817852(Epoch= 2407)\n",
      "train loss=0.10758804278106286(Epoch= 2407)\n",
      "CV loss=0.16822881721375568(Epoch= 2408)\n",
      "train loss=0.10754361270045112(Epoch= 2408)\n",
      "CV loss=0.16788963065480977(Epoch= 2409)\n",
      "train loss=0.10749872393952209(Epoch= 2409)\n",
      "CV loss=0.1679285072642281(Epoch= 2410)\n",
      "train loss=0.10745198074993295(Epoch= 2410)\n",
      "CV loss=0.16793283308524848(Epoch= 2411)\n",
      "train loss=0.1073946203642822(Epoch= 2411)\n",
      "CV loss=0.16791554905037429(Epoch= 2412)\n",
      "train loss=0.10735314124050095(Epoch= 2412)\n",
      "CV loss=0.16778631938897517(Epoch= 2413)\n",
      "train loss=0.10730288668177096(Epoch= 2413)\n",
      "CV loss=0.16775313885217574(Epoch= 2414)\n",
      "train loss=0.10725556304568228(Epoch= 2414)\n",
      "CV loss=0.16779120858666252(Epoch= 2415)\n",
      "train loss=0.10720783614222201(Epoch= 2415)\n",
      "CV loss=0.16771869859002858(Epoch= 2416)\n",
      "train loss=0.10715559651495639(Epoch= 2416)\n",
      "CV loss=0.16761846025655544(Epoch= 2417)\n",
      "train loss=0.10710725333623629(Epoch= 2417)\n",
      "CV loss=0.16754547001605818(Epoch= 2418)\n",
      "train loss=0.10706370604192267(Epoch= 2418)\n",
      "CV loss=0.16778010893080125(Epoch= 2419)\n",
      "train loss=0.10702837978104508(Epoch= 2419)\n",
      "CV loss=0.1674484071552962(Epoch= 2420)\n",
      "train loss=0.10696371718623619(Epoch= 2420)\n",
      "CV loss=0.1676220266227042(Epoch= 2421)\n",
      "train loss=0.10694554482243185(Epoch= 2421)\n",
      "CV loss=0.16754656803341964(Epoch= 2422)\n",
      "train loss=0.1068764087016439(Epoch= 2422)\n",
      "CV loss=0.16739338685453173(Epoch= 2423)\n",
      "train loss=0.10682408138432095(Epoch= 2423)\n",
      "CV loss=0.16725496238695686(Epoch= 2424)\n",
      "train loss=0.10677846291934287(Epoch= 2424)\n",
      "CV loss=0.16722594249517497(Epoch= 2425)\n",
      "train loss=0.10672479604812894(Epoch= 2425)\n",
      "CV loss=0.1672338431086045(Epoch= 2426)\n",
      "train loss=0.10668264738107157(Epoch= 2426)\n",
      "CV loss=0.16727996359563221(Epoch= 2427)\n",
      "train loss=0.10663477673627617(Epoch= 2427)\n",
      "CV loss=0.16727335951481043(Epoch= 2428)\n",
      "train loss=0.10659002797679706(Epoch= 2428)\n",
      "CV loss=0.1671390158517313(Epoch= 2429)\n",
      "train loss=0.10654472248435537(Epoch= 2429)\n",
      "CV loss=0.16707787702981647(Epoch= 2430)\n",
      "train loss=0.10650461602589803(Epoch= 2430)\n",
      "CV loss=0.16700875248130917(Epoch= 2431)\n",
      "train loss=0.10644231888061091(Epoch= 2431)\n",
      "CV loss=0.16720538734622348(Epoch= 2432)\n",
      "train loss=0.10640878716526866(Epoch= 2432)\n",
      "CV loss=0.16690456431434256(Epoch= 2433)\n",
      "train loss=0.10635155394842659(Epoch= 2433)\n",
      "CV loss=0.1668358023632135(Epoch= 2434)\n",
      "train loss=0.10630180303156375(Epoch= 2434)\n",
      "CV loss=0.16693339828803133(Epoch= 2435)\n",
      "train loss=0.10625600736166256(Epoch= 2435)\n",
      "CV loss=0.1666800039650856(Epoch= 2436)\n",
      "train loss=0.10620634314407161(Epoch= 2436)\n",
      "CV loss=0.16677328998854515(Epoch= 2437)\n",
      "train loss=0.10616459411060199(Epoch= 2437)\n",
      "CV loss=0.16676335555611554(Epoch= 2438)\n",
      "train loss=0.1061169200464331(Epoch= 2438)\n",
      "CV loss=0.1667182698001018(Epoch= 2439)\n",
      "train loss=0.10607845542798917(Epoch= 2439)\n",
      "CV loss=0.16657290618817217(Epoch= 2440)\n",
      "train loss=0.10602006412786442(Epoch= 2440)\n",
      "CV loss=0.1664883354490847(Epoch= 2441)\n",
      "train loss=0.10597073151851458(Epoch= 2441)\n",
      "CV loss=0.16637520942719122(Epoch= 2442)\n",
      "train loss=0.10592541135408413(Epoch= 2442)\n",
      "CV loss=0.1664922471405551(Epoch= 2443)\n",
      "train loss=0.1058803821791405(Epoch= 2443)\n",
      "CV loss=0.1663115833102479(Epoch= 2444)\n",
      "train loss=0.10583241754318015(Epoch= 2444)\n",
      "CV loss=0.16628710053037832(Epoch= 2445)\n",
      "train loss=0.10579070372551426(Epoch= 2445)\n",
      "CV loss=0.16625811985282768(Epoch= 2446)\n",
      "train loss=0.1057356466567772(Epoch= 2446)\n",
      "CV loss=0.16612078550027695(Epoch= 2447)\n",
      "train loss=0.105696031448086(Epoch= 2447)\n",
      "CV loss=0.16621445500467974(Epoch= 2448)\n",
      "train loss=0.10564969847561763(Epoch= 2448)\n",
      "CV loss=0.166130030840607(Epoch= 2449)\n",
      "train loss=0.10559939205805169(Epoch= 2449)\n",
      "CV loss=0.16601364089096599(Epoch= 2450)\n",
      "train loss=0.10555764810082865(Epoch= 2450)\n",
      "CV loss=0.1658515722433921(Epoch= 2451)\n",
      "train loss=0.1055210026332119(Epoch= 2451)\n",
      "CV loss=0.16597972583354853(Epoch= 2452)\n",
      "train loss=0.10546578459939913(Epoch= 2452)\n",
      "CV loss=0.16597582516975765(Epoch= 2453)\n",
      "train loss=0.10541378346700755(Epoch= 2453)\n",
      "CV loss=0.16575797520526664(Epoch= 2454)\n",
      "train loss=0.10537706610155588(Epoch= 2454)\n",
      "CV loss=0.16596000575487163(Epoch= 2455)\n",
      "train loss=0.10533597153079914(Epoch= 2455)\n",
      "CV loss=0.1656136042475333(Epoch= 2456)\n",
      "train loss=0.1052813563228438(Epoch= 2456)\n",
      "CV loss=0.1658791488787994(Epoch= 2457)\n",
      "train loss=0.10523148713391217(Epoch= 2457)\n",
      "CV loss=0.16570934557161401(Epoch= 2458)\n",
      "train loss=0.1051853710634351(Epoch= 2458)\n",
      "CV loss=0.1655756192105475(Epoch= 2459)\n",
      "train loss=0.10513741287672533(Epoch= 2459)\n",
      "CV loss=0.16546097696784787(Epoch= 2460)\n",
      "train loss=0.10509320979951074(Epoch= 2460)\n",
      "CV loss=0.16549985318165694(Epoch= 2461)\n",
      "train loss=0.10504447843106922(Epoch= 2461)\n",
      "CV loss=0.16558513706206063(Epoch= 2462)\n",
      "train loss=0.1049998910677874(Epoch= 2462)\n",
      "CV loss=0.16559254581545757(Epoch= 2463)\n",
      "train loss=0.10496300202726727(Epoch= 2463)\n",
      "CV loss=0.16524993778537958(Epoch= 2464)\n",
      "train loss=0.10492118960065738(Epoch= 2464)\n",
      "CV loss=0.1652802244644967(Epoch= 2465)\n",
      "train loss=0.10485941705907575(Epoch= 2465)\n",
      "CV loss=0.16517327879269167(Epoch= 2466)\n",
      "train loss=0.1048126833176415(Epoch= 2466)\n",
      "CV loss=0.1651542151530218(Epoch= 2467)\n",
      "train loss=0.10476810479953563(Epoch= 2467)\n",
      "CV loss=0.1653204588344909(Epoch= 2468)\n",
      "train loss=0.10472732753755842(Epoch= 2468)\n",
      "CV loss=0.16501229912626858(Epoch= 2469)\n",
      "train loss=0.10467794321055425(Epoch= 2469)\n",
      "CV loss=0.1649199497106463(Epoch= 2470)\n",
      "train loss=0.10463530954071511(Epoch= 2470)\n",
      "CV loss=0.16501805933981456(Epoch= 2471)\n",
      "train loss=0.10459321045238174(Epoch= 2471)\n",
      "CV loss=0.16488493076000857(Epoch= 2472)\n",
      "train loss=0.10454010478369022(Epoch= 2472)\n",
      "CV loss=0.1650831341800807(Epoch= 2473)\n",
      "train loss=0.10449387657513645(Epoch= 2473)\n",
      "CV loss=0.16501956508811833(Epoch= 2474)\n",
      "train loss=0.10445385680129737(Epoch= 2474)\n",
      "CV loss=0.16494985770707526(Epoch= 2475)\n",
      "train loss=0.10440640626324997(Epoch= 2475)\n",
      "CV loss=0.16485872020956205(Epoch= 2476)\n",
      "train loss=0.1043563733812811(Epoch= 2476)\n",
      "CV loss=0.16479351436201067(Epoch= 2477)\n",
      "train loss=0.1043114200245538(Epoch= 2477)\n",
      "CV loss=0.1647515038291137(Epoch= 2478)\n",
      "train loss=0.1042631847505789(Epoch= 2478)\n",
      "CV loss=0.16466630126744192(Epoch= 2479)\n",
      "train loss=0.10421929305500457(Epoch= 2479)\n",
      "CV loss=0.16446204353966384(Epoch= 2480)\n",
      "train loss=0.10418302696999253(Epoch= 2480)\n",
      "CV loss=0.16448139945019608(Epoch= 2481)\n",
      "train loss=0.1041306934914758(Epoch= 2481)\n",
      "CV loss=0.16449451940258397(Epoch= 2482)\n",
      "train loss=0.10408423634748717(Epoch= 2482)\n",
      "CV loss=0.16456325864586896(Epoch= 2483)\n",
      "train loss=0.10404254213614471(Epoch= 2483)\n",
      "CV loss=0.16449955047789355(Epoch= 2484)\n",
      "train loss=0.1039955107319916(Epoch= 2484)\n",
      "CV loss=0.1645442777718862(Epoch= 2485)\n",
      "train loss=0.10395576787774082(Epoch= 2485)\n",
      "CV loss=0.164478245092609(Epoch= 2486)\n",
      "train loss=0.10390676863992827(Epoch= 2486)\n",
      "CV loss=0.1643308728727842(Epoch= 2487)\n",
      "train loss=0.10385714066427419(Epoch= 2487)\n",
      "CV loss=0.16418012573804777(Epoch= 2488)\n",
      "train loss=0.10381469978381923(Epoch= 2488)\n",
      "CV loss=0.16428913230062447(Epoch= 2489)\n",
      "train loss=0.1037743184736785(Epoch= 2489)\n",
      "CV loss=0.1642080034206105(Epoch= 2490)\n",
      "train loss=0.1037275027482016(Epoch= 2490)\n",
      "CV loss=0.16412262792493015(Epoch= 2491)\n",
      "train loss=0.10368121277626276(Epoch= 2491)\n",
      "CV loss=0.16406930480711213(Epoch= 2492)\n",
      "train loss=0.10363366764295234(Epoch= 2492)\n",
      "CV loss=0.16396940558630912(Epoch= 2493)\n",
      "train loss=0.10360353460673953(Epoch= 2493)\n",
      "CV loss=0.1640737278823686(Epoch= 2494)\n",
      "train loss=0.10354515592529907(Epoch= 2494)\n",
      "CV loss=0.1638077661136191(Epoch= 2495)\n",
      "train loss=0.10350781101367836(Epoch= 2495)\n",
      "CV loss=0.16389555991105725(Epoch= 2496)\n",
      "train loss=0.1034604264426859(Epoch= 2496)\n",
      "CV loss=0.16375409933759338(Epoch= 2497)\n",
      "train loss=0.10341285748497644(Epoch= 2497)\n",
      "CV loss=0.1638081335821462(Epoch= 2498)\n",
      "train loss=0.10336325058495474(Epoch= 2498)\n",
      "CV loss=0.1637285957711157(Epoch= 2499)\n",
      "train loss=0.10332227842469095(Epoch= 2499)\n",
      "CV loss=0.16358680983544993(Epoch= 2500)\n",
      "train loss=0.10327628357590662(Epoch= 2500)\n",
      "CV loss=0.1635243116524529(Epoch= 2501)\n",
      "train loss=0.10323642430371867(Epoch= 2501)\n",
      "CV loss=0.16373466941610776(Epoch= 2502)\n",
      "train loss=0.10318933327517989(Epoch= 2502)\n",
      "CV loss=0.16349259256685555(Epoch= 2503)\n",
      "train loss=0.10314392550531779(Epoch= 2503)\n",
      "CV loss=0.16365782886384883(Epoch= 2504)\n",
      "train loss=0.1031017131660961(Epoch= 2504)\n",
      "CV loss=0.16355961793393448(Epoch= 2505)\n",
      "train loss=0.10306564441429149(Epoch= 2505)\n",
      "CV loss=0.16355382368894195(Epoch= 2506)\n",
      "train loss=0.10300904285306017(Epoch= 2506)\n",
      "CV loss=0.1632756752185758(Epoch= 2507)\n",
      "train loss=0.10297167178258436(Epoch= 2507)\n",
      "CV loss=0.16332287239547555(Epoch= 2508)\n",
      "train loss=0.10292357313999624(Epoch= 2508)\n",
      "CV loss=0.16310794532113992(Epoch= 2509)\n",
      "train loss=0.10288521446436806(Epoch= 2509)\n",
      "CV loss=0.1631559419947327(Epoch= 2510)\n",
      "train loss=0.10284150004357072(Epoch= 2510)\n",
      "CV loss=0.1633399907562893(Epoch= 2511)\n",
      "train loss=0.10279461237314694(Epoch= 2511)\n",
      "CV loss=0.16330432098916098(Epoch= 2512)\n",
      "train loss=0.10275315982584382(Epoch= 2512)\n",
      "CV loss=0.16300814756352627(Epoch= 2513)\n",
      "train loss=0.10269857312000431(Epoch= 2513)\n",
      "CV loss=0.1631250295770397(Epoch= 2514)\n",
      "train loss=0.10266940914549458(Epoch= 2514)\n",
      "CV loss=0.1629750482403044(Epoch= 2515)\n",
      "train loss=0.10261642781328169(Epoch= 2515)\n",
      "CV loss=0.16309720782561737(Epoch= 2516)\n",
      "train loss=0.10258112846527563(Epoch= 2516)\n",
      "CV loss=0.16287538672004523(Epoch= 2517)\n",
      "train loss=0.10252273024280605(Epoch= 2517)\n",
      "CV loss=0.1628017673179244(Epoch= 2518)\n",
      "train loss=0.10247906201055927(Epoch= 2518)\n",
      "CV loss=0.1628252800166912(Epoch= 2519)\n",
      "train loss=0.10244230143211126(Epoch= 2519)\n",
      "CV loss=0.16276153282907502(Epoch= 2520)\n",
      "train loss=0.10239366654555764(Epoch= 2520)\n",
      "CV loss=0.16277891880557643(Epoch= 2521)\n",
      "train loss=0.10236000592814588(Epoch= 2521)\n",
      "CV loss=0.16253255497580843(Epoch= 2522)\n",
      "train loss=0.10231591339836549(Epoch= 2522)\n",
      "CV loss=0.16254053443655467(Epoch= 2523)\n",
      "train loss=0.10226087951653075(Epoch= 2523)\n",
      "CV loss=0.1625539977563662(Epoch= 2524)\n",
      "train loss=0.10221984417326747(Epoch= 2524)\n",
      "CV loss=0.1626721352202236(Epoch= 2525)\n",
      "train loss=0.10217329661492415(Epoch= 2525)\n",
      "CV loss=0.16247402495139357(Epoch= 2526)\n",
      "train loss=0.10212975012002473(Epoch= 2526)\n",
      "CV loss=0.16251877434505413(Epoch= 2527)\n",
      "train loss=0.1020862804924427(Epoch= 2527)\n",
      "CV loss=0.1624418693526558(Epoch= 2528)\n",
      "train loss=0.1020443384655294(Epoch= 2528)\n",
      "CV loss=0.16227063148304907(Epoch= 2529)\n",
      "train loss=0.10201416056502252(Epoch= 2529)\n",
      "CV loss=0.1623237370393143(Epoch= 2530)\n",
      "train loss=0.10195883165923539(Epoch= 2530)\n",
      "CV loss=0.16238785812895257(Epoch= 2531)\n",
      "train loss=0.10191317918205191(Epoch= 2531)\n",
      "CV loss=0.16227809752542224(Epoch= 2532)\n",
      "train loss=0.10186846784577659(Epoch= 2532)\n",
      "CV loss=0.1621146590481072(Epoch= 2533)\n",
      "train loss=0.1018251714380445(Epoch= 2533)\n",
      "CV loss=0.16210885363525981(Epoch= 2534)\n",
      "train loss=0.1017879675101688(Epoch= 2534)\n",
      "CV loss=0.16199561217262304(Epoch= 2535)\n",
      "train loss=0.10174210771360113(Epoch= 2535)\n",
      "CV loss=0.1620218706217128(Epoch= 2536)\n",
      "train loss=0.10169889905300113(Epoch= 2536)\n",
      "CV loss=0.16208521674786028(Epoch= 2537)\n",
      "train loss=0.10167001666598006(Epoch= 2537)\n",
      "CV loss=0.16187735522571522(Epoch= 2538)\n",
      "train loss=0.10160871457253652(Epoch= 2538)\n",
      "CV loss=0.161933763233644(Epoch= 2539)\n",
      "train loss=0.10156298427281395(Epoch= 2539)\n",
      "CV loss=0.1617948774811408(Epoch= 2540)\n",
      "train loss=0.10152723581665264(Epoch= 2540)\n",
      "CV loss=0.16184173394286622(Epoch= 2541)\n",
      "train loss=0.10147668050632075(Epoch= 2541)\n",
      "CV loss=0.1617068265032066(Epoch= 2542)\n",
      "train loss=0.10144268366702093(Epoch= 2542)\n",
      "CV loss=0.1616169859808037(Epoch= 2543)\n",
      "train loss=0.10139468297042148(Epoch= 2543)\n",
      "CV loss=0.16171693561840098(Epoch= 2544)\n",
      "train loss=0.10135931058460726(Epoch= 2544)\n",
      "CV loss=0.16154104529106883(Epoch= 2545)\n",
      "train loss=0.10132080855757206(Epoch= 2545)\n",
      "CV loss=0.16168044040630228(Epoch= 2546)\n",
      "train loss=0.1012699038504484(Epoch= 2546)\n",
      "CV loss=0.16161698167393068(Epoch= 2547)\n",
      "train loss=0.10122327682266373(Epoch= 2547)\n",
      "CV loss=0.16154326835680122(Epoch= 2548)\n",
      "train loss=0.10117581538472614(Epoch= 2548)\n",
      "CV loss=0.16138023230901893(Epoch= 2549)\n",
      "train loss=0.10113931024968585(Epoch= 2549)\n",
      "CV loss=0.16140102330379116(Epoch= 2550)\n",
      "train loss=0.10110947889283682(Epoch= 2550)\n",
      "CV loss=0.1614878657333419(Epoch= 2551)\n",
      "train loss=0.10105674239188929(Epoch= 2551)\n",
      "CV loss=0.16137305134832824(Epoch= 2552)\n",
      "train loss=0.1010094937263823(Epoch= 2552)\n",
      "CV loss=0.16123821788643083(Epoch= 2553)\n",
      "train loss=0.1009654482396658(Epoch= 2553)\n",
      "CV loss=0.16124000508911437(Epoch= 2554)\n",
      "train loss=0.10092728898500317(Epoch= 2554)\n",
      "CV loss=0.1612156981910084(Epoch= 2555)\n",
      "train loss=0.10087822198193905(Epoch= 2555)\n",
      "CV loss=0.16133100951854923(Epoch= 2556)\n",
      "train loss=0.10084198037947123(Epoch= 2556)\n",
      "CV loss=0.16108926987161737(Epoch= 2557)\n",
      "train loss=0.10079519136332524(Epoch= 2557)\n",
      "CV loss=0.1610045537521595(Epoch= 2558)\n",
      "train loss=0.10075212011811781(Epoch= 2558)\n",
      "CV loss=0.160945299428378(Epoch= 2559)\n",
      "train loss=0.1007143557228858(Epoch= 2559)\n",
      "CV loss=0.1610496388349857(Epoch= 2560)\n",
      "train loss=0.10066621289027411(Epoch= 2560)\n",
      "CV loss=0.16090646040008721(Epoch= 2561)\n",
      "train loss=0.10062458905067609(Epoch= 2561)\n",
      "CV loss=0.16078081250736204(Epoch= 2562)\n",
      "train loss=0.10058943698939836(Epoch= 2562)\n",
      "CV loss=0.16090189824924334(Epoch= 2563)\n",
      "train loss=0.10054055080512651(Epoch= 2563)\n",
      "CV loss=0.16074731629919306(Epoch= 2564)\n",
      "train loss=0.1004992538959585(Epoch= 2564)\n",
      "CV loss=0.16068890918742687(Epoch= 2565)\n",
      "train loss=0.10046569066993857(Epoch= 2565)\n",
      "CV loss=0.16072614501650906(Epoch= 2566)\n",
      "train loss=0.10041323022277675(Epoch= 2566)\n",
      "CV loss=0.16078729909324313(Epoch= 2567)\n",
      "train loss=0.10037379731605606(Epoch= 2567)\n",
      "CV loss=0.16068131589785867(Epoch= 2568)\n",
      "train loss=0.10032804516680757(Epoch= 2568)\n",
      "CV loss=0.1605818508974018(Epoch= 2569)\n",
      "train loss=0.10029126149974305(Epoch= 2569)\n",
      "CV loss=0.16056353518064723(Epoch= 2570)\n",
      "train loss=0.1002517458429(Epoch= 2570)\n",
      "CV loss=0.160551881190801(Epoch= 2571)\n",
      "train loss=0.10020179988743799(Epoch= 2571)\n",
      "CV loss=0.16040026939526653(Epoch= 2572)\n",
      "train loss=0.1001682030436644(Epoch= 2572)\n",
      "CV loss=0.16027956161253185(Epoch= 2573)\n",
      "train loss=0.10013467134979587(Epoch= 2573)\n",
      "CV loss=0.16034779285222694(Epoch= 2574)\n",
      "train loss=0.10008026229044127(Epoch= 2574)\n",
      "CV loss=0.1602657873087661(Epoch= 2575)\n",
      "train loss=0.10003475321333087(Epoch= 2575)\n",
      "CV loss=0.1603071673330071(Epoch= 2576)\n",
      "train loss=0.09999825732393175(Epoch= 2576)\n",
      "CV loss=0.16028903004623646(Epoch= 2577)\n",
      "train loss=0.09995204873675863(Epoch= 2577)\n",
      "CV loss=0.1602106815070213(Epoch= 2578)\n",
      "train loss=0.09991514232650274(Epoch= 2578)\n",
      "CV loss=0.16007102278489801(Epoch= 2579)\n",
      "train loss=0.09987043120619929(Epoch= 2579)\n",
      "CV loss=0.16015199884389814(Epoch= 2580)\n",
      "train loss=0.09983437739468468(Epoch= 2580)\n",
      "CV loss=0.16013993225663653(Epoch= 2581)\n",
      "train loss=0.09978899833125501(Epoch= 2581)\n",
      "CV loss=0.16010928713232164(Epoch= 2582)\n",
      "train loss=0.09974606228691225(Epoch= 2582)\n",
      "CV loss=0.16003777843756395(Epoch= 2583)\n",
      "train loss=0.0997067338369724(Epoch= 2583)\n",
      "CV loss=0.15997953021630368(Epoch= 2584)\n",
      "train loss=0.09966466607562009(Epoch= 2584)\n",
      "CV loss=0.15988307868671658(Epoch= 2585)\n",
      "train loss=0.09962013244654747(Epoch= 2585)\n",
      "CV loss=0.15977380550109727(Epoch= 2586)\n",
      "train loss=0.09958908604033256(Epoch= 2586)\n",
      "CV loss=0.15980557503328957(Epoch= 2587)\n",
      "train loss=0.09953540602731974(Epoch= 2587)\n",
      "CV loss=0.15992440084671716(Epoch= 2588)\n",
      "train loss=0.09949600211184026(Epoch= 2588)\n",
      "CV loss=0.16002875051573656(Epoch= 2589)\n",
      "train loss=0.0994697671319461(Epoch= 2589)\n",
      "CV loss=0.15964205702491338(Epoch= 2590)\n",
      "train loss=0.09943010763960781(Epoch= 2590)\n",
      "CV loss=0.1596407349904092(Epoch= 2591)\n",
      "train loss=0.09937884243338532(Epoch= 2591)\n",
      "CV loss=0.15978774353275788(Epoch= 2592)\n",
      "train loss=0.09933578162674427(Epoch= 2592)\n",
      "CV loss=0.15968129099346448(Epoch= 2593)\n",
      "train loss=0.09928846312114926(Epoch= 2593)\n",
      "CV loss=0.15949350913969237(Epoch= 2594)\n",
      "train loss=0.09924745383598327(Epoch= 2594)\n",
      "CV loss=0.15956933484106495(Epoch= 2595)\n",
      "train loss=0.09920467311255754(Epoch= 2595)\n",
      "CV loss=0.15945915810175643(Epoch= 2596)\n",
      "train loss=0.09916867258032477(Epoch= 2596)\n",
      "CV loss=0.15923731116339984(Epoch= 2597)\n",
      "train loss=0.09912593107196051(Epoch= 2597)\n",
      "CV loss=0.15935470734942941(Epoch= 2598)\n",
      "train loss=0.09908535471994624(Epoch= 2598)\n",
      "CV loss=0.15921700680526157(Epoch= 2599)\n",
      "train loss=0.09904003991805221(Epoch= 2599)\n",
      "CV loss=0.15920612544792015(Epoch= 2600)\n",
      "train loss=0.09899934163107427(Epoch= 2600)\n",
      "CV loss=0.15932576820896055(Epoch= 2601)\n",
      "train loss=0.09896908769356179(Epoch= 2601)\n",
      "CV loss=0.1593435738854773(Epoch= 2602)\n",
      "train loss=0.09892302068088878(Epoch= 2602)\n",
      "CV loss=0.15913513353678052(Epoch= 2603)\n",
      "train loss=0.0988831668065543(Epoch= 2603)\n",
      "CV loss=0.15913385458353477(Epoch= 2604)\n",
      "train loss=0.09883693805985357(Epoch= 2604)\n",
      "CV loss=0.15913660249189532(Epoch= 2605)\n",
      "train loss=0.09880254416523865(Epoch= 2605)\n",
      "CV loss=0.15893672936143965(Epoch= 2606)\n",
      "train loss=0.09875777599374753(Epoch= 2606)\n",
      "CV loss=0.158989003583548(Epoch= 2607)\n",
      "train loss=0.09872044556173874(Epoch= 2607)\n",
      "CV loss=0.1587734607221674(Epoch= 2608)\n",
      "train loss=0.09867703299823634(Epoch= 2608)\n",
      "CV loss=0.1589681850529593(Epoch= 2609)\n",
      "train loss=0.09863722620155771(Epoch= 2609)\n",
      "CV loss=0.1587366670106511(Epoch= 2610)\n",
      "train loss=0.09859698566993769(Epoch= 2610)\n",
      "CV loss=0.1588713991091744(Epoch= 2611)\n",
      "train loss=0.09855813868895943(Epoch= 2611)\n",
      "CV loss=0.1587009178617692(Epoch= 2612)\n",
      "train loss=0.09851291638393567(Epoch= 2612)\n",
      "CV loss=0.15868243493085765(Epoch= 2613)\n",
      "train loss=0.09847206517597341(Epoch= 2613)\n",
      "CV loss=0.15861756206536598(Epoch= 2614)\n",
      "train loss=0.09843190600194737(Epoch= 2614)\n",
      "CV loss=0.1586639778202329(Epoch= 2615)\n",
      "train loss=0.09839801877311434(Epoch= 2615)\n",
      "CV loss=0.15866614735240125(Epoch= 2616)\n",
      "train loss=0.09834903320620643(Epoch= 2616)\n",
      "CV loss=0.1586068849852839(Epoch= 2617)\n",
      "train loss=0.09830990116115754(Epoch= 2617)\n",
      "CV loss=0.15851211824309663(Epoch= 2618)\n",
      "train loss=0.0982704434816012(Epoch= 2618)\n",
      "CV loss=0.15854477671104122(Epoch= 2619)\n",
      "train loss=0.0982287607816085(Epoch= 2619)\n",
      "CV loss=0.15830439756019693(Epoch= 2620)\n",
      "train loss=0.09819274613926027(Epoch= 2620)\n",
      "CV loss=0.15833208951878588(Epoch= 2621)\n",
      "train loss=0.0981532392292248(Epoch= 2621)\n",
      "CV loss=0.1584383114306339(Epoch= 2622)\n",
      "train loss=0.09811161864755598(Epoch= 2622)\n",
      "CV loss=0.15837353772356688(Epoch= 2623)\n",
      "train loss=0.0980779202821377(Epoch= 2623)\n",
      "CV loss=0.15816747740574924(Epoch= 2624)\n",
      "train loss=0.09803785422481381(Epoch= 2624)\n",
      "CV loss=0.15827706695858623(Epoch= 2625)\n",
      "train loss=0.09798753504629935(Epoch= 2625)\n",
      "CV loss=0.15817887105478412(Epoch= 2626)\n",
      "train loss=0.09794579963005659(Epoch= 2626)\n",
      "CV loss=0.1581298811321356(Epoch= 2627)\n",
      "train loss=0.09791274996305122(Epoch= 2627)\n",
      "CV loss=0.15816053053166323(Epoch= 2628)\n",
      "train loss=0.09786787282901416(Epoch= 2628)\n",
      "CV loss=0.15804166698911615(Epoch= 2629)\n",
      "train loss=0.09782850271842661(Epoch= 2629)\n",
      "CV loss=0.15801348858376046(Epoch= 2630)\n",
      "train loss=0.09778404685539616(Epoch= 2630)\n",
      "CV loss=0.1580959869384374(Epoch= 2631)\n",
      "train loss=0.09774995495347298(Epoch= 2631)\n",
      "CV loss=0.1581207159867214(Epoch= 2632)\n",
      "train loss=0.09771806158139652(Epoch= 2632)\n",
      "CV loss=0.157904291501913(Epoch= 2633)\n",
      "train loss=0.09766871026931193(Epoch= 2633)\n",
      "CV loss=0.15785376901223608(Epoch= 2634)\n",
      "train loss=0.09762588660950514(Epoch= 2634)\n",
      "CV loss=0.15778972279495973(Epoch= 2635)\n",
      "train loss=0.09758903190893214(Epoch= 2635)\n",
      "CV loss=0.1578598136083425(Epoch= 2636)\n",
      "train loss=0.09754686171589311(Epoch= 2636)\n",
      "CV loss=0.15776407146722007(Epoch= 2637)\n",
      "train loss=0.0975097420858811(Epoch= 2637)\n",
      "CV loss=0.15785640422938904(Epoch= 2638)\n",
      "train loss=0.09748129481197244(Epoch= 2638)\n",
      "CV loss=0.15769089826993876(Epoch= 2639)\n",
      "train loss=0.09743011279436652(Epoch= 2639)\n",
      "CV loss=0.1576600545652066(Epoch= 2640)\n",
      "train loss=0.09739605172859125(Epoch= 2640)\n",
      "CV loss=0.157559139525034(Epoch= 2641)\n",
      "train loss=0.09734812791458686(Epoch= 2641)\n",
      "CV loss=0.15737845380645873(Epoch= 2642)\n",
      "train loss=0.09731144265194439(Epoch= 2642)\n",
      "CV loss=0.15757992550558286(Epoch= 2643)\n",
      "train loss=0.09727406334650206(Epoch= 2643)\n",
      "CV loss=0.15747935169236413(Epoch= 2644)\n",
      "train loss=0.09722975268960812(Epoch= 2644)\n",
      "CV loss=0.15743995769781238(Epoch= 2645)\n",
      "train loss=0.09718963813006044(Epoch= 2645)\n",
      "CV loss=0.1574555944844866(Epoch= 2646)\n",
      "train loss=0.09715282241412905(Epoch= 2646)\n",
      "CV loss=0.1573695727690838(Epoch= 2647)\n",
      "train loss=0.09711569400003217(Epoch= 2647)\n",
      "CV loss=0.157407129767906(Epoch= 2648)\n",
      "train loss=0.09707327228333859(Epoch= 2648)\n",
      "CV loss=0.157298664365227(Epoch= 2649)\n",
      "train loss=0.0970385345873958(Epoch= 2649)\n",
      "CV loss=0.1570943743049501(Epoch= 2650)\n",
      "train loss=0.09699958775558201(Epoch= 2650)\n",
      "CV loss=0.15713619182825223(Epoch= 2651)\n",
      "train loss=0.0969562577864458(Epoch= 2651)\n",
      "CV loss=0.15707891595899853(Epoch= 2652)\n",
      "train loss=0.09692843079207579(Epoch= 2652)\n",
      "CV loss=0.15698610202590135(Epoch= 2653)\n",
      "train loss=0.09687923209384651(Epoch= 2653)\n",
      "CV loss=0.15714976110994977(Epoch= 2654)\n",
      "train loss=0.09684231332209252(Epoch= 2654)\n",
      "CV loss=0.15692413756724705(Epoch= 2655)\n",
      "train loss=0.09680675863878448(Epoch= 2655)\n",
      "CV loss=0.15690930240531323(Epoch= 2656)\n",
      "train loss=0.09675867955373547(Epoch= 2656)\n",
      "CV loss=0.15700200666225345(Epoch= 2657)\n",
      "train loss=0.09672067428401974(Epoch= 2657)\n",
      "CV loss=0.15694823793243348(Epoch= 2658)\n",
      "train loss=0.09668665771028781(Epoch= 2658)\n",
      "CV loss=0.15692419576125285(Epoch= 2659)\n",
      "train loss=0.09664390170238543(Epoch= 2659)\n",
      "CV loss=0.15686811303966636(Epoch= 2660)\n",
      "train loss=0.09660701885025239(Epoch= 2660)\n",
      "CV loss=0.1567833257431256(Epoch= 2661)\n",
      "train loss=0.09656690075313216(Epoch= 2661)\n",
      "CV loss=0.15680745499457432(Epoch= 2662)\n",
      "train loss=0.09652544933592008(Epoch= 2662)\n",
      "CV loss=0.15684471254751003(Epoch= 2663)\n",
      "train loss=0.0964901129751745(Epoch= 2663)\n",
      "CV loss=0.15659115082251424(Epoch= 2664)\n",
      "train loss=0.09646549540021335(Epoch= 2664)\n",
      "CV loss=0.1566029364347507(Epoch= 2665)\n",
      "train loss=0.09641225931328229(Epoch= 2665)\n",
      "CV loss=0.15650236096108938(Epoch= 2666)\n",
      "train loss=0.09637111064853253(Epoch= 2666)\n",
      "CV loss=0.15643368005592967(Epoch= 2667)\n",
      "train loss=0.09633511903406793(Epoch= 2667)\n",
      "CV loss=0.15661335004326826(Epoch= 2668)\n",
      "train loss=0.09629522857076261(Epoch= 2668)\n",
      "CV loss=0.15650254908930522(Epoch= 2669)\n",
      "train loss=0.09625488909660947(Epoch= 2669)\n",
      "CV loss=0.1563996068214634(Epoch= 2670)\n",
      "train loss=0.09621505186828484(Epoch= 2670)\n",
      "CV loss=0.15634148917692986(Epoch= 2671)\n",
      "train loss=0.09617501114459294(Epoch= 2671)\n",
      "CV loss=0.1562088900542256(Epoch= 2672)\n",
      "train loss=0.096152909370431(Epoch= 2672)\n",
      "CV loss=0.1560923869573883(Epoch= 2673)\n",
      "train loss=0.09612004145446777(Epoch= 2673)\n",
      "CV loss=0.15622650152997195(Epoch= 2674)\n",
      "train loss=0.09606487926660393(Epoch= 2674)\n",
      "CV loss=0.15633406906313213(Epoch= 2675)\n",
      "train loss=0.0960245064929446(Epoch= 2675)\n",
      "CV loss=0.15626790963846696(Epoch= 2676)\n",
      "train loss=0.0959854607449123(Epoch= 2676)\n",
      "CV loss=0.15620053283974705(Epoch= 2677)\n",
      "train loss=0.09594850605503241(Epoch= 2677)\n",
      "CV loss=0.15607708467733208(Epoch= 2678)\n",
      "train loss=0.09590625913008229(Epoch= 2678)\n",
      "CV loss=0.15602414377816629(Epoch= 2679)\n",
      "train loss=0.0958654216326159(Epoch= 2679)\n",
      "CV loss=0.15593165670776118(Epoch= 2680)\n",
      "train loss=0.09582973471692723(Epoch= 2680)\n",
      "CV loss=0.1559806431449094(Epoch= 2681)\n",
      "train loss=0.09579610255861137(Epoch= 2681)\n",
      "CV loss=0.1557414178035102(Epoch= 2682)\n",
      "train loss=0.09576277499362466(Epoch= 2682)\n",
      "CV loss=0.1558936566176004(Epoch= 2683)\n",
      "train loss=0.09571749577493804(Epoch= 2683)\n",
      "CV loss=0.15577552777289577(Epoch= 2684)\n",
      "train loss=0.09567699569897158(Epoch= 2684)\n",
      "CV loss=0.15558963179397473(Epoch= 2685)\n",
      "train loss=0.09564816136081009(Epoch= 2685)\n",
      "CV loss=0.15578812224453295(Epoch= 2686)\n",
      "train loss=0.09560863916343013(Epoch= 2686)\n",
      "CV loss=0.15574702249396954(Epoch= 2687)\n",
      "train loss=0.09556068049738219(Epoch= 2687)\n",
      "CV loss=0.15565692551294433(Epoch= 2688)\n",
      "train loss=0.09552283290048802(Epoch= 2688)\n",
      "CV loss=0.1556179544251983(Epoch= 2689)\n",
      "train loss=0.095486611422694(Epoch= 2689)\n",
      "CV loss=0.15560628935303306(Epoch= 2690)\n",
      "train loss=0.09544864668610623(Epoch= 2690)\n",
      "CV loss=0.1553838896080608(Epoch= 2691)\n",
      "train loss=0.0954231774266436(Epoch= 2691)\n",
      "CV loss=0.1555246157919038(Epoch= 2692)\n",
      "train loss=0.0953811766056825(Epoch= 2692)\n",
      "CV loss=0.15543423244173488(Epoch= 2693)\n",
      "train loss=0.09533475357143456(Epoch= 2693)\n",
      "CV loss=0.1554165492232777(Epoch= 2694)\n",
      "train loss=0.09529979961804212(Epoch= 2694)\n",
      "CV loss=0.15554678976197595(Epoch= 2695)\n",
      "train loss=0.09526340946889417(Epoch= 2695)\n",
      "CV loss=0.1553648071730897(Epoch= 2696)\n",
      "train loss=0.09521951628638346(Epoch= 2696)\n",
      "CV loss=0.15534961826364813(Epoch= 2697)\n",
      "train loss=0.09518590133317088(Epoch= 2697)\n",
      "CV loss=0.1553443352622655(Epoch= 2698)\n",
      "train loss=0.09514459148779009(Epoch= 2698)\n",
      "CV loss=0.1552210436676308(Epoch= 2699)\n",
      "train loss=0.09510579667390273(Epoch= 2699)\n",
      "CV loss=0.15522316511771245(Epoch= 2700)\n",
      "train loss=0.09506834702516073(Epoch= 2700)\n",
      "CV loss=0.15521341470649885(Epoch= 2701)\n",
      "train loss=0.09503244233265556(Epoch= 2701)\n",
      "CV loss=0.15509592753989648(Epoch= 2702)\n",
      "train loss=0.09499630308968815(Epoch= 2702)\n",
      "CV loss=0.15514610138178803(Epoch= 2703)\n",
      "train loss=0.0949575530807434(Epoch= 2703)\n",
      "CV loss=0.15505306501779637(Epoch= 2704)\n",
      "train loss=0.09492911950270741(Epoch= 2704)\n",
      "CV loss=0.15503027591855428(Epoch= 2705)\n",
      "train loss=0.09487955820506785(Epoch= 2705)\n",
      "CV loss=0.15501795052089132(Epoch= 2706)\n",
      "train loss=0.09484462616949592(Epoch= 2706)\n",
      "CV loss=0.15498799868339797(Epoch= 2707)\n",
      "train loss=0.0948083415312565(Epoch= 2707)\n",
      "CV loss=0.15491517514677156(Epoch= 2708)\n",
      "train loss=0.09477118566288872(Epoch= 2708)\n",
      "CV loss=0.15461213978602553(Epoch= 2709)\n",
      "train loss=0.09473975250663583(Epoch= 2709)\n",
      "CV loss=0.15488720343358536(Epoch= 2710)\n",
      "train loss=0.09469640708398978(Epoch= 2710)\n",
      "CV loss=0.1547596925289058(Epoch= 2711)\n",
      "train loss=0.09466035715807232(Epoch= 2711)\n",
      "CV loss=0.15472605558139277(Epoch= 2712)\n",
      "train loss=0.09461685178863988(Epoch= 2712)\n",
      "CV loss=0.1547342094600942(Epoch= 2713)\n",
      "train loss=0.09458748895428781(Epoch= 2713)\n",
      "CV loss=0.15467884318176955(Epoch= 2714)\n",
      "train loss=0.09454470220927436(Epoch= 2714)\n",
      "CV loss=0.15460302620828364(Epoch= 2715)\n",
      "train loss=0.09450784992882566(Epoch= 2715)\n",
      "CV loss=0.15444833763209026(Epoch= 2716)\n",
      "train loss=0.09449355222415695(Epoch= 2716)\n",
      "CV loss=0.154470820036587(Epoch= 2717)\n",
      "train loss=0.09443457547205823(Epoch= 2717)\n",
      "CV loss=0.1544986888915705(Epoch= 2718)\n",
      "train loss=0.09439499166534351(Epoch= 2718)\n",
      "CV loss=0.1543875587679804(Epoch= 2719)\n",
      "train loss=0.09437140729332463(Epoch= 2719)\n",
      "CV loss=0.15425520834895023(Epoch= 2720)\n",
      "train loss=0.09432878851210147(Epoch= 2720)\n",
      "CV loss=0.15447333030981153(Epoch= 2721)\n",
      "train loss=0.09429308414659458(Epoch= 2721)\n",
      "CV loss=0.15442690627720215(Epoch= 2722)\n",
      "train loss=0.0942677530053705(Epoch= 2722)\n",
      "CV loss=0.15431526639234167(Epoch= 2723)\n",
      "train loss=0.09421213698558631(Epoch= 2723)\n",
      "CV loss=0.15437263796800832(Epoch= 2724)\n",
      "train loss=0.09417358193882114(Epoch= 2724)\n",
      "CV loss=0.15422308008383775(Epoch= 2725)\n",
      "train loss=0.09414206929124044(Epoch= 2725)\n",
      "CV loss=0.15415201055016012(Epoch= 2726)\n",
      "train loss=0.09410397073182343(Epoch= 2726)\n",
      "CV loss=0.15408852776888154(Epoch= 2727)\n",
      "train loss=0.09406440950426394(Epoch= 2727)\n",
      "CV loss=0.15436745182671768(Epoch= 2728)\n",
      "train loss=0.09404446256518005(Epoch= 2728)\n",
      "CV loss=0.1541500136274881(Epoch= 2729)\n",
      "train loss=0.09399487838130108(Epoch= 2729)\n",
      "CV loss=0.15404584808411934(Epoch= 2730)\n",
      "train loss=0.09395417086145527(Epoch= 2730)\n",
      "CV loss=0.15396277306693967(Epoch= 2731)\n",
      "train loss=0.09391599318581732(Epoch= 2731)\n",
      "CV loss=0.15406990500623002(Epoch= 2732)\n",
      "train loss=0.09388265075285102(Epoch= 2732)\n",
      "CV loss=0.15396794385515492(Epoch= 2733)\n",
      "train loss=0.09384623134438673(Epoch= 2733)\n",
      "CV loss=0.1539994457553755(Epoch= 2734)\n",
      "train loss=0.09383037379258306(Epoch= 2734)\n",
      "CV loss=0.153789445272281(Epoch= 2735)\n",
      "train loss=0.09376880388775437(Epoch= 2735)\n",
      "CV loss=0.15393170934646833(Epoch= 2736)\n",
      "train loss=0.09373212426488521(Epoch= 2736)\n",
      "CV loss=0.15366447049385434(Epoch= 2737)\n",
      "train loss=0.09370110095278147(Epoch= 2737)\n",
      "CV loss=0.15380201370922464(Epoch= 2738)\n",
      "train loss=0.09366223729902762(Epoch= 2738)\n",
      "CV loss=0.15365925425648808(Epoch= 2739)\n",
      "train loss=0.09362604345698652(Epoch= 2739)\n",
      "CV loss=0.15364824753175116(Epoch= 2740)\n",
      "train loss=0.09358901103164846(Epoch= 2740)\n",
      "CV loss=0.15361764138025477(Epoch= 2741)\n",
      "train loss=0.09355459869113711(Epoch= 2741)\n",
      "CV loss=0.1535975117461636(Epoch= 2742)\n",
      "train loss=0.09351491091012036(Epoch= 2742)\n",
      "CV loss=0.15349187131415898(Epoch= 2743)\n",
      "train loss=0.09348662045295326(Epoch= 2743)\n",
      "CV loss=0.1534976639637206(Epoch= 2744)\n",
      "train loss=0.09344369413316331(Epoch= 2744)\n",
      "CV loss=0.1535997511009921(Epoch= 2745)\n",
      "train loss=0.09341276468417478(Epoch= 2745)\n",
      "CV loss=0.15338277926695754(Epoch= 2746)\n",
      "train loss=0.09337173099998412(Epoch= 2746)\n",
      "CV loss=0.15353669566688707(Epoch= 2747)\n",
      "train loss=0.09334509022604158(Epoch= 2747)\n",
      "CV loss=0.15335352994875215(Epoch= 2748)\n",
      "train loss=0.09329638834998993(Epoch= 2748)\n",
      "CV loss=0.15346204595157115(Epoch= 2749)\n",
      "train loss=0.09326249848483219(Epoch= 2749)\n",
      "CV loss=0.15344574146153134(Epoch= 2750)\n",
      "train loss=0.09323312896242(Epoch= 2750)\n",
      "CV loss=0.15320468308928475(Epoch= 2751)\n",
      "train loss=0.09319455859833047(Epoch= 2751)\n",
      "CV loss=0.15333861057395243(Epoch= 2752)\n",
      "train loss=0.09315128198291447(Epoch= 2752)\n",
      "CV loss=0.1530736832190805(Epoch= 2753)\n",
      "train loss=0.09312426897485934(Epoch= 2753)\n",
      "CV loss=0.15317770481882878(Epoch= 2754)\n",
      "train loss=0.09308141823152968(Epoch= 2754)\n",
      "CV loss=0.15323139267036956(Epoch= 2755)\n",
      "train loss=0.09304890998643053(Epoch= 2755)\n",
      "CV loss=0.15310719252085403(Epoch= 2756)\n",
      "train loss=0.09302907929315106(Epoch= 2756)\n",
      "CV loss=0.1531399830816063(Epoch= 2757)\n",
      "train loss=0.09297695330600815(Epoch= 2757)\n",
      "CV loss=0.1529227433303421(Epoch= 2758)\n",
      "train loss=0.09293872325857747(Epoch= 2758)\n",
      "CV loss=0.1528481232657163(Epoch= 2759)\n",
      "train loss=0.09290210702923477(Epoch= 2759)\n",
      "CV loss=0.1528508856521193(Epoch= 2760)\n",
      "train loss=0.09286549892687491(Epoch= 2760)\n",
      "CV loss=0.15273135243339261(Epoch= 2761)\n",
      "train loss=0.09285008704802102(Epoch= 2761)\n",
      "CV loss=0.15273043334821595(Epoch= 2762)\n",
      "train loss=0.0928065752342092(Epoch= 2762)\n",
      "CV loss=0.1528642571138405(Epoch= 2763)\n",
      "train loss=0.0927614282715725(Epoch= 2763)\n",
      "CV loss=0.15264126805427805(Epoch= 2764)\n",
      "train loss=0.09272711424855166(Epoch= 2764)\n",
      "CV loss=0.15275691981451378(Epoch= 2765)\n",
      "train loss=0.09269546196009926(Epoch= 2765)\n",
      "CV loss=0.15253825611964916(Epoch= 2766)\n",
      "train loss=0.09265080394140102(Epoch= 2766)\n",
      "CV loss=0.1527333501666378(Epoch= 2767)\n",
      "train loss=0.09261678546674845(Epoch= 2767)\n",
      "CV loss=0.15263483287432728(Epoch= 2768)\n",
      "train loss=0.0925820189015633(Epoch= 2768)\n",
      "CV loss=0.15249557025565608(Epoch= 2769)\n",
      "train loss=0.09254452833569866(Epoch= 2769)\n",
      "CV loss=0.1526766700976529(Epoch= 2770)\n",
      "train loss=0.09251082101566807(Epoch= 2770)\n",
      "CV loss=0.15262359402005904(Epoch= 2771)\n",
      "train loss=0.09247828337144767(Epoch= 2771)\n",
      "CV loss=0.15240371711216405(Epoch= 2772)\n",
      "train loss=0.0924356566681132(Epoch= 2772)\n",
      "CV loss=0.15231616407351106(Epoch= 2773)\n",
      "train loss=0.09240476801135876(Epoch= 2773)\n",
      "CV loss=0.1524610926481781(Epoch= 2774)\n",
      "train loss=0.09236765355582625(Epoch= 2774)\n",
      "CV loss=0.15245612810522924(Epoch= 2775)\n",
      "train loss=0.09232908374615487(Epoch= 2775)\n",
      "CV loss=0.15229445736460864(Epoch= 2776)\n",
      "train loss=0.09229484546395458(Epoch= 2776)\n",
      "CV loss=0.15229310749339828(Epoch= 2777)\n",
      "train loss=0.09225942563438291(Epoch= 2777)\n",
      "CV loss=0.15226890058421289(Epoch= 2778)\n",
      "train loss=0.09222743690021686(Epoch= 2778)\n",
      "CV loss=0.1520000914924142(Epoch= 2779)\n",
      "train loss=0.09219511768659229(Epoch= 2779)\n",
      "CV loss=0.15229979946190816(Epoch= 2780)\n",
      "train loss=0.09216004402401755(Epoch= 2780)\n",
      "CV loss=0.1522356343299516(Epoch= 2781)\n",
      "train loss=0.09212194285043689(Epoch= 2781)\n",
      "CV loss=0.15197394141156145(Epoch= 2782)\n",
      "train loss=0.09208873221458329(Epoch= 2782)\n",
      "CV loss=0.15199502053281733(Epoch= 2783)\n",
      "train loss=0.09204772771635893(Epoch= 2783)\n",
      "CV loss=0.15191270006974644(Epoch= 2784)\n",
      "train loss=0.09202714193215322(Epoch= 2784)\n",
      "CV loss=0.15195943674700657(Epoch= 2785)\n",
      "train loss=0.09198876430946129(Epoch= 2785)\n",
      "CV loss=0.15188085934905166(Epoch= 2786)\n",
      "train loss=0.0919523735938662(Epoch= 2786)\n",
      "CV loss=0.1518839827568032(Epoch= 2787)\n",
      "train loss=0.0919177974480411(Epoch= 2787)\n",
      "CV loss=0.15172698152580918(Epoch= 2788)\n",
      "train loss=0.09188109966028118(Epoch= 2788)\n",
      "CV loss=0.15206151083230557(Epoch= 2789)\n",
      "train loss=0.09184341588444152(Epoch= 2789)\n",
      "CV loss=0.15204462651260728(Epoch= 2790)\n",
      "train loss=0.09181612143241247(Epoch= 2790)\n",
      "CV loss=0.1517396085954186(Epoch= 2791)\n",
      "train loss=0.09177245282669289(Epoch= 2791)\n",
      "CV loss=0.15184965274395845(Epoch= 2792)\n",
      "train loss=0.09173083883264971(Epoch= 2792)\n",
      "CV loss=0.15169810568932862(Epoch= 2793)\n",
      "train loss=0.09169463153208884(Epoch= 2793)\n",
      "CV loss=0.1518052526850892(Epoch= 2794)\n",
      "train loss=0.09167105018010095(Epoch= 2794)\n",
      "CV loss=0.1517429813068253(Epoch= 2795)\n",
      "train loss=0.09163472857582805(Epoch= 2795)\n",
      "CV loss=0.15176107268011274(Epoch= 2796)\n",
      "train loss=0.09160237369497129(Epoch= 2796)\n",
      "CV loss=0.15150065186281972(Epoch= 2797)\n",
      "train loss=0.09155854047296585(Epoch= 2797)\n",
      "CV loss=0.15141790202757313(Epoch= 2798)\n",
      "train loss=0.09152451207804767(Epoch= 2798)\n",
      "CV loss=0.15148324782667671(Epoch= 2799)\n",
      "train loss=0.09149096051834925(Epoch= 2799)\n",
      "CV loss=0.1513323725878633(Epoch= 2800)\n",
      "train loss=0.09145556246527527(Epoch= 2800)\n",
      "CV loss=0.15128469060381944(Epoch= 2801)\n",
      "train loss=0.0914222224510268(Epoch= 2801)\n",
      "CV loss=0.15129139053326793(Epoch= 2802)\n",
      "train loss=0.09138931113100943(Epoch= 2802)\n",
      "CV loss=0.15158151264328482(Epoch= 2803)\n",
      "train loss=0.09136016384217041(Epoch= 2803)\n",
      "CV loss=0.15114247568126188(Epoch= 2804)\n",
      "train loss=0.0913295274358963(Epoch= 2804)\n",
      "CV loss=0.1512266499080725(Epoch= 2805)\n",
      "train loss=0.09128567376755231(Epoch= 2805)\n",
      "CV loss=0.15124675098894336(Epoch= 2806)\n",
      "train loss=0.0912699429693019(Epoch= 2806)\n",
      "CV loss=0.15134264233682615(Epoch= 2807)\n",
      "train loss=0.09122262754447524(Epoch= 2807)\n",
      "CV loss=0.15111242052060944(Epoch= 2808)\n",
      "train loss=0.09118682024843311(Epoch= 2808)\n",
      "CV loss=0.15119312499579307(Epoch= 2809)\n",
      "train loss=0.09114350701506849(Epoch= 2809)\n",
      "CV loss=0.15123498741516123(Epoch= 2810)\n",
      "train loss=0.0911106215521215(Epoch= 2810)\n",
      "CV loss=0.15099426795455229(Epoch= 2811)\n",
      "train loss=0.09107851603969604(Epoch= 2811)\n",
      "CV loss=0.15105850129630677(Epoch= 2812)\n",
      "train loss=0.09104197878620951(Epoch= 2812)\n",
      "CV loss=0.15096317475702498(Epoch= 2813)\n",
      "train loss=0.09100998385049525(Epoch= 2813)\n",
      "CV loss=0.1508540609083418(Epoch= 2814)\n",
      "train loss=0.09097303450293903(Epoch= 2814)\n",
      "CV loss=0.15094495806948(Epoch= 2815)\n",
      "train loss=0.09094089841373328(Epoch= 2815)\n",
      "CV loss=0.15109357523301387(Epoch= 2816)\n",
      "train loss=0.09091703178453099(Epoch= 2816)\n",
      "CV loss=0.15088038002016668(Epoch= 2817)\n",
      "train loss=0.09086633310550368(Epoch= 2817)\n",
      "CV loss=0.15085447481970438(Epoch= 2818)\n",
      "train loss=0.09084613897742878(Epoch= 2818)\n",
      "CV loss=0.15081148118188242(Epoch= 2819)\n",
      "train loss=0.09080447792256023(Epoch= 2819)\n",
      "CV loss=0.15070625826191122(Epoch= 2820)\n",
      "train loss=0.09077927908938799(Epoch= 2820)\n",
      "CV loss=0.15071846850323978(Epoch= 2821)\n",
      "train loss=0.09073109812972921(Epoch= 2821)\n",
      "CV loss=0.1508273401194598(Epoch= 2822)\n",
      "train loss=0.09070409067142503(Epoch= 2822)\n",
      "CV loss=0.1505687685927508(Epoch= 2823)\n",
      "train loss=0.09068224689560732(Epoch= 2823)\n",
      "CV loss=0.15064591533859978(Epoch= 2824)\n",
      "train loss=0.09063587259626218(Epoch= 2824)\n",
      "CV loss=0.15067868922296984(Epoch= 2825)\n",
      "train loss=0.09060108092378205(Epoch= 2825)\n",
      "CV loss=0.1503945027561991(Epoch= 2826)\n",
      "train loss=0.09056443195595887(Epoch= 2826)\n",
      "CV loss=0.15050930948685065(Epoch= 2827)\n",
      "train loss=0.0905255112074794(Epoch= 2827)\n",
      "CV loss=0.15048908109406794(Epoch= 2828)\n",
      "train loss=0.09049372873126145(Epoch= 2828)\n",
      "CV loss=0.15036592376466076(Epoch= 2829)\n",
      "train loss=0.09047032818236186(Epoch= 2829)\n",
      "CV loss=0.15044882544031013(Epoch= 2830)\n",
      "train loss=0.09043079341974773(Epoch= 2830)\n",
      "CV loss=0.15044337102449795(Epoch= 2831)\n",
      "train loss=0.09039886914256035(Epoch= 2831)\n",
      "CV loss=0.1502573859344139(Epoch= 2832)\n",
      "train loss=0.09035794517897622(Epoch= 2832)\n",
      "CV loss=0.15036052589043675(Epoch= 2833)\n",
      "train loss=0.09033421580101517(Epoch= 2833)\n",
      "CV loss=0.15016690665242077(Epoch= 2834)\n",
      "train loss=0.09029698397186922(Epoch= 2834)\n",
      "CV loss=0.1502840717159818(Epoch= 2835)\n",
      "train loss=0.09025941528290908(Epoch= 2835)\n",
      "CV loss=0.15023839561492192(Epoch= 2836)\n",
      "train loss=0.09022371083355478(Epoch= 2836)\n",
      "CV loss=0.1501268209792252(Epoch= 2837)\n",
      "train loss=0.09019046406090288(Epoch= 2837)\n",
      "CV loss=0.15014449350825176(Epoch= 2838)\n",
      "train loss=0.09016173955035003(Epoch= 2838)\n",
      "CV loss=0.15009518194404672(Epoch= 2839)\n",
      "train loss=0.09012160509495999(Epoch= 2839)\n",
      "CV loss=0.15000664912983624(Epoch= 2840)\n",
      "train loss=0.09009470269645424(Epoch= 2840)\n",
      "CV loss=0.149948409650436(Epoch= 2841)\n",
      "train loss=0.09006024287743224(Epoch= 2841)\n",
      "CV loss=0.1500693815557538(Epoch= 2842)\n",
      "train loss=0.0900277701942496(Epoch= 2842)\n",
      "CV loss=0.14990719373615835(Epoch= 2843)\n",
      "train loss=0.08999860903313697(Epoch= 2843)\n",
      "CV loss=0.149957829552116(Epoch= 2844)\n",
      "train loss=0.0899549830867623(Epoch= 2844)\n",
      "CV loss=0.14972286808626106(Epoch= 2845)\n",
      "train loss=0.0899328439893612(Epoch= 2845)\n",
      "CV loss=0.14969064045903874(Epoch= 2846)\n",
      "train loss=0.08988979813991099(Epoch= 2846)\n",
      "CV loss=0.14968766846052878(Epoch= 2847)\n",
      "train loss=0.08986110594633837(Epoch= 2847)\n",
      "CV loss=0.14963266156304003(Epoch= 2848)\n",
      "train loss=0.08982086714187773(Epoch= 2848)\n",
      "CV loss=0.14960539776250564(Epoch= 2849)\n",
      "train loss=0.08979260001785307(Epoch= 2849)\n",
      "CV loss=0.14983174438286606(Epoch= 2850)\n",
      "train loss=0.08976603444164895(Epoch= 2850)\n",
      "CV loss=0.14965659142843135(Epoch= 2851)\n",
      "train loss=0.0897204505702408(Epoch= 2851)\n",
      "CV loss=0.14959584111927715(Epoch= 2852)\n",
      "train loss=0.08968712107221401(Epoch= 2852)\n",
      "CV loss=0.1494713001465296(Epoch= 2853)\n",
      "train loss=0.08966411645368381(Epoch= 2853)\n",
      "CV loss=0.1494994378452253(Epoch= 2854)\n",
      "train loss=0.08963636721112336(Epoch= 2854)\n",
      "CV loss=0.14943376473172232(Epoch= 2855)\n",
      "train loss=0.08959498665137668(Epoch= 2855)\n",
      "CV loss=0.14955137180976313(Epoch= 2856)\n",
      "train loss=0.08957022917944073(Epoch= 2856)\n",
      "CV loss=0.14925648305706385(Epoch= 2857)\n",
      "train loss=0.089527590688455(Epoch= 2857)\n",
      "CV loss=0.14938420339008496(Epoch= 2858)\n",
      "train loss=0.08949271553759833(Epoch= 2858)\n",
      "CV loss=0.14934126509150047(Epoch= 2859)\n",
      "train loss=0.08945840635660242(Epoch= 2859)\n",
      "CV loss=0.14933593038332638(Epoch= 2860)\n",
      "train loss=0.08942321646251947(Epoch= 2860)\n",
      "CV loss=0.14925920295209072(Epoch= 2861)\n",
      "train loss=0.08940125744025683(Epoch= 2861)\n",
      "CV loss=0.14933302737524007(Epoch= 2862)\n",
      "train loss=0.08936042848958725(Epoch= 2862)\n",
      "CV loss=0.1493594602517665(Epoch= 2863)\n",
      "train loss=0.08932753317044649(Epoch= 2863)\n",
      "CV loss=0.1493028731342786(Epoch= 2864)\n",
      "train loss=0.08929317951451238(Epoch= 2864)\n",
      "CV loss=0.14921923236744578(Epoch= 2865)\n",
      "train loss=0.08926353779596552(Epoch= 2865)\n",
      "CV loss=0.14919718282881633(Epoch= 2866)\n",
      "train loss=0.08922968599356493(Epoch= 2866)\n",
      "CV loss=0.14898500027062644(Epoch= 2867)\n",
      "train loss=0.08919406889068543(Epoch= 2867)\n",
      "CV loss=0.14902339915318222(Epoch= 2868)\n",
      "train loss=0.08916266942841321(Epoch= 2868)\n",
      "CV loss=0.14918109346720032(Epoch= 2869)\n",
      "train loss=0.08913772089330646(Epoch= 2869)\n",
      "CV loss=0.14890645708210895(Epoch= 2870)\n",
      "train loss=0.08909759261110668(Epoch= 2870)\n",
      "CV loss=0.14899594438926314(Epoch= 2871)\n",
      "train loss=0.08907739934127562(Epoch= 2871)\n",
      "CV loss=0.14911122607490895(Epoch= 2872)\n",
      "train loss=0.08903758976476717(Epoch= 2872)\n",
      "CV loss=0.1489335158749499(Epoch= 2873)\n",
      "train loss=0.08899697847405393(Epoch= 2873)\n",
      "CV loss=0.14884877094391788(Epoch= 2874)\n",
      "train loss=0.08896518510908696(Epoch= 2874)\n",
      "CV loss=0.1488417677994045(Epoch= 2875)\n",
      "train loss=0.08893514185934005(Epoch= 2875)\n",
      "CV loss=0.14874302423702174(Epoch= 2876)\n",
      "train loss=0.0889119824418447(Epoch= 2876)\n",
      "CV loss=0.1486871346160567(Epoch= 2877)\n",
      "train loss=0.088866480584395(Epoch= 2877)\n",
      "CV loss=0.1486436476986275(Epoch= 2878)\n",
      "train loss=0.08883376248395593(Epoch= 2878)\n",
      "CV loss=0.14871026143045835(Epoch= 2879)\n",
      "train loss=0.08880004253903477(Epoch= 2879)\n",
      "CV loss=0.14860273332043272(Epoch= 2880)\n",
      "train loss=0.08877294427068201(Epoch= 2880)\n",
      "CV loss=0.14870396255499946(Epoch= 2881)\n",
      "train loss=0.08874398802088698(Epoch= 2881)\n",
      "CV loss=0.1486859352631158(Epoch= 2882)\n",
      "train loss=0.08870670929080267(Epoch= 2882)\n",
      "CV loss=0.14858342164324673(Epoch= 2883)\n",
      "train loss=0.08867417147463645(Epoch= 2883)\n",
      "CV loss=0.14855024726802202(Epoch= 2884)\n",
      "train loss=0.08864474572774926(Epoch= 2884)\n",
      "CV loss=0.14859773816970204(Epoch= 2885)\n",
      "train loss=0.08861825180251932(Epoch= 2885)\n",
      "CV loss=0.14840574072318122(Epoch= 2886)\n",
      "train loss=0.08857774832538585(Epoch= 2886)\n",
      "CV loss=0.14855866373859794(Epoch= 2887)\n",
      "train loss=0.08854800551346853(Epoch= 2887)\n",
      "CV loss=0.1484634920831952(Epoch= 2888)\n",
      "train loss=0.08851365603001789(Epoch= 2888)\n",
      "CV loss=0.14825288220164645(Epoch= 2889)\n",
      "train loss=0.08848371442591572(Epoch= 2889)\n",
      "CV loss=0.14840407640798264(Epoch= 2890)\n",
      "train loss=0.08844974852425846(Epoch= 2890)\n",
      "CV loss=0.148070574193713(Epoch= 2891)\n",
      "train loss=0.08842021091967908(Epoch= 2891)\n",
      "CV loss=0.14833423358483488(Epoch= 2892)\n",
      "train loss=0.08838734979762258(Epoch= 2892)\n",
      "CV loss=0.1482596439045622(Epoch= 2893)\n",
      "train loss=0.0883508853340436(Epoch= 2893)\n",
      "CV loss=0.1481028336504567(Epoch= 2894)\n",
      "train loss=0.08831969459143732(Epoch= 2894)\n",
      "CV loss=0.1481325481496174(Epoch= 2895)\n",
      "train loss=0.08830093538694046(Epoch= 2895)\n",
      "CV loss=0.14823674005385185(Epoch= 2896)\n",
      "train loss=0.0882601763519226(Epoch= 2896)\n",
      "CV loss=0.14793939787260907(Epoch= 2897)\n",
      "train loss=0.08822734962243808(Epoch= 2897)\n",
      "CV loss=0.14784981907558098(Epoch= 2898)\n",
      "train loss=0.08819358267839807(Epoch= 2898)\n",
      "CV loss=0.1480107650806108(Epoch= 2899)\n",
      "train loss=0.08816616290939565(Epoch= 2899)\n",
      "CV loss=0.14799967982094414(Epoch= 2900)\n",
      "train loss=0.08812920003658659(Epoch= 2900)\n",
      "CV loss=0.1479792233406284(Epoch= 2901)\n",
      "train loss=0.08809863285116247(Epoch= 2901)\n",
      "CV loss=0.1479440404466305(Epoch= 2902)\n",
      "train loss=0.08806587745447017(Epoch= 2902)\n",
      "CV loss=0.1476504168910332(Epoch= 2903)\n",
      "train loss=0.08804213322968796(Epoch= 2903)\n",
      "CV loss=0.14778183099175374(Epoch= 2904)\n",
      "train loss=0.08800448755272038(Epoch= 2904)\n",
      "CV loss=0.14784164466643204(Epoch= 2905)\n",
      "train loss=0.08796499158343522(Epoch= 2905)\n",
      "CV loss=0.14785350125156382(Epoch= 2906)\n",
      "train loss=0.0879386773609182(Epoch= 2906)\n",
      "CV loss=0.1476791405870589(Epoch= 2907)\n",
      "train loss=0.08790719863108598(Epoch= 2907)\n",
      "CV loss=0.14767257481538562(Epoch= 2908)\n",
      "train loss=0.08787397409915526(Epoch= 2908)\n",
      "CV loss=0.14759739874819694(Epoch= 2909)\n",
      "train loss=0.08784062340211905(Epoch= 2909)\n",
      "CV loss=0.1475819956039906(Epoch= 2910)\n",
      "train loss=0.0878117594250513(Epoch= 2910)\n",
      "CV loss=0.14745072176279944(Epoch= 2911)\n",
      "train loss=0.08777935726816823(Epoch= 2911)\n",
      "CV loss=0.14767722666490418(Epoch= 2912)\n",
      "train loss=0.08774464154339526(Epoch= 2912)\n",
      "CV loss=0.1474377890009254(Epoch= 2913)\n",
      "train loss=0.08772545050169807(Epoch= 2913)\n",
      "CV loss=0.14762325869810755(Epoch= 2914)\n",
      "train loss=0.08769124188965839(Epoch= 2914)\n",
      "CV loss=0.14750109228600394(Epoch= 2915)\n",
      "train loss=0.08764923156256862(Epoch= 2915)\n",
      "CV loss=0.1474659846265618(Epoch= 2916)\n",
      "train loss=0.08762460838633032(Epoch= 2916)\n",
      "CV loss=0.14730789581757242(Epoch= 2917)\n",
      "train loss=0.087588849577543(Epoch= 2917)\n",
      "CV loss=0.14725815715571722(Epoch= 2918)\n",
      "train loss=0.08755645603520937(Epoch= 2918)\n",
      "CV loss=0.14745370733195956(Epoch= 2919)\n",
      "train loss=0.08752889883051916(Epoch= 2919)\n",
      "CV loss=0.14744973189201271(Epoch= 2920)\n",
      "train loss=0.08751585486081374(Epoch= 2920)\n",
      "CV loss=0.14719109382816015(Epoch= 2921)\n",
      "train loss=0.08746375241524114(Epoch= 2921)\n",
      "CV loss=0.14710500342731025(Epoch= 2922)\n",
      "train loss=0.08744801841595763(Epoch= 2922)\n",
      "CV loss=0.14730823331771264(Epoch= 2923)\n",
      "train loss=0.08740180195604516(Epoch= 2923)\n",
      "CV loss=0.1471706754645557(Epoch= 2924)\n",
      "train loss=0.08737380804269847(Epoch= 2924)\n",
      "CV loss=0.1472506509477144(Epoch= 2925)\n",
      "train loss=0.08733875425839403(Epoch= 2925)\n",
      "CV loss=0.14709264384182652(Epoch= 2926)\n",
      "train loss=0.08731010782430483(Epoch= 2926)\n",
      "CV loss=0.14715841947159825(Epoch= 2927)\n",
      "train loss=0.0872828743067168(Epoch= 2927)\n",
      "CV loss=0.1470717058094384(Epoch= 2928)\n",
      "train loss=0.08724355363590922(Epoch= 2928)\n",
      "CV loss=0.14722393080660484(Epoch= 2929)\n",
      "train loss=0.08722881151350717(Epoch= 2929)\n",
      "CV loss=0.14707201085198157(Epoch= 2930)\n",
      "train loss=0.08718212139195176(Epoch= 2930)\n",
      "CV loss=0.14687167580037597(Epoch= 2931)\n",
      "train loss=0.08715607149027338(Epoch= 2931)\n",
      "CV loss=0.14689824965344483(Epoch= 2932)\n",
      "train loss=0.08712080593706112(Epoch= 2932)\n",
      "CV loss=0.14689239912161717(Epoch= 2933)\n",
      "train loss=0.0870843683050312(Epoch= 2933)\n",
      "CV loss=0.1469553011259991(Epoch= 2934)\n",
      "train loss=0.08706076487170912(Epoch= 2934)\n",
      "CV loss=0.14669749566246235(Epoch= 2935)\n",
      "train loss=0.08703304908297063(Epoch= 2935)\n",
      "CV loss=0.1466654761537017(Epoch= 2936)\n",
      "train loss=0.08699663079420757(Epoch= 2936)\n",
      "CV loss=0.14680965099244236(Epoch= 2937)\n",
      "train loss=0.08697142886789111(Epoch= 2937)\n",
      "CV loss=0.14666821946355674(Epoch= 2938)\n",
      "train loss=0.0869310395010049(Epoch= 2938)\n",
      "CV loss=0.14660909629258853(Epoch= 2939)\n",
      "train loss=0.08690150066200925(Epoch= 2939)\n",
      "CV loss=0.1466783419307356(Epoch= 2940)\n",
      "train loss=0.08686785434042868(Epoch= 2940)\n",
      "CV loss=0.14675026382790912(Epoch= 2941)\n",
      "train loss=0.08684881197837654(Epoch= 2941)\n",
      "CV loss=0.14673649479811537(Epoch= 2942)\n",
      "train loss=0.08681575626305546(Epoch= 2942)\n",
      "CV loss=0.14654690258201222(Epoch= 2943)\n",
      "train loss=0.08677924804088945(Epoch= 2943)\n",
      "CV loss=0.14668975133597423(Epoch= 2944)\n",
      "train loss=0.08674775880915207(Epoch= 2944)\n",
      "CV loss=0.1464769380736661(Epoch= 2945)\n",
      "train loss=0.08672137791780013(Epoch= 2945)\n",
      "CV loss=0.14651537278351978(Epoch= 2946)\n",
      "train loss=0.08668401436905099(Epoch= 2946)\n",
      "CV loss=0.14658393730293307(Epoch= 2947)\n",
      "train loss=0.08665913655228845(Epoch= 2947)\n",
      "CV loss=0.14643229432596175(Epoch= 2948)\n",
      "train loss=0.08662371156428834(Epoch= 2948)\n",
      "CV loss=0.1465013651758573(Epoch= 2949)\n",
      "train loss=0.08659625126619414(Epoch= 2949)\n",
      "CV loss=0.14642785520427354(Epoch= 2950)\n",
      "train loss=0.08656639658809248(Epoch= 2950)\n",
      "CV loss=0.14651458048778493(Epoch= 2951)\n",
      "train loss=0.08653759323362951(Epoch= 2951)\n",
      "CV loss=0.14622136870606905(Epoch= 2952)\n",
      "train loss=0.08650014193774716(Epoch= 2952)\n",
      "CV loss=0.14627285740602955(Epoch= 2953)\n",
      "train loss=0.08647092931888331(Epoch= 2953)\n",
      "CV loss=0.14615887868468974(Epoch= 2954)\n",
      "train loss=0.08643883462421881(Epoch= 2954)\n",
      "CV loss=0.14609746379272978(Epoch= 2955)\n",
      "train loss=0.08640823209758515(Epoch= 2955)\n",
      "CV loss=0.1461500184542491(Epoch= 2956)\n",
      "train loss=0.08637985525091603(Epoch= 2956)\n",
      "CV loss=0.14608224163538985(Epoch= 2957)\n",
      "train loss=0.0863486153958619(Epoch= 2957)\n",
      "CV loss=0.14593221405978246(Epoch= 2958)\n",
      "train loss=0.0863216875954329(Epoch= 2958)\n",
      "CV loss=0.14598523627290622(Epoch= 2959)\n",
      "train loss=0.08628887506687839(Epoch= 2959)\n",
      "CV loss=0.14603482517112099(Epoch= 2960)\n",
      "train loss=0.08625480374486136(Epoch= 2960)\n",
      "CV loss=0.14606412568398114(Epoch= 2961)\n",
      "train loss=0.08622803083019487(Epoch= 2961)\n",
      "CV loss=0.1460128262034685(Epoch= 2962)\n",
      "train loss=0.08619461583998458(Epoch= 2962)\n",
      "CV loss=0.14583298317717994(Epoch= 2963)\n",
      "train loss=0.08616986466818614(Epoch= 2963)\n",
      "CV loss=0.14584392209905755(Epoch= 2964)\n",
      "train loss=0.08613349693447012(Epoch= 2964)\n",
      "CV loss=0.14603532869113203(Epoch= 2965)\n",
      "train loss=0.08612000802522939(Epoch= 2965)\n",
      "CV loss=0.14595153596265983(Epoch= 2966)\n",
      "train loss=0.0860838103158811(Epoch= 2966)\n",
      "CV loss=0.14579536755447092(Epoch= 2967)\n",
      "train loss=0.08604491333894189(Epoch= 2967)\n",
      "CV loss=0.14576978659928763(Epoch= 2968)\n",
      "train loss=0.08601268145381688(Epoch= 2968)\n",
      "CV loss=0.14579050393219734(Epoch= 2969)\n",
      "train loss=0.08598372926577073(Epoch= 2969)\n",
      "CV loss=0.14563842050782433(Epoch= 2970)\n",
      "train loss=0.08597063026521745(Epoch= 2970)\n",
      "CV loss=0.1457996991100164(Epoch= 2971)\n",
      "train loss=0.08592691282104548(Epoch= 2971)\n",
      "CV loss=0.14563942444617306(Epoch= 2972)\n",
      "train loss=0.08589738732283927(Epoch= 2972)\n",
      "CV loss=0.14563216340345903(Epoch= 2973)\n",
      "train loss=0.08587326037009489(Epoch= 2973)\n",
      "CV loss=0.14567065790529538(Epoch= 2974)\n",
      "train loss=0.08585065122946642(Epoch= 2974)\n",
      "CV loss=0.14551640639363173(Epoch= 2975)\n",
      "train loss=0.08580308440107283(Epoch= 2975)\n",
      "CV loss=0.14559516883365003(Epoch= 2976)\n",
      "train loss=0.08578218863400464(Epoch= 2976)\n",
      "CV loss=0.14544862187513116(Epoch= 2977)\n",
      "train loss=0.08574586236737503(Epoch= 2977)\n",
      "CV loss=0.14554419340596417(Epoch= 2978)\n",
      "train loss=0.08572373929535645(Epoch= 2978)\n",
      "CV loss=0.14543670865922406(Epoch= 2979)\n",
      "train loss=0.08569451498846427(Epoch= 2979)\n",
      "CV loss=0.14537164485507412(Epoch= 2980)\n",
      "train loss=0.08565289061024943(Epoch= 2980)\n",
      "CV loss=0.1453077308415379(Epoch= 2981)\n",
      "train loss=0.0856240724175146(Epoch= 2981)\n",
      "CV loss=0.1452323817625331(Epoch= 2982)\n",
      "train loss=0.08559842407874219(Epoch= 2982)\n",
      "CV loss=0.1455275257632978(Epoch= 2983)\n",
      "train loss=0.08556996480320801(Epoch= 2983)\n",
      "CV loss=0.14527057499491777(Epoch= 2984)\n",
      "train loss=0.08553689134420665(Epoch= 2984)\n",
      "CV loss=0.1452805891660302(Epoch= 2985)\n",
      "train loss=0.08551106039317113(Epoch= 2985)\n",
      "CV loss=0.14509669299164896(Epoch= 2986)\n",
      "train loss=0.08547637160252931(Epoch= 2986)\n",
      "CV loss=0.1453825333278791(Epoch= 2987)\n",
      "train loss=0.08545824596973835(Epoch= 2987)\n",
      "CV loss=0.14516029196884486(Epoch= 2988)\n",
      "train loss=0.08541548889351816(Epoch= 2988)\n",
      "CV loss=0.14510217573445783(Epoch= 2989)\n",
      "train loss=0.08540488881331405(Epoch= 2989)\n",
      "CV loss=0.1451122599486507(Epoch= 2990)\n",
      "train loss=0.08535506173409081(Epoch= 2990)\n",
      "CV loss=0.14503456158794475(Epoch= 2991)\n",
      "train loss=0.08532961501942117(Epoch= 2991)\n",
      "CV loss=0.14497817608336777(Epoch= 2992)\n",
      "train loss=0.0853002293987794(Epoch= 2992)\n",
      "CV loss=0.14518342613500393(Epoch= 2993)\n",
      "train loss=0.08527028250885021(Epoch= 2993)\n",
      "CV loss=0.1449166801054768(Epoch= 2994)\n",
      "train loss=0.08523802507412077(Epoch= 2994)\n",
      "CV loss=0.14489468349453222(Epoch= 2995)\n",
      "train loss=0.08521319258607069(Epoch= 2995)\n",
      "CV loss=0.14489095687835657(Epoch= 2996)\n",
      "train loss=0.0851815423291108(Epoch= 2996)\n",
      "CV loss=0.14491463274356953(Epoch= 2997)\n",
      "train loss=0.08514978379406546(Epoch= 2997)\n",
      "CV loss=0.1448485939835214(Epoch= 2998)\n",
      "train loss=0.08511725467093464(Epoch= 2998)\n",
      "CV loss=0.14471881600353512(Epoch= 2999)\n",
      "train loss=0.08509228842772107(Epoch= 2999)\n",
      "CV loss=0.14473891031286462(Epoch= 3000)\n",
      "train loss=0.08506191902466843(Epoch= 3000)\n",
      "CV loss=0.14471563237305618(Epoch= 3001)\n",
      "train loss=0.08503442615686264(Epoch= 3001)\n",
      "CV loss=0.14487036832281439(Epoch= 3002)\n",
      "train loss=0.08501283950810772(Epoch= 3002)\n",
      "CV loss=0.1446764060859312(Epoch= 3003)\n",
      "train loss=0.08498537056406351(Epoch= 3003)\n",
      "CV loss=0.14470321453325885(Epoch= 3004)\n",
      "train loss=0.0849442524870204(Epoch= 3004)\n",
      "CV loss=0.14470461997475592(Epoch= 3005)\n",
      "train loss=0.08491608143026205(Epoch= 3005)\n",
      "CV loss=0.14467080613433753(Epoch= 3006)\n",
      "train loss=0.08488686846593438(Epoch= 3006)\n",
      "CV loss=0.14451993904501434(Epoch= 3007)\n",
      "train loss=0.08485549757155549(Epoch= 3007)\n",
      "CV loss=0.14443901759026884(Epoch= 3008)\n",
      "train loss=0.08483226929359641(Epoch= 3008)\n",
      "CV loss=0.1446443606909541(Epoch= 3009)\n",
      "train loss=0.08479792584267776(Epoch= 3009)\n",
      "CV loss=0.14441443347727614(Epoch= 3010)\n",
      "train loss=0.08476830632748618(Epoch= 3010)\n",
      "CV loss=0.1443714949416945(Epoch= 3011)\n",
      "train loss=0.08474660263588156(Epoch= 3011)\n",
      "CV loss=0.14439945272582(Epoch= 3012)\n",
      "train loss=0.08472063452956834(Epoch= 3012)\n",
      "CV loss=0.14437879890588315(Epoch= 3013)\n",
      "train loss=0.08467875746553287(Epoch= 3013)\n",
      "CV loss=0.1444037011556646(Epoch= 3014)\n",
      "train loss=0.08465428544892205(Epoch= 3014)\n",
      "CV loss=0.14430407035945964(Epoch= 3015)\n",
      "train loss=0.08462993610363817(Epoch= 3015)\n",
      "CV loss=0.1443084752973475(Epoch= 3016)\n",
      "train loss=0.08459613027363476(Epoch= 3016)\n",
      "CV loss=0.14434463532159553(Epoch= 3017)\n",
      "train loss=0.08456521032935374(Epoch= 3017)\n",
      "CV loss=0.14416274161088583(Epoch= 3018)\n",
      "train loss=0.08454232514324572(Epoch= 3018)\n",
      "CV loss=0.1442891041144319(Epoch= 3019)\n",
      "train loss=0.08450933985043137(Epoch= 3019)\n",
      "CV loss=0.14408735687678728(Epoch= 3020)\n",
      "train loss=0.08448139621768513(Epoch= 3020)\n",
      "CV loss=0.14415853705618062(Epoch= 3021)\n",
      "train loss=0.08445400729307452(Epoch= 3021)\n",
      "CV loss=0.14412452817988813(Epoch= 3022)\n",
      "train loss=0.08441875183398295(Epoch= 3022)\n",
      "CV loss=0.14411379541797986(Epoch= 3023)\n",
      "train loss=0.08439044249769503(Epoch= 3023)\n",
      "CV loss=0.14406220490967697(Epoch= 3024)\n",
      "train loss=0.08436423411649238(Epoch= 3024)\n",
      "CV loss=0.14389836001143322(Epoch= 3025)\n",
      "train loss=0.08434059094873973(Epoch= 3025)\n",
      "CV loss=0.14396452507934168(Epoch= 3026)\n",
      "train loss=0.08430654574909181(Epoch= 3026)\n",
      "CV loss=0.14404288166883256(Epoch= 3027)\n",
      "train loss=0.08427713725682397(Epoch= 3027)\n",
      "CV loss=0.14395336158248473(Epoch= 3028)\n",
      "train loss=0.08424765831740047(Epoch= 3028)\n",
      "CV loss=0.14389450198704193(Epoch= 3029)\n",
      "train loss=0.08422145759933247(Epoch= 3029)\n",
      "CV loss=0.1438732375366227(Epoch= 3030)\n",
      "train loss=0.08420083008901795(Epoch= 3030)\n",
      "CV loss=0.14371530990098697(Epoch= 3031)\n",
      "train loss=0.08417042526115454(Epoch= 3031)\n",
      "CV loss=0.14369725881678155(Epoch= 3032)\n",
      "train loss=0.08414140638981504(Epoch= 3032)\n",
      "CV loss=0.14389739803498852(Epoch= 3033)\n",
      "train loss=0.08410902967414466(Epoch= 3033)\n",
      "CV loss=0.1438074302549494(Epoch= 3034)\n",
      "train loss=0.08407503259930456(Epoch= 3034)\n",
      "CV loss=0.14390021575914985(Epoch= 3035)\n",
      "train loss=0.08405441707264621(Epoch= 3035)\n",
      "CV loss=0.14366442728563422(Epoch= 3036)\n",
      "train loss=0.08402343084111283(Epoch= 3036)\n",
      "CV loss=0.14371307378157835(Epoch= 3037)\n",
      "train loss=0.08399544566509304(Epoch= 3037)\n",
      "CV loss=0.1436636750673928(Epoch= 3038)\n",
      "train loss=0.0839632310574342(Epoch= 3038)\n",
      "CV loss=0.1438144411865921(Epoch= 3039)\n",
      "train loss=0.08394511475089823(Epoch= 3039)\n",
      "CV loss=0.14354428324664825(Epoch= 3040)\n",
      "train loss=0.08390849144293673(Epoch= 3040)\n",
      "CV loss=0.14346234618552461(Epoch= 3041)\n",
      "train loss=0.08387900014628871(Epoch= 3041)\n",
      "CV loss=0.1434207912012608(Epoch= 3042)\n",
      "train loss=0.08387159198636367(Epoch= 3042)\n",
      "CV loss=0.14357901816257446(Epoch= 3043)\n",
      "train loss=0.0838237949908062(Epoch= 3043)\n",
      "CV loss=0.14351207224524104(Epoch= 3044)\n",
      "train loss=0.08379487168820184(Epoch= 3044)\n",
      "CV loss=0.1435057394956234(Epoch= 3045)\n",
      "train loss=0.08376316222949284(Epoch= 3045)\n",
      "CV loss=0.14333571804836856(Epoch= 3046)\n",
      "train loss=0.08373597424312867(Epoch= 3046)\n",
      "CV loss=0.14335547738533244(Epoch= 3047)\n",
      "train loss=0.08370622765432867(Epoch= 3047)\n",
      "CV loss=0.14330466886940568(Epoch= 3048)\n",
      "train loss=0.08367741421302266(Epoch= 3048)\n",
      "CV loss=0.14345974849845716(Epoch= 3049)\n",
      "train loss=0.08365781594248398(Epoch= 3049)\n",
      "CV loss=0.14320229566869458(Epoch= 3050)\n",
      "train loss=0.08363536407994476(Epoch= 3050)\n",
      "CV loss=0.14354923096364441(Epoch= 3051)\n",
      "train loss=0.08361016187587975(Epoch= 3051)\n",
      "CV loss=0.14326031668257178(Epoch= 3052)\n",
      "train loss=0.08356898969009972(Epoch= 3052)\n",
      "CV loss=0.14326226222355049(Epoch= 3053)\n",
      "train loss=0.08353875875752531(Epoch= 3053)\n",
      "CV loss=0.14337003250635694(Epoch= 3054)\n",
      "train loss=0.0835220018211094(Epoch= 3054)\n",
      "CV loss=0.14304589221363517(Epoch= 3055)\n",
      "train loss=0.08348961357757596(Epoch= 3055)\n",
      "CV loss=0.14297835588485625(Epoch= 3056)\n",
      "train loss=0.08346235090384725(Epoch= 3056)\n",
      "CV loss=0.14323563624458835(Epoch= 3057)\n",
      "train loss=0.0834309881074163(Epoch= 3057)\n",
      "CV loss=0.1431963896094079(Epoch= 3058)\n",
      "train loss=0.08340332377186149(Epoch= 3058)\n",
      "CV loss=0.14316577806537784(Epoch= 3059)\n",
      "train loss=0.08337196439534374(Epoch= 3059)\n",
      "CV loss=0.14296443001853495(Epoch= 3060)\n",
      "train loss=0.08334339860506676(Epoch= 3060)\n",
      "CV loss=0.14297933019345993(Epoch= 3061)\n",
      "train loss=0.08332159879682728(Epoch= 3061)\n",
      "CV loss=0.14294912051183004(Epoch= 3062)\n",
      "train loss=0.08328550176840133(Epoch= 3062)\n",
      "CV loss=0.1431128715406804(Epoch= 3063)\n",
      "train loss=0.08326485730883042(Epoch= 3063)\n",
      "CV loss=0.14282528427362612(Epoch= 3064)\n",
      "train loss=0.08322884763939092(Epoch= 3064)\n",
      "CV loss=0.14284364391786414(Epoch= 3065)\n",
      "train loss=0.08320418862494272(Epoch= 3065)\n",
      "CV loss=0.14278870926836434(Epoch= 3066)\n",
      "train loss=0.08318764701536405(Epoch= 3066)\n",
      "CV loss=0.14291964972509735(Epoch= 3067)\n",
      "train loss=0.08315658602805676(Epoch= 3067)\n",
      "CV loss=0.14285113240594538(Epoch= 3068)\n",
      "train loss=0.0831228203819398(Epoch= 3068)\n",
      "CV loss=0.14278833121187035(Epoch= 3069)\n",
      "train loss=0.08309320859112951(Epoch= 3069)\n",
      "CV loss=0.14262686486333293(Epoch= 3070)\n",
      "train loss=0.0830631269965672(Epoch= 3070)\n",
      "CV loss=0.14256261086816563(Epoch= 3071)\n",
      "train loss=0.08304348015464207(Epoch= 3071)\n",
      "CV loss=0.14265568096430203(Epoch= 3072)\n",
      "train loss=0.08300802602889577(Epoch= 3072)\n",
      "CV loss=0.1426184275262612(Epoch= 3073)\n",
      "train loss=0.08298109358832227(Epoch= 3073)\n",
      "CV loss=0.14246938155420924(Epoch= 3074)\n",
      "train loss=0.08295378532291667(Epoch= 3074)\n",
      "CV loss=0.1424707470223303(Epoch= 3075)\n",
      "train loss=0.08293068582580182(Epoch= 3075)\n",
      "CV loss=0.1425013960616236(Epoch= 3076)\n",
      "train loss=0.08289822783006512(Epoch= 3076)\n",
      "CV loss=0.14251091974435104(Epoch= 3077)\n",
      "train loss=0.0828691426496772(Epoch= 3077)\n",
      "CV loss=0.1425614033060834(Epoch= 3078)\n",
      "train loss=0.08284276450141874(Epoch= 3078)\n",
      "CV loss=0.14247216465618864(Epoch= 3079)\n",
      "train loss=0.08281457184541301(Epoch= 3079)\n",
      "CV loss=0.1424182283759961(Epoch= 3080)\n",
      "train loss=0.08278578271394901(Epoch= 3080)\n",
      "CV loss=0.14238810150521997(Epoch= 3081)\n",
      "train loss=0.08276065187561439(Epoch= 3081)\n",
      "CV loss=0.14231589354233604(Epoch= 3082)\n",
      "train loss=0.0827316429293499(Epoch= 3082)\n",
      "CV loss=0.1424414111219625(Epoch= 3083)\n",
      "train loss=0.08271120728882911(Epoch= 3083)\n",
      "CV loss=0.14235999027686724(Epoch= 3084)\n",
      "train loss=0.08267717049281387(Epoch= 3084)\n",
      "CV loss=0.14226159866463803(Epoch= 3085)\n",
      "train loss=0.08265434007744724(Epoch= 3085)\n",
      "CV loss=0.14221064174048398(Epoch= 3086)\n",
      "train loss=0.0826286307156994(Epoch= 3086)\n",
      "CV loss=0.14229692148654347(Epoch= 3087)\n",
      "train loss=0.08259687614756511(Epoch= 3087)\n",
      "CV loss=0.14209681444388148(Epoch= 3088)\n",
      "train loss=0.08257051720242126(Epoch= 3088)\n",
      "CV loss=0.14218989906389556(Epoch= 3089)\n",
      "train loss=0.08254830475431826(Epoch= 3089)\n",
      "CV loss=0.14221013859355064(Epoch= 3090)\n",
      "train loss=0.08251186382666503(Epoch= 3090)\n",
      "CV loss=0.14218723115405335(Epoch= 3091)\n",
      "train loss=0.08248726078405165(Epoch= 3091)\n",
      "CV loss=0.14210172605397103(Epoch= 3092)\n",
      "train loss=0.08245648769292126(Epoch= 3092)\n",
      "CV loss=0.1420095218983006(Epoch= 3093)\n",
      "train loss=0.08243853742629419(Epoch= 3093)\n",
      "CV loss=0.14203270464231302(Epoch= 3094)\n",
      "train loss=0.08240657250615517(Epoch= 3094)\n",
      "CV loss=0.14207750362290886(Epoch= 3095)\n",
      "train loss=0.0823794662045408(Epoch= 3095)\n",
      "CV loss=0.14189438400048865(Epoch= 3096)\n",
      "train loss=0.08236170588222873(Epoch= 3096)\n",
      "CV loss=0.14192303820376606(Epoch= 3097)\n",
      "train loss=0.08232467168811279(Epoch= 3097)\n",
      "CV loss=0.1419541423615673(Epoch= 3098)\n",
      "train loss=0.08229545550924348(Epoch= 3098)\n",
      "CV loss=0.1419371597723075(Epoch= 3099)\n",
      "train loss=0.08227987074304603(Epoch= 3099)\n",
      "CV loss=0.141768870360826(Epoch= 3100)\n",
      "train loss=0.08224498980505429(Epoch= 3100)\n",
      "CV loss=0.14182118104220415(Epoch= 3101)\n",
      "train loss=0.0822268230215061(Epoch= 3101)\n",
      "CV loss=0.1418016446985031(Epoch= 3102)\n",
      "train loss=0.08220211514913503(Epoch= 3102)\n",
      "CV loss=0.14190671757506662(Epoch= 3103)\n",
      "train loss=0.0821617897554387(Epoch= 3103)\n",
      "CV loss=0.14182165477913494(Epoch= 3104)\n",
      "train loss=0.08213524624118301(Epoch= 3104)\n",
      "CV loss=0.14181385894288082(Epoch= 3105)\n",
      "train loss=0.08211189695599358(Epoch= 3105)\n",
      "CV loss=0.14187269404921235(Epoch= 3106)\n",
      "train loss=0.08208392368382497(Epoch= 3106)\n",
      "CV loss=0.14183324126174812(Epoch= 3107)\n",
      "train loss=0.08205676091613683(Epoch= 3107)\n",
      "CV loss=0.1417475222304092(Epoch= 3108)\n",
      "train loss=0.08202718452673873(Epoch= 3108)\n",
      "CV loss=0.1418475195705241(Epoch= 3109)\n",
      "train loss=0.08202336350676169(Epoch= 3109)\n",
      "CV loss=0.14161668356115187(Epoch= 3110)\n",
      "train loss=0.08198385199689856(Epoch= 3110)\n",
      "CV loss=0.14169062551149061(Epoch= 3111)\n",
      "train loss=0.08194923419014226(Epoch= 3111)\n",
      "CV loss=0.14149227695211108(Epoch= 3112)\n",
      "train loss=0.08191988006716053(Epoch= 3112)\n",
      "CV loss=0.1415430960460748(Epoch= 3113)\n",
      "train loss=0.08190664697679316(Epoch= 3113)\n",
      "CV loss=0.1415646275450727(Epoch= 3114)\n",
      "train loss=0.0818676610507373(Epoch= 3114)\n",
      "CV loss=0.14155154128769404(Epoch= 3115)\n",
      "train loss=0.0818440260134605(Epoch= 3115)\n",
      "CV loss=0.14165001865209587(Epoch= 3116)\n",
      "train loss=0.08181631605161044(Epoch= 3116)\n",
      "CV loss=0.14157249073844697(Epoch= 3117)\n",
      "train loss=0.08179525446758354(Epoch= 3117)\n",
      "CV loss=0.14147310322227136(Epoch= 3118)\n",
      "train loss=0.08175769759479448(Epoch= 3118)\n",
      "CV loss=0.14134688335267223(Epoch= 3119)\n",
      "train loss=0.08172977247690534(Epoch= 3119)\n",
      "CV loss=0.14129876780041012(Epoch= 3120)\n",
      "train loss=0.08170462569124215(Epoch= 3120)\n",
      "CV loss=0.14134590748971354(Epoch= 3121)\n",
      "train loss=0.08167987132571285(Epoch= 3121)\n",
      "CV loss=0.14132330189261938(Epoch= 3122)\n",
      "train loss=0.08166261756765295(Epoch= 3122)\n",
      "CV loss=0.14134622758072646(Epoch= 3123)\n",
      "train loss=0.08162663281758349(Epoch= 3123)\n",
      "CV loss=0.14126749854686127(Epoch= 3124)\n",
      "train loss=0.08159937012300503(Epoch= 3124)\n",
      "CV loss=0.14123454470551194(Epoch= 3125)\n",
      "train loss=0.08157464323087743(Epoch= 3125)\n",
      "CV loss=0.1413663783160355(Epoch= 3126)\n",
      "train loss=0.08155688309079953(Epoch= 3126)\n",
      "CV loss=0.14108484661268175(Epoch= 3127)\n",
      "train loss=0.08152022291570066(Epoch= 3127)\n",
      "CV loss=0.14128262974355857(Epoch= 3128)\n",
      "train loss=0.0814974616992041(Epoch= 3128)\n",
      "CV loss=0.14104223101342375(Epoch= 3129)\n",
      "train loss=0.08146884710291108(Epoch= 3129)\n",
      "CV loss=0.14111537652470435(Epoch= 3130)\n",
      "train loss=0.08144093840690532(Epoch= 3130)\n",
      "CV loss=0.14096936286752978(Epoch= 3131)\n",
      "train loss=0.08141671926920767(Epoch= 3131)\n",
      "CV loss=0.14120845106870278(Epoch= 3132)\n",
      "train loss=0.08140191421925898(Epoch= 3132)\n",
      "CV loss=0.1409460693458866(Epoch= 3133)\n",
      "train loss=0.08135984982161458(Epoch= 3133)\n",
      "CV loss=0.1409344192856489(Epoch= 3134)\n",
      "train loss=0.08133828067947924(Epoch= 3134)\n",
      "CV loss=0.1411701108041459(Epoch= 3135)\n",
      "train loss=0.08131650300396788(Epoch= 3135)\n",
      "CV loss=0.14088639356095614(Epoch= 3136)\n",
      "train loss=0.0812894459689814(Epoch= 3136)\n",
      "CV loss=0.14084701754133366(Epoch= 3137)\n",
      "train loss=0.08125914641958723(Epoch= 3137)\n",
      "CV loss=0.1409901964682538(Epoch= 3138)\n",
      "train loss=0.08123256486168219(Epoch= 3138)\n",
      "CV loss=0.14075420105705805(Epoch= 3139)\n",
      "train loss=0.08120811296104216(Epoch= 3139)\n",
      "CV loss=0.1407775935391474(Epoch= 3140)\n",
      "train loss=0.0811857808288651(Epoch= 3140)\n",
      "CV loss=0.1408850190507469(Epoch= 3141)\n",
      "train loss=0.08115275927299653(Epoch= 3141)\n",
      "CV loss=0.1406819978415712(Epoch= 3142)\n",
      "train loss=0.08112683717172231(Epoch= 3142)\n",
      "CV loss=0.14066794216274894(Epoch= 3143)\n",
      "train loss=0.08110223133659576(Epoch= 3143)\n",
      "CV loss=0.14075245585886348(Epoch= 3144)\n",
      "train loss=0.08107323224409348(Epoch= 3144)\n",
      "CV loss=0.14071357291687864(Epoch= 3145)\n",
      "train loss=0.08104943258692736(Epoch= 3145)\n",
      "CV loss=0.14068911096412268(Epoch= 3146)\n",
      "train loss=0.08102540437307747(Epoch= 3146)\n",
      "CV loss=0.14067225568204528(Epoch= 3147)\n",
      "train loss=0.08099882860135094(Epoch= 3147)\n",
      "CV loss=0.14071843525689137(Epoch= 3148)\n",
      "train loss=0.08097591131516708(Epoch= 3148)\n",
      "CV loss=0.14056911761381735(Epoch= 3149)\n",
      "train loss=0.08094349570132116(Epoch= 3149)\n",
      "CV loss=0.14039709509626253(Epoch= 3150)\n",
      "train loss=0.08092221880706126(Epoch= 3150)\n",
      "CV loss=0.14049644766484945(Epoch= 3151)\n",
      "train loss=0.08089070358400226(Epoch= 3151)\n",
      "CV loss=0.1405123192169978(Epoch= 3152)\n",
      "train loss=0.08086307892688807(Epoch= 3152)\n",
      "CV loss=0.14041966838809677(Epoch= 3153)\n",
      "train loss=0.08085804682103132(Epoch= 3153)\n",
      "CV loss=0.14035234009197833(Epoch= 3154)\n",
      "train loss=0.08081582396729001(Epoch= 3154)\n",
      "CV loss=0.14036618180125943(Epoch= 3155)\n",
      "train loss=0.08078625430530538(Epoch= 3155)\n",
      "CV loss=0.14037189943806658(Epoch= 3156)\n",
      "train loss=0.08076723293912924(Epoch= 3156)\n",
      "CV loss=0.140465123460588(Epoch= 3157)\n",
      "train loss=0.08074938386282582(Epoch= 3157)\n",
      "CV loss=0.14054640945446895(Epoch= 3158)\n",
      "train loss=0.08071444685315851(Epoch= 3158)\n",
      "CV loss=0.14024713233327774(Epoch= 3159)\n",
      "train loss=0.08068476918867258(Epoch= 3159)\n",
      "CV loss=0.1402521078570752(Epoch= 3160)\n",
      "train loss=0.08065810527039365(Epoch= 3160)\n",
      "CV loss=0.1401847435578349(Epoch= 3161)\n",
      "train loss=0.08064116292062595(Epoch= 3161)\n",
      "CV loss=0.14032730202279542(Epoch= 3162)\n",
      "train loss=0.08061150571523884(Epoch= 3162)\n",
      "CV loss=0.14015758607913748(Epoch= 3163)\n",
      "train loss=0.08057878076551488(Epoch= 3163)\n",
      "CV loss=0.14034688874799411(Epoch= 3164)\n",
      "train loss=0.08056131176741056(Epoch= 3164)\n",
      "CV loss=0.1401394945751825(Epoch= 3165)\n",
      "train loss=0.08053061200310481(Epoch= 3165)\n",
      "CV loss=0.1401669993053435(Epoch= 3166)\n",
      "train loss=0.08050711999417406(Epoch= 3166)\n",
      "CV loss=0.1400804584371445(Epoch= 3167)\n",
      "train loss=0.0804774005398248(Epoch= 3167)\n",
      "CV loss=0.14001525664774017(Epoch= 3168)\n",
      "train loss=0.08046309593971017(Epoch= 3168)\n",
      "CV loss=0.13993602394322657(Epoch= 3169)\n",
      "train loss=0.08043871094329656(Epoch= 3169)\n",
      "CV loss=0.13984092134000273(Epoch= 3170)\n",
      "train loss=0.08040688094349371(Epoch= 3170)\n",
      "CV loss=0.1399315976492078(Epoch= 3171)\n",
      "train loss=0.08040996093126626(Epoch= 3171)\n",
      "CV loss=0.13993750084626072(Epoch= 3172)\n",
      "train loss=0.08035593283365641(Epoch= 3172)\n",
      "CV loss=0.13995915307928208(Epoch= 3173)\n",
      "train loss=0.08032511039526039(Epoch= 3173)\n",
      "CV loss=0.13999036517378655(Epoch= 3174)\n",
      "train loss=0.0803024700934702(Epoch= 3174)\n",
      "CV loss=0.13974977262145996(Epoch= 3175)\n",
      "train loss=0.08027939647092691(Epoch= 3175)\n",
      "CV loss=0.13982650275261346(Epoch= 3176)\n",
      "train loss=0.08025097921994774(Epoch= 3176)\n",
      "CV loss=0.1399305924997532(Epoch= 3177)\n",
      "train loss=0.0802295352348869(Epoch= 3177)\n",
      "CV loss=0.13980324084695972(Epoch= 3178)\n",
      "train loss=0.0801977202756405(Epoch= 3178)\n",
      "CV loss=0.1397891801663566(Epoch= 3179)\n",
      "train loss=0.08017924418599633(Epoch= 3179)\n",
      "CV loss=0.13962985837735745(Epoch= 3180)\n",
      "train loss=0.08016045586027447(Epoch= 3180)\n",
      "CV loss=0.13985856928434132(Epoch= 3181)\n",
      "train loss=0.08012763508865489(Epoch= 3181)\n",
      "CV loss=0.13966565722207047(Epoch= 3182)\n",
      "train loss=0.08010056860633964(Epoch= 3182)\n",
      "CV loss=0.13977291079750348(Epoch= 3183)\n",
      "train loss=0.08008089030940647(Epoch= 3183)\n",
      "CV loss=0.1396471916253923(Epoch= 3184)\n",
      "train loss=0.0800497494115164(Epoch= 3184)\n",
      "CV loss=0.1396141932802789(Epoch= 3185)\n",
      "train loss=0.08002611351727389(Epoch= 3185)\n",
      "CV loss=0.13962019828240202(Epoch= 3186)\n",
      "train loss=0.07999787771947667(Epoch= 3186)\n",
      "CV loss=0.13969447868854723(Epoch= 3187)\n",
      "train loss=0.07997276983791937(Epoch= 3187)\n",
      "CV loss=0.13949844829576463(Epoch= 3188)\n",
      "train loss=0.07994767100799588(Epoch= 3188)\n",
      "CV loss=0.13954066622302141(Epoch= 3189)\n",
      "train loss=0.07991908364972014(Epoch= 3189)\n",
      "CV loss=0.13952153491437796(Epoch= 3190)\n",
      "train loss=0.07989414500525309(Epoch= 3190)\n",
      "CV loss=0.13947577098665442(Epoch= 3191)\n",
      "train loss=0.0798694895037668(Epoch= 3191)\n",
      "CV loss=0.1393703793964319(Epoch= 3192)\n",
      "train loss=0.07984756927631671(Epoch= 3192)\n",
      "CV loss=0.13947417813517698(Epoch= 3193)\n",
      "train loss=0.07982252493701167(Epoch= 3193)\n",
      "CV loss=0.13948032806607893(Epoch= 3194)\n",
      "train loss=0.07980504980978702(Epoch= 3194)\n",
      "CV loss=0.13939248550977956(Epoch= 3195)\n",
      "train loss=0.0797734830573541(Epoch= 3195)\n",
      "CV loss=0.1393169752301887(Epoch= 3196)\n",
      "train loss=0.07974727025497007(Epoch= 3196)\n",
      "CV loss=0.13948535225818817(Epoch= 3197)\n",
      "train loss=0.07973491503449834(Epoch= 3197)\n",
      "CV loss=0.13937782936810716(Epoch= 3198)\n",
      "train loss=0.07970557100715797(Epoch= 3198)\n",
      "CV loss=0.13925871411135715(Epoch= 3199)\n",
      "train loss=0.07968659956407904(Epoch= 3199)\n",
      "CV loss=0.13926613699633197(Epoch= 3200)\n",
      "train loss=0.07964801339837883(Epoch= 3200)\n",
      "CV loss=0.13923949302494049(Epoch= 3201)\n",
      "train loss=0.07962120260282755(Epoch= 3201)\n",
      "CV loss=0.13926557188154276(Epoch= 3202)\n",
      "train loss=0.07960432972707521(Epoch= 3202)\n",
      "CV loss=0.1391826192459864(Epoch= 3203)\n",
      "train loss=0.07957290810297192(Epoch= 3203)\n",
      "CV loss=0.1390494289067002(Epoch= 3204)\n",
      "train loss=0.07954635434407871(Epoch= 3204)\n",
      "CV loss=0.13904590034379552(Epoch= 3205)\n",
      "train loss=0.07953022589297232(Epoch= 3205)\n",
      "CV loss=0.13899770392941369(Epoch= 3206)\n",
      "train loss=0.07950580938370165(Epoch= 3206)\n",
      "CV loss=0.1390936951340952(Epoch= 3207)\n",
      "train loss=0.07947186610322805(Epoch= 3207)\n",
      "CV loss=0.13914046410856482(Epoch= 3208)\n",
      "train loss=0.07945965610597734(Epoch= 3208)\n",
      "CV loss=0.13882944611883158(Epoch= 3209)\n",
      "train loss=0.07943499197238395(Epoch= 3209)\n",
      "CV loss=0.13917079210250405(Epoch= 3210)\n",
      "train loss=0.07940514427635388(Epoch= 3210)\n",
      "CV loss=0.13891066421690365(Epoch= 3211)\n",
      "train loss=0.07937508242698013(Epoch= 3211)\n",
      "CV loss=0.13903187938466904(Epoch= 3212)\n",
      "train loss=0.07935061092685938(Epoch= 3212)\n",
      "CV loss=0.13887686823078446(Epoch= 3213)\n",
      "train loss=0.07932532415760231(Epoch= 3213)\n",
      "CV loss=0.13903374517872485(Epoch= 3214)\n",
      "train loss=0.07930613907118614(Epoch= 3214)\n",
      "CV loss=0.1388089599879246(Epoch= 3215)\n",
      "train loss=0.07927634325410729(Epoch= 3215)\n",
      "CV loss=0.13893288577536378(Epoch= 3216)\n",
      "train loss=0.07925391594656146(Epoch= 3216)\n",
      "CV loss=0.13884606738797456(Epoch= 3217)\n",
      "train loss=0.07922654621842805(Epoch= 3217)\n",
      "CV loss=0.139038677391067(Epoch= 3218)\n",
      "train loss=0.07922291810824594(Epoch= 3218)\n",
      "CV loss=0.13873947904875594(Epoch= 3219)\n",
      "train loss=0.07917468654108185(Epoch= 3219)\n",
      "CV loss=0.13883047826831563(Epoch= 3220)\n",
      "train loss=0.07915483098703513(Epoch= 3220)\n",
      "CV loss=0.13867696031106974(Epoch= 3221)\n",
      "train loss=0.07912975029669156(Epoch= 3221)\n",
      "CV loss=0.1388113102326931(Epoch= 3222)\n",
      "train loss=0.0791108091303663(Epoch= 3222)\n",
      "CV loss=0.13878254115879318(Epoch= 3223)\n",
      "train loss=0.07908120541911902(Epoch= 3223)\n",
      "CV loss=0.13862863391232966(Epoch= 3224)\n",
      "train loss=0.07905359205768292(Epoch= 3224)\n",
      "CV loss=0.13877323534645997(Epoch= 3225)\n",
      "train loss=0.0790341874319126(Epoch= 3225)\n",
      "CV loss=0.13850985514470643(Epoch= 3226)\n",
      "train loss=0.07900623830579061(Epoch= 3226)\n",
      "CV loss=0.13853743813969532(Epoch= 3227)\n",
      "train loss=0.0789836328501361(Epoch= 3227)\n",
      "CV loss=0.1383822071221396(Epoch= 3228)\n",
      "train loss=0.07896468900243675(Epoch= 3228)\n",
      "CV loss=0.13842069500252127(Epoch= 3229)\n",
      "train loss=0.07894558977577427(Epoch= 3229)\n",
      "CV loss=0.1382671479338652(Epoch= 3230)\n",
      "train loss=0.07891912488857354(Epoch= 3230)\n",
      "CV loss=0.13847633427281608(Epoch= 3231)\n",
      "train loss=0.07890805659636817(Epoch= 3231)\n",
      "CV loss=0.13828451370662043(Epoch= 3232)\n",
      "train loss=0.07886709490964444(Epoch= 3232)\n",
      "CV loss=0.13857588232708767(Epoch= 3233)\n",
      "train loss=0.078844430999355(Epoch= 3233)\n",
      "CV loss=0.13838749951153306(Epoch= 3234)\n",
      "train loss=0.07881492342037347(Epoch= 3234)\n",
      "CV loss=0.1382580888579913(Epoch= 3235)\n",
      "train loss=0.078793517144298(Epoch= 3235)\n",
      "CV loss=0.13825107588141788(Epoch= 3236)\n",
      "train loss=0.07876534986356123(Epoch= 3236)\n",
      "CV loss=0.13822642491538273(Epoch= 3237)\n",
      "train loss=0.07874166314092956(Epoch= 3237)\n",
      "CV loss=0.1383216513064624(Epoch= 3238)\n",
      "train loss=0.07871778248831263(Epoch= 3238)\n",
      "CV loss=0.13818018813339253(Epoch= 3239)\n",
      "train loss=0.07869719659592449(Epoch= 3239)\n",
      "CV loss=0.13817880730366672(Epoch= 3240)\n",
      "train loss=0.07866748119482787(Epoch= 3240)\n",
      "CV loss=0.13818107353980438(Epoch= 3241)\n",
      "train loss=0.07865539291363131(Epoch= 3241)\n",
      "CV loss=0.13813553017824104(Epoch= 3242)\n",
      "train loss=0.07861807227855994(Epoch= 3242)\n",
      "CV loss=0.1381892196663715(Epoch= 3243)\n",
      "train loss=0.07859383369271329(Epoch= 3243)\n",
      "CV loss=0.13827390933070444(Epoch= 3244)\n",
      "train loss=0.0785905006787382(Epoch= 3244)\n",
      "CV loss=0.1379974928713949(Epoch= 3245)\n",
      "train loss=0.07854770045690501(Epoch= 3245)\n",
      "CV loss=0.1380104755282811(Epoch= 3246)\n",
      "train loss=0.0785272477232221(Epoch= 3246)\n",
      "CV loss=0.13808738467113274(Epoch= 3247)\n",
      "train loss=0.07849881006891457(Epoch= 3247)\n",
      "CV loss=0.13809460483451436(Epoch= 3248)\n",
      "train loss=0.07847907634935512(Epoch= 3248)\n",
      "CV loss=0.13798215366329158(Epoch= 3249)\n",
      "train loss=0.07845057603704503(Epoch= 3249)\n",
      "CV loss=0.1379702369581825(Epoch= 3250)\n",
      "train loss=0.0784346332572072(Epoch= 3250)\n",
      "CV loss=0.13794821435889615(Epoch= 3251)\n",
      "train loss=0.07840682838136816(Epoch= 3251)\n",
      "CV loss=0.1380277396367843(Epoch= 3252)\n",
      "train loss=0.07838107227455322(Epoch= 3252)\n",
      "CV loss=0.1378325600348049(Epoch= 3253)\n",
      "train loss=0.0783697917543918(Epoch= 3253)\n",
      "CV loss=0.13792186995941086(Epoch= 3254)\n",
      "train loss=0.07834250470035665(Epoch= 3254)\n",
      "CV loss=0.13786444765305(Epoch= 3255)\n",
      "train loss=0.07830778801672185(Epoch= 3255)\n",
      "CV loss=0.1379988602490595(Epoch= 3256)\n",
      "train loss=0.07828864297545804(Epoch= 3256)\n",
      "CV loss=0.13789332231354168(Epoch= 3257)\n",
      "train loss=0.07826357617693717(Epoch= 3257)\n",
      "CV loss=0.13776900795577884(Epoch= 3258)\n",
      "train loss=0.07823994142978649(Epoch= 3258)\n",
      "CV loss=0.1377253675513262(Epoch= 3259)\n",
      "train loss=0.07823209838909839(Epoch= 3259)\n",
      "CV loss=0.1375359683363195(Epoch= 3260)\n",
      "train loss=0.07819854983985298(Epoch= 3260)\n",
      "CV loss=0.13760498987520378(Epoch= 3261)\n",
      "train loss=0.07816935779635781(Epoch= 3261)\n",
      "CV loss=0.13771156716501423(Epoch= 3262)\n",
      "train loss=0.07814221795724396(Epoch= 3262)\n",
      "CV loss=0.13775213115947005(Epoch= 3263)\n",
      "train loss=0.0781222093913576(Epoch= 3263)\n",
      "CV loss=0.13754584879306464(Epoch= 3264)\n",
      "train loss=0.07809821515487964(Epoch= 3264)\n",
      "CV loss=0.13774228696468735(Epoch= 3265)\n",
      "train loss=0.07808081753539303(Epoch= 3265)\n",
      "CV loss=0.13760635680302502(Epoch= 3266)\n",
      "train loss=0.07805503084645726(Epoch= 3266)\n",
      "CV loss=0.13749029169128887(Epoch= 3267)\n",
      "train loss=0.07803098572864636(Epoch= 3267)\n",
      "CV loss=0.13758621702473156(Epoch= 3268)\n",
      "train loss=0.0780121630834389(Epoch= 3268)\n",
      "CV loss=0.1375129041597381(Epoch= 3269)\n",
      "train loss=0.0779777015970905(Epoch= 3269)\n",
      "CV loss=0.13746147239342732(Epoch= 3270)\n",
      "train loss=0.07795173077346088(Epoch= 3270)\n",
      "CV loss=0.13745616147420847(Epoch= 3271)\n",
      "train loss=0.07792764430677637(Epoch= 3271)\n",
      "CV loss=0.1374101938618016(Epoch= 3272)\n",
      "train loss=0.07791124153035307(Epoch= 3272)\n",
      "CV loss=0.13746686843410083(Epoch= 3273)\n",
      "train loss=0.07788511893348139(Epoch= 3273)\n",
      "CV loss=0.13733035462067705(Epoch= 3274)\n",
      "train loss=0.07787205288785948(Epoch= 3274)\n",
      "CV loss=0.1372697093444291(Epoch= 3275)\n",
      "train loss=0.07783482030699884(Epoch= 3275)\n",
      "CV loss=0.1373881290929565(Epoch= 3276)\n",
      "train loss=0.07782047012955859(Epoch= 3276)\n",
      "CV loss=0.13725065733738234(Epoch= 3277)\n",
      "train loss=0.07780147321428382(Epoch= 3277)\n",
      "CV loss=0.1373169726602748(Epoch= 3278)\n",
      "train loss=0.07777242911254413(Epoch= 3278)\n",
      "CV loss=0.1371725735636432(Epoch= 3279)\n",
      "train loss=0.07774456844923548(Epoch= 3279)\n",
      "CV loss=0.13713132865040545(Epoch= 3280)\n",
      "train loss=0.07772358137783274(Epoch= 3280)\n",
      "CV loss=0.1372313719742898(Epoch= 3281)\n",
      "train loss=0.07770105354733788(Epoch= 3281)\n",
      "CV loss=0.13729836042619345(Epoch= 3282)\n",
      "train loss=0.07767571910438333(Epoch= 3282)\n",
      "CV loss=0.137086101502168(Epoch= 3283)\n",
      "train loss=0.07765803612189172(Epoch= 3283)\n",
      "CV loss=0.13704681449115091(Epoch= 3284)\n",
      "train loss=0.0776399644818755(Epoch= 3284)\n",
      "CV loss=0.1372148245371633(Epoch= 3285)\n",
      "train loss=0.0776025861271565(Epoch= 3285)\n",
      "CV loss=0.1371239946932024(Epoch= 3286)\n",
      "train loss=0.07757929885400941(Epoch= 3286)\n",
      "CV loss=0.13719705217829326(Epoch= 3287)\n",
      "train loss=0.07756835928804849(Epoch= 3287)\n",
      "CV loss=0.1370962225548023(Epoch= 3288)\n",
      "train loss=0.07754502929137673(Epoch= 3288)\n",
      "CV loss=0.1369480199229003(Epoch= 3289)\n",
      "train loss=0.07752189012965885(Epoch= 3289)\n",
      "CV loss=0.13685247914506232(Epoch= 3290)\n",
      "train loss=0.07749506618095314(Epoch= 3290)\n",
      "CV loss=0.13709543979408345(Epoch= 3291)\n",
      "train loss=0.07746902387230117(Epoch= 3291)\n",
      "CV loss=0.13689137896374826(Epoch= 3292)\n",
      "train loss=0.07744601778958306(Epoch= 3292)\n",
      "CV loss=0.1367999574983346(Epoch= 3293)\n",
      "train loss=0.07741657932309849(Epoch= 3293)\n",
      "CV loss=0.13721129480669642(Epoch= 3294)\n",
      "train loss=0.07741061350263381(Epoch= 3294)\n",
      "CV loss=0.13690049164453022(Epoch= 3295)\n",
      "train loss=0.07737626032233465(Epoch= 3295)\n",
      "CV loss=0.1368490070209722(Epoch= 3296)\n",
      "train loss=0.07735262998114534(Epoch= 3296)\n",
      "CV loss=0.13688338361668204(Epoch= 3297)\n",
      "train loss=0.07732819506459772(Epoch= 3297)\n",
      "CV loss=0.13697051184357595(Epoch= 3298)\n",
      "train loss=0.07730735861027593(Epoch= 3298)\n",
      "CV loss=0.13680928507900997(Epoch= 3299)\n",
      "train loss=0.07728189659229442(Epoch= 3299)\n",
      "CV loss=0.13662435314706728(Epoch= 3300)\n",
      "train loss=0.07726580224554354(Epoch= 3300)\n",
      "CV loss=0.1366745491741429(Epoch= 3301)\n",
      "train loss=0.07723402152767617(Epoch= 3301)\n",
      "CV loss=0.13685255364954246(Epoch= 3302)\n",
      "train loss=0.07722133255734093(Epoch= 3302)\n",
      "CV loss=0.13668011297086569(Epoch= 3303)\n",
      "train loss=0.0771941419621694(Epoch= 3303)\n",
      "CV loss=0.13678107206027498(Epoch= 3304)\n",
      "train loss=0.0771655913631867(Epoch= 3304)\n",
      "CV loss=0.13663081067111116(Epoch= 3305)\n",
      "train loss=0.07714712093726994(Epoch= 3305)\n",
      "CV loss=0.1366622865720769(Epoch= 3306)\n",
      "train loss=0.07711881341006624(Epoch= 3306)\n",
      "CV loss=0.13661231786592382(Epoch= 3307)\n",
      "train loss=0.07709981931219918(Epoch= 3307)\n",
      "CV loss=0.136404246397518(Epoch= 3308)\n",
      "train loss=0.07708669399683529(Epoch= 3308)\n",
      "CV loss=0.13653777555898405(Epoch= 3309)\n",
      "train loss=0.07706356803435435(Epoch= 3309)\n",
      "CV loss=0.1364645885937783(Epoch= 3310)\n",
      "train loss=0.07703952247728588(Epoch= 3310)\n",
      "CV loss=0.13654295330772187(Epoch= 3311)\n",
      "train loss=0.07700859891124606(Epoch= 3311)\n",
      "CV loss=0.1364617464988242(Epoch= 3312)\n",
      "train loss=0.07698292204979043(Epoch= 3312)\n",
      "CV loss=0.13649940736694993(Epoch= 3313)\n",
      "train loss=0.07696677164586253(Epoch= 3313)\n",
      "CV loss=0.13653854898464562(Epoch= 3314)\n",
      "train loss=0.07694040417686464(Epoch= 3314)\n",
      "CV loss=0.13626929383681605(Epoch= 3315)\n",
      "train loss=0.07692022157863711(Epoch= 3315)\n",
      "CV loss=0.1363444702049979(Epoch= 3316)\n",
      "train loss=0.07689268808349067(Epoch= 3316)\n",
      "CV loss=0.13640221369319822(Epoch= 3317)\n",
      "train loss=0.07686860902904018(Epoch= 3317)\n",
      "CV loss=0.13629928057952811(Epoch= 3318)\n",
      "train loss=0.07684629548673487(Epoch= 3318)\n",
      "CV loss=0.13615967036525511(Epoch= 3319)\n",
      "train loss=0.07682957158044368(Epoch= 3319)\n",
      "CV loss=0.1362983506244938(Epoch= 3320)\n",
      "train loss=0.07679964153739292(Epoch= 3320)\n",
      "CV loss=0.13622778449645181(Epoch= 3321)\n",
      "train loss=0.07677929836513155(Epoch= 3321)\n",
      "CV loss=0.1361725094802177(Epoch= 3322)\n",
      "train loss=0.07675768085834449(Epoch= 3322)\n",
      "CV loss=0.13655379787216376(Epoch= 3323)\n",
      "train loss=0.07675058359876083(Epoch= 3323)\n",
      "CV loss=0.13611850371395517(Epoch= 3324)\n",
      "train loss=0.07671590831437365(Epoch= 3324)\n",
      "CV loss=0.13616384844526744(Epoch= 3325)\n",
      "train loss=0.0766891667278573(Epoch= 3325)\n",
      "CV loss=0.1362944176436061(Epoch= 3326)\n",
      "train loss=0.07667065388260337(Epoch= 3326)\n",
      "CV loss=0.13608379251187577(Epoch= 3327)\n",
      "train loss=0.07665274109570183(Epoch= 3327)\n",
      "CV loss=0.1361448094868589(Epoch= 3328)\n",
      "train loss=0.07663199358839441(Epoch= 3328)\n",
      "CV loss=0.13604624351953326(Epoch= 3329)\n",
      "train loss=0.07660267915674858(Epoch= 3329)\n",
      "CV loss=0.1360551598378737(Epoch= 3330)\n",
      "train loss=0.07657921161030651(Epoch= 3330)\n",
      "CV loss=0.13599395627068447(Epoch= 3331)\n",
      "train loss=0.07655920409495791(Epoch= 3331)\n",
      "CV loss=0.1360663654694696(Epoch= 3332)\n",
      "train loss=0.0765415618447404(Epoch= 3332)\n",
      "CV loss=0.1360254538046013(Epoch= 3333)\n",
      "train loss=0.07650916703462707(Epoch= 3333)\n",
      "CV loss=0.13579218683146405(Epoch= 3334)\n",
      "train loss=0.07650094633368267(Epoch= 3334)\n",
      "CV loss=0.1357921226413501(Epoch= 3335)\n",
      "train loss=0.07647502519857523(Epoch= 3335)\n",
      "CV loss=0.13583160423346674(Epoch= 3336)\n",
      "train loss=0.07644345431653712(Epoch= 3336)\n",
      "CV loss=0.13588486492383736(Epoch= 3337)\n",
      "train loss=0.07642148653768342(Epoch= 3337)\n",
      "CV loss=0.1359124641608021(Epoch= 3338)\n",
      "train loss=0.07640275732085514(Epoch= 3338)\n",
      "CV loss=0.13586106038928003(Epoch= 3339)\n",
      "train loss=0.07637771792390441(Epoch= 3339)\n",
      "CV loss=0.13581515009485395(Epoch= 3340)\n",
      "train loss=0.07635439188275472(Epoch= 3340)\n",
      "CV loss=0.13590104462591646(Epoch= 3341)\n",
      "train loss=0.07633697139871923(Epoch= 3341)\n",
      "CV loss=0.13593145461696032(Epoch= 3342)\n",
      "train loss=0.07631034224245063(Epoch= 3342)\n",
      "CV loss=0.13585855518724582(Epoch= 3343)\n",
      "train loss=0.07629126375166187(Epoch= 3343)\n",
      "CV loss=0.13571327549819653(Epoch= 3344)\n",
      "train loss=0.0762680233968711(Epoch= 3344)\n",
      "CV loss=0.13578375226860306(Epoch= 3345)\n",
      "train loss=0.07624289514329707(Epoch= 3345)\n",
      "CV loss=0.13582679223553876(Epoch= 3346)\n",
      "train loss=0.07622596668071714(Epoch= 3346)\n",
      "CV loss=0.13583635135038497(Epoch= 3347)\n",
      "train loss=0.07620266717713653(Epoch= 3347)\n",
      "CV loss=0.13571587254103892(Epoch= 3348)\n",
      "train loss=0.07617645450119576(Epoch= 3348)\n",
      "CV loss=0.1355837505844119(Epoch= 3349)\n",
      "train loss=0.07616078543517446(Epoch= 3349)\n",
      "CV loss=0.1356741810734846(Epoch= 3350)\n",
      "train loss=0.07613228585572321(Epoch= 3350)\n",
      "CV loss=0.13570270166194628(Epoch= 3351)\n",
      "train loss=0.07611136025838222(Epoch= 3351)\n",
      "CV loss=0.13551413307116364(Epoch= 3352)\n",
      "train loss=0.07609104955436848(Epoch= 3352)\n",
      "CV loss=0.13555092807475694(Epoch= 3353)\n",
      "train loss=0.07606708371347064(Epoch= 3353)\n",
      "CV loss=0.13535908204918257(Epoch= 3354)\n",
      "train loss=0.07604930133237608(Epoch= 3354)\n",
      "CV loss=0.13545794949449536(Epoch= 3355)\n",
      "train loss=0.0760254085399349(Epoch= 3355)\n",
      "CV loss=0.13537948442766018(Epoch= 3356)\n",
      "train loss=0.07600348303829314(Epoch= 3356)\n",
      "CV loss=0.13546774988005805(Epoch= 3357)\n",
      "train loss=0.07597817445613324(Epoch= 3357)\n",
      "CV loss=0.13554246563528588(Epoch= 3358)\n",
      "train loss=0.07596630138045055(Epoch= 3358)\n",
      "CV loss=0.13543097584227967(Epoch= 3359)\n",
      "train loss=0.07593801502147268(Epoch= 3359)\n",
      "CV loss=0.13543920161252085(Epoch= 3360)\n",
      "train loss=0.07591340442099648(Epoch= 3360)\n",
      "CV loss=0.13527397056019538(Epoch= 3361)\n",
      "train loss=0.07589629584828736(Epoch= 3361)\n",
      "CV loss=0.13528621429590415(Epoch= 3362)\n",
      "train loss=0.07587035927307102(Epoch= 3362)\n",
      "CV loss=0.13528148297753698(Epoch= 3363)\n",
      "train loss=0.07585663916706413(Epoch= 3363)\n",
      "CV loss=0.13544461907589395(Epoch= 3364)\n",
      "train loss=0.07583142788037911(Epoch= 3364)\n",
      "CV loss=0.13525937466594312(Epoch= 3365)\n",
      "train loss=0.07580989767154371(Epoch= 3365)\n",
      "CV loss=0.13526705532695438(Epoch= 3366)\n",
      "train loss=0.07578168393285323(Epoch= 3366)\n",
      "CV loss=0.13527765777963124(Epoch= 3367)\n",
      "train loss=0.07576022816911969(Epoch= 3367)\n",
      "CV loss=0.1352620045697904(Epoch= 3368)\n",
      "train loss=0.07574549269142514(Epoch= 3368)\n",
      "CV loss=0.1352352838620432(Epoch= 3369)\n",
      "train loss=0.07572392457781818(Epoch= 3369)\n",
      "CV loss=0.1354214873186242(Epoch= 3370)\n",
      "train loss=0.0757043745037377(Epoch= 3370)\n",
      "CV loss=0.13511444829664132(Epoch= 3371)\n",
      "train loss=0.07567381278998303(Epoch= 3371)\n",
      "CV loss=0.1352007313130302(Epoch= 3372)\n",
      "train loss=0.07565725832258816(Epoch= 3372)\n",
      "CV loss=0.13500976297453826(Epoch= 3373)\n",
      "train loss=0.07563428095280854(Epoch= 3373)\n",
      "CV loss=0.1351126491713087(Epoch= 3374)\n",
      "train loss=0.07561524164487715(Epoch= 3374)\n",
      "CV loss=0.13511898270244466(Epoch= 3375)\n",
      "train loss=0.07559991287506128(Epoch= 3375)\n",
      "CV loss=0.13510760163296695(Epoch= 3376)\n",
      "train loss=0.0755703339224335(Epoch= 3376)\n",
      "CV loss=0.13502830138111133(Epoch= 3377)\n",
      "train loss=0.07554431300382283(Epoch= 3377)\n",
      "CV loss=0.13507883757349898(Epoch= 3378)\n",
      "train loss=0.0755319306265049(Epoch= 3378)\n",
      "CV loss=0.13506972811524515(Epoch= 3379)\n",
      "train loss=0.07550758385001691(Epoch= 3379)\n",
      "CV loss=0.13507204927927502(Epoch= 3380)\n",
      "train loss=0.07549103397328544(Epoch= 3380)\n",
      "CV loss=0.13485712602667194(Epoch= 3381)\n",
      "train loss=0.07545871555787888(Epoch= 3381)\n",
      "CV loss=0.13508636416451145(Epoch= 3382)\n",
      "train loss=0.07544606706328885(Epoch= 3382)\n",
      "CV loss=0.135071018614488(Epoch= 3383)\n",
      "train loss=0.07543003951391322(Epoch= 3383)\n",
      "CV loss=0.1349360958747322(Epoch= 3384)\n",
      "train loss=0.07539919539692931(Epoch= 3384)\n",
      "CV loss=0.13484910136593564(Epoch= 3385)\n",
      "train loss=0.07537752016092583(Epoch= 3385)\n",
      "CV loss=0.134839251884236(Epoch= 3386)\n",
      "train loss=0.0753525315509589(Epoch= 3386)\n",
      "CV loss=0.13472473871085788(Epoch= 3387)\n",
      "train loss=0.07533622199449921(Epoch= 3387)\n",
      "CV loss=0.13473875121416012(Epoch= 3388)\n",
      "train loss=0.07531006731063215(Epoch= 3388)\n",
      "CV loss=0.1348016262760307(Epoch= 3389)\n",
      "train loss=0.07528940774020051(Epoch= 3389)\n",
      "CV loss=0.1347478655038732(Epoch= 3390)\n",
      "train loss=0.07527446114983466(Epoch= 3390)\n",
      "CV loss=0.1348253179880151(Epoch= 3391)\n",
      "train loss=0.07525079653763239(Epoch= 3391)\n",
      "CV loss=0.13462634322567912(Epoch= 3392)\n",
      "train loss=0.07523210634960899(Epoch= 3392)\n",
      "CV loss=0.13452445050772147(Epoch= 3393)\n",
      "train loss=0.07520854183303244(Epoch= 3393)\n",
      "CV loss=0.13459006551757297(Epoch= 3394)\n",
      "train loss=0.07518475653664417(Epoch= 3394)\n",
      "CV loss=0.1345510231551312(Epoch= 3395)\n",
      "train loss=0.07516160976690094(Epoch= 3395)\n",
      "CV loss=0.13457438405135974(Epoch= 3396)\n",
      "train loss=0.07514445827658013(Epoch= 3396)\n",
      "CV loss=0.13453948137713576(Epoch= 3397)\n",
      "train loss=0.07512092819662912(Epoch= 3397)\n",
      "CV loss=0.1344737687050839(Epoch= 3398)\n",
      "train loss=0.07510176803755353(Epoch= 3398)\n",
      "CV loss=0.13455169453152063(Epoch= 3399)\n",
      "train loss=0.0750850437286629(Epoch= 3399)\n",
      "CV loss=0.13445788206966083(Epoch= 3400)\n",
      "train loss=0.075056857259159(Epoch= 3400)\n",
      "CV loss=0.13444698970801622(Epoch= 3401)\n",
      "train loss=0.07503907542115112(Epoch= 3401)\n",
      "CV loss=0.13448697313995506(Epoch= 3402)\n",
      "train loss=0.07501346505384021(Epoch= 3402)\n",
      "CV loss=0.13446845403783897(Epoch= 3403)\n",
      "train loss=0.07500127570759947(Epoch= 3403)\n",
      "CV loss=0.1345974557904873(Epoch= 3404)\n",
      "train loss=0.07497592849504767(Epoch= 3404)\n",
      "CV loss=0.13435330488107083(Epoch= 3405)\n",
      "train loss=0.07494786543214504(Epoch= 3405)\n",
      "CV loss=0.13429910271475637(Epoch= 3406)\n",
      "train loss=0.07493257155396289(Epoch= 3406)\n",
      "CV loss=0.1342897401603365(Epoch= 3407)\n",
      "train loss=0.07490706382108539(Epoch= 3407)\n",
      "CV loss=0.13436946604868938(Epoch= 3408)\n",
      "train loss=0.07489357541821232(Epoch= 3408)\n",
      "CV loss=0.13432699103397538(Epoch= 3409)\n",
      "train loss=0.07487116204227229(Epoch= 3409)\n",
      "CV loss=0.13413337562380998(Epoch= 3410)\n",
      "train loss=0.07485048404340709(Epoch= 3410)\n",
      "CV loss=0.13416169645070447(Epoch= 3411)\n",
      "train loss=0.0748249882254566(Epoch= 3411)\n",
      "CV loss=0.13424185738290162(Epoch= 3412)\n",
      "train loss=0.0748046247651216(Epoch= 3412)\n",
      "CV loss=0.13431817794503825(Epoch= 3413)\n",
      "train loss=0.07478552777625322(Epoch= 3413)\n",
      "CV loss=0.13417901023888298(Epoch= 3414)\n",
      "train loss=0.07476962297573916(Epoch= 3414)\n",
      "CV loss=0.13414212956902294(Epoch= 3415)\n",
      "train loss=0.07475034469192618(Epoch= 3415)\n",
      "CV loss=0.1342179916895314(Epoch= 3416)\n",
      "train loss=0.07471895430702925(Epoch= 3416)\n",
      "CV loss=0.13404186358140513(Epoch= 3417)\n",
      "train loss=0.0746975210104765(Epoch= 3417)\n",
      "CV loss=0.1340757422007256(Epoch= 3418)\n",
      "train loss=0.07469364488053981(Epoch= 3418)\n",
      "CV loss=0.13419404776955113(Epoch= 3419)\n",
      "train loss=0.07466521674045405(Epoch= 3419)\n",
      "CV loss=0.13405437912209744(Epoch= 3420)\n",
      "train loss=0.0746386308032539(Epoch= 3420)\n",
      "CV loss=0.13403378620929823(Epoch= 3421)\n",
      "train loss=0.074616127619439(Epoch= 3421)\n",
      "CV loss=0.13410964268207243(Epoch= 3422)\n",
      "train loss=0.074601582146306(Epoch= 3422)\n",
      "CV loss=0.1339632124799648(Epoch= 3423)\n",
      "train loss=0.0745740624220262(Epoch= 3423)\n",
      "CV loss=0.13400607058489816(Epoch= 3424)\n",
      "train loss=0.07455425066471913(Epoch= 3424)\n",
      "CV loss=0.13388996326278396(Epoch= 3425)\n",
      "train loss=0.07453283871929195(Epoch= 3425)\n",
      "CV loss=0.13386231398482057(Epoch= 3426)\n",
      "train loss=0.07451801720064911(Epoch= 3426)\n",
      "CV loss=0.13376504226952193(Epoch= 3427)\n",
      "train loss=0.07450284456296735(Epoch= 3427)\n",
      "CV loss=0.1338664063370282(Epoch= 3428)\n",
      "train loss=0.07447163664706222(Epoch= 3428)\n",
      "CV loss=0.1337699422723879(Epoch= 3429)\n",
      "train loss=0.0744623775322615(Epoch= 3429)\n",
      "CV loss=0.13373108696049252(Epoch= 3430)\n",
      "train loss=0.07443718966205093(Epoch= 3430)\n",
      "CV loss=0.1339007513714311(Epoch= 3431)\n",
      "train loss=0.0744101852991967(Epoch= 3431)\n",
      "CV loss=0.13385398410305668(Epoch= 3432)\n",
      "train loss=0.07439059972008151(Epoch= 3432)\n",
      "CV loss=0.13380459954547774(Epoch= 3433)\n",
      "train loss=0.0743669043502639(Epoch= 3433)\n",
      "CV loss=0.13370652460349636(Epoch= 3434)\n",
      "train loss=0.07434644740150152(Epoch= 3434)\n",
      "CV loss=0.13360257784898463(Epoch= 3435)\n",
      "train loss=0.07433109474751415(Epoch= 3435)\n",
      "CV loss=0.13387736304318498(Epoch= 3436)\n",
      "train loss=0.0743159011664102(Epoch= 3436)\n",
      "CV loss=0.13373955834644624(Epoch= 3437)\n",
      "train loss=0.07429299644878301(Epoch= 3437)\n",
      "CV loss=0.13373767448426643(Epoch= 3438)\n",
      "train loss=0.07427121921779974(Epoch= 3438)\n",
      "CV loss=0.13376721075945852(Epoch= 3439)\n",
      "train loss=0.0742514421134874(Epoch= 3439)\n",
      "CV loss=0.13343948931174263(Epoch= 3440)\n",
      "train loss=0.07423817603220785(Epoch= 3440)\n",
      "CV loss=0.13353935126195365(Epoch= 3441)\n",
      "train loss=0.07421442962891957(Epoch= 3441)\n",
      "CV loss=0.13344964358296624(Epoch= 3442)\n",
      "train loss=0.07418854318232496(Epoch= 3442)\n",
      "CV loss=0.13354778974640535(Epoch= 3443)\n",
      "train loss=0.07416361592117021(Epoch= 3443)\n",
      "CV loss=0.13360824829045997(Epoch= 3444)\n",
      "train loss=0.07414248374207444(Epoch= 3444)\n",
      "CV loss=0.13356227379370317(Epoch= 3445)\n",
      "train loss=0.0741321372696161(Epoch= 3445)\n",
      "CV loss=0.133379774147416(Epoch= 3446)\n",
      "train loss=0.0741040646810424(Epoch= 3446)\n",
      "CV loss=0.13347373711362082(Epoch= 3447)\n",
      "train loss=0.0740835001274567(Epoch= 3447)\n",
      "CV loss=0.13347072267811774(Epoch= 3448)\n",
      "train loss=0.074066104138073(Epoch= 3448)\n",
      "CV loss=0.13365770501462382(Epoch= 3449)\n",
      "train loss=0.07405649788252933(Epoch= 3449)\n",
      "CV loss=0.13346603605779533(Epoch= 3450)\n",
      "train loss=0.07402644084588426(Epoch= 3450)\n",
      "CV loss=0.1333504284912177(Epoch= 3451)\n",
      "train loss=0.07400324526212193(Epoch= 3451)\n",
      "CV loss=0.13355792168782843(Epoch= 3452)\n",
      "train loss=0.07398478314791343(Epoch= 3452)\n",
      "CV loss=0.13323414057910252(Epoch= 3453)\n",
      "train loss=0.07396305676306969(Epoch= 3453)\n",
      "CV loss=0.13330817310549137(Epoch= 3454)\n",
      "train loss=0.07394219836572827(Epoch= 3454)\n",
      "CV loss=0.13325719079312637(Epoch= 3455)\n",
      "train loss=0.07391866587443936(Epoch= 3455)\n",
      "CV loss=0.133150683589008(Epoch= 3456)\n",
      "train loss=0.07390455949321398(Epoch= 3456)\n",
      "CV loss=0.13332553450164167(Epoch= 3457)\n",
      "train loss=0.073881792550901(Epoch= 3457)\n",
      "CV loss=0.13323344962579556(Epoch= 3458)\n",
      "train loss=0.07386152038429614(Epoch= 3458)\n",
      "CV loss=0.13333314917549904(Epoch= 3459)\n",
      "train loss=0.0738509916343676(Epoch= 3459)\n",
      "CV loss=0.13319876441078804(Epoch= 3460)\n",
      "train loss=0.07382771388463583(Epoch= 3460)\n",
      "CV loss=0.13326570511302438(Epoch= 3461)\n",
      "train loss=0.07380056748233305(Epoch= 3461)\n",
      "CV loss=0.1332670325656988(Epoch= 3462)\n",
      "train loss=0.07377886201719(Epoch= 3462)\n",
      "CV loss=0.13300744570861842(Epoch= 3463)\n",
      "train loss=0.07376754574375964(Epoch= 3463)\n",
      "CV loss=0.13311696604641726(Epoch= 3464)\n",
      "train loss=0.07374206333053256(Epoch= 3464)\n",
      "CV loss=0.1330986181138794(Epoch= 3465)\n",
      "train loss=0.07371878699442229(Epoch= 3465)\n",
      "CV loss=0.13306235191499277(Epoch= 3466)\n",
      "train loss=0.07370105091934058(Epoch= 3466)\n",
      "CV loss=0.1331029456652447(Epoch= 3467)\n",
      "train loss=0.07368326682773049(Epoch= 3467)\n",
      "CV loss=0.13282695967138294(Epoch= 3468)\n",
      "train loss=0.07366448459267272(Epoch= 3468)\n",
      "CV loss=0.13314135632645763(Epoch= 3469)\n",
      "train loss=0.0736434797320322(Epoch= 3469)\n",
      "CV loss=0.1331150739306458(Epoch= 3470)\n",
      "train loss=0.07363232784693913(Epoch= 3470)\n",
      "CV loss=0.13303789405016392(Epoch= 3471)\n",
      "train loss=0.07359886157658288(Epoch= 3471)\n",
      "CV loss=0.132913548258684(Epoch= 3472)\n",
      "train loss=0.07357980044724116(Epoch= 3472)\n",
      "CV loss=0.13294622619256607(Epoch= 3473)\n",
      "train loss=0.07356051197928432(Epoch= 3473)\n",
      "CV loss=0.13291237845472842(Epoch= 3474)\n",
      "train loss=0.0735443641766386(Epoch= 3474)\n",
      "CV loss=0.13283209041928495(Epoch= 3475)\n",
      "train loss=0.07352380632093175(Epoch= 3475)\n",
      "CV loss=0.13284369770898832(Epoch= 3476)\n",
      "train loss=0.07349992770100491(Epoch= 3476)\n",
      "CV loss=0.13280471416288836(Epoch= 3477)\n",
      "train loss=0.07348526187170785(Epoch= 3477)\n",
      "CV loss=0.13290639630557816(Epoch= 3478)\n",
      "train loss=0.07346088328927312(Epoch= 3478)\n",
      "CV loss=0.13286978451143816(Epoch= 3479)\n",
      "train loss=0.07344597193019206(Epoch= 3479)\n",
      "CV loss=0.1327339482476583(Epoch= 3480)\n",
      "train loss=0.07342476710988761(Epoch= 3480)\n",
      "CV loss=0.13281810909959818(Epoch= 3481)\n",
      "train loss=0.07340546337646234(Epoch= 3481)\n",
      "CV loss=0.13264243361201183(Epoch= 3482)\n",
      "train loss=0.07338432350938373(Epoch= 3482)\n",
      "CV loss=0.13289493402671979(Epoch= 3483)\n",
      "train loss=0.07337054003550661(Epoch= 3483)\n",
      "CV loss=0.132743317659408(Epoch= 3484)\n",
      "train loss=0.07335656997812615(Epoch= 3484)\n",
      "CV loss=0.13261070598831293(Epoch= 3485)\n",
      "train loss=0.07332470978479189(Epoch= 3485)\n",
      "CV loss=0.13261253134373926(Epoch= 3486)\n",
      "train loss=0.07330291240575988(Epoch= 3486)\n",
      "CV loss=0.13270820910603942(Epoch= 3487)\n",
      "train loss=0.07328175014730272(Epoch= 3487)\n",
      "CV loss=0.1326030086510317(Epoch= 3488)\n",
      "train loss=0.07326252911958162(Epoch= 3488)\n",
      "CV loss=0.13267872955302334(Epoch= 3489)\n",
      "train loss=0.07324923694278054(Epoch= 3489)\n",
      "CV loss=0.1326384551231789(Epoch= 3490)\n",
      "train loss=0.07322961398173967(Epoch= 3490)\n",
      "CV loss=0.1325566525476431(Epoch= 3491)\n",
      "train loss=0.07320236786930572(Epoch= 3491)\n",
      "CV loss=0.13250592174514447(Epoch= 3492)\n",
      "train loss=0.0731858757248084(Epoch= 3492)\n",
      "CV loss=0.1324846482816487(Epoch= 3493)\n",
      "train loss=0.0731684864998139(Epoch= 3493)\n",
      "CV loss=0.13247239903765007(Epoch= 3494)\n",
      "train loss=0.0731490793526918(Epoch= 3494)\n",
      "CV loss=0.13246462129015285(Epoch= 3495)\n",
      "train loss=0.07312932975322586(Epoch= 3495)\n",
      "CV loss=0.13238116947198575(Epoch= 3496)\n",
      "train loss=0.07311203423246079(Epoch= 3496)\n",
      "CV loss=0.13251695726211532(Epoch= 3497)\n",
      "train loss=0.07309015008435057(Epoch= 3497)\n",
      "CV loss=0.13261852521935608(Epoch= 3498)\n",
      "train loss=0.07308274395638602(Epoch= 3498)\n",
      "CV loss=0.1324198085927954(Epoch= 3499)\n",
      "train loss=0.07304938036452407(Epoch= 3499)\n",
      "CV loss=0.13228031053442285(Epoch= 3500)\n",
      "train loss=0.07302997186559126(Epoch= 3500)\n",
      "CV loss=0.13232794365596798(Epoch= 3501)\n",
      "train loss=0.07301856381680204(Epoch= 3501)\n",
      "CV loss=0.1322144088752329(Epoch= 3502)\n",
      "train loss=0.07299303302190602(Epoch= 3502)\n",
      "CV loss=0.13240275338670246(Epoch= 3503)\n",
      "train loss=0.07297769321636036(Epoch= 3503)\n",
      "CV loss=0.1324556425387289(Epoch= 3504)\n",
      "train loss=0.07295485834541902(Epoch= 3504)\n",
      "CV loss=0.1322685405067256(Epoch= 3505)\n",
      "train loss=0.07293971896376768(Epoch= 3505)\n",
      "CV loss=0.1321747219478998(Epoch= 3506)\n",
      "train loss=0.07291581076478106(Epoch= 3506)\n",
      "CV loss=0.13208117132862102(Epoch= 3507)\n",
      "train loss=0.07290043639142177(Epoch= 3507)\n",
      "CV loss=0.13231267191989168(Epoch= 3508)\n",
      "train loss=0.07287903353602034(Epoch= 3508)\n",
      "CV loss=0.13232527231440194(Epoch= 3509)\n",
      "train loss=0.07285874158796078(Epoch= 3509)\n",
      "CV loss=0.132335458535084(Epoch= 3510)\n",
      "train loss=0.07283893410548056(Epoch= 3510)\n",
      "CV loss=0.13216403599620563(Epoch= 3511)\n",
      "train loss=0.07282362352031843(Epoch= 3511)\n",
      "CV loss=0.1319514657430662(Epoch= 3512)\n",
      "train loss=0.0728013037935808(Epoch= 3512)\n",
      "CV loss=0.13204172624859528(Epoch= 3513)\n",
      "train loss=0.07277872861153087(Epoch= 3513)\n",
      "CV loss=0.13186442727401199(Epoch= 3514)\n",
      "train loss=0.07276659816090586(Epoch= 3514)\n",
      "CV loss=0.1320510490703019(Epoch= 3515)\n",
      "train loss=0.07273757012347222(Epoch= 3515)\n",
      "CV loss=0.13226117317401767(Epoch= 3516)\n",
      "train loss=0.07273587004152306(Epoch= 3516)\n",
      "CV loss=0.13200821138870455(Epoch= 3517)\n",
      "train loss=0.07270113789468946(Epoch= 3517)\n",
      "CV loss=0.13184619239108392(Epoch= 3518)\n",
      "train loss=0.07268216694404817(Epoch= 3518)\n",
      "CV loss=0.1318810167997993(Epoch= 3519)\n",
      "train loss=0.07266261561755874(Epoch= 3519)\n",
      "CV loss=0.13185123142338584(Epoch= 3520)\n",
      "train loss=0.07264366157465824(Epoch= 3520)\n",
      "CV loss=0.1319241455245054(Epoch= 3521)\n",
      "train loss=0.07262257976350293(Epoch= 3521)\n",
      "CV loss=0.1320414634158001(Epoch= 3522)\n",
      "train loss=0.07261109706839662(Epoch= 3522)\n",
      "CV loss=0.13184922152707068(Epoch= 3523)\n",
      "train loss=0.07258935472301054(Epoch= 3523)\n",
      "CV loss=0.13183484328013428(Epoch= 3524)\n",
      "train loss=0.07256618813130293(Epoch= 3524)\n",
      "CV loss=0.1318682715149928(Epoch= 3525)\n",
      "train loss=0.07254957835960271(Epoch= 3525)\n",
      "CV loss=0.13187483535408956(Epoch= 3526)\n",
      "train loss=0.07253068486119475(Epoch= 3526)\n",
      "CV loss=0.13181576398290878(Epoch= 3527)\n",
      "train loss=0.07251416414199746(Epoch= 3527)\n",
      "CV loss=0.1318625653263764(Epoch= 3528)\n",
      "train loss=0.07249484274731435(Epoch= 3528)\n",
      "CV loss=0.13189385203987974(Epoch= 3529)\n",
      "train loss=0.07247289934917794(Epoch= 3529)\n",
      "CV loss=0.13177197486878245(Epoch= 3530)\n",
      "train loss=0.07245856184345753(Epoch= 3530)\n",
      "CV loss=0.13180207325944515(Epoch= 3531)\n",
      "train loss=0.07243643290677708(Epoch= 3531)\n",
      "CV loss=0.1316925518481423(Epoch= 3532)\n",
      "train loss=0.0724139798572671(Epoch= 3532)\n",
      "CV loss=0.13185872453218112(Epoch= 3533)\n",
      "train loss=0.0724009447346602(Epoch= 3533)\n",
      "CV loss=0.13159738115140335(Epoch= 3534)\n",
      "train loss=0.07238881725340535(Epoch= 3534)\n",
      "CV loss=0.13169693319645626(Epoch= 3535)\n",
      "train loss=0.07236723805252135(Epoch= 3535)\n",
      "CV loss=0.13155850496822163(Epoch= 3536)\n",
      "train loss=0.0723397274607759(Epoch= 3536)\n",
      "CV loss=0.13162620225163382(Epoch= 3537)\n",
      "train loss=0.07232604720458119(Epoch= 3537)\n",
      "CV loss=0.1315803264191004(Epoch= 3538)\n",
      "train loss=0.07230261348278114(Epoch= 3538)\n",
      "CV loss=0.1317617449215417(Epoch= 3539)\n",
      "train loss=0.07229205585403016(Epoch= 3539)\n",
      "CV loss=0.13146833430431185(Epoch= 3540)\n",
      "train loss=0.07226679611905773(Epoch= 3540)\n",
      "CV loss=0.1315521374419219(Epoch= 3541)\n",
      "train loss=0.07225334491802587(Epoch= 3541)\n",
      "CV loss=0.1312706010545355(Epoch= 3542)\n",
      "train loss=0.07226225719615181(Epoch= 3542)\n",
      "CV loss=0.13158974285943476(Epoch= 3543)\n",
      "train loss=0.07220905281498531(Epoch= 3543)\n",
      "CV loss=0.13153615455838893(Epoch= 3544)\n",
      "train loss=0.07219119392180784(Epoch= 3544)\n",
      "CV loss=0.13153496890079877(Epoch= 3545)\n",
      "train loss=0.07217443598026359(Epoch= 3545)\n",
      "CV loss=0.1313302397224408(Epoch= 3546)\n",
      "train loss=0.07216718347311038(Epoch= 3546)\n",
      "CV loss=0.13132912341595276(Epoch= 3547)\n",
      "train loss=0.07213992722555924(Epoch= 3547)\n",
      "CV loss=0.13154212573384422(Epoch= 3548)\n",
      "train loss=0.07211836880919503(Epoch= 3548)\n",
      "CV loss=0.1313352850642114(Epoch= 3549)\n",
      "train loss=0.07209834935660282(Epoch= 3549)\n",
      "CV loss=0.13126250959456054(Epoch= 3550)\n",
      "train loss=0.07207911084098728(Epoch= 3550)\n",
      "CV loss=0.13123237387998687(Epoch= 3551)\n",
      "train loss=0.07206023081627566(Epoch= 3551)\n",
      "CV loss=0.13129077842480152(Epoch= 3552)\n",
      "train loss=0.07204238091778814(Epoch= 3552)\n",
      "CV loss=0.1311936112697401(Epoch= 3553)\n",
      "train loss=0.07202247576296457(Epoch= 3553)\n",
      "CV loss=0.13126463706089922(Epoch= 3554)\n",
      "train loss=0.07200681459755455(Epoch= 3554)\n",
      "CV loss=0.13110299524171473(Epoch= 3555)\n",
      "train loss=0.07199393491580017(Epoch= 3555)\n",
      "CV loss=0.13135001308620048(Epoch= 3556)\n",
      "train loss=0.07196842093376822(Epoch= 3556)\n",
      "CV loss=0.13101931124682167(Epoch= 3557)\n",
      "train loss=0.07195335504670156(Epoch= 3557)\n",
      "CV loss=0.13117372016404016(Epoch= 3558)\n",
      "train loss=0.07192689638926138(Epoch= 3558)\n",
      "CV loss=0.1311012202325395(Epoch= 3559)\n",
      "train loss=0.07191644920805008(Epoch= 3559)\n",
      "CV loss=0.13107049512036265(Epoch= 3560)\n",
      "train loss=0.07189469616903119(Epoch= 3560)\n",
      "CV loss=0.13118818956111028(Epoch= 3561)\n",
      "train loss=0.07187680608150546(Epoch= 3561)\n",
      "CV loss=0.1310999335590723(Epoch= 3562)\n",
      "train loss=0.07186342679679227(Epoch= 3562)\n",
      "CV loss=0.13113067219124533(Epoch= 3563)\n",
      "train loss=0.07183837848287321(Epoch= 3563)\n",
      "CV loss=0.13118811193702137(Epoch= 3564)\n",
      "train loss=0.07182052684229176(Epoch= 3564)\n",
      "CV loss=0.1311856176998606(Epoch= 3565)\n",
      "train loss=0.07180081393708236(Epoch= 3565)\n",
      "CV loss=0.130974912041945(Epoch= 3566)\n",
      "train loss=0.07178490295300884(Epoch= 3566)\n",
      "CV loss=0.1309179357529926(Epoch= 3567)\n",
      "train loss=0.07176341791446349(Epoch= 3567)\n",
      "CV loss=0.13100298261893023(Epoch= 3568)\n",
      "train loss=0.07175653229148102(Epoch= 3568)\n",
      "CV loss=0.13098576160924674(Epoch= 3569)\n",
      "train loss=0.07172294912461061(Epoch= 3569)\n",
      "CV loss=0.13103061773868016(Epoch= 3570)\n",
      "train loss=0.07171681663316336(Epoch= 3570)\n",
      "CV loss=0.13110415707968665(Epoch= 3571)\n",
      "train loss=0.07169179229187886(Epoch= 3571)\n",
      "CV loss=0.13102702819761625(Epoch= 3572)\n",
      "train loss=0.07167163770396097(Epoch= 3572)\n",
      "CV loss=0.13086166808485708(Epoch= 3573)\n",
      "train loss=0.07164905504297045(Epoch= 3573)\n",
      "CV loss=0.13085458191765498(Epoch= 3574)\n",
      "train loss=0.07163451704772801(Epoch= 3574)\n",
      "CV loss=0.1308585923561712(Epoch= 3575)\n",
      "train loss=0.07161424752617485(Epoch= 3575)\n",
      "CV loss=0.1308797998746501(Epoch= 3576)\n",
      "train loss=0.07159558768774536(Epoch= 3576)\n",
      "CV loss=0.13086477052886397(Epoch= 3577)\n",
      "train loss=0.07157800281490082(Epoch= 3577)\n",
      "CV loss=0.1307522661747437(Epoch= 3578)\n",
      "train loss=0.07155957222275024(Epoch= 3578)\n",
      "CV loss=0.1307739503609977(Epoch= 3579)\n",
      "train loss=0.07154203954333682(Epoch= 3579)\n",
      "CV loss=0.13082248226640475(Epoch= 3580)\n",
      "train loss=0.07152416097372712(Epoch= 3580)\n",
      "CV loss=0.13067855704379627(Epoch= 3581)\n",
      "train loss=0.07151810858023593(Epoch= 3581)\n",
      "CV loss=0.13077892015892134(Epoch= 3582)\n",
      "train loss=0.07149151692654222(Epoch= 3582)\n",
      "CV loss=0.13062866732858855(Epoch= 3583)\n",
      "train loss=0.07147143934657678(Epoch= 3583)\n",
      "CV loss=0.13092535727758786(Epoch= 3584)\n",
      "train loss=0.07146664661405006(Epoch= 3584)\n",
      "CV loss=0.13064484589144726(Epoch= 3585)\n",
      "train loss=0.07143598468918398(Epoch= 3585)\n",
      "CV loss=0.13065472895592511(Epoch= 3586)\n",
      "train loss=0.07142238343189942(Epoch= 3586)\n",
      "CV loss=0.1305944457528146(Epoch= 3587)\n",
      "train loss=0.07140013948808081(Epoch= 3587)\n",
      "CV loss=0.13052787171004282(Epoch= 3588)\n",
      "train loss=0.07138080301703148(Epoch= 3588)\n",
      "CV loss=0.1305819235577348(Epoch= 3589)\n",
      "train loss=0.07136384142600388(Epoch= 3589)\n",
      "CV loss=0.13053057741797608(Epoch= 3590)\n",
      "train loss=0.0713497622621378(Epoch= 3590)\n",
      "CV loss=0.13072270663459476(Epoch= 3591)\n",
      "train loss=0.0713290783706606(Epoch= 3591)\n",
      "CV loss=0.1305088755473539(Epoch= 3592)\n",
      "train loss=0.07131075548925961(Epoch= 3592)\n",
      "CV loss=0.13052601896819588(Epoch= 3593)\n",
      "train loss=0.07131088938295832(Epoch= 3593)\n",
      "CV loss=0.1303643793895968(Epoch= 3594)\n",
      "train loss=0.07127322896319765(Epoch= 3594)\n",
      "CV loss=0.13038501122080504(Epoch= 3595)\n",
      "train loss=0.07125163030466596(Epoch= 3595)\n",
      "CV loss=0.13035212541599245(Epoch= 3596)\n",
      "train loss=0.07125040623276525(Epoch= 3596)\n",
      "CV loss=0.13055038889241394(Epoch= 3597)\n",
      "train loss=0.07121976757727438(Epoch= 3597)\n",
      "CV loss=0.13042438238387716(Epoch= 3598)\n",
      "train loss=0.07119743732933656(Epoch= 3598)\n",
      "CV loss=0.13027060315999023(Epoch= 3599)\n",
      "train loss=0.07118551311853694(Epoch= 3599)\n",
      "CV loss=0.13039515810745705(Epoch= 3600)\n",
      "train loss=0.0711764358811974(Epoch= 3600)\n",
      "CV loss=0.13019662356588996(Epoch= 3601)\n",
      "train loss=0.0711506938094036(Epoch= 3601)\n",
      "CV loss=0.13025089371024776(Epoch= 3602)\n",
      "train loss=0.07112615070673763(Epoch= 3602)\n",
      "CV loss=0.13044250329984827(Epoch= 3603)\n",
      "train loss=0.0711099434610933(Epoch= 3603)\n",
      "CV loss=0.13033557981464816(Epoch= 3604)\n",
      "train loss=0.07108898711648502(Epoch= 3604)\n",
      "CV loss=0.13031300702303994(Epoch= 3605)\n",
      "train loss=0.07107676040187118(Epoch= 3605)\n",
      "CV loss=0.13028262269084745(Epoch= 3606)\n",
      "train loss=0.07106132808597532(Epoch= 3606)\n",
      "CV loss=0.1301788270150961(Epoch= 3607)\n",
      "train loss=0.07103667057784023(Epoch= 3607)\n",
      "CV loss=0.13026566923235938(Epoch= 3608)\n",
      "train loss=0.07102622112133185(Epoch= 3608)\n",
      "CV loss=0.1302645847092192(Epoch= 3609)\n",
      "train loss=0.07100472302437359(Epoch= 3609)\n",
      "CV loss=0.1300654810695303(Epoch= 3610)\n",
      "train loss=0.0709896118355736(Epoch= 3610)\n",
      "CV loss=0.13012388253972312(Epoch= 3611)\n",
      "train loss=0.07096968721690988(Epoch= 3611)\n",
      "CV loss=0.13008906716970758(Epoch= 3612)\n",
      "train loss=0.07094913270074245(Epoch= 3612)\n",
      "CV loss=0.13011480035281742(Epoch= 3613)\n",
      "train loss=0.0709339912323056(Epoch= 3613)\n",
      "CV loss=0.13013247503273553(Epoch= 3614)\n",
      "train loss=0.070926758642892(Epoch= 3614)\n",
      "CV loss=0.13007081918397972(Epoch= 3615)\n",
      "train loss=0.07089361013436132(Epoch= 3615)\n",
      "CV loss=0.13016136094305594(Epoch= 3616)\n",
      "train loss=0.07088207314628765(Epoch= 3616)\n",
      "CV loss=0.13004163504773714(Epoch= 3617)\n",
      "train loss=0.07086437604318598(Epoch= 3617)\n",
      "CV loss=0.1298729363988937(Epoch= 3618)\n",
      "train loss=0.07084379223011325(Epoch= 3618)\n",
      "CV loss=0.13013186864605064(Epoch= 3619)\n",
      "train loss=0.07082801322833615(Epoch= 3619)\n",
      "CV loss=0.13000274117627883(Epoch= 3620)\n",
      "train loss=0.07080946989417498(Epoch= 3620)\n",
      "CV loss=0.13007950669714102(Epoch= 3621)\n",
      "train loss=0.07079067442417963(Epoch= 3621)\n",
      "CV loss=0.12996139128165707(Epoch= 3622)\n",
      "train loss=0.0707716292858076(Epoch= 3622)\n",
      "CV loss=0.129952700915397(Epoch= 3623)\n",
      "train loss=0.07076472251794186(Epoch= 3623)\n",
      "CV loss=0.13005404864574027(Epoch= 3624)\n",
      "train loss=0.07073962285128724(Epoch= 3624)\n",
      "CV loss=0.12998829991018063(Epoch= 3625)\n",
      "train loss=0.07072494565516725(Epoch= 3625)\n",
      "CV loss=0.1298993782199549(Epoch= 3626)\n",
      "train loss=0.07070899940513872(Epoch= 3626)\n",
      "CV loss=0.1297971420092334(Epoch= 3627)\n",
      "train loss=0.07069735122428056(Epoch= 3627)\n",
      "CV loss=0.12989010562626824(Epoch= 3628)\n",
      "train loss=0.07066817204346657(Epoch= 3628)\n",
      "CV loss=0.12979830349254579(Epoch= 3629)\n",
      "train loss=0.07065202426141909(Epoch= 3629)\n",
      "CV loss=0.1298940140494695(Epoch= 3630)\n",
      "train loss=0.07063214979697786(Epoch= 3630)\n",
      "CV loss=0.1298768938575944(Epoch= 3631)\n",
      "train loss=0.07061693069432194(Epoch= 3631)\n",
      "CV loss=0.12977579264502553(Epoch= 3632)\n",
      "train loss=0.07059905971705656(Epoch= 3632)\n",
      "CV loss=0.1298587031515572(Epoch= 3633)\n",
      "train loss=0.07058128524900074(Epoch= 3633)\n",
      "CV loss=0.12960140620203275(Epoch= 3634)\n",
      "train loss=0.07056505742662199(Epoch= 3634)\n",
      "CV loss=0.12974316919580647(Epoch= 3635)\n",
      "train loss=0.07054313554714063(Epoch= 3635)\n",
      "CV loss=0.12977813460355264(Epoch= 3636)\n",
      "train loss=0.0705273942440947(Epoch= 3636)\n",
      "CV loss=0.12975413700522928(Epoch= 3637)\n",
      "train loss=0.0705079131468255(Epoch= 3637)\n",
      "CV loss=0.12965677798215325(Epoch= 3638)\n",
      "train loss=0.07049240668147237(Epoch= 3638)\n",
      "CV loss=0.12957405725044496(Epoch= 3639)\n",
      "train loss=0.07047427511264306(Epoch= 3639)\n",
      "CV loss=0.12973584394530302(Epoch= 3640)\n",
      "train loss=0.07045956931616965(Epoch= 3640)\n",
      "CV loss=0.12959942104714509(Epoch= 3641)\n",
      "train loss=0.07044023525795073(Epoch= 3641)\n",
      "CV loss=0.12964345867111532(Epoch= 3642)\n",
      "train loss=0.07042297479158845(Epoch= 3642)\n",
      "CV loss=0.12956342029724888(Epoch= 3643)\n",
      "train loss=0.07040689058513407(Epoch= 3643)\n",
      "CV loss=0.12961637816064414(Epoch= 3644)\n",
      "train loss=0.07039510848864951(Epoch= 3644)\n",
      "CV loss=0.12951235186301013(Epoch= 3645)\n",
      "train loss=0.07037292415557339(Epoch= 3645)\n",
      "CV loss=0.12957713788757153(Epoch= 3646)\n",
      "train loss=0.07035433934732312(Epoch= 3646)\n",
      "CV loss=0.1295087578418167(Epoch= 3647)\n",
      "train loss=0.07034106677020276(Epoch= 3647)\n",
      "CV loss=0.12952447556292807(Epoch= 3648)\n",
      "train loss=0.07032417844693603(Epoch= 3648)\n",
      "CV loss=0.12934036849392255(Epoch= 3649)\n",
      "train loss=0.07030398093298022(Epoch= 3649)\n",
      "CV loss=0.12940834236478144(Epoch= 3650)\n",
      "train loss=0.07029047444819204(Epoch= 3650)\n",
      "CV loss=0.1293461157310113(Epoch= 3651)\n",
      "train loss=0.07027268159583418(Epoch= 3651)\n",
      "CV loss=0.12943006996847775(Epoch= 3652)\n",
      "train loss=0.07025258970671369(Epoch= 3652)\n",
      "CV loss=0.12936716748485672(Epoch= 3653)\n",
      "train loss=0.07023441095823958(Epoch= 3653)\n",
      "CV loss=0.12949745387246997(Epoch= 3654)\n",
      "train loss=0.07021684179265054(Epoch= 3654)\n",
      "CV loss=0.12936736072303912(Epoch= 3655)\n",
      "train loss=0.07020670444516564(Epoch= 3655)\n",
      "CV loss=0.12940860308907692(Epoch= 3656)\n",
      "train loss=0.07018706195927146(Epoch= 3656)\n",
      "CV loss=0.1292993414946998(Epoch= 3657)\n",
      "train loss=0.07016806021449845(Epoch= 3657)\n",
      "CV loss=0.12933037233949185(Epoch= 3658)\n",
      "train loss=0.07014852497777023(Epoch= 3658)\n",
      "CV loss=0.1292414162032075(Epoch= 3659)\n",
      "train loss=0.07014226844463756(Epoch= 3659)\n",
      "CV loss=0.1292069465496067(Epoch= 3660)\n",
      "train loss=0.07012392390913352(Epoch= 3660)\n",
      "CV loss=0.1292412420671738(Epoch= 3661)\n",
      "train loss=0.07009802663724206(Epoch= 3661)\n",
      "CV loss=0.12948618620169416(Epoch= 3662)\n",
      "train loss=0.07009606417930658(Epoch= 3662)\n",
      "CV loss=0.12922590917814233(Epoch= 3663)\n",
      "train loss=0.0700626064275965(Epoch= 3663)\n",
      "CV loss=0.1292999651547618(Epoch= 3664)\n",
      "train loss=0.07006339571706482(Epoch= 3664)\n",
      "CV loss=0.12907184899807297(Epoch= 3665)\n",
      "train loss=0.0700387974023691(Epoch= 3665)\n",
      "CV loss=0.12901339664248318(Epoch= 3666)\n",
      "train loss=0.07001671671978128(Epoch= 3666)\n",
      "CV loss=0.1291849850328089(Epoch= 3667)\n",
      "train loss=0.06999271534971244(Epoch= 3667)\n",
      "CV loss=0.12908852117500888(Epoch= 3668)\n",
      "train loss=0.06997782260222978(Epoch= 3668)\n",
      "CV loss=0.12911899871016141(Epoch= 3669)\n",
      "train loss=0.06996754515806673(Epoch= 3669)\n",
      "CV loss=0.12896049244641478(Epoch= 3670)\n",
      "train loss=0.06995553633089083(Epoch= 3670)\n",
      "CV loss=0.12912066490742194(Epoch= 3671)\n",
      "train loss=0.06993439916707475(Epoch= 3671)\n",
      "CV loss=0.12901319658615754(Epoch= 3672)\n",
      "train loss=0.06991311673933495(Epoch= 3672)\n",
      "CV loss=0.12892007221305415(Epoch= 3673)\n",
      "train loss=0.06989575481576621(Epoch= 3673)\n",
      "CV loss=0.12897806355869285(Epoch= 3674)\n",
      "train loss=0.06987688754811217(Epoch= 3674)\n",
      "CV loss=0.12906208462284371(Epoch= 3675)\n",
      "train loss=0.0698615498410626(Epoch= 3675)\n",
      "CV loss=0.12899435944668464(Epoch= 3676)\n",
      "train loss=0.06984140636564241(Epoch= 3676)\n",
      "CV loss=0.1290043994574637(Epoch= 3677)\n",
      "train loss=0.0698375749213889(Epoch= 3677)\n",
      "CV loss=0.12891894393125852(Epoch= 3678)\n",
      "train loss=0.06980938947075524(Epoch= 3678)\n",
      "CV loss=0.12893025351307055(Epoch= 3679)\n",
      "train loss=0.06980675493207834(Epoch= 3679)\n",
      "CV loss=0.12886861733078403(Epoch= 3680)\n",
      "train loss=0.0697899362372754(Epoch= 3680)\n",
      "CV loss=0.12896220373391565(Epoch= 3681)\n",
      "train loss=0.06975694740233103(Epoch= 3681)\n",
      "CV loss=0.1291188532880892(Epoch= 3682)\n",
      "train loss=0.06975010147654946(Epoch= 3682)\n",
      "CV loss=0.12899893261648365(Epoch= 3683)\n",
      "train loss=0.06973537362345417(Epoch= 3683)\n",
      "CV loss=0.12888666322353928(Epoch= 3684)\n",
      "train loss=0.06971170221331216(Epoch= 3684)\n",
      "CV loss=0.12899502450971415(Epoch= 3685)\n",
      "train loss=0.0696979164251415(Epoch= 3685)\n",
      "CV loss=0.1289265729204782(Epoch= 3686)\n",
      "train loss=0.06968078765957451(Epoch= 3686)\n",
      "CV loss=0.12877468337301284(Epoch= 3687)\n",
      "train loss=0.06966704334554784(Epoch= 3687)\n",
      "CV loss=0.12896455369645582(Epoch= 3688)\n",
      "train loss=0.0696521921577091(Epoch= 3688)\n",
      "CV loss=0.12876423246713886(Epoch= 3689)\n",
      "train loss=0.06962393166513615(Epoch= 3689)\n",
      "CV loss=0.12875987169513528(Epoch= 3690)\n",
      "train loss=0.06960900197251982(Epoch= 3690)\n",
      "CV loss=0.1287644842962542(Epoch= 3691)\n",
      "train loss=0.06960258377032978(Epoch= 3691)\n",
      "CV loss=0.12882338310590585(Epoch= 3692)\n",
      "train loss=0.06958500989809473(Epoch= 3692)\n",
      "CV loss=0.12862493792272722(Epoch= 3693)\n",
      "train loss=0.06956187306851747(Epoch= 3693)\n",
      "CV loss=0.12869280351445755(Epoch= 3694)\n",
      "train loss=0.06955397654810296(Epoch= 3694)\n",
      "CV loss=0.1287053825571472(Epoch= 3695)\n",
      "train loss=0.06952507371408648(Epoch= 3695)\n",
      "CV loss=0.12872874223414948(Epoch= 3696)\n",
      "train loss=0.06951222430963545(Epoch= 3696)\n",
      "CV loss=0.12879959508979832(Epoch= 3697)\n",
      "train loss=0.06950046517630286(Epoch= 3697)\n",
      "CV loss=0.12853478032995308(Epoch= 3698)\n",
      "train loss=0.06948039737970345(Epoch= 3698)\n",
      "CV loss=0.12870909690058208(Epoch= 3699)\n",
      "train loss=0.06946144246370589(Epoch= 3699)\n",
      "CV loss=0.12858859173731668(Epoch= 3700)\n",
      "train loss=0.06945634288628018(Epoch= 3700)\n",
      "CV loss=0.1285674311239311(Epoch= 3701)\n",
      "train loss=0.06942748817569258(Epoch= 3701)\n",
      "CV loss=0.1285327737943405(Epoch= 3702)\n",
      "train loss=0.06941182503667763(Epoch= 3702)\n",
      "CV loss=0.12841789710681026(Epoch= 3703)\n",
      "train loss=0.06939436680363176(Epoch= 3703)\n",
      "CV loss=0.12844196505443126(Epoch= 3704)\n",
      "train loss=0.06937840433113666(Epoch= 3704)\n",
      "CV loss=0.12835207996102252(Epoch= 3705)\n",
      "train loss=0.06936657195197017(Epoch= 3705)\n",
      "CV loss=0.12840764523789278(Epoch= 3706)\n",
      "train loss=0.0693450627001287(Epoch= 3706)\n",
      "CV loss=0.12847761905127164(Epoch= 3707)\n",
      "train loss=0.06932715800604838(Epoch= 3707)\n",
      "CV loss=0.128503659448976(Epoch= 3708)\n",
      "train loss=0.06931154173497081(Epoch= 3708)\n",
      "CV loss=0.12827135496141345(Epoch= 3709)\n",
      "train loss=0.06929840456549648(Epoch= 3709)\n",
      "CV loss=0.12842066260339274(Epoch= 3710)\n",
      "train loss=0.06928084874819333(Epoch= 3710)\n",
      "CV loss=0.12848131028544185(Epoch= 3711)\n",
      "train loss=0.06926787943349325(Epoch= 3711)\n",
      "CV loss=0.12820723009444418(Epoch= 3712)\n",
      "train loss=0.0692604500251093(Epoch= 3712)\n",
      "CV loss=0.1282916807809255(Epoch= 3713)\n",
      "train loss=0.06923037006343345(Epoch= 3713)\n",
      "CV loss=0.12854514536934197(Epoch= 3714)\n",
      "train loss=0.06922422313003601(Epoch= 3714)\n",
      "CV loss=0.12826530620803595(Epoch= 3715)\n",
      "train loss=0.06919620677479574(Epoch= 3715)\n",
      "CV loss=0.12830937391493258(Epoch= 3716)\n",
      "train loss=0.06918530009604584(Epoch= 3716)\n",
      "CV loss=0.1283550327342876(Epoch= 3717)\n",
      "train loss=0.06916727496522841(Epoch= 3717)\n",
      "CV loss=0.1283200031001009(Epoch= 3718)\n",
      "train loss=0.06914969766201112(Epoch= 3718)\n",
      "CV loss=0.12836359589864726(Epoch= 3719)\n",
      "train loss=0.06915245798458536(Epoch= 3719)\n",
      "CV loss=0.12829950353415953(Epoch= 3720)\n",
      "train loss=0.06912283908385825(Epoch= 3720)\n",
      "CV loss=0.12817097601481675(Epoch= 3721)\n",
      "train loss=0.06910057940656253(Epoch= 3721)\n",
      "CV loss=0.12807721265076935(Epoch= 3722)\n",
      "train loss=0.0690879799070632(Epoch= 3722)\n",
      "CV loss=0.12804702821614944(Epoch= 3723)\n",
      "train loss=0.06907146697508942(Epoch= 3723)\n",
      "CV loss=0.12812158534601564(Epoch= 3724)\n",
      "train loss=0.06905667046143966(Epoch= 3724)\n",
      "CV loss=0.12810069532780488(Epoch= 3725)\n",
      "train loss=0.0690373473495325(Epoch= 3725)\n",
      "CV loss=0.12801533401628062(Epoch= 3726)\n",
      "train loss=0.06902344500114435(Epoch= 3726)\n",
      "CV loss=0.12819840337769334(Epoch= 3727)\n",
      "train loss=0.06900185818433476(Epoch= 3727)\n",
      "CV loss=0.12805690316199342(Epoch= 3728)\n",
      "train loss=0.06898686122041099(Epoch= 3728)\n",
      "CV loss=0.12799432212169135(Epoch= 3729)\n",
      "train loss=0.06897057816215604(Epoch= 3729)\n",
      "CV loss=0.12790030946690228(Epoch= 3730)\n",
      "train loss=0.06897236492172773(Epoch= 3730)\n",
      "CV loss=0.12813564862522495(Epoch= 3731)\n",
      "train loss=0.06894102942723378(Epoch= 3731)\n",
      "CV loss=0.1282511927014514(Epoch= 3732)\n",
      "train loss=0.0689338499419893(Epoch= 3732)\n",
      "CV loss=0.12796581427869108(Epoch= 3733)\n",
      "train loss=0.06891366898485118(Epoch= 3733)\n",
      "CV loss=0.12798512981321486(Epoch= 3734)\n",
      "train loss=0.06889035362347012(Epoch= 3734)\n",
      "CV loss=0.1278887305243845(Epoch= 3735)\n",
      "train loss=0.06888014420358562(Epoch= 3735)\n",
      "CV loss=0.1280997529310866(Epoch= 3736)\n",
      "train loss=0.06886593187000425(Epoch= 3736)\n",
      "CV loss=0.12808142695215186(Epoch= 3737)\n",
      "train loss=0.06885370485242005(Epoch= 3737)\n",
      "CV loss=0.12795972374657538(Epoch= 3738)\n",
      "train loss=0.06882850788171854(Epoch= 3738)\n",
      "CV loss=0.12790314880713505(Epoch= 3739)\n",
      "train loss=0.06881125790928812(Epoch= 3739)\n",
      "CV loss=0.12785990183365825(Epoch= 3740)\n",
      "train loss=0.06879372188882912(Epoch= 3740)\n",
      "CV loss=0.1277648799242772(Epoch= 3741)\n",
      "train loss=0.06877847299727965(Epoch= 3741)\n",
      "CV loss=0.12794629407786245(Epoch= 3742)\n",
      "train loss=0.06876341900955309(Epoch= 3742)\n",
      "CV loss=0.12781377942754837(Epoch= 3743)\n",
      "train loss=0.06874704864445114(Epoch= 3743)\n",
      "CV loss=0.12775640155951817(Epoch= 3744)\n",
      "train loss=0.0687339529458473(Epoch= 3744)\n",
      "CV loss=0.12797247385416044(Epoch= 3745)\n",
      "train loss=0.06872313634507408(Epoch= 3745)\n",
      "CV loss=0.127636294032958(Epoch= 3746)\n",
      "train loss=0.06870066102215658(Epoch= 3746)\n",
      "CV loss=0.1277752330097789(Epoch= 3747)\n",
      "train loss=0.06868032296888947(Epoch= 3747)\n",
      "CV loss=0.12762283265766244(Epoch= 3748)\n",
      "train loss=0.06867441100177576(Epoch= 3748)\n",
      "CV loss=0.12785169514320197(Epoch= 3749)\n",
      "train loss=0.06864938972444966(Epoch= 3749)\n",
      "CV loss=0.12792454040096163(Epoch= 3750)\n",
      "train loss=0.06864687614587286(Epoch= 3750)\n",
      "CV loss=0.1276840312484584(Epoch= 3751)\n",
      "train loss=0.06862761800220976(Epoch= 3751)\n",
      "CV loss=0.12764004509352223(Epoch= 3752)\n",
      "train loss=0.0686021334388898(Epoch= 3752)\n",
      "CV loss=0.1277985978881538(Epoch= 3753)\n",
      "train loss=0.06858974726079495(Epoch= 3753)\n",
      "CV loss=0.12753537980356447(Epoch= 3754)\n",
      "train loss=0.06857791079405018(Epoch= 3754)\n",
      "CV loss=0.12770814464789124(Epoch= 3755)\n",
      "train loss=0.06856390571589929(Epoch= 3755)\n",
      "CV loss=0.12755053447026654(Epoch= 3756)\n",
      "train loss=0.068547025047671(Epoch= 3756)\n",
      "CV loss=0.12759477655780158(Epoch= 3757)\n",
      "train loss=0.06854443331729802(Epoch= 3757)\n",
      "CV loss=0.12759882456871133(Epoch= 3758)\n",
      "train loss=0.06850869486228799(Epoch= 3758)\n",
      "CV loss=0.1277096513631034(Epoch= 3759)\n",
      "train loss=0.0684975967269538(Epoch= 3759)\n",
      "CV loss=0.127679588497217(Epoch= 3760)\n",
      "train loss=0.06847897639450944(Epoch= 3760)\n",
      "CV loss=0.1274493147372707(Epoch= 3761)\n",
      "train loss=0.06846564140154768(Epoch= 3761)\n",
      "CV loss=0.12757262615316933(Epoch= 3762)\n",
      "train loss=0.06845740349517693(Epoch= 3762)\n",
      "CV loss=0.12751009779237016(Epoch= 3763)\n",
      "train loss=0.06842670828055186(Epoch= 3763)\n",
      "CV loss=0.12744907578025463(Epoch= 3764)\n",
      "train loss=0.06841222678851978(Epoch= 3764)\n",
      "CV loss=0.1274624234256952(Epoch= 3765)\n",
      "train loss=0.06840340621985765(Epoch= 3765)\n",
      "CV loss=0.12748102963018737(Epoch= 3766)\n",
      "train loss=0.06839095448447517(Epoch= 3766)\n",
      "CV loss=0.12751390169544788(Epoch= 3767)\n",
      "train loss=0.06836691806282091(Epoch= 3767)\n",
      "CV loss=0.12726806708763339(Epoch= 3768)\n",
      "train loss=0.06835465961749732(Epoch= 3768)\n",
      "CV loss=0.12742855140545417(Epoch= 3769)\n",
      "train loss=0.06833529985911105(Epoch= 3769)\n",
      "CV loss=0.12741059049271744(Epoch= 3770)\n",
      "train loss=0.06831958317466456(Epoch= 3770)\n",
      "CV loss=0.12730638779647727(Epoch= 3771)\n",
      "train loss=0.06830193361506864(Epoch= 3771)\n",
      "CV loss=0.1273537067063414(Epoch= 3772)\n",
      "train loss=0.0682905109528159(Epoch= 3772)\n",
      "CV loss=0.12740467361872063(Epoch= 3773)\n",
      "train loss=0.0682767804181307(Epoch= 3773)\n",
      "CV loss=0.12717410685281091(Epoch= 3774)\n",
      "train loss=0.06826688488313619(Epoch= 3774)\n",
      "CV loss=0.12721134555905672(Epoch= 3775)\n",
      "train loss=0.06824474515734189(Epoch= 3775)\n",
      "CV loss=0.12717826665932963(Epoch= 3776)\n",
      "train loss=0.06822754864456937(Epoch= 3776)\n",
      "CV loss=0.1272736522090365(Epoch= 3777)\n",
      "train loss=0.06821434632596507(Epoch= 3777)\n",
      "CV loss=0.1273197269529622(Epoch= 3778)\n",
      "train loss=0.06819726122046998(Epoch= 3778)\n",
      "CV loss=0.127050347076525(Epoch= 3779)\n",
      "train loss=0.06819585285484515(Epoch= 3779)\n",
      "CV loss=0.12704859575512684(Epoch= 3780)\n",
      "train loss=0.06817095378847526(Epoch= 3780)\n",
      "CV loss=0.1272385310819362(Epoch= 3781)\n",
      "train loss=0.06814705453094427(Epoch= 3781)\n",
      "CV loss=0.12698249029888364(Epoch= 3782)\n",
      "train loss=0.06814189122867026(Epoch= 3782)\n",
      "CV loss=0.12696537505975314(Epoch= 3783)\n",
      "train loss=0.06813231742559517(Epoch= 3783)\n",
      "CV loss=0.1272004957235664(Epoch= 3784)\n",
      "train loss=0.06810321510062424(Epoch= 3784)\n",
      "CV loss=0.1272546847633533(Epoch= 3785)\n",
      "train loss=0.06808718027726623(Epoch= 3785)\n",
      "CV loss=0.12696681990263242(Epoch= 3786)\n",
      "train loss=0.06808293395997009(Epoch= 3786)\n",
      "CV loss=0.1272410316541676(Epoch= 3787)\n",
      "train loss=0.06805770886276352(Epoch= 3787)\n",
      "CV loss=0.127042777200209(Epoch= 3788)\n",
      "train loss=0.06805092116268396(Epoch= 3788)\n",
      "CV loss=0.1272548219812439(Epoch= 3789)\n",
      "train loss=0.0680306796845254(Epoch= 3789)\n",
      "CV loss=0.12710698277009905(Epoch= 3790)\n",
      "train loss=0.06801064603454637(Epoch= 3790)\n",
      "CV loss=0.12712225968865998(Epoch= 3791)\n",
      "train loss=0.06800187565953694(Epoch= 3791)\n",
      "CV loss=0.1269767244384124(Epoch= 3792)\n",
      "train loss=0.06798046142831311(Epoch= 3792)\n",
      "CV loss=0.1269982926974415(Epoch= 3793)\n",
      "train loss=0.06796430726688844(Epoch= 3793)\n",
      "CV loss=0.12703620181236247(Epoch= 3794)\n",
      "train loss=0.0679526191133162(Epoch= 3794)\n",
      "CV loss=0.12697900352720307(Epoch= 3795)\n",
      "train loss=0.06795027196019963(Epoch= 3795)\n",
      "CV loss=0.12684447994629386(Epoch= 3796)\n",
      "train loss=0.06791756420032352(Epoch= 3796)\n",
      "CV loss=0.12704502910796267(Epoch= 3797)\n",
      "train loss=0.06791110119918957(Epoch= 3797)\n",
      "CV loss=0.12680526379836213(Epoch= 3798)\n",
      "train loss=0.06789167795532536(Epoch= 3798)\n",
      "CV loss=0.12688462962164604(Epoch= 3799)\n",
      "train loss=0.0678750815242922(Epoch= 3799)\n",
      "CV loss=0.12695701616717742(Epoch= 3800)\n",
      "train loss=0.0678617371897863(Epoch= 3800)\n",
      "CV loss=0.12674837989663856(Epoch= 3801)\n",
      "train loss=0.06784934326612463(Epoch= 3801)\n",
      "CV loss=0.1267340258125303(Epoch= 3802)\n",
      "train loss=0.06782831458727445(Epoch= 3802)\n",
      "CV loss=0.12693230481475098(Epoch= 3803)\n",
      "train loss=0.06781532321035659(Epoch= 3803)\n",
      "CV loss=0.12701272797956362(Epoch= 3804)\n",
      "train loss=0.0678015885691654(Epoch= 3804)\n",
      "CV loss=0.12680939589309928(Epoch= 3805)\n",
      "train loss=0.0677817097200723(Epoch= 3805)\n",
      "CV loss=0.1267255715800224(Epoch= 3806)\n",
      "train loss=0.06776792780799996(Epoch= 3806)\n",
      "CV loss=0.12666455434411583(Epoch= 3807)\n",
      "train loss=0.06775889798134782(Epoch= 3807)\n",
      "CV loss=0.12677095208899633(Epoch= 3808)\n",
      "train loss=0.06773641818331565(Epoch= 3808)\n",
      "CV loss=0.12675576064279664(Epoch= 3809)\n",
      "train loss=0.06772004054846582(Epoch= 3809)\n",
      "CV loss=0.12682310827463028(Epoch= 3810)\n",
      "train loss=0.0677195772242511(Epoch= 3810)\n",
      "CV loss=0.12684360697927108(Epoch= 3811)\n",
      "train loss=0.06769646999213261(Epoch= 3811)\n",
      "CV loss=0.12668277553703364(Epoch= 3812)\n",
      "train loss=0.06768073945464187(Epoch= 3812)\n",
      "CV loss=0.12683593504890064(Epoch= 3813)\n",
      "train loss=0.06766602254643787(Epoch= 3813)\n",
      "CV loss=0.12652236746412326(Epoch= 3814)\n",
      "train loss=0.0676541915679589(Epoch= 3814)\n",
      "CV loss=0.12674825733566347(Epoch= 3815)\n",
      "train loss=0.06763055445374484(Epoch= 3815)\n",
      "CV loss=0.12670346514964625(Epoch= 3816)\n",
      "train loss=0.0676151555491322(Epoch= 3816)\n",
      "CV loss=0.1266303913759072(Epoch= 3817)\n",
      "train loss=0.06760015043002802(Epoch= 3817)\n",
      "CV loss=0.12656187266182795(Epoch= 3818)\n",
      "train loss=0.0675887064333224(Epoch= 3818)\n",
      "CV loss=0.12656359969335557(Epoch= 3819)\n",
      "train loss=0.06757046636864228(Epoch= 3819)\n",
      "CV loss=0.12659347720222755(Epoch= 3820)\n",
      "train loss=0.06756098654684317(Epoch= 3820)\n",
      "CV loss=0.12664515977799137(Epoch= 3821)\n",
      "train loss=0.06753905434463046(Epoch= 3821)\n",
      "CV loss=0.1265335151927091(Epoch= 3822)\n",
      "train loss=0.06752423668300525(Epoch= 3822)\n",
      "CV loss=0.12655399282909824(Epoch= 3823)\n",
      "train loss=0.06750960250449327(Epoch= 3823)\n",
      "CV loss=0.12650509060742884(Epoch= 3824)\n",
      "train loss=0.06750504696785471(Epoch= 3824)\n",
      "CV loss=0.12641194078577084(Epoch= 3825)\n",
      "train loss=0.06748338628619147(Epoch= 3825)\n",
      "CV loss=0.12648535402424027(Epoch= 3826)\n",
      "train loss=0.06747471261376846(Epoch= 3826)\n",
      "CV loss=0.12648983248919177(Epoch= 3827)\n",
      "train loss=0.0674520645397369(Epoch= 3827)\n",
      "CV loss=0.1265175947588899(Epoch= 3828)\n",
      "train loss=0.0674433187539794(Epoch= 3828)\n",
      "CV loss=0.12644315445441107(Epoch= 3829)\n",
      "train loss=0.06742247653287295(Epoch= 3829)\n",
      "CV loss=0.1265412194366491(Epoch= 3830)\n",
      "train loss=0.06740641581649462(Epoch= 3830)\n",
      "CV loss=0.12635711601173208(Epoch= 3831)\n",
      "train loss=0.06739276569958973(Epoch= 3831)\n",
      "CV loss=0.12639992896980956(Epoch= 3832)\n",
      "train loss=0.0673780240746267(Epoch= 3832)\n",
      "CV loss=0.12644548786530746(Epoch= 3833)\n",
      "train loss=0.067361523292132(Epoch= 3833)\n",
      "CV loss=0.12652308280818864(Epoch= 3834)\n",
      "train loss=0.06734990970267933(Epoch= 3834)\n",
      "CV loss=0.12629812775648(Epoch= 3835)\n",
      "train loss=0.06733582328304645(Epoch= 3835)\n",
      "CV loss=0.1264237178779652(Epoch= 3836)\n",
      "train loss=0.06732085363445181(Epoch= 3836)\n",
      "CV loss=0.12648995200547244(Epoch= 3837)\n",
      "train loss=0.06730561304737519(Epoch= 3837)\n",
      "CV loss=0.12629653207587133(Epoch= 3838)\n",
      "train loss=0.06728835373238384(Epoch= 3838)\n",
      "CV loss=0.12631334479130463(Epoch= 3839)\n",
      "train loss=0.06727702274156964(Epoch= 3839)\n",
      "CV loss=0.12623247331752582(Epoch= 3840)\n",
      "train loss=0.06726906707448053(Epoch= 3840)\n",
      "CV loss=0.12613621418551185(Epoch= 3841)\n",
      "train loss=0.06724684351797272(Epoch= 3841)\n",
      "CV loss=0.12633029115670627(Epoch= 3842)\n",
      "train loss=0.06723741099483176(Epoch= 3842)\n",
      "CV loss=0.12626561304734313(Epoch= 3843)\n",
      "train loss=0.06721320296048713(Epoch= 3843)\n",
      "CV loss=0.12617911988840264(Epoch= 3844)\n",
      "train loss=0.06719857596546319(Epoch= 3844)\n",
      "CV loss=0.12641182829957462(Epoch= 3845)\n",
      "train loss=0.067197755850853(Epoch= 3845)\n",
      "CV loss=0.1264589208308185(Epoch= 3846)\n",
      "train loss=0.06717851856766192(Epoch= 3846)\n",
      "CV loss=0.12613600728605556(Epoch= 3847)\n",
      "train loss=0.06715442307484276(Epoch= 3847)\n",
      "CV loss=0.12620966678768158(Epoch= 3848)\n",
      "train loss=0.06713992814964662(Epoch= 3848)\n",
      "CV loss=0.1260535280691027(Epoch= 3849)\n",
      "train loss=0.0671291583633536(Epoch= 3849)\n",
      "CV loss=0.1260866724237552(Epoch= 3850)\n",
      "train loss=0.06711343620571558(Epoch= 3850)\n",
      "CV loss=0.12616085355307535(Epoch= 3851)\n",
      "train loss=0.06709763142887742(Epoch= 3851)\n",
      "CV loss=0.12595965143676288(Epoch= 3852)\n",
      "train loss=0.06708925179615856(Epoch= 3852)\n",
      "CV loss=0.12607841262643082(Epoch= 3853)\n",
      "train loss=0.06707040610081996(Epoch= 3853)\n",
      "CV loss=0.12594235433906364(Epoch= 3854)\n",
      "train loss=0.06705661906470256(Epoch= 3854)\n",
      "CV loss=0.1260186706196505(Epoch= 3855)\n",
      "train loss=0.06704806434034652(Epoch= 3855)\n",
      "CV loss=0.1260045222143136(Epoch= 3856)\n",
      "train loss=0.06702936489786933(Epoch= 3856)\n",
      "CV loss=0.1260189818236534(Epoch= 3857)\n",
      "train loss=0.06701497674459525(Epoch= 3857)\n",
      "CV loss=0.1259910277565617(Epoch= 3858)\n",
      "train loss=0.06699635789801571(Epoch= 3858)\n",
      "CV loss=0.1259474676551748(Epoch= 3859)\n",
      "train loss=0.0669883859078217(Epoch= 3859)\n",
      "CV loss=0.1259940955025018(Epoch= 3860)\n",
      "train loss=0.06697986529354812(Epoch= 3860)\n",
      "CV loss=0.12610232311262454(Epoch= 3861)\n",
      "train loss=0.06695764436463802(Epoch= 3861)\n",
      "CV loss=0.1259596763989564(Epoch= 3862)\n",
      "train loss=0.06693712101004581(Epoch= 3862)\n",
      "CV loss=0.12599461210709104(Epoch= 3863)\n",
      "train loss=0.06692651546952379(Epoch= 3863)\n",
      "CV loss=0.12579891376558947(Epoch= 3864)\n",
      "train loss=0.06692361152092967(Epoch= 3864)\n",
      "CV loss=0.12598450280400433(Epoch= 3865)\n",
      "train loss=0.06689906051568341(Epoch= 3865)\n",
      "CV loss=0.12581229764735086(Epoch= 3866)\n",
      "train loss=0.06688177073866669(Epoch= 3866)\n",
      "CV loss=0.1258469100079791(Epoch= 3867)\n",
      "train loss=0.06686351013594172(Epoch= 3867)\n",
      "CV loss=0.12583458081175944(Epoch= 3868)\n",
      "train loss=0.06685357039452616(Epoch= 3868)\n",
      "CV loss=0.12569524048158628(Epoch= 3869)\n",
      "train loss=0.06684036865291032(Epoch= 3869)\n",
      "CV loss=0.1257526053598117(Epoch= 3870)\n",
      "train loss=0.06682146827063833(Epoch= 3870)\n",
      "CV loss=0.12584183244387553(Epoch= 3871)\n",
      "train loss=0.06681214954561027(Epoch= 3871)\n",
      "CV loss=0.12582308226930722(Epoch= 3872)\n",
      "train loss=0.06679363152902984(Epoch= 3872)\n",
      "CV loss=0.12592074264907505(Epoch= 3873)\n",
      "train loss=0.06678708133684859(Epoch= 3873)\n",
      "CV loss=0.12561965000707326(Epoch= 3874)\n",
      "train loss=0.0667719246845831(Epoch= 3874)\n",
      "CV loss=0.12570076114033307(Epoch= 3875)\n",
      "train loss=0.06674851704302155(Epoch= 3875)\n",
      "CV loss=0.12575906841390663(Epoch= 3876)\n",
      "train loss=0.06673705969264444(Epoch= 3876)\n",
      "CV loss=0.1255902993322755(Epoch= 3877)\n",
      "train loss=0.0667396525328744(Epoch= 3877)\n",
      "CV loss=0.12572096791565723(Epoch= 3878)\n",
      "train loss=0.06670814843149096(Epoch= 3878)\n",
      "CV loss=0.12551401464653522(Epoch= 3879)\n",
      "train loss=0.0667081453185744(Epoch= 3879)\n",
      "CV loss=0.12560502323341827(Epoch= 3880)\n",
      "train loss=0.06667994255144225(Epoch= 3880)\n",
      "CV loss=0.1256447917150642(Epoch= 3881)\n",
      "train loss=0.06666237131092706(Epoch= 3881)\n",
      "CV loss=0.1256519097260198(Epoch= 3882)\n",
      "train loss=0.06664974561944857(Epoch= 3882)\n",
      "CV loss=0.1256583307052423(Epoch= 3883)\n",
      "train loss=0.06663603735106756(Epoch= 3883)\n",
      "CV loss=0.1256578463699612(Epoch= 3884)\n",
      "train loss=0.06662828161804253(Epoch= 3884)\n",
      "CV loss=0.12556133725887217(Epoch= 3885)\n",
      "train loss=0.06661071889014387(Epoch= 3885)\n",
      "CV loss=0.12552686831827303(Epoch= 3886)\n",
      "train loss=0.06659345961742459(Epoch= 3886)\n",
      "CV loss=0.12574270645312702(Epoch= 3887)\n",
      "train loss=0.0665893729945468(Epoch= 3887)\n",
      "CV loss=0.12546849978981012(Epoch= 3888)\n",
      "train loss=0.06656511642675852(Epoch= 3888)\n",
      "CV loss=0.12564241697851367(Epoch= 3889)\n",
      "train loss=0.06655541316790484(Epoch= 3889)\n",
      "CV loss=0.12567030356212894(Epoch= 3890)\n",
      "train loss=0.06654408884319338(Epoch= 3890)\n",
      "CV loss=0.12559531567629298(Epoch= 3891)\n",
      "train loss=0.06652737440186188(Epoch= 3891)\n",
      "CV loss=0.1254898326131536(Epoch= 3892)\n",
      "train loss=0.06650765823023606(Epoch= 3892)\n",
      "CV loss=0.1254326130904126(Epoch= 3893)\n",
      "train loss=0.066502752108839(Epoch= 3893)\n",
      "CV loss=0.12561977384616335(Epoch= 3894)\n",
      "train loss=0.06648773823409801(Epoch= 3894)\n",
      "CV loss=0.1253818737725282(Epoch= 3895)\n",
      "train loss=0.06646547500681492(Epoch= 3895)\n",
      "CV loss=0.12556673698602328(Epoch= 3896)\n",
      "train loss=0.0664557833954308(Epoch= 3896)\n",
      "CV loss=0.12551584026865373(Epoch= 3897)\n",
      "train loss=0.06644080961226666(Epoch= 3897)\n",
      "CV loss=0.12530412143683467(Epoch= 3898)\n",
      "train loss=0.06642471222283765(Epoch= 3898)\n",
      "CV loss=0.12528706998457(Epoch= 3899)\n",
      "train loss=0.06641352486855912(Epoch= 3899)\n",
      "CV loss=0.12543142963202603(Epoch= 3900)\n",
      "train loss=0.06639785328813978(Epoch= 3900)\n",
      "CV loss=0.12539308324238718(Epoch= 3901)\n",
      "train loss=0.06638343581224941(Epoch= 3901)\n",
      "CV loss=0.12545567764414944(Epoch= 3902)\n",
      "train loss=0.06637738924178622(Epoch= 3902)\n",
      "CV loss=0.1253412009913682(Epoch= 3903)\n",
      "train loss=0.06635466699102298(Epoch= 3903)\n",
      "CV loss=0.12527572882542748(Epoch= 3904)\n",
      "train loss=0.06633903538878001(Epoch= 3904)\n",
      "CV loss=0.12526445341366987(Epoch= 3905)\n",
      "train loss=0.06632944384570502(Epoch= 3905)\n",
      "CV loss=0.12528741384468878(Epoch= 3906)\n",
      "train loss=0.0663137769647466(Epoch= 3906)\n",
      "CV loss=0.12533551492821043(Epoch= 3907)\n",
      "train loss=0.06630336449758191(Epoch= 3907)\n",
      "CV loss=0.12535006815835437(Epoch= 3908)\n",
      "train loss=0.06628771209849264(Epoch= 3908)\n",
      "CV loss=0.12535644178692518(Epoch= 3909)\n",
      "train loss=0.0662763874751463(Epoch= 3909)\n",
      "CV loss=0.1252725180597184(Epoch= 3910)\n",
      "train loss=0.06626335843034169(Epoch= 3910)\n",
      "CV loss=0.12508113147798228(Epoch= 3911)\n",
      "train loss=0.06624878959213341(Epoch= 3911)\n",
      "CV loss=0.1251177233426195(Epoch= 3912)\n",
      "train loss=0.06623328772562138(Epoch= 3912)\n",
      "CV loss=0.12546554894556833(Epoch= 3913)\n",
      "train loss=0.06625039469996913(Epoch= 3913)\n",
      "CV loss=0.12521590282789824(Epoch= 3914)\n",
      "train loss=0.06620429637807931(Epoch= 3914)\n",
      "CV loss=0.12510407628368986(Epoch= 3915)\n",
      "train loss=0.06618629806995976(Epoch= 3915)\n",
      "CV loss=0.12500622839380912(Epoch= 3916)\n",
      "train loss=0.06618179869405688(Epoch= 3916)\n",
      "CV loss=0.1252067373913569(Epoch= 3917)\n",
      "train loss=0.06616486692291947(Epoch= 3917)\n",
      "CV loss=0.1251565294237738(Epoch= 3918)\n",
      "train loss=0.06614831002106676(Epoch= 3918)\n",
      "CV loss=0.12496418622897146(Epoch= 3919)\n",
      "train loss=0.06613415800097676(Epoch= 3919)\n",
      "CV loss=0.12517566997364538(Epoch= 3920)\n",
      "train loss=0.06612434755129067(Epoch= 3920)\n",
      "CV loss=0.12496599004849657(Epoch= 3921)\n",
      "train loss=0.06610541875879057(Epoch= 3921)\n",
      "CV loss=0.1251816768992285(Epoch= 3922)\n",
      "train loss=0.06609410767585197(Epoch= 3922)\n",
      "CV loss=0.12496804700547542(Epoch= 3923)\n",
      "train loss=0.06608097561872799(Epoch= 3923)\n",
      "CV loss=0.1251585329023125(Epoch= 3924)\n",
      "train loss=0.06607176090554752(Epoch= 3924)\n",
      "CV loss=0.12494571248386019(Epoch= 3925)\n",
      "train loss=0.06604877261718445(Epoch= 3925)\n",
      "CV loss=0.12520661212009682(Epoch= 3926)\n",
      "train loss=0.06605143369165367(Epoch= 3926)\n",
      "CV loss=0.1250520243069156(Epoch= 3927)\n",
      "train loss=0.0660259864071937(Epoch= 3927)\n",
      "CV loss=0.1249462744946945(Epoch= 3928)\n",
      "train loss=0.06600777889792786(Epoch= 3928)\n",
      "CV loss=0.12495026941907489(Epoch= 3929)\n",
      "train loss=0.06599509155242117(Epoch= 3929)\n",
      "CV loss=0.12480100882772674(Epoch= 3930)\n",
      "train loss=0.06598498465936387(Epoch= 3930)\n",
      "CV loss=0.12499081086779237(Epoch= 3931)\n",
      "train loss=0.06596882846029889(Epoch= 3931)\n",
      "CV loss=0.12505173092695748(Epoch= 3932)\n",
      "train loss=0.06596030044338358(Epoch= 3932)\n",
      "CV loss=0.12493172163818028(Epoch= 3933)\n",
      "train loss=0.06593984526762299(Epoch= 3933)\n",
      "CV loss=0.12486919908070165(Epoch= 3934)\n",
      "train loss=0.06592474080932084(Epoch= 3934)\n",
      "CV loss=0.12471606557909812(Epoch= 3935)\n",
      "train loss=0.06591516028769155(Epoch= 3935)\n",
      "CV loss=0.12477710524865473(Epoch= 3936)\n",
      "train loss=0.0659005193600216(Epoch= 3936)\n",
      "CV loss=0.12483419534453769(Epoch= 3937)\n",
      "train loss=0.06588655078235933(Epoch= 3937)\n",
      "CV loss=0.12463963089311422(Epoch= 3938)\n",
      "train loss=0.06587490705464562(Epoch= 3938)\n",
      "CV loss=0.124668721912464(Epoch= 3939)\n",
      "train loss=0.06585764533307684(Epoch= 3939)\n",
      "CV loss=0.12475408978421731(Epoch= 3940)\n",
      "train loss=0.06584619862090121(Epoch= 3940)\n",
      "CV loss=0.12476335281468424(Epoch= 3941)\n",
      "train loss=0.06583180149462296(Epoch= 3941)\n",
      "CV loss=0.12475945639803371(Epoch= 3942)\n",
      "train loss=0.06582160390526827(Epoch= 3942)\n",
      "CV loss=0.1245892336948567(Epoch= 3943)\n",
      "train loss=0.06580783045380979(Epoch= 3943)\n",
      "CV loss=0.12485689354192045(Epoch= 3944)\n",
      "train loss=0.06579378438318066(Epoch= 3944)\n",
      "CV loss=0.12461669877927245(Epoch= 3945)\n",
      "train loss=0.06577756963699176(Epoch= 3945)\n",
      "CV loss=0.12455937066658186(Epoch= 3946)\n",
      "train loss=0.0657680391330179(Epoch= 3946)\n",
      "CV loss=0.12463683738569378(Epoch= 3947)\n",
      "train loss=0.06574905090497597(Epoch= 3947)\n",
      "CV loss=0.1247451037990907(Epoch= 3948)\n",
      "train loss=0.06573926891208115(Epoch= 3948)\n",
      "CV loss=0.12476327815086843(Epoch= 3949)\n",
      "train loss=0.06572175608200526(Epoch= 3949)\n",
      "CV loss=0.12463592626444168(Epoch= 3950)\n",
      "train loss=0.06570994445023333(Epoch= 3950)\n",
      "CV loss=0.1244299460357301(Epoch= 3951)\n",
      "train loss=0.06570516763755649(Epoch= 3951)\n",
      "CV loss=0.124652068708999(Epoch= 3952)\n",
      "train loss=0.06568208488647381(Epoch= 3952)\n",
      "CV loss=0.12467426655234251(Epoch= 3953)\n",
      "train loss=0.06567350474448544(Epoch= 3953)\n",
      "CV loss=0.12451674928301604(Epoch= 3954)\n",
      "train loss=0.06565654282645833(Epoch= 3954)\n",
      "CV loss=0.12447497137632862(Epoch= 3955)\n",
      "train loss=0.06564576625646781(Epoch= 3955)\n",
      "CV loss=0.1245845506834665(Epoch= 3956)\n",
      "train loss=0.06563152969254048(Epoch= 3956)\n",
      "CV loss=0.12451823118053465(Epoch= 3957)\n",
      "train loss=0.0656165366876857(Epoch= 3957)\n",
      "CV loss=0.1244134469911815(Epoch= 3958)\n",
      "train loss=0.06560334345718542(Epoch= 3958)\n",
      "CV loss=0.12443573510089956(Epoch= 3959)\n",
      "train loss=0.0656060244081801(Epoch= 3959)\n",
      "CV loss=0.12448107838538859(Epoch= 3960)\n",
      "train loss=0.06557611463076488(Epoch= 3960)\n",
      "CV loss=0.12437240679769702(Epoch= 3961)\n",
      "train loss=0.06556214548578805(Epoch= 3961)\n",
      "CV loss=0.12442562517676631(Epoch= 3962)\n",
      "train loss=0.06555025509267037(Epoch= 3962)\n",
      "CV loss=0.12441876948069389(Epoch= 3963)\n",
      "train loss=0.06553353140361308(Epoch= 3963)\n",
      "CV loss=0.12449649990868746(Epoch= 3964)\n",
      "train loss=0.0655229583423141(Epoch= 3964)\n",
      "CV loss=0.12419713645265557(Epoch= 3965)\n",
      "train loss=0.0655159678712929(Epoch= 3965)\n",
      "CV loss=0.12442900899582827(Epoch= 3966)\n",
      "train loss=0.06549781982572371(Epoch= 3966)\n",
      "CV loss=0.124315748340068(Epoch= 3967)\n",
      "train loss=0.06548441013938142(Epoch= 3967)\n",
      "CV loss=0.12429627204913352(Epoch= 3968)\n",
      "train loss=0.0654775559482215(Epoch= 3968)\n",
      "CV loss=0.12419254047776537(Epoch= 3969)\n",
      "train loss=0.06545844803426225(Epoch= 3969)\n",
      "CV loss=0.12444060492608157(Epoch= 3970)\n",
      "train loss=0.06544495256714289(Epoch= 3970)\n",
      "CV loss=0.12441701714739466(Epoch= 3971)\n",
      "train loss=0.06543286871482303(Epoch= 3971)\n",
      "CV loss=0.12442889611965166(Epoch= 3972)\n",
      "train loss=0.06542755590761752(Epoch= 3972)\n",
      "CV loss=0.12427642158146748(Epoch= 3973)\n",
      "train loss=0.06540168639691181(Epoch= 3973)\n",
      "CV loss=0.12425139658319648(Epoch= 3974)\n",
      "train loss=0.06539826977404034(Epoch= 3974)\n",
      "CV loss=0.1243160610583852(Epoch= 3975)\n",
      "train loss=0.06537480629700627(Epoch= 3975)\n",
      "CV loss=0.12416577039427129(Epoch= 3976)\n",
      "train loss=0.06536482515200684(Epoch= 3976)\n",
      "CV loss=0.12422502904433737(Epoch= 3977)\n",
      "train loss=0.0653496450943136(Epoch= 3977)\n",
      "CV loss=0.12438801705504089(Epoch= 3978)\n",
      "train loss=0.0653463761352224(Epoch= 3978)\n",
      "CV loss=0.1242553838639075(Epoch= 3979)\n",
      "train loss=0.0653293163594593(Epoch= 3979)\n",
      "CV loss=0.1241381136069383(Epoch= 3980)\n",
      "train loss=0.06531225723227563(Epoch= 3980)\n",
      "CV loss=0.12417120597130629(Epoch= 3981)\n",
      "train loss=0.06529997730637947(Epoch= 3981)\n",
      "CV loss=0.12443154387159173(Epoch= 3982)\n",
      "train loss=0.0652999888712531(Epoch= 3982)\n",
      "CV loss=0.1242632524726873(Epoch= 3983)\n",
      "train loss=0.06527197074867791(Epoch= 3983)\n",
      "CV loss=0.12437485225958075(Epoch= 3984)\n",
      "train loss=0.06526425762319049(Epoch= 3984)\n",
      "CV loss=0.12419973157206515(Epoch= 3985)\n",
      "train loss=0.06525790431733916(Epoch= 3985)\n",
      "CV loss=0.12427251473733893(Epoch= 3986)\n",
      "train loss=0.06523717633378584(Epoch= 3986)\n",
      "CV loss=0.1242408145342326(Epoch= 3987)\n",
      "train loss=0.06523338330241293(Epoch= 3987)\n",
      "CV loss=0.12405287707274375(Epoch= 3988)\n",
      "train loss=0.06520259738635298(Epoch= 3988)\n",
      "CV loss=0.12407859602282405(Epoch= 3989)\n",
      "train loss=0.06519607779398975(Epoch= 3989)\n",
      "CV loss=0.12405591277697556(Epoch= 3990)\n",
      "train loss=0.06518455568689739(Epoch= 3990)\n",
      "CV loss=0.12396136546307282(Epoch= 3991)\n",
      "train loss=0.06516992365493524(Epoch= 3991)\n",
      "CV loss=0.12408434635347984(Epoch= 3992)\n",
      "train loss=0.06515309802492941(Epoch= 3992)\n",
      "CV loss=0.12395652097470736(Epoch= 3993)\n",
      "train loss=0.06514125772868098(Epoch= 3993)\n",
      "CV loss=0.12400601103543751(Epoch= 3994)\n",
      "train loss=0.06513371209227826(Epoch= 3994)\n",
      "CV loss=0.1239052461233727(Epoch= 3995)\n",
      "train loss=0.06511577097752001(Epoch= 3995)\n",
      "CV loss=0.12404655827295626(Epoch= 3996)\n",
      "train loss=0.06510012984195257(Epoch= 3996)\n",
      "CV loss=0.12407629165211488(Epoch= 3997)\n",
      "train loss=0.0650937574270306(Epoch= 3997)\n",
      "CV loss=0.12393921456053963(Epoch= 3998)\n",
      "train loss=0.06509175219512571(Epoch= 3998)\n",
      "CV loss=0.12405642397870886(Epoch= 3999)\n",
      "train loss=0.06506213475234206(Epoch= 3999)\n",
      "CV loss=0.12384611442362595(Epoch= 4000)\n",
      "train loss=0.06505733513253871(Epoch= 4000)\n",
      "CV loss=0.1239762646682329(Epoch= 4001)\n",
      "train loss=0.06503873070848551(Epoch= 4001)\n",
      "CV loss=0.12402562140873971(Epoch= 4002)\n",
      "train loss=0.06502792768465507(Epoch= 4002)\n",
      "CV loss=0.12389825327538329(Epoch= 4003)\n",
      "train loss=0.06501002241255815(Epoch= 4003)\n",
      "CV loss=0.12385096891854319(Epoch= 4004)\n",
      "train loss=0.06500396184769397(Epoch= 4004)\n",
      "CV loss=0.12378571838118574(Epoch= 4005)\n",
      "train loss=0.06498765279191815(Epoch= 4005)\n",
      "CV loss=0.12383638409905105(Epoch= 4006)\n",
      "train loss=0.06497724507262005(Epoch= 4006)\n",
      "CV loss=0.1239420958698852(Epoch= 4007)\n",
      "train loss=0.06496322803562869(Epoch= 4007)\n",
      "CV loss=0.12383888660116779(Epoch= 4008)\n",
      "train loss=0.06495288350222078(Epoch= 4008)\n",
      "CV loss=0.12370435320524728(Epoch= 4009)\n",
      "train loss=0.06493821429813496(Epoch= 4009)\n",
      "CV loss=0.12371937793886598(Epoch= 4010)\n",
      "train loss=0.06492310231907567(Epoch= 4010)\n",
      "CV loss=0.12399109157986624(Epoch= 4011)\n",
      "train loss=0.06492438619244788(Epoch= 4011)\n",
      "CV loss=0.12374900385875928(Epoch= 4012)\n",
      "train loss=0.06489270475409756(Epoch= 4012)\n",
      "CV loss=0.1238698353790741(Epoch= 4013)\n",
      "train loss=0.06488804950583957(Epoch= 4013)\n",
      "CV loss=0.12369219861836149(Epoch= 4014)\n",
      "train loss=0.06487017146007615(Epoch= 4014)\n",
      "CV loss=0.12372890165958453(Epoch= 4015)\n",
      "train loss=0.0648575968107044(Epoch= 4015)\n",
      "CV loss=0.12358860463603547(Epoch= 4016)\n",
      "train loss=0.0648447228543894(Epoch= 4016)\n",
      "CV loss=0.12394416670880351(Epoch= 4017)\n",
      "train loss=0.06484887997743854(Epoch= 4017)\n",
      "CV loss=0.12369762406658702(Epoch= 4018)\n",
      "train loss=0.06482576170694475(Epoch= 4018)\n",
      "CV loss=0.12371682539080071(Epoch= 4019)\n",
      "train loss=0.0648104819851789(Epoch= 4019)\n",
      "CV loss=0.1234971150510688(Epoch= 4020)\n",
      "train loss=0.06480544274013901(Epoch= 4020)\n",
      "CV loss=0.12358460335319407(Epoch= 4021)\n",
      "train loss=0.06477958934366548(Epoch= 4021)\n",
      "CV loss=0.12364231822876215(Epoch= 4022)\n",
      "train loss=0.06476544188042845(Epoch= 4022)\n",
      "CV loss=0.12355977328570866(Epoch= 4023)\n",
      "train loss=0.06475990040601097(Epoch= 4023)\n",
      "CV loss=0.1236231081600276(Epoch= 4024)\n",
      "train loss=0.06474522359130368(Epoch= 4024)\n",
      "CV loss=0.12384368354996605(Epoch= 4025)\n",
      "train loss=0.06474024585801741(Epoch= 4025)\n",
      "CV loss=0.12350972713193764(Epoch= 4026)\n",
      "train loss=0.0647233009943598(Epoch= 4026)\n",
      "CV loss=0.12370768008899619(Epoch= 4027)\n",
      "train loss=0.064708091634505(Epoch= 4027)\n",
      "CV loss=0.12359830975957373(Epoch= 4028)\n",
      "train loss=0.06469962612720269(Epoch= 4028)\n",
      "CV loss=0.12364885214612174(Epoch= 4029)\n",
      "train loss=0.06468444489092112(Epoch= 4029)\n",
      "CV loss=0.12353549105910155(Epoch= 4030)\n",
      "train loss=0.06467158093244486(Epoch= 4030)\n",
      "CV loss=0.1234282133246836(Epoch= 4031)\n",
      "train loss=0.06465396405365927(Epoch= 4031)\n",
      "CV loss=0.12351150238787123(Epoch= 4032)\n",
      "train loss=0.06464194289441158(Epoch= 4032)\n",
      "CV loss=0.12369390199717674(Epoch= 4033)\n",
      "train loss=0.06463806667177638(Epoch= 4033)\n",
      "CV loss=0.12356911575178836(Epoch= 4034)\n",
      "train loss=0.06461679128990702(Epoch= 4034)\n",
      "CV loss=0.12353445150871119(Epoch= 4035)\n",
      "train loss=0.06460205616788166(Epoch= 4035)\n",
      "CV loss=0.12357530412250198(Epoch= 4036)\n",
      "train loss=0.06459382454252754(Epoch= 4036)\n",
      "CV loss=0.12352586626013327(Epoch= 4037)\n",
      "train loss=0.06458274274965081(Epoch= 4037)\n",
      "CV loss=0.12339142698151195(Epoch= 4038)\n",
      "train loss=0.06456309088001774(Epoch= 4038)\n",
      "CV loss=0.12334278409855903(Epoch= 4039)\n",
      "train loss=0.06455706429942139(Epoch= 4039)\n",
      "CV loss=0.12324742438266165(Epoch= 4040)\n",
      "train loss=0.06454470415058564(Epoch= 4040)\n",
      "CV loss=0.12342304266034726(Epoch= 4041)\n",
      "train loss=0.06453243401472257(Epoch= 4041)\n",
      "CV loss=0.12325035394221548(Epoch= 4042)\n",
      "train loss=0.0645158552722498(Epoch= 4042)\n",
      "CV loss=0.12344399139871132(Epoch= 4043)\n",
      "train loss=0.06451793185719935(Epoch= 4043)\n",
      "CV loss=0.12335928122872493(Epoch= 4044)\n",
      "train loss=0.0644876212220817(Epoch= 4044)\n",
      "CV loss=0.12329039292512886(Epoch= 4045)\n",
      "train loss=0.06447850442780506(Epoch= 4045)\n",
      "CV loss=0.12332710382147205(Epoch= 4046)\n",
      "train loss=0.06447121220629835(Epoch= 4046)\n",
      "CV loss=0.12313874672968779(Epoch= 4047)\n",
      "train loss=0.0644840997561556(Epoch= 4047)\n",
      "CV loss=0.12340955205826665(Epoch= 4048)\n",
      "train loss=0.06444608966380234(Epoch= 4048)\n",
      "CV loss=0.12337265053285991(Epoch= 4049)\n",
      "train loss=0.06443443424052171(Epoch= 4049)\n",
      "CV loss=0.12333854013690596(Epoch= 4050)\n",
      "train loss=0.06441714690528744(Epoch= 4050)\n",
      "CV loss=0.12306340730550777(Epoch= 4051)\n",
      "train loss=0.06441822111021052(Epoch= 4051)\n",
      "CV loss=0.12322839572541704(Epoch= 4052)\n",
      "train loss=0.06438969216879895(Epoch= 4052)\n",
      "CV loss=0.12315183876310526(Epoch= 4053)\n",
      "train loss=0.06437549724766937(Epoch= 4053)\n",
      "CV loss=0.12317839953015758(Epoch= 4054)\n",
      "train loss=0.06437115232065913(Epoch= 4054)\n",
      "CV loss=0.12323671809023534(Epoch= 4055)\n",
      "train loss=0.06435860212780148(Epoch= 4055)\n",
      "CV loss=0.12327472668037084(Epoch= 4056)\n",
      "train loss=0.06434502175137442(Epoch= 4056)\n",
      "CV loss=0.12319222166557697(Epoch= 4057)\n",
      "train loss=0.0643301774836908(Epoch= 4057)\n",
      "CV loss=0.12316157292137586(Epoch= 4058)\n",
      "train loss=0.06431775727813364(Epoch= 4058)\n",
      "CV loss=0.12334853136293122(Epoch= 4059)\n",
      "train loss=0.06431641854502307(Epoch= 4059)\n",
      "CV loss=0.12309924584590895(Epoch= 4060)\n",
      "train loss=0.0642892653747928(Epoch= 4060)\n",
      "CV loss=0.12312813023920048(Epoch= 4061)\n",
      "train loss=0.06428460160076407(Epoch= 4061)\n",
      "CV loss=0.12293799795008505(Epoch= 4062)\n",
      "train loss=0.06426999349873155(Epoch= 4062)\n",
      "CV loss=0.12309136781733641(Epoch= 4063)\n",
      "train loss=0.06425288831423007(Epoch= 4063)\n",
      "CV loss=0.12311428729361057(Epoch= 4064)\n",
      "train loss=0.06424159633310213(Epoch= 4064)\n",
      "CV loss=0.12306653517379018(Epoch= 4065)\n",
      "train loss=0.06424433048523676(Epoch= 4065)\n",
      "CV loss=0.122978575220371(Epoch= 4066)\n",
      "train loss=0.06421757354623632(Epoch= 4066)\n",
      "CV loss=0.12308167172618258(Epoch= 4067)\n",
      "train loss=0.06420203418249236(Epoch= 4067)\n",
      "CV loss=0.12305996804214278(Epoch= 4068)\n",
      "train loss=0.06419457500400479(Epoch= 4068)\n",
      "CV loss=0.12285797710757765(Epoch= 4069)\n",
      "train loss=0.0641918316492268(Epoch= 4069)\n",
      "CV loss=0.12301207561955634(Epoch= 4070)\n",
      "train loss=0.06416882182312365(Epoch= 4070)\n",
      "CV loss=0.12295441052595825(Epoch= 4071)\n",
      "train loss=0.06416424376612316(Epoch= 4071)\n",
      "CV loss=0.1229141843445461(Epoch= 4072)\n",
      "train loss=0.0641446313908913(Epoch= 4072)\n",
      "CV loss=0.1228245437510771(Epoch= 4073)\n",
      "train loss=0.06413655717461104(Epoch= 4073)\n",
      "CV loss=0.12288368101134173(Epoch= 4074)\n",
      "train loss=0.06412184410247601(Epoch= 4074)\n",
      "CV loss=0.1228494695347214(Epoch= 4075)\n",
      "train loss=0.06410661606028117(Epoch= 4075)\n",
      "CV loss=0.12308191323798218(Epoch= 4076)\n",
      "train loss=0.0641054431331239(Epoch= 4076)\n",
      "CV loss=0.1229380164814061(Epoch= 4077)\n",
      "train loss=0.0640823805240278(Epoch= 4077)\n",
      "CV loss=0.12296013402184598(Epoch= 4078)\n",
      "train loss=0.06407020142972286(Epoch= 4078)\n",
      "CV loss=0.12290043781102565(Epoch= 4079)\n",
      "train loss=0.06406122141302453(Epoch= 4079)\n",
      "CV loss=0.12297609491301613(Epoch= 4080)\n",
      "train loss=0.06405203244219099(Epoch= 4080)\n",
      "CV loss=0.12285157381985774(Epoch= 4081)\n",
      "train loss=0.06403388542828403(Epoch= 4081)\n",
      "CV loss=0.12276051921995812(Epoch= 4082)\n",
      "train loss=0.06402166224902274(Epoch= 4082)\n",
      "CV loss=0.12281208479672301(Epoch= 4083)\n",
      "train loss=0.06400768098990971(Epoch= 4083)\n",
      "CV loss=0.12297607131320501(Epoch= 4084)\n",
      "train loss=0.06400233903485515(Epoch= 4084)\n",
      "CV loss=0.12274466428662456(Epoch= 4085)\n",
      "train loss=0.06398418366637235(Epoch= 4085)\n",
      "CV loss=0.12265444734708758(Epoch= 4086)\n",
      "train loss=0.0639786960285275(Epoch= 4086)\n",
      "CV loss=0.12271876328715045(Epoch= 4087)\n",
      "train loss=0.06396140342958757(Epoch= 4087)\n",
      "CV loss=0.12282028449331278(Epoch= 4088)\n",
      "train loss=0.0639567159310875(Epoch= 4088)\n",
      "CV loss=0.12265488236451633(Epoch= 4089)\n",
      "train loss=0.06394187109368636(Epoch= 4089)\n",
      "CV loss=0.12270928875298141(Epoch= 4090)\n",
      "train loss=0.06392278194167762(Epoch= 4090)\n",
      "CV loss=0.12267414727325282(Epoch= 4091)\n",
      "train loss=0.06391196877928682(Epoch= 4091)\n",
      "CV loss=0.1226895037941293(Epoch= 4092)\n",
      "train loss=0.0639067096473805(Epoch= 4092)\n",
      "CV loss=0.12275886857723259(Epoch= 4093)\n",
      "train loss=0.063891289549908(Epoch= 4093)\n",
      "CV loss=0.12274819011517128(Epoch= 4094)\n",
      "train loss=0.06387846989443267(Epoch= 4094)\n",
      "CV loss=0.12251506120732317(Epoch= 4095)\n",
      "train loss=0.06386546072137196(Epoch= 4095)\n",
      "CV loss=0.12261184802163475(Epoch= 4096)\n",
      "train loss=0.06386058041687225(Epoch= 4096)\n",
      "CV loss=0.12272537029766357(Epoch= 4097)\n",
      "train loss=0.06384070588944578(Epoch= 4097)\n",
      "CV loss=0.12267621349227253(Epoch= 4098)\n",
      "train loss=0.06383115866111254(Epoch= 4098)\n",
      "CV loss=0.1226843985153828(Epoch= 4099)\n",
      "train loss=0.06382097765517109(Epoch= 4099)\n",
      "CV loss=0.12271266728484703(Epoch= 4100)\n",
      "train loss=0.06380803476871254(Epoch= 4100)\n",
      "CV loss=0.12253879084079947(Epoch= 4101)\n",
      "train loss=0.06379600944084217(Epoch= 4101)\n",
      "CV loss=0.12259267088871344(Epoch= 4102)\n",
      "train loss=0.0637845418387584(Epoch= 4102)\n",
      "CV loss=0.12251179363992779(Epoch= 4103)\n",
      "train loss=0.06377577706551092(Epoch= 4103)\n",
      "CV loss=0.12250526480360635(Epoch= 4104)\n",
      "train loss=0.06376232006216061(Epoch= 4104)\n",
      "CV loss=0.12241001352948487(Epoch= 4105)\n",
      "train loss=0.063746593662335(Epoch= 4105)\n",
      "CV loss=0.1225752210399628(Epoch= 4106)\n",
      "train loss=0.06373238228460068(Epoch= 4106)\n",
      "CV loss=0.12252937184742124(Epoch= 4107)\n",
      "train loss=0.06372369646866204(Epoch= 4107)\n",
      "CV loss=0.12259501387731611(Epoch= 4108)\n",
      "train loss=0.06370811463837951(Epoch= 4108)\n",
      "CV loss=0.122498551455507(Epoch= 4109)\n",
      "train loss=0.06370127063611043(Epoch= 4109)\n",
      "CV loss=0.12246630214136281(Epoch= 4110)\n",
      "train loss=0.06368441726626899(Epoch= 4110)\n",
      "CV loss=0.12233025081547094(Epoch= 4111)\n",
      "train loss=0.06368235746700648(Epoch= 4111)\n",
      "CV loss=0.12247223110307523(Epoch= 4112)\n",
      "train loss=0.06365965891653196(Epoch= 4112)\n",
      "CV loss=0.12261788708970356(Epoch= 4113)\n",
      "train loss=0.0636552647189935(Epoch= 4113)\n",
      "CV loss=0.12255111848480404(Epoch= 4114)\n",
      "train loss=0.06363801831454494(Epoch= 4114)\n",
      "CV loss=0.12247767255085117(Epoch= 4115)\n",
      "train loss=0.06363339180257958(Epoch= 4115)\n",
      "CV loss=0.12236301826974325(Epoch= 4116)\n",
      "train loss=0.06362149211078087(Epoch= 4116)\n",
      "CV loss=0.1222874867618998(Epoch= 4117)\n",
      "train loss=0.06360318281203589(Epoch= 4117)\n",
      "CV loss=0.12231842567945113(Epoch= 4118)\n",
      "train loss=0.06359476690186315(Epoch= 4118)\n",
      "CV loss=0.12240070127551557(Epoch= 4119)\n",
      "train loss=0.06358219136844173(Epoch= 4119)\n",
      "CV loss=0.12237343573631387(Epoch= 4120)\n",
      "train loss=0.06356972591643648(Epoch= 4120)\n",
      "CV loss=0.12234137965001358(Epoch= 4121)\n",
      "train loss=0.0635567117183215(Epoch= 4121)\n",
      "CV loss=0.12219129081105692(Epoch= 4122)\n",
      "train loss=0.06355188233658166(Epoch= 4122)\n",
      "CV loss=0.12235392114781338(Epoch= 4123)\n",
      "train loss=0.06353041696438447(Epoch= 4123)\n",
      "CV loss=0.12234031894623132(Epoch= 4124)\n",
      "train loss=0.06351771453971528(Epoch= 4124)\n",
      "CV loss=0.1224659152155115(Epoch= 4125)\n",
      "train loss=0.0635223299792678(Epoch= 4125)\n",
      "CV loss=0.12226346305152713(Epoch= 4126)\n",
      "train loss=0.06349626447939584(Epoch= 4126)\n",
      "CV loss=0.12223015562273712(Epoch= 4127)\n",
      "train loss=0.06348423546346198(Epoch= 4127)\n",
      "CV loss=0.12223781271459491(Epoch= 4128)\n",
      "train loss=0.06347203787073485(Epoch= 4128)\n",
      "CV loss=0.1224152677830668(Epoch= 4129)\n",
      "train loss=0.06347107939608201(Epoch= 4129)\n",
      "CV loss=0.1221264506339435(Epoch= 4130)\n",
      "train loss=0.06345703791863484(Epoch= 4130)\n",
      "CV loss=0.12229181366959832(Epoch= 4131)\n",
      "train loss=0.06344032705653882(Epoch= 4131)\n",
      "CV loss=0.12220998202310196(Epoch= 4132)\n",
      "train loss=0.06343247707668756(Epoch= 4132)\n",
      "CV loss=0.12207374140146765(Epoch= 4133)\n",
      "train loss=0.06341770531374054(Epoch= 4133)\n",
      "CV loss=0.12218553083723625(Epoch= 4134)\n",
      "train loss=0.0634086143638069(Epoch= 4134)\n",
      "CV loss=0.12217859583941287(Epoch= 4135)\n",
      "train loss=0.06338952927132695(Epoch= 4135)\n",
      "CV loss=0.12229359868018924(Epoch= 4136)\n",
      "train loss=0.06338859409865337(Epoch= 4136)\n",
      "CV loss=0.1219997225689639(Epoch= 4137)\n",
      "train loss=0.06338221237199816(Epoch= 4137)\n",
      "CV loss=0.12217513220709733(Epoch= 4138)\n",
      "train loss=0.0633573016472725(Epoch= 4138)\n",
      "CV loss=0.12207099424898757(Epoch= 4139)\n",
      "train loss=0.06335421736890426(Epoch= 4139)\n",
      "CV loss=0.12215495772564275(Epoch= 4140)\n",
      "train loss=0.06333822391643014(Epoch= 4140)\n",
      "CV loss=0.12202519554814192(Epoch= 4141)\n",
      "train loss=0.06332429205693306(Epoch= 4141)\n",
      "CV loss=0.12201683028742426(Epoch= 4142)\n",
      "train loss=0.06331118548341337(Epoch= 4142)\n",
      "CV loss=0.1221509492871663(Epoch= 4143)\n",
      "train loss=0.06329886960342558(Epoch= 4143)\n",
      "CV loss=0.12189862093782214(Epoch= 4144)\n",
      "train loss=0.06329470823670763(Epoch= 4144)\n",
      "CV loss=0.12210599584578946(Epoch= 4145)\n",
      "train loss=0.06327935014078315(Epoch= 4145)\n",
      "CV loss=0.1222292906831455(Epoch= 4146)\n",
      "train loss=0.06328147086547062(Epoch= 4146)\n",
      "CV loss=0.12188542798653554(Epoch= 4147)\n",
      "train loss=0.063260386732947(Epoch= 4147)\n",
      "CV loss=0.12197737695767369(Epoch= 4148)\n",
      "train loss=0.06324008136052811(Epoch= 4148)\n",
      "CV loss=0.1219617254194873(Epoch= 4149)\n",
      "train loss=0.063229327795834(Epoch= 4149)\n",
      "CV loss=0.12202521271577224(Epoch= 4150)\n",
      "train loss=0.06321856010810933(Epoch= 4150)\n",
      "CV loss=0.12203400840870757(Epoch= 4151)\n",
      "train loss=0.06320742937629739(Epoch= 4151)\n",
      "CV loss=0.12208560069293001(Epoch= 4152)\n",
      "train loss=0.0632162890420005(Epoch= 4152)\n",
      "CV loss=0.1218372951023271(Epoch= 4153)\n",
      "train loss=0.0631895378582568(Epoch= 4153)\n",
      "CV loss=0.12191185833007859(Epoch= 4154)\n",
      "train loss=0.06317388492170586(Epoch= 4154)\n",
      "CV loss=0.12198773093821744(Epoch= 4155)\n",
      "train loss=0.06316527161077988(Epoch= 4155)\n",
      "CV loss=0.12181661188468479(Epoch= 4156)\n",
      "train loss=0.06315184730380453(Epoch= 4156)\n",
      "CV loss=0.12181801772394212(Epoch= 4157)\n",
      "train loss=0.06314191667449591(Epoch= 4157)\n",
      "CV loss=0.1218208416660026(Epoch= 4158)\n",
      "train loss=0.06312461117608166(Epoch= 4158)\n",
      "CV loss=0.12192109061353715(Epoch= 4159)\n",
      "train loss=0.06311479495113614(Epoch= 4159)\n",
      "CV loss=0.1219400397044327(Epoch= 4160)\n",
      "train loss=0.06311750329073156(Epoch= 4160)\n",
      "CV loss=0.12183385213759185(Epoch= 4161)\n",
      "train loss=0.06309268788850868(Epoch= 4161)\n",
      "CV loss=0.12174356753091366(Epoch= 4162)\n",
      "train loss=0.06308181799292312(Epoch= 4162)\n",
      "CV loss=0.12181343882799078(Epoch= 4163)\n",
      "train loss=0.06307636484007405(Epoch= 4163)\n",
      "CV loss=0.1218688820196432(Epoch= 4164)\n",
      "train loss=0.06306489517736087(Epoch= 4164)\n",
      "CV loss=0.12183294319953447(Epoch= 4165)\n",
      "train loss=0.06304745694666943(Epoch= 4165)\n",
      "CV loss=0.12155083151384144(Epoch= 4166)\n",
      "train loss=0.06305824313153961(Epoch= 4166)\n",
      "CV loss=0.12189384315660232(Epoch= 4167)\n",
      "train loss=0.06302667808709979(Epoch= 4167)\n",
      "CV loss=0.1217655202452178(Epoch= 4168)\n",
      "train loss=0.06301396111294913(Epoch= 4168)\n",
      "CV loss=0.12161012839235728(Epoch= 4169)\n",
      "train loss=0.06300778113357863(Epoch= 4169)\n",
      "CV loss=0.12186495082675322(Epoch= 4170)\n",
      "train loss=0.06299559544253139(Epoch= 4170)\n",
      "CV loss=0.12193637704355327(Epoch= 4171)\n",
      "train loss=0.06298678895725654(Epoch= 4171)\n",
      "CV loss=0.12176918318307621(Epoch= 4172)\n",
      "train loss=0.06296611635480774(Epoch= 4172)\n",
      "CV loss=0.12166834928095319(Epoch= 4173)\n",
      "train loss=0.06296104282541774(Epoch= 4173)\n",
      "CV loss=0.12172324438133775(Epoch= 4174)\n",
      "train loss=0.06294351782798485(Epoch= 4174)\n",
      "CV loss=0.12165841555369336(Epoch= 4175)\n",
      "train loss=0.06293543668459502(Epoch= 4175)\n",
      "CV loss=0.12166670679151875(Epoch= 4176)\n",
      "train loss=0.06292165945387106(Epoch= 4176)\n",
      "CV loss=0.1216433624521395(Epoch= 4177)\n",
      "train loss=0.06291446796946705(Epoch= 4177)\n",
      "CV loss=0.12166909827680839(Epoch= 4178)\n",
      "train loss=0.06290701318835491(Epoch= 4178)\n",
      "CV loss=0.12158845759716674(Epoch= 4179)\n",
      "train loss=0.06289016761028798(Epoch= 4179)\n",
      "CV loss=0.12164023032598485(Epoch= 4180)\n",
      "train loss=0.0628800478581384(Epoch= 4180)\n",
      "CV loss=0.12162003763634491(Epoch= 4181)\n",
      "train loss=0.06286405278919233(Epoch= 4181)\n",
      "CV loss=0.12171909158866659(Epoch= 4182)\n",
      "train loss=0.06285689328281888(Epoch= 4182)\n",
      "CV loss=0.12169998321919578(Epoch= 4183)\n",
      "train loss=0.06284665710784057(Epoch= 4183)\n",
      "CV loss=0.12165399906743002(Epoch= 4184)\n",
      "train loss=0.06283161919028088(Epoch= 4184)\n",
      "CV loss=0.121754504416298(Epoch= 4185)\n",
      "train loss=0.06282828626391801(Epoch= 4185)\n",
      "CV loss=0.12159227530630679(Epoch= 4186)\n",
      "train loss=0.06281416302421895(Epoch= 4186)\n",
      "CV loss=0.12147146048510313(Epoch= 4187)\n",
      "train loss=0.06280182470288344(Epoch= 4187)\n",
      "CV loss=0.1214980129100516(Epoch= 4188)\n",
      "train loss=0.0627883837416477(Epoch= 4188)\n",
      "CV loss=0.12167707545263194(Epoch= 4189)\n",
      "train loss=0.06278682145694758(Epoch= 4189)\n",
      "CV loss=0.12158176436132964(Epoch= 4190)\n",
      "train loss=0.0627644116317802(Epoch= 4190)\n",
      "CV loss=0.1216154611408117(Epoch= 4191)\n",
      "train loss=0.06275386806661816(Epoch= 4191)\n",
      "CV loss=0.12141009465421637(Epoch= 4192)\n",
      "train loss=0.06274254636087942(Epoch= 4192)\n",
      "CV loss=0.12147947823955142(Epoch= 4193)\n",
      "train loss=0.06273494604885517(Epoch= 4193)\n",
      "CV loss=0.12165684356088188(Epoch= 4194)\n",
      "train loss=0.06273256766447935(Epoch= 4194)\n",
      "CV loss=0.12139045101019741(Epoch= 4195)\n",
      "train loss=0.06270801827131652(Epoch= 4195)\n",
      "CV loss=0.12160597073641743(Epoch= 4196)\n",
      "train loss=0.06270645853208491(Epoch= 4196)\n",
      "CV loss=0.12134280271279385(Epoch= 4197)\n",
      "train loss=0.06268989056559843(Epoch= 4197)\n",
      "CV loss=0.1214257168926805(Epoch= 4198)\n",
      "train loss=0.06270428021170942(Epoch= 4198)\n",
      "CV loss=0.12147507089575482(Epoch= 4199)\n",
      "train loss=0.06266672745993236(Epoch= 4199)\n",
      "CV loss=0.12128257188309924(Epoch= 4200)\n",
      "train loss=0.0626576539618728(Epoch= 4200)\n",
      "CV loss=0.1212324065081615(Epoch= 4201)\n",
      "train loss=0.06264475808466446(Epoch= 4201)\n",
      "CV loss=0.12143844974296802(Epoch= 4202)\n",
      "train loss=0.06263444458369839(Epoch= 4202)\n",
      "CV loss=0.12148606142694345(Epoch= 4203)\n",
      "train loss=0.06262403235199704(Epoch= 4203)\n",
      "CV loss=0.12133030982715959(Epoch= 4204)\n",
      "train loss=0.06261036376637139(Epoch= 4204)\n",
      "CV loss=0.12135109280781281(Epoch= 4205)\n",
      "train loss=0.06260197438798171(Epoch= 4205)\n",
      "CV loss=0.12133330267882958(Epoch= 4206)\n",
      "train loss=0.06258934623875179(Epoch= 4206)\n",
      "CV loss=0.12139906656219371(Epoch= 4207)\n",
      "train loss=0.0625873463471881(Epoch= 4207)\n",
      "CV loss=0.12115729880049209(Epoch= 4208)\n",
      "train loss=0.0625689323375791(Epoch= 4208)\n",
      "CV loss=0.1214051930552002(Epoch= 4209)\n",
      "train loss=0.06255688707664128(Epoch= 4209)\n",
      "CV loss=0.12131508736964455(Epoch= 4210)\n",
      "train loss=0.06254284980434541(Epoch= 4210)\n",
      "CV loss=0.1211593860717377(Epoch= 4211)\n",
      "train loss=0.0625382297652403(Epoch= 4211)\n",
      "CV loss=0.12110682884441534(Epoch= 4212)\n",
      "train loss=0.0625311235496662(Epoch= 4212)\n",
      "CV loss=0.1212067031186861(Epoch= 4213)\n",
      "train loss=0.06252314394911732(Epoch= 4213)\n",
      "CV loss=0.12131317236471473(Epoch= 4214)\n",
      "train loss=0.06250998574159879(Epoch= 4214)\n",
      "CV loss=0.12121835257338567(Epoch= 4215)\n",
      "train loss=0.06248921765116463(Epoch= 4215)\n",
      "CV loss=0.12119467911486694(Epoch= 4216)\n",
      "train loss=0.062483248197356546(Epoch= 4216)\n",
      "CV loss=0.1212148959850976(Epoch= 4217)\n",
      "train loss=0.062468048273702385(Epoch= 4217)\n",
      "CV loss=0.12125019078324653(Epoch= 4218)\n",
      "train loss=0.06246823066445256(Epoch= 4218)\n",
      "CV loss=0.12105785654684767(Epoch= 4219)\n",
      "train loss=0.0624458252363247(Epoch= 4219)\n",
      "CV loss=0.12108267568845059(Epoch= 4220)\n",
      "train loss=0.06243408097613983(Epoch= 4220)\n",
      "CV loss=0.12100147258376506(Epoch= 4221)\n",
      "train loss=0.062429185096179216(Epoch= 4221)\n",
      "CV loss=0.12125584759276883(Epoch= 4222)\n",
      "train loss=0.062421419118330955(Epoch= 4222)\n",
      "CV loss=0.12126607394556609(Epoch= 4223)\n",
      "train loss=0.062404304531106125(Epoch= 4223)\n",
      "CV loss=0.12108609892464073(Epoch= 4224)\n",
      "train loss=0.0623941527876926(Epoch= 4224)\n",
      "CV loss=0.1212321486207126(Epoch= 4225)\n",
      "train loss=0.06238917609693088(Epoch= 4225)\n",
      "CV loss=0.12122767860637415(Epoch= 4226)\n",
      "train loss=0.062370957656898186(Epoch= 4226)\n",
      "CV loss=0.12114425951057073(Epoch= 4227)\n",
      "train loss=0.062358205365147024(Epoch= 4227)\n",
      "CV loss=0.12102420157242033(Epoch= 4228)\n",
      "train loss=0.062352607480471084(Epoch= 4228)\n",
      "CV loss=0.12100641747914839(Epoch= 4229)\n",
      "train loss=0.06233917088516771(Epoch= 4229)\n",
      "CV loss=0.12101250921285372(Epoch= 4230)\n",
      "train loss=0.062327242230820736(Epoch= 4230)\n",
      "CV loss=0.12110248668168816(Epoch= 4231)\n",
      "train loss=0.0623202600108668(Epoch= 4231)\n",
      "CV loss=0.1210164019810668(Epoch= 4232)\n",
      "train loss=0.06231154580401717(Epoch= 4232)\n",
      "CV loss=0.12109101803696998(Epoch= 4233)\n",
      "train loss=0.06229708833627155(Epoch= 4233)\n",
      "CV loss=0.12091241511584164(Epoch= 4234)\n",
      "train loss=0.06228983330212179(Epoch= 4234)\n",
      "CV loss=0.12105329096042788(Epoch= 4235)\n",
      "train loss=0.06227534223052064(Epoch= 4235)\n",
      "CV loss=0.12091586734097758(Epoch= 4236)\n",
      "train loss=0.06226553060114094(Epoch= 4236)\n",
      "CV loss=0.1209072573258656(Epoch= 4237)\n",
      "train loss=0.06225038683586866(Epoch= 4237)\n",
      "CV loss=0.1208696040534009(Epoch= 4238)\n",
      "train loss=0.06224035091054399(Epoch= 4238)\n",
      "CV loss=0.12098503831328682(Epoch= 4239)\n",
      "train loss=0.06223130295470078(Epoch= 4239)\n",
      "CV loss=0.12079147416275098(Epoch= 4240)\n",
      "train loss=0.06222490280935353(Epoch= 4240)\n",
      "CV loss=0.12088455216357609(Epoch= 4241)\n",
      "train loss=0.06220984849712263(Epoch= 4241)\n",
      "CV loss=0.12087863889424785(Epoch= 4242)\n",
      "train loss=0.0621989128744599(Epoch= 4242)\n",
      "CV loss=0.1208532296133141(Epoch= 4243)\n",
      "train loss=0.06219386777838461(Epoch= 4243)\n",
      "CV loss=0.12093432753229008(Epoch= 4244)\n",
      "train loss=0.06217661487952354(Epoch= 4244)\n",
      "CV loss=0.12084336309763222(Epoch= 4245)\n",
      "train loss=0.06218861693118798(Epoch= 4245)\n",
      "CV loss=0.12079583613097798(Epoch= 4246)\n",
      "train loss=0.062153455666060786(Epoch= 4246)\n",
      "CV loss=0.12080293054869816(Epoch= 4247)\n",
      "train loss=0.06214292467945624(Epoch= 4247)\n",
      "CV loss=0.12072116770072663(Epoch= 4248)\n",
      "train loss=0.06213718650464883(Epoch= 4248)\n",
      "CV loss=0.1207990572197665(Epoch= 4249)\n",
      "train loss=0.06212102489442904(Epoch= 4249)\n",
      "CV loss=0.12074090166616266(Epoch= 4250)\n",
      "train loss=0.06213643486600868(Epoch= 4250)\n",
      "CV loss=0.12083979936321679(Epoch= 4251)\n",
      "train loss=0.06210854023768268(Epoch= 4251)\n",
      "CV loss=0.12082708962980515(Epoch= 4252)\n",
      "train loss=0.06210358331922393(Epoch= 4252)\n",
      "CV loss=0.1207407737164714(Epoch= 4253)\n",
      "train loss=0.06208782365502757(Epoch= 4253)\n",
      "CV loss=0.12083952058081121(Epoch= 4254)\n",
      "train loss=0.06207458239843519(Epoch= 4254)\n",
      "CV loss=0.12079891771091097(Epoch= 4255)\n",
      "train loss=0.062060625675172156(Epoch= 4255)\n",
      "CV loss=0.12070381503931504(Epoch= 4256)\n",
      "train loss=0.062051973578181124(Epoch= 4256)\n",
      "CV loss=0.12092013235942305(Epoch= 4257)\n",
      "train loss=0.062050514229787794(Epoch= 4257)\n",
      "CV loss=0.12070058983800024(Epoch= 4258)\n",
      "train loss=0.06202788999009362(Epoch= 4258)\n",
      "CV loss=0.12065990423538404(Epoch= 4259)\n",
      "train loss=0.06202293815135151(Epoch= 4259)\n",
      "CV loss=0.12064343080806073(Epoch= 4260)\n",
      "train loss=0.062012027658781325(Epoch= 4260)\n",
      "CV loss=0.12060558373546633(Epoch= 4261)\n",
      "train loss=0.06200450499096414(Epoch= 4261)\n",
      "CV loss=0.12064270847543723(Epoch= 4262)\n",
      "train loss=0.0619845265466323(Epoch= 4262)\n",
      "CV loss=0.12059898831569915(Epoch= 4263)\n",
      "train loss=0.06198166470814571(Epoch= 4263)\n",
      "CV loss=0.1206712966176326(Epoch= 4264)\n",
      "train loss=0.06196677765102606(Epoch= 4264)\n",
      "CV loss=0.12062255126500959(Epoch= 4265)\n",
      "train loss=0.061962474825829256(Epoch= 4265)\n",
      "CV loss=0.12063539875366874(Epoch= 4266)\n",
      "train loss=0.06195740907496741(Epoch= 4266)\n",
      "CV loss=0.12060375264845465(Epoch= 4267)\n",
      "train loss=0.061940503248081265(Epoch= 4267)\n",
      "CV loss=0.12073996788284977(Epoch= 4268)\n",
      "train loss=0.06192697280268181(Epoch= 4268)\n",
      "CV loss=0.12073288426180391(Epoch= 4269)\n",
      "train loss=0.06191599751458434(Epoch= 4269)\n",
      "CV loss=0.12078078307973128(Epoch= 4270)\n",
      "train loss=0.061910239693940494(Epoch= 4270)\n",
      "CV loss=0.12050711396647465(Epoch= 4271)\n",
      "train loss=0.061900996371819485(Epoch= 4271)\n",
      "CV loss=0.12051479247745553(Epoch= 4272)\n",
      "train loss=0.06188144507611218(Epoch= 4272)\n",
      "CV loss=0.12080607503754734(Epoch= 4273)\n",
      "train loss=0.061887282244691345(Epoch= 4273)\n",
      "CV loss=0.12073196249565823(Epoch= 4274)\n",
      "train loss=0.06187243510506429(Epoch= 4274)\n",
      "CV loss=0.12044447814053487(Epoch= 4275)\n",
      "train loss=0.06185100352184309(Epoch= 4275)\n",
      "CV loss=0.12049259225934873(Epoch= 4276)\n",
      "train loss=0.06183926117693491(Epoch= 4276)\n",
      "CV loss=0.12040725631992155(Epoch= 4277)\n",
      "train loss=0.061835471863324325(Epoch= 4277)\n",
      "CV loss=0.12065274425224126(Epoch= 4278)\n",
      "train loss=0.061826559377128484(Epoch= 4278)\n",
      "CV loss=0.12061016513362219(Epoch= 4279)\n",
      "train loss=0.06180865399277727(Epoch= 4279)\n",
      "CV loss=0.12051754765408082(Epoch= 4280)\n",
      "train loss=0.06179810951700446(Epoch= 4280)\n",
      "CV loss=0.12042778270957896(Epoch= 4281)\n",
      "train loss=0.06180773587298513(Epoch= 4281)\n",
      "CV loss=0.12066812990468731(Epoch= 4282)\n",
      "train loss=0.06178175571863734(Epoch= 4282)\n",
      "CV loss=0.12044310747004122(Epoch= 4283)\n",
      "train loss=0.0617711268021976(Epoch= 4283)\n",
      "CV loss=0.12032383561004983(Epoch= 4284)\n",
      "train loss=0.06175798946453368(Epoch= 4284)\n",
      "CV loss=0.12042330524766291(Epoch= 4285)\n",
      "train loss=0.06174780779898486(Epoch= 4285)\n",
      "CV loss=0.12057228511070159(Epoch= 4286)\n",
      "train loss=0.06174509186107783(Epoch= 4286)\n",
      "CV loss=0.12042842171156812(Epoch= 4287)\n",
      "train loss=0.06172812743380093(Epoch= 4287)\n",
      "CV loss=0.12044544076759758(Epoch= 4288)\n",
      "train loss=0.06172218911381088(Epoch= 4288)\n",
      "CV loss=0.12033766862003506(Epoch= 4289)\n",
      "train loss=0.061708562052850226(Epoch= 4289)\n",
      "CV loss=0.12037898466842953(Epoch= 4290)\n",
      "train loss=0.06169416947162776(Epoch= 4290)\n",
      "CV loss=0.12032930642130628(Epoch= 4291)\n",
      "train loss=0.061684054822720366(Epoch= 4291)\n",
      "CV loss=0.12040533133836336(Epoch= 4292)\n",
      "train loss=0.06167740078218918(Epoch= 4292)\n",
      "CV loss=0.12043368601667101(Epoch= 4293)\n",
      "train loss=0.06166692710691998(Epoch= 4293)\n",
      "CV loss=0.12026801981590843(Epoch= 4294)\n",
      "train loss=0.061664579413861276(Epoch= 4294)\n",
      "CV loss=0.12044235933499467(Epoch= 4295)\n",
      "train loss=0.061646669514209454(Epoch= 4295)\n",
      "CV loss=0.12026747125185527(Epoch= 4296)\n",
      "train loss=0.06163503836652139(Epoch= 4296)\n",
      "CV loss=0.12017848912653287(Epoch= 4297)\n",
      "train loss=0.061633129541170555(Epoch= 4297)\n",
      "CV loss=0.1202236536452377(Epoch= 4298)\n",
      "train loss=0.06161378181964701(Epoch= 4298)\n",
      "CV loss=0.12022451626974023(Epoch= 4299)\n",
      "train loss=0.06160667465135498(Epoch= 4299)\n",
      "CV loss=0.12018929581006442(Epoch= 4300)\n",
      "train loss=0.061591673068593526(Epoch= 4300)\n",
      "CV loss=0.12023214033325923(Epoch= 4301)\n",
      "train loss=0.061584359424614356(Epoch= 4301)\n",
      "CV loss=0.1202358728268468(Epoch= 4302)\n",
      "train loss=0.061576473276337744(Epoch= 4302)\n",
      "CV loss=0.12016841363161238(Epoch= 4303)\n",
      "train loss=0.06156584646461691(Epoch= 4303)\n",
      "CV loss=0.12009544750975024(Epoch= 4304)\n",
      "train loss=0.06155220382387797(Epoch= 4304)\n",
      "CV loss=0.1200571459819204(Epoch= 4305)\n",
      "train loss=0.06155135929213645(Epoch= 4305)\n",
      "CV loss=0.12000034047617185(Epoch= 4306)\n",
      "train loss=0.06153746570422425(Epoch= 4306)\n",
      "CV loss=0.12018279091944807(Epoch= 4307)\n",
      "train loss=0.06152251675869381(Epoch= 4307)\n",
      "CV loss=0.1201070010628783(Epoch= 4308)\n",
      "train loss=0.06151317293702821(Epoch= 4308)\n",
      "CV loss=0.12010445726025781(Epoch= 4309)\n",
      "train loss=0.06150050748789966(Epoch= 4309)\n",
      "CV loss=0.12004401826024048(Epoch= 4310)\n",
      "train loss=0.06150201519645052(Epoch= 4310)\n",
      "CV loss=0.1201848597621088(Epoch= 4311)\n",
      "train loss=0.061480608320393385(Epoch= 4311)\n",
      "CV loss=0.1202997438166987(Epoch= 4312)\n",
      "train loss=0.061483586451019666(Epoch= 4312)\n",
      "CV loss=0.12007123675613054(Epoch= 4313)\n",
      "train loss=0.06145872392929461(Epoch= 4313)\n",
      "CV loss=0.12013557086068527(Epoch= 4314)\n",
      "train loss=0.061452403055400576(Epoch= 4314)\n",
      "CV loss=0.11992206021429819(Epoch= 4315)\n",
      "train loss=0.06144533521731787(Epoch= 4315)\n",
      "CV loss=0.12013034240626039(Epoch= 4316)\n",
      "train loss=0.06143157233796614(Epoch= 4316)\n",
      "CV loss=0.1200035009019747(Epoch= 4317)\n",
      "train loss=0.06142321039279555(Epoch= 4317)\n",
      "CV loss=0.11994554124981163(Epoch= 4318)\n",
      "train loss=0.06141114317672754(Epoch= 4318)\n",
      "CV loss=0.1200229385367703(Epoch= 4319)\n",
      "train loss=0.06140096788424933(Epoch= 4319)\n",
      "CV loss=0.11992290154970936(Epoch= 4320)\n",
      "train loss=0.061407427801843016(Epoch= 4320)\n",
      "CV loss=0.1200705227906246(Epoch= 4321)\n",
      "train loss=0.061382648081287045(Epoch= 4321)\n",
      "CV loss=0.11992987133528787(Epoch= 4322)\n",
      "train loss=0.06137063960750926(Epoch= 4322)\n",
      "CV loss=0.11992389334821954(Epoch= 4323)\n",
      "train loss=0.06136427896926504(Epoch= 4323)\n",
      "CV loss=0.1199465372392884(Epoch= 4324)\n",
      "train loss=0.06135536399276095(Epoch= 4324)\n",
      "CV loss=0.12006226802304033(Epoch= 4325)\n",
      "train loss=0.06133938154490338(Epoch= 4325)\n",
      "CV loss=0.11990372130015275(Epoch= 4326)\n",
      "train loss=0.06132980715846932(Epoch= 4326)\n",
      "CV loss=0.11983882794516498(Epoch= 4327)\n",
      "train loss=0.06132220344764398(Epoch= 4327)\n",
      "CV loss=0.11995103696320186(Epoch= 4328)\n",
      "train loss=0.061310117085105524(Epoch= 4328)\n",
      "CV loss=0.11987430028790691(Epoch= 4329)\n",
      "train loss=0.06130202956038501(Epoch= 4329)\n",
      "CV loss=0.11979908833555362(Epoch= 4330)\n",
      "train loss=0.0612976078181693(Epoch= 4330)\n",
      "CV loss=0.12022037606461072(Epoch= 4331)\n",
      "train loss=0.06129464420058526(Epoch= 4331)\n",
      "CV loss=0.12003036285231361(Epoch= 4332)\n",
      "train loss=0.06127674979749466(Epoch= 4332)\n",
      "CV loss=0.11982927190641768(Epoch= 4333)\n",
      "train loss=0.06126190805931219(Epoch= 4333)\n",
      "CV loss=0.11989477422491446(Epoch= 4334)\n",
      "train loss=0.06124942977664823(Epoch= 4334)\n",
      "CV loss=0.11998525450442135(Epoch= 4335)\n",
      "train loss=0.06124182049160698(Epoch= 4335)\n",
      "CV loss=0.11981376050127565(Epoch= 4336)\n",
      "train loss=0.06123343493218608(Epoch= 4336)\n",
      "CV loss=0.11989458201630424(Epoch= 4337)\n",
      "train loss=0.06122054999061957(Epoch= 4337)\n",
      "CV loss=0.1197892484716028(Epoch= 4338)\n",
      "train loss=0.06120785373885227(Epoch= 4338)\n",
      "CV loss=0.119875792244824(Epoch= 4339)\n",
      "train loss=0.06120178985892088(Epoch= 4339)\n",
      "CV loss=0.11985618919529081(Epoch= 4340)\n",
      "train loss=0.061191763181559805(Epoch= 4340)\n",
      "CV loss=0.11975579754918814(Epoch= 4341)\n",
      "train loss=0.061186574277654955(Epoch= 4341)\n",
      "CV loss=0.11981997543920578(Epoch= 4342)\n",
      "train loss=0.06117003958397564(Epoch= 4342)\n",
      "CV loss=0.11981061437544774(Epoch= 4343)\n",
      "train loss=0.06116941576699165(Epoch= 4343)\n",
      "CV loss=0.11989233849927158(Epoch= 4344)\n",
      "train loss=0.06116181152738659(Epoch= 4344)\n",
      "CV loss=0.11971181476949322(Epoch= 4345)\n",
      "train loss=0.061140703260590956(Epoch= 4345)\n",
      "CV loss=0.11992888488774357(Epoch= 4346)\n",
      "train loss=0.061135630298407136(Epoch= 4346)\n",
      "CV loss=0.11974446040693043(Epoch= 4347)\n",
      "train loss=0.061130176766246(Epoch= 4347)\n",
      "CV loss=0.11972857811154858(Epoch= 4348)\n",
      "train loss=0.06110986218465026(Epoch= 4348)\n",
      "CV loss=0.11977012869888233(Epoch= 4349)\n",
      "train loss=0.06110785108297103(Epoch= 4349)\n",
      "CV loss=0.11983735824029759(Epoch= 4350)\n",
      "train loss=0.061096975623783446(Epoch= 4350)\n",
      "CV loss=0.11982504700165163(Epoch= 4351)\n",
      "train loss=0.061087929405160654(Epoch= 4351)\n",
      "CV loss=0.11969550798442666(Epoch= 4352)\n",
      "train loss=0.061090486102296414(Epoch= 4352)\n",
      "CV loss=0.11979252192636065(Epoch= 4353)\n",
      "train loss=0.06106411945509289(Epoch= 4353)\n",
      "CV loss=0.11957777102723455(Epoch= 4354)\n",
      "train loss=0.06106469170999483(Epoch= 4354)\n",
      "CV loss=0.11968122635984035(Epoch= 4355)\n",
      "train loss=0.06104218665724206(Epoch= 4355)\n",
      "CV loss=0.11961377156800555(Epoch= 4356)\n",
      "train loss=0.061033040729867226(Epoch= 4356)\n",
      "CV loss=0.11954910387003734(Epoch= 4357)\n",
      "train loss=0.06102940614955879(Epoch= 4357)\n",
      "CV loss=0.1195471456201151(Epoch= 4358)\n",
      "train loss=0.06101397681992986(Epoch= 4358)\n",
      "CV loss=0.11968524642111199(Epoch= 4359)\n",
      "train loss=0.06100677126710512(Epoch= 4359)\n",
      "CV loss=0.1195751481597839(Epoch= 4360)\n",
      "train loss=0.060998986653418824(Epoch= 4360)\n",
      "CV loss=0.11962979641289208(Epoch= 4361)\n",
      "train loss=0.060986085651131935(Epoch= 4361)\n",
      "CV loss=0.1194942381318062(Epoch= 4362)\n",
      "train loss=0.060976379965148736(Epoch= 4362)\n",
      "CV loss=0.11958387080323518(Epoch= 4363)\n",
      "train loss=0.06096458654180968(Epoch= 4363)\n",
      "CV loss=0.11945650540445044(Epoch= 4364)\n",
      "train loss=0.060955732269012956(Epoch= 4364)\n",
      "CV loss=0.11938996226065235(Epoch= 4365)\n",
      "train loss=0.060956897737101674(Epoch= 4365)\n",
      "CV loss=0.11952020993217273(Epoch= 4366)\n",
      "train loss=0.06093485735623054(Epoch= 4366)\n",
      "CV loss=0.11939690013484128(Epoch= 4367)\n",
      "train loss=0.060935498204042364(Epoch= 4367)\n",
      "CV loss=0.11953522536483743(Epoch= 4368)\n",
      "train loss=0.0609242660960673(Epoch= 4368)\n",
      "CV loss=0.11957111659826522(Epoch= 4369)\n",
      "train loss=0.06090699763429324(Epoch= 4369)\n",
      "CV loss=0.11938896341508995(Epoch= 4370)\n",
      "train loss=0.06090117622512124(Epoch= 4370)\n",
      "CV loss=0.11951108693115814(Epoch= 4371)\n",
      "train loss=0.06089003546829591(Epoch= 4371)\n",
      "CV loss=0.11942815713094457(Epoch= 4372)\n",
      "train loss=0.06087952452122987(Epoch= 4372)\n",
      "CV loss=0.11949399667137553(Epoch= 4373)\n",
      "train loss=0.060871628740311566(Epoch= 4373)\n",
      "CV loss=0.11957772845666254(Epoch= 4374)\n",
      "train loss=0.06086277499336167(Epoch= 4374)\n",
      "CV loss=0.11954563308603769(Epoch= 4375)\n",
      "train loss=0.06084993790643605(Epoch= 4375)\n",
      "CV loss=0.11943706230255569(Epoch= 4376)\n",
      "train loss=0.06084938279813916(Epoch= 4376)\n",
      "CV loss=0.11939426430290039(Epoch= 4377)\n",
      "train loss=0.060836153407522485(Epoch= 4377)\n",
      "CV loss=0.11928851896898227(Epoch= 4378)\n",
      "train loss=0.060831764890378566(Epoch= 4378)\n",
      "CV loss=0.11919312978826316(Epoch= 4379)\n",
      "train loss=0.060821090181817514(Epoch= 4379)\n",
      "CV loss=0.11933788906697686(Epoch= 4380)\n",
      "train loss=0.06080086965158561(Epoch= 4380)\n",
      "CV loss=0.11934857241050043(Epoch= 4381)\n",
      "train loss=0.0607911296863808(Epoch= 4381)\n",
      "CV loss=0.11934987518502634(Epoch= 4382)\n",
      "train loss=0.06077947107396861(Epoch= 4382)\n",
      "CV loss=0.11960177141237721(Epoch= 4383)\n",
      "train loss=0.060786795069874035(Epoch= 4383)\n",
      "CV loss=0.1193678329264668(Epoch= 4384)\n",
      "train loss=0.06076260305693932(Epoch= 4384)\n",
      "CV loss=0.11931466421091395(Epoch= 4385)\n",
      "train loss=0.06075563912408966(Epoch= 4385)\n",
      "CV loss=0.11929158874693005(Epoch= 4386)\n",
      "train loss=0.06074315241386735(Epoch= 4386)\n",
      "CV loss=0.1194567563551471(Epoch= 4387)\n",
      "train loss=0.060735942379388105(Epoch= 4387)\n",
      "CV loss=0.11932145805692324(Epoch= 4388)\n",
      "train loss=0.0607256706005805(Epoch= 4388)\n",
      "CV loss=0.1192076314916587(Epoch= 4389)\n",
      "train loss=0.06072173704713852(Epoch= 4389)\n",
      "CV loss=0.11930420333138535(Epoch= 4390)\n",
      "train loss=0.06070395230926633(Epoch= 4390)\n",
      "CV loss=0.11925304452658406(Epoch= 4391)\n",
      "train loss=0.06071021047368633(Epoch= 4391)\n",
      "CV loss=0.11936690813745021(Epoch= 4392)\n",
      "train loss=0.060694519788074816(Epoch= 4392)\n",
      "CV loss=0.11939095972225289(Epoch= 4393)\n",
      "train loss=0.06068512613592262(Epoch= 4393)\n",
      "CV loss=0.11918285908227885(Epoch= 4394)\n",
      "train loss=0.06067065678739238(Epoch= 4394)\n",
      "CV loss=0.11905250450433791(Epoch= 4395)\n",
      "train loss=0.06068483944982101(Epoch= 4395)\n",
      "CV loss=0.11917873847412276(Epoch= 4396)\n",
      "train loss=0.06065798185121551(Epoch= 4396)\n",
      "CV loss=0.11938569587458611(Epoch= 4397)\n",
      "train loss=0.06064450402541455(Epoch= 4397)\n",
      "CV loss=0.11921477337006336(Epoch= 4398)\n",
      "train loss=0.060636818445242345(Epoch= 4398)\n",
      "CV loss=0.11923797755312834(Epoch= 4399)\n",
      "train loss=0.06062601573494461(Epoch= 4399)\n",
      "CV loss=0.11925377884356932(Epoch= 4400)\n",
      "train loss=0.06062067266889256(Epoch= 4400)\n",
      "CV loss=0.11910855837159112(Epoch= 4401)\n",
      "train loss=0.0606004292951057(Epoch= 4401)\n",
      "CV loss=0.11918222148918392(Epoch= 4402)\n",
      "train loss=0.060597633502218196(Epoch= 4402)\n",
      "CV loss=0.11914755274956229(Epoch= 4403)\n",
      "train loss=0.06058255749710861(Epoch= 4403)\n",
      "CV loss=0.11918430342320195(Epoch= 4404)\n",
      "train loss=0.060571547819985624(Epoch= 4404)\n",
      "CV loss=0.11913093563951144(Epoch= 4405)\n",
      "train loss=0.06056459378435446(Epoch= 4405)\n",
      "CV loss=0.11895604960847657(Epoch= 4406)\n",
      "train loss=0.06056563898019607(Epoch= 4406)\n",
      "CV loss=0.11904667850395462(Epoch= 4407)\n",
      "train loss=0.06054243469294346(Epoch= 4407)\n",
      "CV loss=0.11900118165683597(Epoch= 4408)\n",
      "train loss=0.060537814080503946(Epoch= 4408)\n",
      "CV loss=0.11912069474119623(Epoch= 4409)\n",
      "train loss=0.06054092383249228(Epoch= 4409)\n",
      "CV loss=0.11907594520322276(Epoch= 4410)\n",
      "train loss=0.06052005069915137(Epoch= 4410)\n",
      "CV loss=0.11904862354865778(Epoch= 4411)\n",
      "train loss=0.06050878540657616(Epoch= 4411)\n",
      "CV loss=0.11928165809098842(Epoch= 4412)\n",
      "train loss=0.06050477857322944(Epoch= 4412)\n",
      "CV loss=0.11908014492192975(Epoch= 4413)\n",
      "train loss=0.060486761824492113(Epoch= 4413)\n",
      "CV loss=0.11897911704536722(Epoch= 4414)\n",
      "train loss=0.060489265573940076(Epoch= 4414)\n",
      "CV loss=0.11897207188929755(Epoch= 4415)\n",
      "train loss=0.06047017641952323(Epoch= 4415)\n",
      "CV loss=0.11912300609980009(Epoch= 4416)\n",
      "train loss=0.06046233960129206(Epoch= 4416)\n",
      "CV loss=0.11892374070299541(Epoch= 4417)\n",
      "train loss=0.06046417043714311(Epoch= 4417)\n",
      "CV loss=0.11902860767849043(Epoch= 4418)\n",
      "train loss=0.06044125595267412(Epoch= 4418)\n",
      "CV loss=0.11904897919260111(Epoch= 4419)\n",
      "train loss=0.060434701672057466(Epoch= 4419)\n",
      "CV loss=0.11890819211489853(Epoch= 4420)\n",
      "train loss=0.06042321114736729(Epoch= 4420)\n",
      "CV loss=0.11894218844640933(Epoch= 4421)\n",
      "train loss=0.060423720150836684(Epoch= 4421)\n",
      "CV loss=0.11900516085103446(Epoch= 4422)\n",
      "train loss=0.06041298034790108(Epoch= 4422)\n",
      "CV loss=0.11887106865756766(Epoch= 4423)\n",
      "train loss=0.06039800557222572(Epoch= 4423)\n",
      "CV loss=0.11883988338180441(Epoch= 4424)\n",
      "train loss=0.060390673849619436(Epoch= 4424)\n",
      "CV loss=0.11885958267995249(Epoch= 4425)\n",
      "train loss=0.06038209210605948(Epoch= 4425)\n",
      "CV loss=0.11914660719665807(Epoch= 4426)\n",
      "train loss=0.060379318427121624(Epoch= 4426)\n",
      "CV loss=0.11879074970332192(Epoch= 4427)\n",
      "train loss=0.06036138151693841(Epoch= 4427)\n",
      "CV loss=0.11891012465120143(Epoch= 4428)\n",
      "train loss=0.06034851547567189(Epoch= 4428)\n",
      "CV loss=0.11893811849891103(Epoch= 4429)\n",
      "train loss=0.06033911545212144(Epoch= 4429)\n",
      "CV loss=0.11885983428890437(Epoch= 4430)\n",
      "train loss=0.06033101999654117(Epoch= 4430)\n",
      "CV loss=0.11889474527499042(Epoch= 4431)\n",
      "train loss=0.060320551328226606(Epoch= 4431)\n",
      "CV loss=0.11881499609722997(Epoch= 4432)\n",
      "train loss=0.060327919886516125(Epoch= 4432)\n",
      "CV loss=0.11871266941752658(Epoch= 4433)\n",
      "train loss=0.060314800019975405(Epoch= 4433)\n",
      "CV loss=0.11893890485580431(Epoch= 4434)\n",
      "train loss=0.06029714713567281(Epoch= 4434)\n",
      "CV loss=0.11887429375943062(Epoch= 4435)\n",
      "train loss=0.06028375124148888(Epoch= 4435)\n",
      "CV loss=0.1188600299826964(Epoch= 4436)\n",
      "train loss=0.060273425985020536(Epoch= 4436)\n",
      "CV loss=0.11876629866174127(Epoch= 4437)\n",
      "train loss=0.06026542123561319(Epoch= 4437)\n",
      "CV loss=0.11875056305894924(Epoch= 4438)\n",
      "train loss=0.060258656508164386(Epoch= 4438)\n",
      "CV loss=0.1186188537167327(Epoch= 4439)\n",
      "train loss=0.0602577725300579(Epoch= 4439)\n",
      "CV loss=0.11876432635922168(Epoch= 4440)\n",
      "train loss=0.0602449928881679(Epoch= 4440)\n",
      "CV loss=0.11878790772139228(Epoch= 4441)\n",
      "train loss=0.060230804061127526(Epoch= 4441)\n",
      "CV loss=0.11874968011650795(Epoch= 4442)\n",
      "train loss=0.06021946849863928(Epoch= 4442)\n",
      "CV loss=0.11868979650737624(Epoch= 4443)\n",
      "train loss=0.06021335190575876(Epoch= 4443)\n",
      "CV loss=0.11865717129393606(Epoch= 4444)\n",
      "train loss=0.06020243854953632(Epoch= 4444)\n",
      "CV loss=0.11873163295711994(Epoch= 4445)\n",
      "train loss=0.06019594210479895(Epoch= 4445)\n",
      "CV loss=0.11872989413913534(Epoch= 4446)\n",
      "train loss=0.060184623030895515(Epoch= 4446)\n",
      "CV loss=0.11880608037795237(Epoch= 4447)\n",
      "train loss=0.0601791365079325(Epoch= 4447)\n",
      "CV loss=0.11862890011525107(Epoch= 4448)\n",
      "train loss=0.0601793175987573(Epoch= 4448)\n",
      "CV loss=0.11862854494390354(Epoch= 4449)\n",
      "train loss=0.06015587715054037(Epoch= 4449)\n",
      "CV loss=0.1187803650866847(Epoch= 4450)\n",
      "train loss=0.060148649311598655(Epoch= 4450)\n",
      "CV loss=0.11873549910381323(Epoch= 4451)\n",
      "train loss=0.060137443359509914(Epoch= 4451)\n",
      "CV loss=0.11882567628914464(Epoch= 4452)\n",
      "train loss=0.06013066352028859(Epoch= 4452)\n",
      "CV loss=0.1187617160560212(Epoch= 4453)\n",
      "train loss=0.06012421787255054(Epoch= 4453)\n",
      "CV loss=0.11859244210236128(Epoch= 4454)\n",
      "train loss=0.06011536393706786(Epoch= 4454)\n",
      "CV loss=0.11861458856065141(Epoch= 4455)\n",
      "train loss=0.060104562499691254(Epoch= 4455)\n",
      "CV loss=0.1186979316910618(Epoch= 4456)\n",
      "train loss=0.06009580600730087(Epoch= 4456)\n",
      "CV loss=0.11866562819693569(Epoch= 4457)\n",
      "train loss=0.060085606869331634(Epoch= 4457)\n",
      "CV loss=0.1186488524718945(Epoch= 4458)\n",
      "train loss=0.06008539733572784(Epoch= 4458)\n",
      "CV loss=0.11856739569553003(Epoch= 4459)\n",
      "train loss=0.060076096482826906(Epoch= 4459)\n",
      "CV loss=0.11853951513795127(Epoch= 4460)\n",
      "train loss=0.06006019235688771(Epoch= 4460)\n",
      "CV loss=0.11866792073719093(Epoch= 4461)\n",
      "train loss=0.06005223616418565(Epoch= 4461)\n",
      "CV loss=0.11857895793219933(Epoch= 4462)\n",
      "train loss=0.06003720807756461(Epoch= 4462)\n",
      "CV loss=0.11854793412765033(Epoch= 4463)\n",
      "train loss=0.06002952929661291(Epoch= 4463)\n",
      "CV loss=0.11855907395942711(Epoch= 4464)\n",
      "train loss=0.060029816944514806(Epoch= 4464)\n",
      "CV loss=0.11856486899568489(Epoch= 4465)\n",
      "train loss=0.06002475446959987(Epoch= 4465)\n",
      "CV loss=0.11847217488016129(Epoch= 4466)\n",
      "train loss=0.060008572474735136(Epoch= 4466)\n",
      "CV loss=0.11857132811195079(Epoch= 4467)\n",
      "train loss=0.05999577206511141(Epoch= 4467)\n",
      "CV loss=0.1184339161242768(Epoch= 4468)\n",
      "train loss=0.059985254031107785(Epoch= 4468)\n",
      "CV loss=0.11845976993987074(Epoch= 4469)\n",
      "train loss=0.059983938738178394(Epoch= 4469)\n",
      "CV loss=0.11846676577334296(Epoch= 4470)\n",
      "train loss=0.059968612559849144(Epoch= 4470)\n",
      "CV loss=0.11834461704434092(Epoch= 4471)\n",
      "train loss=0.059964007614265666(Epoch= 4471)\n",
      "CV loss=0.11868935141568215(Epoch= 4472)\n",
      "train loss=0.05996107965099907(Epoch= 4472)\n",
      "CV loss=0.1184413473339496(Epoch= 4473)\n",
      "train loss=0.05994164244270711(Epoch= 4473)\n",
      "CV loss=0.11826460616866258(Epoch= 4474)\n",
      "train loss=0.05993700512681937(Epoch= 4474)\n",
      "CV loss=0.11831211119382856(Epoch= 4475)\n",
      "train loss=0.059930534271209195(Epoch= 4475)\n",
      "CV loss=0.1185158060779885(Epoch= 4476)\n",
      "train loss=0.059914622445050275(Epoch= 4476)\n",
      "CV loss=0.11854246562492742(Epoch= 4477)\n",
      "train loss=0.05990707508772699(Epoch= 4477)\n",
      "CV loss=0.1183871415632215(Epoch= 4478)\n",
      "train loss=0.059894477658317125(Epoch= 4478)\n",
      "CV loss=0.11853651892084435(Epoch= 4479)\n",
      "train loss=0.05989037800160173(Epoch= 4479)\n",
      "CV loss=0.118547055556858(Epoch= 4480)\n",
      "train loss=0.05988271691460666(Epoch= 4480)\n",
      "CV loss=0.11831357761432157(Epoch= 4481)\n",
      "train loss=0.059870876142848584(Epoch= 4481)\n",
      "CV loss=0.1186560714777641(Epoch= 4482)\n",
      "train loss=0.05987303882209119(Epoch= 4482)\n",
      "CV loss=0.11831831224036155(Epoch= 4483)\n",
      "train loss=0.059851797241895516(Epoch= 4483)\n",
      "CV loss=0.11849097436071393(Epoch= 4484)\n",
      "train loss=0.05984967153039468(Epoch= 4484)\n",
      "CV loss=0.1182195623596373(Epoch= 4485)\n",
      "train loss=0.059839599442060826(Epoch= 4485)\n",
      "CV loss=0.11843106328641702(Epoch= 4486)\n",
      "train loss=0.059828865206240096(Epoch= 4486)\n",
      "CV loss=0.1182533314391842(Epoch= 4487)\n",
      "train loss=0.05981723070506806(Epoch= 4487)\n",
      "CV loss=0.11830825718302393(Epoch= 4488)\n",
      "train loss=0.0598150188474043(Epoch= 4488)\n",
      "CV loss=0.11811574616862777(Epoch= 4489)\n",
      "train loss=0.05980349407765339(Epoch= 4489)\n",
      "CV loss=0.11832381304283515(Epoch= 4490)\n",
      "train loss=0.05979093840934468(Epoch= 4490)\n",
      "CV loss=0.11821677682324024(Epoch= 4491)\n",
      "train loss=0.05978674606471532(Epoch= 4491)\n",
      "CV loss=0.11829600085065434(Epoch= 4492)\n",
      "train loss=0.0597745325555921(Epoch= 4492)\n",
      "CV loss=0.11832954515295399(Epoch= 4493)\n",
      "train loss=0.059769978644611935(Epoch= 4493)\n",
      "CV loss=0.1182697402087091(Epoch= 4494)\n",
      "train loss=0.05975473703961651(Epoch= 4494)\n",
      "CV loss=0.11819282955471316(Epoch= 4495)\n",
      "train loss=0.0597522750794267(Epoch= 4495)\n",
      "CV loss=0.11824849703297252(Epoch= 4496)\n",
      "train loss=0.05973975541903965(Epoch= 4496)\n",
      "CV loss=0.11822247617696921(Epoch= 4497)\n",
      "train loss=0.05973300405478773(Epoch= 4497)\n",
      "CV loss=0.11841581474901827(Epoch= 4498)\n",
      "train loss=0.05972631314554613(Epoch= 4498)\n",
      "CV loss=0.11815063846477776(Epoch= 4499)\n",
      "train loss=0.05970986362798916(Epoch= 4499)\n",
      "CV loss=0.11813310942580663(Epoch= 4500)\n",
      "train loss=0.05970332069910325(Epoch= 4500)\n",
      "CV loss=0.1181700759872265(Epoch= 4501)\n",
      "train loss=0.05970501905996608(Epoch= 4501)\n",
      "CV loss=0.11809747058867882(Epoch= 4502)\n",
      "train loss=0.05968372180192531(Epoch= 4502)\n",
      "CV loss=0.1181387974713867(Epoch= 4503)\n",
      "train loss=0.05967821123811406(Epoch= 4503)\n",
      "CV loss=0.11801291501639685(Epoch= 4504)\n",
      "train loss=0.05966884460511583(Epoch= 4504)\n",
      "CV loss=0.11818395958393951(Epoch= 4505)\n",
      "train loss=0.0596621787783055(Epoch= 4505)\n",
      "CV loss=0.1181447495464237(Epoch= 4506)\n",
      "train loss=0.05964707757171293(Epoch= 4506)\n",
      "CV loss=0.11820818789682754(Epoch= 4507)\n",
      "train loss=0.059649680008749456(Epoch= 4507)\n",
      "CV loss=0.11802800497491621(Epoch= 4508)\n",
      "train loss=0.05963588974773531(Epoch= 4508)\n",
      "CV loss=0.11819149452894437(Epoch= 4509)\n",
      "train loss=0.059632273004210395(Epoch= 4509)\n",
      "CV loss=0.11828682755215923(Epoch= 4510)\n",
      "train loss=0.05963170613914324(Epoch= 4510)\n",
      "CV loss=0.11827204939737988(Epoch= 4511)\n",
      "train loss=0.05961724765118542(Epoch= 4511)\n",
      "CV loss=0.11810002442834178(Epoch= 4512)\n",
      "train loss=0.05959610970942297(Epoch= 4512)\n",
      "CV loss=0.11805238715208327(Epoch= 4513)\n",
      "train loss=0.05960426657855162(Epoch= 4513)\n",
      "CV loss=0.11803498684281885(Epoch= 4514)\n",
      "train loss=0.05958743322821556(Epoch= 4514)\n",
      "CV loss=0.118003284080819(Epoch= 4515)\n",
      "train loss=0.05957937664671263(Epoch= 4515)\n",
      "CV loss=0.1181495806196093(Epoch= 4516)\n",
      "train loss=0.05956450159067573(Epoch= 4516)\n",
      "CV loss=0.11795314768924745(Epoch= 4517)\n",
      "train loss=0.05955724709398604(Epoch= 4517)\n",
      "CV loss=0.11804974982300839(Epoch= 4518)\n",
      "train loss=0.05955214094767856(Epoch= 4518)\n",
      "CV loss=0.11795205216060599(Epoch= 4519)\n",
      "train loss=0.059542316134826065(Epoch= 4519)\n",
      "CV loss=0.11800467138748769(Epoch= 4520)\n",
      "train loss=0.05953093378627342(Epoch= 4520)\n",
      "CV loss=0.11788036264129609(Epoch= 4521)\n",
      "train loss=0.05952316455695621(Epoch= 4521)\n",
      "CV loss=0.11795056195395492(Epoch= 4522)\n",
      "train loss=0.059515894308875834(Epoch= 4522)\n",
      "CV loss=0.11809426519160943(Epoch= 4523)\n",
      "train loss=0.05950554854086487(Epoch= 4523)\n",
      "CV loss=0.1180030569044862(Epoch= 4524)\n",
      "train loss=0.059500743930091596(Epoch= 4524)\n",
      "CV loss=0.11782145905097802(Epoch= 4525)\n",
      "train loss=0.05949230933078427(Epoch= 4525)\n",
      "CV loss=0.1178683662485232(Epoch= 4526)\n",
      "train loss=0.05947599145452171(Epoch= 4526)\n",
      "CV loss=0.11793915872803207(Epoch= 4527)\n",
      "train loss=0.05946725559590262(Epoch= 4527)\n",
      "CV loss=0.1180171917037203(Epoch= 4528)\n",
      "train loss=0.059465032839176965(Epoch= 4528)\n",
      "CV loss=0.11785999526866942(Epoch= 4529)\n",
      "train loss=0.05945363998188643(Epoch= 4529)\n",
      "CV loss=0.11793719062349123(Epoch= 4530)\n",
      "train loss=0.05944552499641266(Epoch= 4530)\n",
      "CV loss=0.11783591646231689(Epoch= 4531)\n",
      "train loss=0.0594377516290067(Epoch= 4531)\n",
      "CV loss=0.11812281565890498(Epoch= 4532)\n",
      "train loss=0.05944558834771743(Epoch= 4532)\n",
      "CV loss=0.11795255602890423(Epoch= 4533)\n",
      "train loss=0.05942222902622432(Epoch= 4533)\n",
      "CV loss=0.11796663915594821(Epoch= 4534)\n",
      "train loss=0.05940867763147882(Epoch= 4534)\n",
      "CV loss=0.11781929490545645(Epoch= 4535)\n",
      "train loss=0.05939914897737675(Epoch= 4535)\n",
      "CV loss=0.11792566360875703(Epoch= 4536)\n",
      "train loss=0.05939584429754448(Epoch= 4536)\n",
      "CV loss=0.11773072260857756(Epoch= 4537)\n",
      "train loss=0.05938575731400551(Epoch= 4537)\n",
      "CV loss=0.11759269512098844(Epoch= 4538)\n",
      "train loss=0.05938875283872937(Epoch= 4538)\n",
      "CV loss=0.1177542465657784(Epoch= 4539)\n",
      "train loss=0.05936974370399125(Epoch= 4539)\n",
      "CV loss=0.11777707213538993(Epoch= 4540)\n",
      "train loss=0.05935835132846096(Epoch= 4540)\n",
      "CV loss=0.11783041936799504(Epoch= 4541)\n",
      "train loss=0.05936311064753309(Epoch= 4541)\n",
      "CV loss=0.11778747490334252(Epoch= 4542)\n",
      "train loss=0.059345376907313986(Epoch= 4542)\n",
      "CV loss=0.11775896955951398(Epoch= 4543)\n",
      "train loss=0.059332814794627174(Epoch= 4543)\n",
      "CV loss=0.11770423616665991(Epoch= 4544)\n",
      "train loss=0.059323153852717754(Epoch= 4544)\n",
      "CV loss=0.1177979578410843(Epoch= 4545)\n",
      "train loss=0.0593193324164275(Epoch= 4545)\n",
      "CV loss=0.11784429851340007(Epoch= 4546)\n",
      "train loss=0.059307896616443645(Epoch= 4546)\n",
      "CV loss=0.11779172139626354(Epoch= 4547)\n",
      "train loss=0.05929713007675444(Epoch= 4547)\n",
      "CV loss=0.11772334105030371(Epoch= 4548)\n",
      "train loss=0.059296109806901816(Epoch= 4548)\n",
      "CV loss=0.11776030057914463(Epoch= 4549)\n",
      "train loss=0.059284030885925175(Epoch= 4549)\n",
      "CV loss=0.11779573374584976(Epoch= 4550)\n",
      "train loss=0.05927336012544522(Epoch= 4550)\n",
      "CV loss=0.11759705265636469(Epoch= 4551)\n",
      "train loss=0.05926928314094813(Epoch= 4551)\n",
      "CV loss=0.11772779655245176(Epoch= 4552)\n",
      "train loss=0.05926032467885031(Epoch= 4552)\n",
      "CV loss=0.11762979676753386(Epoch= 4553)\n",
      "train loss=0.05924892853446622(Epoch= 4553)\n",
      "CV loss=0.11766773883592814(Epoch= 4554)\n",
      "train loss=0.05923782837253915(Epoch= 4554)\n",
      "CV loss=0.11753487470094881(Epoch= 4555)\n",
      "train loss=0.059240981629549716(Epoch= 4555)\n",
      "CV loss=0.1176292684309854(Epoch= 4556)\n",
      "train loss=0.0592260081762629(Epoch= 4556)\n",
      "CV loss=0.117802751564941(Epoch= 4557)\n",
      "train loss=0.0592192281579329(Epoch= 4557)\n",
      "CV loss=0.11767778268140092(Epoch= 4558)\n",
      "train loss=0.0592060736278547(Epoch= 4558)\n",
      "CV loss=0.11780316605493156(Epoch= 4559)\n",
      "train loss=0.05920504352232988(Epoch= 4559)\n",
      "CV loss=0.11747893790312966(Epoch= 4560)\n",
      "train loss=0.05919332980901585(Epoch= 4560)\n",
      "CV loss=0.1176699912211894(Epoch= 4561)\n",
      "train loss=0.05918588789227085(Epoch= 4561)\n",
      "CV loss=0.11748427027043906(Epoch= 4562)\n",
      "train loss=0.05918197042804433(Epoch= 4562)\n",
      "CV loss=0.11742118239586681(Epoch= 4563)\n",
      "train loss=0.05917187163177155(Epoch= 4563)\n",
      "CV loss=0.11758692469833476(Epoch= 4564)\n",
      "train loss=0.0591551127760714(Epoch= 4564)\n",
      "CV loss=0.11765408352042642(Epoch= 4565)\n",
      "train loss=0.05915159617576216(Epoch= 4565)\n",
      "CV loss=0.11753837386920099(Epoch= 4566)\n",
      "train loss=0.059139314696204336(Epoch= 4566)\n",
      "CV loss=0.11755429146965872(Epoch= 4567)\n",
      "train loss=0.059131291371398546(Epoch= 4567)\n",
      "CV loss=0.11749506336470805(Epoch= 4568)\n",
      "train loss=0.05912472609266908(Epoch= 4568)\n",
      "CV loss=0.11754079815301613(Epoch= 4569)\n",
      "train loss=0.05911618789802171(Epoch= 4569)\n",
      "CV loss=0.11736517308912253(Epoch= 4570)\n",
      "train loss=0.059108861049607025(Epoch= 4570)\n",
      "CV loss=0.11753008038348617(Epoch= 4571)\n",
      "train loss=0.05909572256786577(Epoch= 4571)\n",
      "CV loss=0.11745763608127911(Epoch= 4572)\n",
      "train loss=0.05909131533553953(Epoch= 4572)\n",
      "CV loss=0.11751416337878756(Epoch= 4573)\n",
      "train loss=0.059080004784369484(Epoch= 4573)\n",
      "CV loss=0.11736845992971891(Epoch= 4574)\n",
      "train loss=0.05908456110185112(Epoch= 4574)\n",
      "CV loss=0.11752755489305836(Epoch= 4575)\n",
      "train loss=0.05906709663044232(Epoch= 4575)\n",
      "CV loss=0.11753919478988623(Epoch= 4576)\n",
      "train loss=0.05906063642477281(Epoch= 4576)\n",
      "CV loss=0.11752422137813112(Epoch= 4577)\n",
      "train loss=0.059046106882351096(Epoch= 4577)\n",
      "CV loss=0.11760185740059584(Epoch= 4578)\n",
      "train loss=0.05904556615792565(Epoch= 4578)\n",
      "CV loss=0.11761992664904494(Epoch= 4579)\n",
      "train loss=0.05903491701166078(Epoch= 4579)\n",
      "CV loss=0.11747596641153556(Epoch= 4580)\n",
      "train loss=0.05903624994243207(Epoch= 4580)\n",
      "CV loss=0.1174399295424647(Epoch= 4581)\n",
      "train loss=0.05901652018361267(Epoch= 4581)\n",
      "CV loss=0.11744249500326817(Epoch= 4582)\n",
      "train loss=0.05900492930137995(Epoch= 4582)\n",
      "CV loss=0.11759442848911045(Epoch= 4583)\n",
      "train loss=0.05901033654015728(Epoch= 4583)\n",
      "CV loss=0.11737353018637543(Epoch= 4584)\n",
      "train loss=0.058992723742643485(Epoch= 4584)\n",
      "CV loss=0.11746576649044202(Epoch= 4585)\n",
      "train loss=0.058982588505751055(Epoch= 4585)\n",
      "CV loss=0.1174146568295912(Epoch= 4586)\n",
      "train loss=0.058980551276951385(Epoch= 4586)\n",
      "CV loss=0.11734586888647731(Epoch= 4587)\n",
      "train loss=0.05896662249461413(Epoch= 4587)\n",
      "CV loss=0.11738599144963449(Epoch= 4588)\n",
      "train loss=0.058960972679086614(Epoch= 4588)\n",
      "CV loss=0.1174118247065786(Epoch= 4589)\n",
      "train loss=0.05895369075215119(Epoch= 4589)\n",
      "CV loss=0.11727459670463412(Epoch= 4590)\n",
      "train loss=0.05894371578387527(Epoch= 4590)\n",
      "CV loss=0.1173913642678821(Epoch= 4591)\n",
      "train loss=0.05893679560959109(Epoch= 4591)\n",
      "CV loss=0.11737654236519547(Epoch= 4592)\n",
      "train loss=0.05893399635568213(Epoch= 4592)\n",
      "CV loss=0.11740850889157953(Epoch= 4593)\n",
      "train loss=0.058934353224586324(Epoch= 4593)\n",
      "CV loss=0.11731735076012627(Epoch= 4594)\n",
      "train loss=0.058907897169849135(Epoch= 4594)\n",
      "CV loss=0.11732073978199051(Epoch= 4595)\n",
      "train loss=0.05890559418908401(Epoch= 4595)\n",
      "CV loss=0.11726716285641087(Epoch= 4596)\n",
      "train loss=0.05889214230804006(Epoch= 4596)\n",
      "CV loss=0.11721914501612832(Epoch= 4597)\n",
      "train loss=0.058888760235664986(Epoch= 4597)\n",
      "CV loss=0.11724862887767612(Epoch= 4598)\n",
      "train loss=0.058879267773789934(Epoch= 4598)\n",
      "CV loss=0.11716570652137134(Epoch= 4599)\n",
      "train loss=0.05887385931473032(Epoch= 4599)\n",
      "CV loss=0.11748216642303966(Epoch= 4600)\n",
      "train loss=0.058879295433144886(Epoch= 4600)\n",
      "CV loss=0.11722810471542783(Epoch= 4601)\n",
      "train loss=0.05886240332457532(Epoch= 4601)\n",
      "CV loss=0.11717399944096382(Epoch= 4602)\n",
      "train loss=0.058848063376768504(Epoch= 4602)\n",
      "CV loss=0.11733209091685222(Epoch= 4603)\n",
      "train loss=0.05883958633141409(Epoch= 4603)\n",
      "CV loss=0.11727559785109501(Epoch= 4604)\n",
      "train loss=0.0588294883100286(Epoch= 4604)\n",
      "CV loss=0.11720436178516475(Epoch= 4605)\n",
      "train loss=0.05881904643835388(Epoch= 4605)\n",
      "CV loss=0.11714546091746691(Epoch= 4606)\n",
      "train loss=0.058811249724341735(Epoch= 4606)\n",
      "CV loss=0.11715342448362295(Epoch= 4607)\n",
      "train loss=0.05881373199666449(Epoch= 4607)\n",
      "CV loss=0.11745164054811387(Epoch= 4608)\n",
      "train loss=0.05880710640706373(Epoch= 4608)\n",
      "CV loss=0.11722829273997734(Epoch= 4609)\n",
      "train loss=0.05880479413446552(Epoch= 4609)\n",
      "CV loss=0.11721539047988774(Epoch= 4610)\n",
      "train loss=0.05877762475940697(Epoch= 4610)\n",
      "CV loss=0.11725055229036008(Epoch= 4611)\n",
      "train loss=0.0587749870258913(Epoch= 4611)\n",
      "CV loss=0.11725474587811108(Epoch= 4612)\n",
      "train loss=0.05876300258184355(Epoch= 4612)\n",
      "CV loss=0.1172131369072251(Epoch= 4613)\n",
      "train loss=0.058754548335070926(Epoch= 4613)\n",
      "CV loss=0.11720991869014571(Epoch= 4614)\n",
      "train loss=0.05875518686101375(Epoch= 4614)\n",
      "CV loss=0.11698807334107564(Epoch= 4615)\n",
      "train loss=0.0587465876598977(Epoch= 4615)\n",
      "CV loss=0.11691400212070491(Epoch= 4616)\n",
      "train loss=0.058737029011094774(Epoch= 4616)\n",
      "CV loss=0.11711475737921986(Epoch= 4617)\n",
      "train loss=0.05873801916655027(Epoch= 4617)\n",
      "CV loss=0.11703452889886914(Epoch= 4618)\n",
      "train loss=0.05871548602247548(Epoch= 4618)\n",
      "CV loss=0.11713387168062925(Epoch= 4619)\n",
      "train loss=0.05871209415210616(Epoch= 4619)\n",
      "CV loss=0.11700106595141609(Epoch= 4620)\n",
      "train loss=0.05870021365895497(Epoch= 4620)\n",
      "CV loss=0.11709437058730054(Epoch= 4621)\n",
      "train loss=0.05869159255651747(Epoch= 4621)\n",
      "CV loss=0.11688174569998896(Epoch= 4622)\n",
      "train loss=0.05869339401861637(Epoch= 4622)\n",
      "CV loss=0.11717220463904365(Epoch= 4623)\n",
      "train loss=0.05868258667076544(Epoch= 4623)\n",
      "CV loss=0.11704724682658536(Epoch= 4624)\n",
      "train loss=0.058674440533481975(Epoch= 4624)\n",
      "CV loss=0.11713488817800492(Epoch= 4625)\n",
      "train loss=0.05866739652142739(Epoch= 4625)\n",
      "CV loss=0.11711791343030106(Epoch= 4626)\n",
      "train loss=0.05865448575515167(Epoch= 4626)\n",
      "CV loss=0.11700869089389064(Epoch= 4627)\n",
      "train loss=0.0586442197194908(Epoch= 4627)\n",
      "CV loss=0.11709885316896379(Epoch= 4628)\n",
      "train loss=0.05863732409079819(Epoch= 4628)\n",
      "CV loss=0.11685439835361373(Epoch= 4629)\n",
      "train loss=0.0586344133963302(Epoch= 4629)\n",
      "CV loss=0.11719323275029699(Epoch= 4630)\n",
      "train loss=0.05862650784077735(Epoch= 4630)\n",
      "CV loss=0.11705828953693169(Epoch= 4631)\n",
      "train loss=0.0586150744786598(Epoch= 4631)\n",
      "CV loss=0.11680828906848306(Epoch= 4632)\n",
      "train loss=0.0586112554754148(Epoch= 4632)\n",
      "CV loss=0.11710831596642296(Epoch= 4633)\n",
      "train loss=0.05860338642338004(Epoch= 4633)\n",
      "CV loss=0.11696702913913604(Epoch= 4634)\n",
      "train loss=0.05858865081690344(Epoch= 4634)\n",
      "CV loss=0.11692540510737513(Epoch= 4635)\n",
      "train loss=0.058580411576888494(Epoch= 4635)\n",
      "CV loss=0.11705128544274165(Epoch= 4636)\n",
      "train loss=0.05857386415457501(Epoch= 4636)\n",
      "CV loss=0.11691995860883841(Epoch= 4637)\n",
      "train loss=0.05857214093592231(Epoch= 4637)\n",
      "CV loss=0.11693642429144478(Epoch= 4638)\n",
      "train loss=0.05856750364253779(Epoch= 4638)\n",
      "CV loss=0.11695393794949746(Epoch= 4639)\n",
      "train loss=0.05854850700638827(Epoch= 4639)\n",
      "CV loss=0.11693949298314951(Epoch= 4640)\n",
      "train loss=0.05854445061699867(Epoch= 4640)\n",
      "CV loss=0.11700709692723833(Epoch= 4641)\n",
      "train loss=0.058545823336287744(Epoch= 4641)\n",
      "CV loss=0.11676390892029517(Epoch= 4642)\n",
      "train loss=0.05854943217528716(Epoch= 4642)\n",
      "CV loss=0.11688876026654385(Epoch= 4643)\n",
      "train loss=0.058525847597697606(Epoch= 4643)\n",
      "CV loss=0.11692027957448659(Epoch= 4644)\n",
      "train loss=0.058511443323476256(Epoch= 4644)\n",
      "CV loss=0.11691741047956639(Epoch= 4645)\n",
      "train loss=0.05851406671835033(Epoch= 4645)\n",
      "CV loss=0.11686750343645322(Epoch= 4646)\n",
      "train loss=0.058505638333087376(Epoch= 4646)\n",
      "CV loss=0.1166883366989302(Epoch= 4647)\n",
      "train loss=0.05849077730883273(Epoch= 4647)\n",
      "CV loss=0.11694964733842309(Epoch= 4648)\n",
      "train loss=0.05848170276930589(Epoch= 4648)\n",
      "CV loss=0.11694440140497435(Epoch= 4649)\n",
      "train loss=0.05848149694161735(Epoch= 4649)\n",
      "CV loss=0.11690056607951402(Epoch= 4650)\n",
      "train loss=0.0584624161050631(Epoch= 4650)\n",
      "CV loss=0.1170037310563597(Epoch= 4651)\n",
      "train loss=0.05846640107903256(Epoch= 4651)\n",
      "CV loss=0.11670913687088141(Epoch= 4652)\n",
      "train loss=0.05844934101977912(Epoch= 4652)\n",
      "CV loss=0.11676336022810996(Epoch= 4653)\n",
      "train loss=0.058451755748654646(Epoch= 4653)\n",
      "CV loss=0.117076035284642(Epoch= 4654)\n",
      "train loss=0.0584483125832761(Epoch= 4654)\n",
      "CV loss=0.11665789068612613(Epoch= 4655)\n",
      "train loss=0.058430047288000816(Epoch= 4655)\n",
      "CV loss=0.1168405997662636(Epoch= 4656)\n",
      "train loss=0.058428221988125986(Epoch= 4656)\n",
      "CV loss=0.11671532540580577(Epoch= 4657)\n",
      "train loss=0.05841268382148713(Epoch= 4657)\n",
      "CV loss=0.11673955002981816(Epoch= 4658)\n",
      "train loss=0.0584003543155704(Epoch= 4658)\n",
      "CV loss=0.11682167133726333(Epoch= 4659)\n",
      "train loss=0.05839629629293672(Epoch= 4659)\n",
      "CV loss=0.11668553605760301(Epoch= 4660)\n",
      "train loss=0.0583862542426716(Epoch= 4660)\n",
      "CV loss=0.11677705699592258(Epoch= 4661)\n",
      "train loss=0.05838037776933686(Epoch= 4661)\n",
      "CV loss=0.11674179847463968(Epoch= 4662)\n",
      "train loss=0.058371603100540244(Epoch= 4662)\n",
      "CV loss=0.11658331693615265(Epoch= 4663)\n",
      "train loss=0.05837045043106692(Epoch= 4663)\n",
      "CV loss=0.11675149508927654(Epoch= 4664)\n",
      "train loss=0.05836129061017998(Epoch= 4664)\n",
      "CV loss=0.11678956554870479(Epoch= 4665)\n",
      "train loss=0.05836134495561433(Epoch= 4665)\n",
      "CV loss=0.11680368435816715(Epoch= 4666)\n",
      "train loss=0.058340953512483626(Epoch= 4666)\n",
      "CV loss=0.11670659986232347(Epoch= 4667)\n",
      "train loss=0.058333869781654994(Epoch= 4667)\n",
      "CV loss=0.11676353594887959(Epoch= 4668)\n",
      "train loss=0.05833379096157899(Epoch= 4668)\n",
      "CV loss=0.1166674675282385(Epoch= 4669)\n",
      "train loss=0.058322224041904513(Epoch= 4669)\n",
      "CV loss=0.11662176778410102(Epoch= 4670)\n",
      "train loss=0.05830977431798095(Epoch= 4670)\n",
      "CV loss=0.11687389826000649(Epoch= 4671)\n",
      "train loss=0.05830903973399607(Epoch= 4671)\n",
      "CV loss=0.11652008150188986(Epoch= 4672)\n",
      "train loss=0.05829708856702349(Epoch= 4672)\n",
      "CV loss=0.11668814030670628(Epoch= 4673)\n",
      "train loss=0.0582861268127797(Epoch= 4673)\n",
      "CV loss=0.11656015946950354(Epoch= 4674)\n",
      "train loss=0.05828012515727273(Epoch= 4674)\n",
      "CV loss=0.11660833182719027(Epoch= 4675)\n",
      "train loss=0.05827101795033072(Epoch= 4675)\n",
      "CV loss=0.11654099593113298(Epoch= 4676)\n",
      "train loss=0.05826329379983384(Epoch= 4676)\n",
      "CV loss=0.11678580731850371(Epoch= 4677)\n",
      "train loss=0.05826408544764343(Epoch= 4677)\n",
      "CV loss=0.11652079974908569(Epoch= 4678)\n",
      "train loss=0.05824628062728543(Epoch= 4678)\n",
      "CV loss=0.11661458969946482(Epoch= 4679)\n",
      "train loss=0.058240577562914356(Epoch= 4679)\n",
      "CV loss=0.11664640280295989(Epoch= 4680)\n",
      "train loss=0.05823802626315624(Epoch= 4680)\n",
      "CV loss=0.11661710303786932(Epoch= 4681)\n",
      "train loss=0.05822377473028092(Epoch= 4681)\n",
      "CV loss=0.11676126911130309(Epoch= 4682)\n",
      "train loss=0.05822632157537936(Epoch= 4682)\n",
      "CV loss=0.11650536675350862(Epoch= 4683)\n",
      "train loss=0.05820965493983987(Epoch= 4683)\n",
      "CV loss=0.11657513816768217(Epoch= 4684)\n",
      "train loss=0.05820241181380704(Epoch= 4684)\n",
      "CV loss=0.11670214836785867(Epoch= 4685)\n",
      "train loss=0.05821282922186763(Epoch= 4685)\n",
      "CV loss=0.11646796134977022(Epoch= 4686)\n",
      "train loss=0.058191027519307825(Epoch= 4686)\n",
      "CV loss=0.116656102441244(Epoch= 4687)\n",
      "train loss=0.05819032323319211(Epoch= 4687)\n",
      "CV loss=0.11645659591372262(Epoch= 4688)\n",
      "train loss=0.058170228215937196(Epoch= 4688)\n",
      "CV loss=0.11643206120497852(Epoch= 4689)\n",
      "train loss=0.05816715831507728(Epoch= 4689)\n",
      "CV loss=0.1164433220205261(Epoch= 4690)\n",
      "train loss=0.058161754029705526(Epoch= 4690)\n",
      "CV loss=0.11661010325269093(Epoch= 4691)\n",
      "train loss=0.058153552288501435(Epoch= 4691)\n",
      "CV loss=0.11648830922380418(Epoch= 4692)\n",
      "train loss=0.05814556282971063(Epoch= 4692)\n",
      "CV loss=0.11664812216536896(Epoch= 4693)\n",
      "train loss=0.05814155771195196(Epoch= 4693)\n",
      "CV loss=0.11644000642134122(Epoch= 4694)\n",
      "train loss=0.0581335943844023(Epoch= 4694)\n",
      "CV loss=0.11630425048366047(Epoch= 4695)\n",
      "train loss=0.05812403437440768(Epoch= 4695)\n",
      "CV loss=0.11636127397525033(Epoch= 4696)\n",
      "train loss=0.05811928824287557(Epoch= 4696)\n",
      "CV loss=0.11625457069263964(Epoch= 4697)\n",
      "train loss=0.05811179380168084(Epoch= 4697)\n",
      "CV loss=0.11645922996489173(Epoch= 4698)\n",
      "train loss=0.05809744676222591(Epoch= 4698)\n",
      "CV loss=0.1165324136488432(Epoch= 4699)\n",
      "train loss=0.05809225594254015(Epoch= 4699)\n",
      "CV loss=0.11645403195303497(Epoch= 4700)\n",
      "train loss=0.05808050563310067(Epoch= 4700)\n",
      "CV loss=0.11639604886603444(Epoch= 4701)\n",
      "train loss=0.05807575623869614(Epoch= 4701)\n",
      "CV loss=0.11623613338197578(Epoch= 4702)\n",
      "train loss=0.05807220639020908(Epoch= 4702)\n",
      "CV loss=0.1163839071756894(Epoch= 4703)\n",
      "train loss=0.05805977449033997(Epoch= 4703)\n",
      "CV loss=0.11643236191941461(Epoch= 4704)\n",
      "train loss=0.05805466972151491(Epoch= 4704)\n",
      "CV loss=0.11627265236023156(Epoch= 4705)\n",
      "train loss=0.05804725333472669(Epoch= 4705)\n",
      "CV loss=0.11633973135150624(Epoch= 4706)\n",
      "train loss=0.058039440032954984(Epoch= 4706)\n",
      "CV loss=0.11631409840634636(Epoch= 4707)\n",
      "train loss=0.0580309159498332(Epoch= 4707)\n",
      "CV loss=0.11657755575639239(Epoch= 4708)\n",
      "train loss=0.05803409474906844(Epoch= 4708)\n",
      "CV loss=0.1163151862073617(Epoch= 4709)\n",
      "train loss=0.058017881012796306(Epoch= 4709)\n",
      "CV loss=0.11641518616566132(Epoch= 4710)\n",
      "train loss=0.058008873576774(Epoch= 4710)\n",
      "CV loss=0.1163373270235598(Epoch= 4711)\n",
      "train loss=0.05800289595730804(Epoch= 4711)\n",
      "CV loss=0.11625595839685451(Epoch= 4712)\n",
      "train loss=0.0579956502283496(Epoch= 4712)\n",
      "CV loss=0.1162049551982233(Epoch= 4713)\n",
      "train loss=0.057992842051964526(Epoch= 4713)\n",
      "CV loss=0.11630328872613882(Epoch= 4714)\n",
      "train loss=0.057983629900649376(Epoch= 4714)\n",
      "CV loss=0.11633778745792511(Epoch= 4715)\n",
      "train loss=0.05797014982101132(Epoch= 4715)\n",
      "CV loss=0.11626225541702544(Epoch= 4716)\n",
      "train loss=0.05797007861570738(Epoch= 4716)\n",
      "CV loss=0.11655862000478895(Epoch= 4717)\n",
      "train loss=0.057974387176854915(Epoch= 4717)\n",
      "CV loss=0.11638934101359882(Epoch= 4718)\n",
      "train loss=0.05795570962765992(Epoch= 4718)\n",
      "CV loss=0.11622319372279942(Epoch= 4719)\n",
      "train loss=0.057939796529571194(Epoch= 4719)\n",
      "CV loss=0.11612763710263801(Epoch= 4720)\n",
      "train loss=0.05793867251159728(Epoch= 4720)\n",
      "CV loss=0.1164202503310873(Epoch= 4721)\n",
      "train loss=0.05795085419753611(Epoch= 4721)\n",
      "CV loss=0.11637293015090203(Epoch= 4722)\n",
      "train loss=0.0579206907130314(Epoch= 4722)\n",
      "CV loss=0.11629660262634776(Epoch= 4723)\n",
      "train loss=0.05791895119135973(Epoch= 4723)\n",
      "CV loss=0.11614515684531954(Epoch= 4724)\n",
      "train loss=0.05790323796993593(Epoch= 4724)\n",
      "CV loss=0.11621411120646069(Epoch= 4725)\n",
      "train loss=0.0578969924072354(Epoch= 4725)\n",
      "CV loss=0.1160486539739305(Epoch= 4726)\n",
      "train loss=0.05789447812442258(Epoch= 4726)\n",
      "CV loss=0.11626545502788918(Epoch= 4727)\n",
      "train loss=0.0578888711692899(Epoch= 4727)\n",
      "CV loss=0.11614429798604145(Epoch= 4728)\n",
      "train loss=0.057878132423412415(Epoch= 4728)\n",
      "CV loss=0.11602022544202137(Epoch= 4729)\n",
      "train loss=0.05787245032186916(Epoch= 4729)\n",
      "CV loss=0.11614082981185303(Epoch= 4730)\n",
      "train loss=0.05785920076082314(Epoch= 4730)\n",
      "CV loss=0.11607391256310072(Epoch= 4731)\n",
      "train loss=0.057856412861476025(Epoch= 4731)\n",
      "CV loss=0.11602779599279708(Epoch= 4732)\n",
      "train loss=0.05784885627042019(Epoch= 4732)\n",
      "CV loss=0.11605129943513383(Epoch= 4733)\n",
      "train loss=0.05784183663275706(Epoch= 4733)\n",
      "CV loss=0.11609114757093264(Epoch= 4734)\n",
      "train loss=0.057834395239597165(Epoch= 4734)\n",
      "CV loss=0.11610405100951593(Epoch= 4735)\n",
      "train loss=0.057822209752577486(Epoch= 4735)\n",
      "CV loss=0.11604820479747588(Epoch= 4736)\n",
      "train loss=0.05782055904918791(Epoch= 4736)\n",
      "CV loss=0.11598693937169102(Epoch= 4737)\n",
      "train loss=0.05781455944100476(Epoch= 4737)\n",
      "CV loss=0.11614047657962842(Epoch= 4738)\n",
      "train loss=0.05780420465353263(Epoch= 4738)\n",
      "CV loss=0.11610669276340548(Epoch= 4739)\n",
      "train loss=0.05779907805796087(Epoch= 4739)\n",
      "CV loss=0.11624493377997994(Epoch= 4740)\n",
      "train loss=0.057791814276644544(Epoch= 4740)\n",
      "CV loss=0.11618664166482637(Epoch= 4741)\n",
      "train loss=0.05778184056174029(Epoch= 4741)\n",
      "CV loss=0.1159899340993506(Epoch= 4742)\n",
      "train loss=0.057774772939431084(Epoch= 4742)\n",
      "CV loss=0.11603502023216504(Epoch= 4743)\n",
      "train loss=0.05776576455186831(Epoch= 4743)\n",
      "CV loss=0.11604730538144756(Epoch= 4744)\n",
      "train loss=0.05775774484771709(Epoch= 4744)\n",
      "CV loss=0.1161999568587417(Epoch= 4745)\n",
      "train loss=0.0577687533259469(Epoch= 4745)\n",
      "CV loss=0.11593226687861372(Epoch= 4746)\n",
      "train loss=0.057751969882216425(Epoch= 4746)\n",
      "CV loss=0.11606600455826996(Epoch= 4747)\n",
      "train loss=0.057734926862600455(Epoch= 4747)\n",
      "CV loss=0.11603086793669555(Epoch= 4748)\n",
      "train loss=0.05773511335516034(Epoch= 4748)\n",
      "CV loss=0.11600276582931748(Epoch= 4749)\n",
      "train loss=0.057723981674312544(Epoch= 4749)\n",
      "CV loss=0.11597026529723276(Epoch= 4750)\n",
      "train loss=0.05771391703761311(Epoch= 4750)\n",
      "CV loss=0.11594812202453304(Epoch= 4751)\n",
      "train loss=0.05770575970344827(Epoch= 4751)\n",
      "CV loss=0.11595612035197189(Epoch= 4752)\n",
      "train loss=0.057709615119757925(Epoch= 4752)\n",
      "CV loss=0.11596695087988151(Epoch= 4753)\n",
      "train loss=0.0576952417407964(Epoch= 4753)\n",
      "CV loss=0.11585476544201867(Epoch= 4754)\n",
      "train loss=0.05769556887927026(Epoch= 4754)\n",
      "CV loss=0.1159920706946273(Epoch= 4755)\n",
      "train loss=0.05768010192265983(Epoch= 4755)\n",
      "CV loss=0.11601041497694474(Epoch= 4756)\n",
      "train loss=0.05767217141436462(Epoch= 4756)\n",
      "CV loss=0.11590209525210957(Epoch= 4757)\n",
      "train loss=0.057663035551329504(Epoch= 4757)\n",
      "CV loss=0.11614513570987468(Epoch= 4758)\n",
      "train loss=0.05767599468642036(Epoch= 4758)\n",
      "CV loss=0.11581875579886(Epoch= 4759)\n",
      "train loss=0.05765954557764633(Epoch= 4759)\n",
      "CV loss=0.11592431170394144(Epoch= 4760)\n",
      "train loss=0.05764362291724497(Epoch= 4760)\n",
      "CV loss=0.11593662748871683(Epoch= 4761)\n",
      "train loss=0.057634527640150715(Epoch= 4761)\n",
      "CV loss=0.11595508773016852(Epoch= 4762)\n",
      "train loss=0.05762800702983015(Epoch= 4762)\n",
      "CV loss=0.11581216027910685(Epoch= 4763)\n",
      "train loss=0.0576217184531513(Epoch= 4763)\n",
      "CV loss=0.11597646026112549(Epoch= 4764)\n",
      "train loss=0.057614611700602066(Epoch= 4764)\n",
      "CV loss=0.11583211833590518(Epoch= 4765)\n",
      "train loss=0.05761073197370547(Epoch= 4765)\n",
      "CV loss=0.11578610109726287(Epoch= 4766)\n",
      "train loss=0.05760247247593299(Epoch= 4766)\n",
      "CV loss=0.11577668344635819(Epoch= 4767)\n",
      "train loss=0.05759207616720533(Epoch= 4767)\n",
      "CV loss=0.11587383970594099(Epoch= 4768)\n",
      "train loss=0.057583876799188256(Epoch= 4768)\n",
      "CV loss=0.11599445380967424(Epoch= 4769)\n",
      "train loss=0.05759311145076636(Epoch= 4769)\n",
      "CV loss=0.11592246403814636(Epoch= 4770)\n",
      "train loss=0.057580351775898604(Epoch= 4770)\n",
      "CV loss=0.11598776943017032(Epoch= 4771)\n",
      "train loss=0.05757967788693296(Epoch= 4771)\n",
      "CV loss=0.11572738522469649(Epoch= 4772)\n",
      "train loss=0.05756104026642265(Epoch= 4772)\n",
      "CV loss=0.11571859489127356(Epoch= 4773)\n",
      "train loss=0.057550604278010786(Epoch= 4773)\n",
      "CV loss=0.1157758349511423(Epoch= 4774)\n",
      "train loss=0.057542277716760765(Epoch= 4774)\n",
      "CV loss=0.11579356947989732(Epoch= 4775)\n",
      "train loss=0.05753681709536405(Epoch= 4775)\n",
      "CV loss=0.11597802475671304(Epoch= 4776)\n",
      "train loss=0.057536222579463836(Epoch= 4776)\n",
      "CV loss=0.11592029399222678(Epoch= 4777)\n",
      "train loss=0.05753402872032079(Epoch= 4777)\n",
      "CV loss=0.11579977506567737(Epoch= 4778)\n",
      "train loss=0.05751929518239774(Epoch= 4778)\n",
      "CV loss=0.11571482997400039(Epoch= 4779)\n",
      "train loss=0.057520943880260744(Epoch= 4779)\n",
      "CV loss=0.11590921927109192(Epoch= 4780)\n",
      "train loss=0.05750622390880855(Epoch= 4780)\n",
      "CV loss=0.11590872073028077(Epoch= 4781)\n",
      "train loss=0.05749970546947328(Epoch= 4781)\n",
      "CV loss=0.11577565671690375(Epoch= 4782)\n",
      "train loss=0.05748798931883031(Epoch= 4782)\n",
      "CV loss=0.11571951069578075(Epoch= 4783)\n",
      "train loss=0.05747875993527998(Epoch= 4783)\n",
      "CV loss=0.11580004811950725(Epoch= 4784)\n",
      "train loss=0.05747622035210228(Epoch= 4784)\n",
      "CV loss=0.115720419546843(Epoch= 4785)\n",
      "train loss=0.05747436015516182(Epoch= 4785)\n",
      "CV loss=0.11573581402503447(Epoch= 4786)\n",
      "train loss=0.05746642520895114(Epoch= 4786)\n",
      "CV loss=0.1156832027707419(Epoch= 4787)\n",
      "train loss=0.05745705399581491(Epoch= 4787)\n",
      "CV loss=0.11576309466534335(Epoch= 4788)\n",
      "train loss=0.05744984626038647(Epoch= 4788)\n",
      "CV loss=0.11571295446362256(Epoch= 4789)\n",
      "train loss=0.05743830345103521(Epoch= 4789)\n",
      "CV loss=0.11585365962340867(Epoch= 4790)\n",
      "train loss=0.05743457414642588(Epoch= 4790)\n",
      "CV loss=0.11567549525993695(Epoch= 4791)\n",
      "train loss=0.05742359402763383(Epoch= 4791)\n",
      "CV loss=0.11582631766240789(Epoch= 4792)\n",
      "train loss=0.05742334948310298(Epoch= 4792)\n",
      "CV loss=0.1154804240063481(Epoch= 4793)\n",
      "train loss=0.05742668071465238(Epoch= 4793)\n",
      "CV loss=0.11573348400285702(Epoch= 4794)\n",
      "train loss=0.057407525623649154(Epoch= 4794)\n",
      "CV loss=0.1156500920385212(Epoch= 4795)\n",
      "train loss=0.05739581239500788(Epoch= 4795)\n",
      "CV loss=0.1155611634947005(Epoch= 4796)\n",
      "train loss=0.057388356197002485(Epoch= 4796)\n",
      "CV loss=0.11561830096035564(Epoch= 4797)\n",
      "train loss=0.0573797914301874(Epoch= 4797)\n",
      "CV loss=0.11565466850295161(Epoch= 4798)\n",
      "train loss=0.05737333274203907(Epoch= 4798)\n",
      "CV loss=0.1155749890520791(Epoch= 4799)\n",
      "train loss=0.05736611012653699(Epoch= 4799)\n",
      "CV loss=0.11548932804581377(Epoch= 4800)\n",
      "train loss=0.05736624056825375(Epoch= 4800)\n",
      "CV loss=0.11553762519431117(Epoch= 4801)\n",
      "train loss=0.057354771964942194(Epoch= 4801)\n",
      "CV loss=0.11551171470805313(Epoch= 4802)\n",
      "train loss=0.057350708929524(Epoch= 4802)\n",
      "CV loss=0.11558971139632448(Epoch= 4803)\n",
      "train loss=0.05735191487373643(Epoch= 4803)\n",
      "CV loss=0.11555828583390199(Epoch= 4804)\n",
      "train loss=0.05733376525231633(Epoch= 4804)\n",
      "CV loss=0.11558243688853095(Epoch= 4805)\n",
      "train loss=0.05732791523961041(Epoch= 4805)\n",
      "CV loss=0.11575465288459989(Epoch= 4806)\n",
      "train loss=0.057323860733785564(Epoch= 4806)\n",
      "CV loss=0.11549283475386471(Epoch= 4807)\n",
      "train loss=0.057315505039514394(Epoch= 4807)\n",
      "CV loss=0.1155703226562617(Epoch= 4808)\n",
      "train loss=0.05731585171725179(Epoch= 4808)\n",
      "CV loss=0.11544892091488858(Epoch= 4809)\n",
      "train loss=0.05730268901389459(Epoch= 4809)\n",
      "CV loss=0.11555013485603896(Epoch= 4810)\n",
      "train loss=0.05729773833417616(Epoch= 4810)\n",
      "CV loss=0.11558868957800388(Epoch= 4811)\n",
      "train loss=0.057285043759111615(Epoch= 4811)\n",
      "CV loss=0.11551732919582766(Epoch= 4812)\n",
      "train loss=0.05727694379510415(Epoch= 4812)\n",
      "CV loss=0.11549567794558407(Epoch= 4813)\n",
      "train loss=0.05727197706807441(Epoch= 4813)\n",
      "CV loss=0.11554884877060703(Epoch= 4814)\n",
      "train loss=0.05726883576290505(Epoch= 4814)\n",
      "CV loss=0.11543939304652989(Epoch= 4815)\n",
      "train loss=0.057256162898437955(Epoch= 4815)\n",
      "CV loss=0.11550285951374276(Epoch= 4816)\n",
      "train loss=0.05724957933591951(Epoch= 4816)\n",
      "CV loss=0.11551940112515635(Epoch= 4817)\n",
      "train loss=0.05724222390217565(Epoch= 4817)\n",
      "CV loss=0.1155654497556948(Epoch= 4818)\n",
      "train loss=0.05725065915226506(Epoch= 4818)\n",
      "CV loss=0.11552786021546765(Epoch= 4819)\n",
      "train loss=0.057230705808183714(Epoch= 4819)\n",
      "CV loss=0.11540925150962966(Epoch= 4820)\n",
      "train loss=0.05722292975339275(Epoch= 4820)\n",
      "CV loss=0.1153829784692921(Epoch= 4821)\n",
      "train loss=0.05721546488365792(Epoch= 4821)\n",
      "CV loss=0.11556222287923573(Epoch= 4822)\n",
      "train loss=0.05722452107922147(Epoch= 4822)\n",
      "CV loss=0.11548634864433857(Epoch= 4823)\n",
      "train loss=0.0572032404896761(Epoch= 4823)\n",
      "CV loss=0.11539562474125084(Epoch= 4824)\n",
      "train loss=0.05719463831991599(Epoch= 4824)\n",
      "CV loss=0.11553222870784616(Epoch= 4825)\n",
      "train loss=0.05718952730159456(Epoch= 4825)\n",
      "CV loss=0.11546984845841225(Epoch= 4826)\n",
      "train loss=0.05718676365948326(Epoch= 4826)\n",
      "CV loss=0.11535967912545027(Epoch= 4827)\n",
      "train loss=0.057177972773677185(Epoch= 4827)\n",
      "CV loss=0.11528768839485548(Epoch= 4828)\n",
      "train loss=0.05717684972498593(Epoch= 4828)\n",
      "CV loss=0.11546148655049733(Epoch= 4829)\n",
      "train loss=0.057163053771689024(Epoch= 4829)\n",
      "CV loss=0.11538400483370273(Epoch= 4830)\n",
      "train loss=0.05715374610846519(Epoch= 4830)\n",
      "CV loss=0.11532398981041625(Epoch= 4831)\n",
      "train loss=0.057148193610586304(Epoch= 4831)\n",
      "CV loss=0.1153688950799392(Epoch= 4832)\n",
      "train loss=0.0571517731217835(Epoch= 4832)\n",
      "CV loss=0.11522281139469484(Epoch= 4833)\n",
      "train loss=0.05713773890172776(Epoch= 4833)\n",
      "CV loss=0.11535973340722772(Epoch= 4834)\n",
      "train loss=0.05712937155689327(Epoch= 4834)\n",
      "CV loss=0.11544648270545142(Epoch= 4835)\n",
      "train loss=0.057122244212710734(Epoch= 4835)\n",
      "CV loss=0.11538094239620426(Epoch= 4836)\n",
      "train loss=0.05711305159161876(Epoch= 4836)\n",
      "CV loss=0.11520693999932348(Epoch= 4837)\n",
      "train loss=0.057114490640018424(Epoch= 4837)\n",
      "CV loss=0.1152585426952981(Epoch= 4838)\n",
      "train loss=0.05711042018064759(Epoch= 4838)\n",
      "CV loss=0.11526539016898253(Epoch= 4839)\n",
      "train loss=0.057101109865346666(Epoch= 4839)\n",
      "CV loss=0.11537507982540668(Epoch= 4840)\n",
      "train loss=0.05709653013238998(Epoch= 4840)\n",
      "CV loss=0.11539560759940592(Epoch= 4841)\n",
      "train loss=0.057082693242459646(Epoch= 4841)\n",
      "CV loss=0.11526078973961665(Epoch= 4842)\n",
      "train loss=0.05707563665516397(Epoch= 4842)\n",
      "CV loss=0.1154573404187394(Epoch= 4843)\n",
      "train loss=0.05707181541441851(Epoch= 4843)\n",
      "CV loss=0.11551312386451576(Epoch= 4844)\n",
      "train loss=0.057068214406201946(Epoch= 4844)\n",
      "CV loss=0.11520837664307534(Epoch= 4845)\n",
      "train loss=0.057059382685661214(Epoch= 4845)\n",
      "CV loss=0.11511189259651608(Epoch= 4846)\n",
      "train loss=0.05706302223849673(Epoch= 4846)\n",
      "CV loss=0.11511664462266633(Epoch= 4847)\n",
      "train loss=0.05705613699873753(Epoch= 4847)\n",
      "CV loss=0.11538959937088138(Epoch= 4848)\n",
      "train loss=0.05704296816072091(Epoch= 4848)\n",
      "CV loss=0.11526203982300681(Epoch= 4849)\n",
      "train loss=0.05702852600741118(Epoch= 4849)\n",
      "CV loss=0.11526924869897698(Epoch= 4850)\n",
      "train loss=0.05701883759392878(Epoch= 4850)\n",
      "CV loss=0.11540004744751514(Epoch= 4851)\n",
      "train loss=0.057024210046064734(Epoch= 4851)\n",
      "CV loss=0.11510004339975614(Epoch= 4852)\n",
      "train loss=0.057007068964778106(Epoch= 4852)\n",
      "CV loss=0.11507025709694102(Epoch= 4853)\n",
      "train loss=0.05700354135064734(Epoch= 4853)\n",
      "CV loss=0.1151307173867741(Epoch= 4854)\n",
      "train loss=0.05699557683774259(Epoch= 4854)\n",
      "CV loss=0.11521867908428515(Epoch= 4855)\n",
      "train loss=0.05699119053302204(Epoch= 4855)\n",
      "CV loss=0.11523569365153154(Epoch= 4856)\n",
      "train loss=0.056983895212549596(Epoch= 4856)\n",
      "CV loss=0.11516249292646219(Epoch= 4857)\n",
      "train loss=0.056972481574324337(Epoch= 4857)\n",
      "CV loss=0.11513712466968841(Epoch= 4858)\n",
      "train loss=0.056968172030962126(Epoch= 4858)\n",
      "CV loss=0.11513355424929894(Epoch= 4859)\n",
      "train loss=0.0569588993362594(Epoch= 4859)\n",
      "CV loss=0.1152499593181692(Epoch= 4860)\n",
      "train loss=0.056959295298002935(Epoch= 4860)\n",
      "CV loss=0.11514590931846605(Epoch= 4861)\n",
      "train loss=0.056966525589132865(Epoch= 4861)\n",
      "CV loss=0.115155162566067(Epoch= 4862)\n",
      "train loss=0.056942889687762885(Epoch= 4862)\n",
      "CV loss=0.11511517994571491(Epoch= 4863)\n",
      "train loss=0.05694433729927374(Epoch= 4863)\n",
      "CV loss=0.11516964461260135(Epoch= 4864)\n",
      "train loss=0.05692982279862806(Epoch= 4864)\n",
      "CV loss=0.1151401457604054(Epoch= 4865)\n",
      "train loss=0.056925512122639056(Epoch= 4865)\n",
      "CV loss=0.11501453378064638(Epoch= 4866)\n",
      "train loss=0.05691569109320811(Epoch= 4866)\n",
      "CV loss=0.11525402260787009(Epoch= 4867)\n",
      "train loss=0.05691238292243143(Epoch= 4867)\n",
      "CV loss=0.11498074690293836(Epoch= 4868)\n",
      "train loss=0.056903867277678516(Epoch= 4868)\n",
      "CV loss=0.11519156491006183(Epoch= 4869)\n",
      "train loss=0.05689636080717615(Epoch= 4869)\n",
      "CV loss=0.11501989700328491(Epoch= 4870)\n",
      "train loss=0.05688859646429993(Epoch= 4870)\n",
      "CV loss=0.1151153024300424(Epoch= 4871)\n",
      "train loss=0.056881795897893606(Epoch= 4871)\n",
      "CV loss=0.11514208193850711(Epoch= 4872)\n",
      "train loss=0.056872722421005155(Epoch= 4872)\n",
      "CV loss=0.11508789661608751(Epoch= 4873)\n",
      "train loss=0.05687205528617807(Epoch= 4873)\n",
      "CV loss=0.11518820980427213(Epoch= 4874)\n",
      "train loss=0.05686459786452932(Epoch= 4874)\n",
      "CV loss=0.11491030989923867(Epoch= 4875)\n",
      "train loss=0.05686699309309762(Epoch= 4875)\n",
      "CV loss=0.11490073388753104(Epoch= 4876)\n",
      "train loss=0.05685736164096753(Epoch= 4876)\n",
      "CV loss=0.11489084626419516(Epoch= 4877)\n",
      "train loss=0.05684566975418527(Epoch= 4877)\n",
      "CV loss=0.11490602934681715(Epoch= 4878)\n",
      "train loss=0.05685390571287805(Epoch= 4878)\n",
      "CV loss=0.11508017212578932(Epoch= 4879)\n",
      "train loss=0.056833595797707706(Epoch= 4879)\n",
      "CV loss=0.11503164354456444(Epoch= 4880)\n",
      "train loss=0.05682277754627432(Epoch= 4880)\n",
      "CV loss=0.11488455777168441(Epoch= 4881)\n",
      "train loss=0.05682487768318596(Epoch= 4881)\n",
      "CV loss=0.11489369113589815(Epoch= 4882)\n",
      "train loss=0.05681596281643765(Epoch= 4882)\n",
      "CV loss=0.11511871912560719(Epoch= 4883)\n",
      "train loss=0.05680656427937971(Epoch= 4883)\n",
      "CV loss=0.11489706866481099(Epoch= 4884)\n",
      "train loss=0.05679871141399682(Epoch= 4884)\n",
      "CV loss=0.11491343256428924(Epoch= 4885)\n",
      "train loss=0.05678741072012568(Epoch= 4885)\n",
      "CV loss=0.11496350114656095(Epoch= 4886)\n",
      "train loss=0.05679200937568472(Epoch= 4886)\n",
      "CV loss=0.11494187708550183(Epoch= 4887)\n",
      "train loss=0.05677849689086121(Epoch= 4887)\n",
      "CV loss=0.11499714023822313(Epoch= 4888)\n",
      "train loss=0.0567730753257716(Epoch= 4888)\n",
      "CV loss=0.1149061441725108(Epoch= 4889)\n",
      "train loss=0.05676334915041754(Epoch= 4889)\n",
      "CV loss=0.11486055532875025(Epoch= 4890)\n",
      "train loss=0.05675767980665858(Epoch= 4890)\n",
      "CV loss=0.11494044671342704(Epoch= 4891)\n",
      "train loss=0.05674990840799295(Epoch= 4891)\n",
      "CV loss=0.11508215837802044(Epoch= 4892)\n",
      "train loss=0.05675592702391326(Epoch= 4892)\n",
      "CV loss=0.11506162652791341(Epoch= 4893)\n",
      "train loss=0.05674738006841901(Epoch= 4893)\n",
      "CV loss=0.11500241901274255(Epoch= 4894)\n",
      "train loss=0.056734163129912474(Epoch= 4894)\n",
      "CV loss=0.11491855482550666(Epoch= 4895)\n",
      "train loss=0.05672805400682683(Epoch= 4895)\n",
      "CV loss=0.114885866253278(Epoch= 4896)\n",
      "train loss=0.05671864124761171(Epoch= 4896)\n",
      "CV loss=0.11480640642737841(Epoch= 4897)\n",
      "train loss=0.05671316860765263(Epoch= 4897)\n",
      "CV loss=0.11494669739370403(Epoch= 4898)\n",
      "train loss=0.056707348905367515(Epoch= 4898)\n",
      "CV loss=0.11492489655314177(Epoch= 4899)\n",
      "train loss=0.0566973998647015(Epoch= 4899)\n",
      "CV loss=0.11495418108674246(Epoch= 4900)\n",
      "train loss=0.056692967105140525(Epoch= 4900)\n",
      "CV loss=0.11478006824466488(Epoch= 4901)\n",
      "train loss=0.05668923190078307(Epoch= 4901)\n",
      "CV loss=0.11477070090458465(Epoch= 4902)\n",
      "train loss=0.05668464097119741(Epoch= 4902)\n",
      "CV loss=0.11494070718679839(Epoch= 4903)\n",
      "train loss=0.05667727818048248(Epoch= 4903)\n",
      "CV loss=0.11484837474169046(Epoch= 4904)\n",
      "train loss=0.056670099131869045(Epoch= 4904)\n",
      "CV loss=0.11482280057891381(Epoch= 4905)\n",
      "train loss=0.05666694712647698(Epoch= 4905)\n",
      "CV loss=0.11491473140273772(Epoch= 4906)\n",
      "train loss=0.056653560463408206(Epoch= 4906)\n",
      "CV loss=0.11499611051236386(Epoch= 4907)\n",
      "train loss=0.05665484321265466(Epoch= 4907)\n",
      "CV loss=0.11488674018943035(Epoch= 4908)\n",
      "train loss=0.05664035102659955(Epoch= 4908)\n",
      "CV loss=0.1149460407064636(Epoch= 4909)\n",
      "train loss=0.056637181263753514(Epoch= 4909)\n",
      "CV loss=0.11486246987273405(Epoch= 4910)\n",
      "train loss=0.05662752845680733(Epoch= 4910)\n",
      "CV loss=0.11487617266471478(Epoch= 4911)\n",
      "train loss=0.056622239638522226(Epoch= 4911)\n",
      "CV loss=0.11475986523030536(Epoch= 4912)\n",
      "train loss=0.056620563264023094(Epoch= 4912)\n",
      "CV loss=0.11484338309571439(Epoch= 4913)\n",
      "train loss=0.056609144566417774(Epoch= 4913)\n",
      "CV loss=0.1146999422996309(Epoch= 4914)\n",
      "train loss=0.056610820292094556(Epoch= 4914)\n",
      "CV loss=0.11476761112391046(Epoch= 4915)\n",
      "train loss=0.05659806481593798(Epoch= 4915)\n",
      "CV loss=0.11465208073433766(Epoch= 4916)\n",
      "train loss=0.0565930071131246(Epoch= 4916)\n",
      "CV loss=0.1147757474599306(Epoch= 4917)\n",
      "train loss=0.05658153317962698(Epoch= 4917)\n",
      "CV loss=0.11469170245070881(Epoch= 4918)\n",
      "train loss=0.05657721664472822(Epoch= 4918)\n",
      "CV loss=0.11474825206156784(Epoch= 4919)\n",
      "train loss=0.056576711553622025(Epoch= 4919)\n",
      "CV loss=0.11468297020508457(Epoch= 4920)\n",
      "train loss=0.056563372404310724(Epoch= 4920)\n",
      "CV loss=0.11482824076342378(Epoch= 4921)\n",
      "train loss=0.05656694848998718(Epoch= 4921)\n",
      "CV loss=0.11474556083251419(Epoch= 4922)\n",
      "train loss=0.05656562177275701(Epoch= 4922)\n",
      "CV loss=0.11474609225133534(Epoch= 4923)\n",
      "train loss=0.056547456133654285(Epoch= 4923)\n",
      "CV loss=0.11481056421958535(Epoch= 4924)\n",
      "train loss=0.056543392131278916(Epoch= 4924)\n",
      "CV loss=0.11469127604492291(Epoch= 4925)\n",
      "train loss=0.05653577513422228(Epoch= 4925)\n",
      "CV loss=0.11477215730093931(Epoch= 4926)\n",
      "train loss=0.05652917191529245(Epoch= 4926)\n",
      "CV loss=0.11484005079293583(Epoch= 4927)\n",
      "train loss=0.05652415569730378(Epoch= 4927)\n",
      "CV loss=0.1148119154474657(Epoch= 4928)\n",
      "train loss=0.0565197601771244(Epoch= 4928)\n",
      "CV loss=0.11478737555840704(Epoch= 4929)\n",
      "train loss=0.056513750693060645(Epoch= 4929)\n",
      "CV loss=0.11459835211161794(Epoch= 4930)\n",
      "train loss=0.0565052398973052(Epoch= 4930)\n",
      "CV loss=0.11466747084516227(Epoch= 4931)\n",
      "train loss=0.05649597600272485(Epoch= 4931)\n",
      "CV loss=0.11471430023586399(Epoch= 4932)\n",
      "train loss=0.05649048893070654(Epoch= 4932)\n",
      "CV loss=0.11479334327092053(Epoch= 4933)\n",
      "train loss=0.056484337344856694(Epoch= 4933)\n",
      "CV loss=0.11455631318782142(Epoch= 4934)\n",
      "train loss=0.05647724837938865(Epoch= 4934)\n",
      "CV loss=0.11450207769755091(Epoch= 4935)\n",
      "train loss=0.05647572416059754(Epoch= 4935)\n",
      "CV loss=0.11466368151670236(Epoch= 4936)\n",
      "train loss=0.0564707647603802(Epoch= 4936)\n",
      "CV loss=0.11458513170108703(Epoch= 4937)\n",
      "train loss=0.056457053802341756(Epoch= 4937)\n",
      "CV loss=0.11467408869844828(Epoch= 4938)\n",
      "train loss=0.05645167781685063(Epoch= 4938)\n",
      "CV loss=0.11469171688495758(Epoch= 4939)\n",
      "train loss=0.056458972592108504(Epoch= 4939)\n",
      "CV loss=0.11473011769953823(Epoch= 4940)\n",
      "train loss=0.056443610385990016(Epoch= 4940)\n",
      "CV loss=0.11453411924968981(Epoch= 4941)\n",
      "train loss=0.05643530278670848(Epoch= 4941)\n",
      "CV loss=0.11461149774664542(Epoch= 4942)\n",
      "train loss=0.0564249066198387(Epoch= 4942)\n",
      "CV loss=0.11464925139645384(Epoch= 4943)\n",
      "train loss=0.05642536543735965(Epoch= 4943)\n",
      "CV loss=0.11457968168290929(Epoch= 4944)\n",
      "train loss=0.05641151732378414(Epoch= 4944)\n",
      "CV loss=0.11454810641199573(Epoch= 4945)\n",
      "train loss=0.05640681708801802(Epoch= 4945)\n",
      "CV loss=0.11462597615308802(Epoch= 4946)\n",
      "train loss=0.05640263033793122(Epoch= 4946)\n",
      "CV loss=0.11457375977666495(Epoch= 4947)\n",
      "train loss=0.05639327046130048(Epoch= 4947)\n",
      "CV loss=0.11469739487885795(Epoch= 4948)\n",
      "train loss=0.056396984849991394(Epoch= 4948)\n",
      "CV loss=0.11454810918863786(Epoch= 4949)\n",
      "train loss=0.05638511380973753(Epoch= 4949)\n",
      "CV loss=0.11449854441812343(Epoch= 4950)\n",
      "train loss=0.056376273777372046(Epoch= 4950)\n",
      "CV loss=0.11443008099887067(Epoch= 4951)\n",
      "train loss=0.056374106724451464(Epoch= 4951)\n",
      "CV loss=0.11450498718366824(Epoch= 4952)\n",
      "train loss=0.05637314144560422(Epoch= 4952)\n",
      "CV loss=0.11459462058350384(Epoch= 4953)\n",
      "train loss=0.056359955525954075(Epoch= 4953)\n",
      "CV loss=0.11437236264279016(Epoch= 4954)\n",
      "train loss=0.05635384089962118(Epoch= 4954)\n",
      "CV loss=0.11454207717249909(Epoch= 4955)\n",
      "train loss=0.05635123449435135(Epoch= 4955)\n",
      "CV loss=0.11455059988447053(Epoch= 4956)\n",
      "train loss=0.05633813884443227(Epoch= 4956)\n",
      "CV loss=0.1146318059223283(Epoch= 4957)\n",
      "train loss=0.05633503474192803(Epoch= 4957)\n",
      "CV loss=0.1144719927535438(Epoch= 4958)\n",
      "train loss=0.0563352405178049(Epoch= 4958)\n",
      "CV loss=0.11471416745149456(Epoch= 4959)\n",
      "train loss=0.05633360522264846(Epoch= 4959)\n",
      "CV loss=0.11451737045928298(Epoch= 4960)\n",
      "train loss=0.05632311932375022(Epoch= 4960)\n",
      "CV loss=0.11459229796275441(Epoch= 4961)\n",
      "train loss=0.056310725080342315(Epoch= 4961)\n",
      "CV loss=0.11450826075673058(Epoch= 4962)\n",
      "train loss=0.05630552259304648(Epoch= 4962)\n",
      "CV loss=0.11453837776464752(Epoch= 4963)\n",
      "train loss=0.056296637813932585(Epoch= 4963)\n",
      "CV loss=0.11441029356630886(Epoch= 4964)\n",
      "train loss=0.05629710960756291(Epoch= 4964)\n",
      "CV loss=0.11449116895353709(Epoch= 4965)\n",
      "train loss=0.056287239451409024(Epoch= 4965)\n",
      "CV loss=0.11434667460595155(Epoch= 4966)\n",
      "train loss=0.05628809020331446(Epoch= 4966)\n",
      "CV loss=0.11457075477081519(Epoch= 4967)\n",
      "train loss=0.056273206913741(Epoch= 4967)\n",
      "CV loss=0.11455448651949088(Epoch= 4968)\n",
      "train loss=0.05626828978664529(Epoch= 4968)\n",
      "CV loss=0.11451954233594262(Epoch= 4969)\n",
      "train loss=0.05626262705980152(Epoch= 4969)\n",
      "CV loss=0.11453699989414401(Epoch= 4970)\n",
      "train loss=0.05625492068302993(Epoch= 4970)\n",
      "CV loss=0.11436685330228376(Epoch= 4971)\n",
      "train loss=0.056250905840087254(Epoch= 4971)\n",
      "CV loss=0.11441803463054553(Epoch= 4972)\n",
      "train loss=0.05624149973229151(Epoch= 4972)\n",
      "CV loss=0.11479020054733602(Epoch= 4973)\n",
      "train loss=0.05626485167799389(Epoch= 4973)\n",
      "CV loss=0.11425086519883762(Epoch= 4974)\n",
      "train loss=0.05623006610457802(Epoch= 4974)\n",
      "CV loss=0.11434609019441393(Epoch= 4975)\n",
      "train loss=0.05622499581052165(Epoch= 4975)\n",
      "CV loss=0.11425648561219137(Epoch= 4976)\n",
      "train loss=0.05621914545313409(Epoch= 4976)\n",
      "CV loss=0.11432558234604533(Epoch= 4977)\n",
      "train loss=0.056209538231349436(Epoch= 4977)\n",
      "CV loss=0.11432092183327494(Epoch= 4978)\n",
      "train loss=0.05620585256804561(Epoch= 4978)\n",
      "CV loss=0.11440124710811186(Epoch= 4979)\n",
      "train loss=0.056202623641311296(Epoch= 4979)\n",
      "CV loss=0.11445778432403933(Epoch= 4980)\n",
      "train loss=0.056206334141791645(Epoch= 4980)\n",
      "CV loss=0.11424778912800292(Epoch= 4981)\n",
      "train loss=0.05621673508287666(Epoch= 4981)\n",
      "CV loss=0.11436547085872803(Epoch= 4982)\n",
      "train loss=0.05618290941299964(Epoch= 4982)\n",
      "CV loss=0.11416511148890178(Epoch= 4983)\n",
      "train loss=0.05617982505944442(Epoch= 4983)\n",
      "CV loss=0.11424943504438301(Epoch= 4984)\n",
      "train loss=0.05618191065795607(Epoch= 4984)\n",
      "CV loss=0.11421967785220397(Epoch= 4985)\n",
      "train loss=0.05616538192584219(Epoch= 4985)\n",
      "CV loss=0.11428313166725201(Epoch= 4986)\n",
      "train loss=0.05616458289924468(Epoch= 4986)\n",
      "CV loss=0.1142103602216138(Epoch= 4987)\n",
      "train loss=0.0561498476685931(Epoch= 4987)\n",
      "CV loss=0.11435864054101516(Epoch= 4988)\n",
      "train loss=0.05614544900690806(Epoch= 4988)\n",
      "CV loss=0.1142881271356358(Epoch= 4989)\n",
      "train loss=0.05613955056592495(Epoch= 4989)\n",
      "CV loss=0.1142686344443731(Epoch= 4990)\n",
      "train loss=0.056134248122137284(Epoch= 4990)\n",
      "CV loss=0.11432600516474731(Epoch= 4991)\n",
      "train loss=0.05613142478864786(Epoch= 4991)\n",
      "CV loss=0.11421287369429593(Epoch= 4992)\n",
      "train loss=0.05612301622953347(Epoch= 4992)\n",
      "CV loss=0.11437835139606867(Epoch= 4993)\n",
      "train loss=0.056122859131775636(Epoch= 4993)\n",
      "CV loss=0.11418745998182864(Epoch= 4994)\n",
      "train loss=0.05611252070169938(Epoch= 4994)\n",
      "CV loss=0.11421862179927872(Epoch= 4995)\n",
      "train loss=0.056101428198312826(Epoch= 4995)\n",
      "CV loss=0.11415777610832772(Epoch= 4996)\n",
      "train loss=0.05609687882093463(Epoch= 4996)\n",
      "CV loss=0.11427747338338631(Epoch= 4997)\n",
      "train loss=0.056090093239649697(Epoch= 4997)\n",
      "CV loss=0.11412134297625935(Epoch= 4998)\n",
      "train loss=0.05608717187099756(Epoch= 4998)\n",
      "CV loss=0.11412768739438386(Epoch= 4999)\n",
      "train loss=0.05608305649553281(Epoch= 4999)\n",
      "CV loss=0.11415931097278535(Epoch= 5000)\n",
      "train loss=0.05607266335761792(Epoch= 5000)\n",
      "CV loss=0.11430596491569768(Epoch= 5001)\n",
      "train loss=0.05606720306566268(Epoch= 5001)\n",
      "CV loss=0.11419741373983763(Epoch= 5002)\n",
      "train loss=0.05605997929537885(Epoch= 5002)\n",
      "CV loss=0.11420112055009382(Epoch= 5003)\n",
      "train loss=0.05605389325460233(Epoch= 5003)\n",
      "CV loss=0.11422799772687614(Epoch= 5004)\n",
      "train loss=0.05605092100854368(Epoch= 5004)\n",
      "CV loss=0.11436374525609042(Epoch= 5005)\n",
      "train loss=0.056047010788701855(Epoch= 5005)\n",
      "CV loss=0.11423370504967485(Epoch= 5006)\n",
      "train loss=0.056036400951758804(Epoch= 5006)\n",
      "CV loss=0.11420343664732271(Epoch= 5007)\n",
      "train loss=0.0560338578329843(Epoch= 5007)\n",
      "CV loss=0.11415500887402538(Epoch= 5008)\n",
      "train loss=0.05602628279267999(Epoch= 5008)\n",
      "CV loss=0.1142504782092886(Epoch= 5009)\n",
      "train loss=0.056020840622241046(Epoch= 5009)\n",
      "CV loss=0.11402647797317614(Epoch= 5010)\n",
      "train loss=0.05601915588660419(Epoch= 5010)\n",
      "CV loss=0.1142211276151148(Epoch= 5011)\n",
      "train loss=0.056012041746960776(Epoch= 5011)\n",
      "CV loss=0.11413322688116162(Epoch= 5012)\n",
      "train loss=0.0559985144854646(Epoch= 5012)\n",
      "CV loss=0.11414773768143985(Epoch= 5013)\n",
      "train loss=0.055998880710485435(Epoch= 5013)\n",
      "CV loss=0.11413732544758601(Epoch= 5014)\n",
      "train loss=0.055990135919660636(Epoch= 5014)\n",
      "CV loss=0.1140766358700176(Epoch= 5015)\n",
      "train loss=0.05599317965211644(Epoch= 5015)\n",
      "CV loss=0.1140178186135477(Epoch= 5016)\n",
      "train loss=0.05598505865846638(Epoch= 5016)\n",
      "CV loss=0.11405833029325216(Epoch= 5017)\n",
      "train loss=0.0559738512885698(Epoch= 5017)\n",
      "CV loss=0.11412714901742235(Epoch= 5018)\n",
      "train loss=0.05596549083020777(Epoch= 5018)\n",
      "CV loss=0.11397898680823075(Epoch= 5019)\n",
      "train loss=0.055965957669602356(Epoch= 5019)\n",
      "CV loss=0.11417097844552183(Epoch= 5020)\n",
      "train loss=0.05595585344718296(Epoch= 5020)\n",
      "CV loss=0.11419263424133713(Epoch= 5021)\n",
      "train loss=0.05594924310598335(Epoch= 5021)\n",
      "CV loss=0.1141785120238501(Epoch= 5022)\n",
      "train loss=0.05594737103229894(Epoch= 5022)\n",
      "CV loss=0.11402844945336835(Epoch= 5023)\n",
      "train loss=0.05593648586787239(Epoch= 5023)\n",
      "CV loss=0.11408042319713141(Epoch= 5024)\n",
      "train loss=0.05593161313971209(Epoch= 5024)\n",
      "CV loss=0.11399968688305827(Epoch= 5025)\n",
      "train loss=0.05593034085133591(Epoch= 5025)\n",
      "CV loss=0.11413891377399314(Epoch= 5026)\n",
      "train loss=0.0559260696805046(Epoch= 5026)\n",
      "CV loss=0.11412574788013287(Epoch= 5027)\n",
      "train loss=0.055915665577370204(Epoch= 5027)\n",
      "CV loss=0.11396888059144039(Epoch= 5028)\n",
      "train loss=0.055910100821933754(Epoch= 5028)\n",
      "CV loss=0.11391133764480121(Epoch= 5029)\n",
      "train loss=0.05590449957812266(Epoch= 5029)\n",
      "CV loss=0.11400000799573104(Epoch= 5030)\n",
      "train loss=0.05589820129709119(Epoch= 5030)\n",
      "CV loss=0.1140817052585381(Epoch= 5031)\n",
      "train loss=0.05589147610648871(Epoch= 5031)\n",
      "CV loss=0.11401171456274142(Epoch= 5032)\n",
      "train loss=0.05588337932872948(Epoch= 5032)\n",
      "CV loss=0.11403541191780323(Epoch= 5033)\n",
      "train loss=0.05588290888782237(Epoch= 5033)\n",
      "CV loss=0.11402683283541723(Epoch= 5034)\n",
      "train loss=0.05587441830126625(Epoch= 5034)\n",
      "CV loss=0.11394011223998396(Epoch= 5035)\n",
      "train loss=0.05587027389011394(Epoch= 5035)\n",
      "CV loss=0.11401974882461254(Epoch= 5036)\n",
      "train loss=0.055861698025743194(Epoch= 5036)\n",
      "CV loss=0.11398308930226692(Epoch= 5037)\n",
      "train loss=0.055861131552535824(Epoch= 5037)\n",
      "CV loss=0.11409848233988287(Epoch= 5038)\n",
      "train loss=0.05585354746030684(Epoch= 5038)\n",
      "CV loss=0.11402473856810538(Epoch= 5039)\n",
      "train loss=0.05584420795352506(Epoch= 5039)\n",
      "CV loss=0.11401800838822237(Epoch= 5040)\n",
      "train loss=0.055843431180775366(Epoch= 5040)\n",
      "CV loss=0.11396814740661015(Epoch= 5041)\n",
      "train loss=0.05584194927093819(Epoch= 5041)\n",
      "CV loss=0.11402456825906437(Epoch= 5042)\n",
      "train loss=0.05583834795409096(Epoch= 5042)\n",
      "CV loss=0.11410800355839823(Epoch= 5043)\n",
      "train loss=0.05582855419017195(Epoch= 5043)\n",
      "CV loss=0.11391605307310512(Epoch= 5044)\n",
      "train loss=0.0558135928066064(Epoch= 5044)\n",
      "CV loss=0.1139602265264506(Epoch= 5045)\n",
      "train loss=0.05581112929924577(Epoch= 5045)\n",
      "CV loss=0.11384961020511966(Epoch= 5046)\n",
      "train loss=0.05580560038132887(Epoch= 5046)\n",
      "CV loss=0.11386475803851362(Epoch= 5047)\n",
      "train loss=0.05579917560072308(Epoch= 5047)\n",
      "CV loss=0.11394190355338497(Epoch= 5048)\n",
      "train loss=0.05579186860089522(Epoch= 5048)\n",
      "CV loss=0.1138549335315028(Epoch= 5049)\n",
      "train loss=0.05579022503633692(Epoch= 5049)\n",
      "CV loss=0.11388781251459232(Epoch= 5050)\n",
      "train loss=0.05578128701610001(Epoch= 5050)\n",
      "CV loss=0.11384872440268513(Epoch= 5051)\n",
      "train loss=0.05578359178295727(Epoch= 5051)\n",
      "CV loss=0.11387989243266741(Epoch= 5052)\n",
      "train loss=0.05576936506143314(Epoch= 5052)\n",
      "CV loss=0.11397409460489905(Epoch= 5053)\n",
      "train loss=0.05577182459068545(Epoch= 5053)\n",
      "CV loss=0.11374926955735942(Epoch= 5054)\n",
      "train loss=0.05576747290795842(Epoch= 5054)\n",
      "CV loss=0.11391019322955709(Epoch= 5055)\n",
      "train loss=0.055752870097377144(Epoch= 5055)\n",
      "CV loss=0.11376172756144044(Epoch= 5056)\n",
      "train loss=0.05574662638751572(Epoch= 5056)\n",
      "CV loss=0.11371443251623854(Epoch= 5057)\n",
      "train loss=0.05574441750519528(Epoch= 5057)\n",
      "CV loss=0.11378253697272449(Epoch= 5058)\n",
      "train loss=0.055735893871675345(Epoch= 5058)\n",
      "CV loss=0.11369268233403(Epoch= 5059)\n",
      "train loss=0.05573216397922824(Epoch= 5059)\n",
      "CV loss=0.11370433187317(Epoch= 5060)\n",
      "train loss=0.055726922097349495(Epoch= 5060)\n",
      "CV loss=0.11379072213094504(Epoch= 5061)\n",
      "train loss=0.055717464282829254(Epoch= 5061)\n",
      "CV loss=0.11395275141275368(Epoch= 5062)\n",
      "train loss=0.05571507181074701(Epoch= 5062)\n",
      "CV loss=0.1137195689189489(Epoch= 5063)\n",
      "train loss=0.055714307279262576(Epoch= 5063)\n",
      "CV loss=0.1138097465627381(Epoch= 5064)\n",
      "train loss=0.05570030937359026(Epoch= 5064)\n",
      "CV loss=0.11373606597262653(Epoch= 5065)\n",
      "train loss=0.05570273539500513(Epoch= 5065)\n",
      "CV loss=0.11381108209642672(Epoch= 5066)\n",
      "train loss=0.05569582883118017(Epoch= 5066)\n",
      "CV loss=0.11358451618042566(Epoch= 5067)\n",
      "train loss=0.05568805931414645(Epoch= 5067)\n",
      "CV loss=0.11376364660174704(Epoch= 5068)\n",
      "train loss=0.055677007802180836(Epoch= 5068)\n",
      "CV loss=0.11391145149853973(Epoch= 5069)\n",
      "train loss=0.055676209661256706(Epoch= 5069)\n",
      "CV loss=0.11365815404748751(Epoch= 5070)\n",
      "train loss=0.05566660649936104(Epoch= 5070)\n",
      "CV loss=0.11376267291793256(Epoch= 5071)\n",
      "train loss=0.05565908497539418(Epoch= 5071)\n",
      "CV loss=0.11377879572026006(Epoch= 5072)\n",
      "train loss=0.055659131529882326(Epoch= 5072)\n",
      "CV loss=0.11382280327336793(Epoch= 5073)\n",
      "train loss=0.05564950090858365(Epoch= 5073)\n",
      "CV loss=0.1137138587673705(Epoch= 5074)\n",
      "train loss=0.05564995125481918(Epoch= 5074)\n",
      "CV loss=0.11377527109819668(Epoch= 5075)\n",
      "train loss=0.05563722727845313(Epoch= 5075)\n",
      "CV loss=0.11372323338350881(Epoch= 5076)\n",
      "train loss=0.05563174193676312(Epoch= 5076)\n",
      "CV loss=0.11365603555688075(Epoch= 5077)\n",
      "train loss=0.05563248436003388(Epoch= 5077)\n",
      "CV loss=0.11370457457008912(Epoch= 5078)\n",
      "train loss=0.05562271705619189(Epoch= 5078)\n",
      "CV loss=0.11385242670794708(Epoch= 5079)\n",
      "train loss=0.05561926668697566(Epoch= 5079)\n",
      "CV loss=0.11387180231742473(Epoch= 5080)\n",
      "train loss=0.05561933963561472(Epoch= 5080)\n",
      "CV loss=0.11356542366907955(Epoch= 5081)\n",
      "train loss=0.055609526176103395(Epoch= 5081)\n",
      "CV loss=0.11370128643240995(Epoch= 5082)\n",
      "train loss=0.055596902832877895(Epoch= 5082)\n",
      "CV loss=0.11359631474099283(Epoch= 5083)\n",
      "train loss=0.055597388390774424(Epoch= 5083)\n",
      "CV loss=0.11357683309925291(Epoch= 5084)\n",
      "train loss=0.05558775987120437(Epoch= 5084)\n",
      "CV loss=0.11363007388656457(Epoch= 5085)\n",
      "train loss=0.055585804294708926(Epoch= 5085)\n",
      "CV loss=0.11357828184086233(Epoch= 5086)\n",
      "train loss=0.055577661150147006(Epoch= 5086)\n",
      "CV loss=0.11373465032791658(Epoch= 5087)\n",
      "train loss=0.055571871949655016(Epoch= 5087)\n",
      "CV loss=0.11364582860114278(Epoch= 5088)\n",
      "train loss=0.05556616274948731(Epoch= 5088)\n",
      "CV loss=0.113606658111773(Epoch= 5089)\n",
      "train loss=0.05555827448845071(Epoch= 5089)\n",
      "CV loss=0.11368922611940388(Epoch= 5090)\n",
      "train loss=0.055556514901179264(Epoch= 5090)\n",
      "CV loss=0.11364456654897115(Epoch= 5091)\n",
      "train loss=0.05555545027448172(Epoch= 5091)\n",
      "CV loss=0.11379311657735194(Epoch= 5092)\n",
      "train loss=0.05555035980449294(Epoch= 5092)\n",
      "CV loss=0.11368383280411387(Epoch= 5093)\n",
      "train loss=0.05554285477579878(Epoch= 5093)\n",
      "CV loss=0.1135080456760342(Epoch= 5094)\n",
      "train loss=0.05553374135500922(Epoch= 5094)\n",
      "CV loss=0.11357275922638507(Epoch= 5095)\n",
      "train loss=0.055524852134366705(Epoch= 5095)\n",
      "CV loss=0.11361680391407387(Epoch= 5096)\n",
      "train loss=0.05552051638915097(Epoch= 5096)\n",
      "CV loss=0.11353954076509765(Epoch= 5097)\n",
      "train loss=0.055532349379473434(Epoch= 5097)\n",
      "CV loss=0.11366270373269108(Epoch= 5098)\n",
      "train loss=0.05551140305434546(Epoch= 5098)\n",
      "CV loss=0.1135175373672142(Epoch= 5099)\n",
      "train loss=0.05550685499573568(Epoch= 5099)\n",
      "CV loss=0.11348791339343504(Epoch= 5100)\n",
      "train loss=0.055497649665921(Epoch= 5100)\n",
      "CV loss=0.11342861056177439(Epoch= 5101)\n",
      "train loss=0.05550239829777291(Epoch= 5101)\n",
      "CV loss=0.11363769270689966(Epoch= 5102)\n",
      "train loss=0.05549040233964638(Epoch= 5102)\n",
      "CV loss=0.11342777892456729(Epoch= 5103)\n",
      "train loss=0.055486463978531145(Epoch= 5103)\n",
      "CV loss=0.11357509244866112(Epoch= 5104)\n",
      "train loss=0.0554770901542433(Epoch= 5104)\n",
      "CV loss=0.11345090594525781(Epoch= 5105)\n",
      "train loss=0.05547282470026857(Epoch= 5105)\n",
      "CV loss=0.11364049264022175(Epoch= 5106)\n",
      "train loss=0.05548127513081421(Epoch= 5106)\n",
      "CV loss=0.11343206861364974(Epoch= 5107)\n",
      "train loss=0.05546401830934605(Epoch= 5107)\n",
      "CV loss=0.11365480444764914(Epoch= 5108)\n",
      "train loss=0.055466027608460325(Epoch= 5108)\n",
      "CV loss=0.11349187566392704(Epoch= 5109)\n",
      "train loss=0.05545773512281641(Epoch= 5109)\n",
      "CV loss=0.11350599633320071(Epoch= 5110)\n",
      "train loss=0.055442047279651435(Epoch= 5110)\n",
      "CV loss=0.1135354960483106(Epoch= 5111)\n",
      "train loss=0.055438253890099154(Epoch= 5111)\n",
      "CV loss=0.11353225798936459(Epoch= 5112)\n",
      "train loss=0.05543121963608148(Epoch= 5112)\n",
      "CV loss=0.11356400704439312(Epoch= 5113)\n",
      "train loss=0.05542700267928639(Epoch= 5113)\n",
      "CV loss=0.11366637472915467(Epoch= 5114)\n",
      "train loss=0.05542580604137434(Epoch= 5114)\n",
      "CV loss=0.11350096194050209(Epoch= 5115)\n",
      "train loss=0.05542185773943338(Epoch= 5115)\n",
      "CV loss=0.11377698797025443(Epoch= 5116)\n",
      "train loss=0.05542415002330228(Epoch= 5116)\n",
      "CV loss=0.11333462430601049(Epoch= 5117)\n",
      "train loss=0.05541271283571299(Epoch= 5117)\n",
      "CV loss=0.11329576832646797(Epoch= 5118)\n",
      "train loss=0.0554122269710151(Epoch= 5118)\n",
      "CV loss=0.11359103996551068(Epoch= 5119)\n",
      "train loss=0.05540133656253097(Epoch= 5119)\n",
      "CV loss=0.1136969613098589(Epoch= 5120)\n",
      "train loss=0.05540491093583758(Epoch= 5120)\n",
      "CV loss=0.11351649549574333(Epoch= 5121)\n",
      "train loss=0.05538497380973112(Epoch= 5121)\n",
      "CV loss=0.1133940433912213(Epoch= 5122)\n",
      "train loss=0.05537932740432966(Epoch= 5122)\n",
      "CV loss=0.11343227815832464(Epoch= 5123)\n",
      "train loss=0.05537330987223661(Epoch= 5123)\n",
      "CV loss=0.11327142686829116(Epoch= 5124)\n",
      "train loss=0.05536821364336118(Epoch= 5124)\n",
      "CV loss=0.11327009945609978(Epoch= 5125)\n",
      "train loss=0.05536676468649222(Epoch= 5125)\n",
      "CV loss=0.11343717735785944(Epoch= 5126)\n",
      "train loss=0.05535392150853612(Epoch= 5126)\n",
      "CV loss=0.11321670309247703(Epoch= 5127)\n",
      "train loss=0.05536125159583949(Epoch= 5127)\n",
      "CV loss=0.11341079452628654(Epoch= 5128)\n",
      "train loss=0.05534730860198622(Epoch= 5128)\n",
      "CV loss=0.11348315906310122(Epoch= 5129)\n",
      "train loss=0.05534140537015134(Epoch= 5129)\n",
      "CV loss=0.11332553846569729(Epoch= 5130)\n",
      "train loss=0.05534064368304839(Epoch= 5130)\n",
      "CV loss=0.11334607377541828(Epoch= 5131)\n",
      "train loss=0.05532856055441606(Epoch= 5131)\n",
      "CV loss=0.11327147289373024(Epoch= 5132)\n",
      "train loss=0.05532996600841473(Epoch= 5132)\n",
      "CV loss=0.11355636243948411(Epoch= 5133)\n",
      "train loss=0.055331002257056686(Epoch= 5133)\n",
      "CV loss=0.11328714872780335(Epoch= 5134)\n",
      "train loss=0.055315100662541763(Epoch= 5134)\n",
      "CV loss=0.11332345713233133(Epoch= 5135)\n",
      "train loss=0.05531400032962538(Epoch= 5135)\n",
      "CV loss=0.11336698303237439(Epoch= 5136)\n",
      "train loss=0.0553005471171242(Epoch= 5136)\n",
      "CV loss=0.11359987450713044(Epoch= 5137)\n",
      "train loss=0.05531420840591577(Epoch= 5137)\n",
      "CV loss=0.11328801744438657(Epoch= 5138)\n",
      "train loss=0.055289405218327044(Epoch= 5138)\n",
      "CV loss=0.11327933783266961(Epoch= 5139)\n",
      "train loss=0.055287586166467756(Epoch= 5139)\n",
      "CV loss=0.11333950065117493(Epoch= 5140)\n",
      "train loss=0.055280705258311136(Epoch= 5140)\n",
      "CV loss=0.1132650364113969(Epoch= 5141)\n",
      "train loss=0.055275004549211915(Epoch= 5141)\n",
      "CV loss=0.11332041115523521(Epoch= 5142)\n",
      "train loss=0.055269510414308845(Epoch= 5142)\n",
      "CV loss=0.11332458820524038(Epoch= 5143)\n",
      "train loss=0.055262915893112954(Epoch= 5143)\n",
      "CV loss=0.11327771573585252(Epoch= 5144)\n",
      "train loss=0.055262975472179815(Epoch= 5144)\n",
      "CV loss=0.11327067324331791(Epoch= 5145)\n",
      "train loss=0.05525215143765703(Epoch= 5145)\n",
      "CV loss=0.11323399023945402(Epoch= 5146)\n",
      "train loss=0.05525547019586697(Epoch= 5146)\n",
      "CV loss=0.1133541246096584(Epoch= 5147)\n",
      "train loss=0.05524668262327744(Epoch= 5147)\n",
      "CV loss=0.11338450753976279(Epoch= 5148)\n",
      "train loss=0.05524785980359742(Epoch= 5148)\n",
      "CV loss=0.11331758849647422(Epoch= 5149)\n",
      "train loss=0.055231652212230524(Epoch= 5149)\n",
      "CV loss=0.11323626958592448(Epoch= 5150)\n",
      "train loss=0.05523341141580475(Epoch= 5150)\n",
      "CV loss=0.11331461099504664(Epoch= 5151)\n",
      "train loss=0.055224832711207456(Epoch= 5151)\n",
      "CV loss=0.11315247251371374(Epoch= 5152)\n",
      "train loss=0.055216121392631635(Epoch= 5152)\n",
      "CV loss=0.11326162796051889(Epoch= 5153)\n",
      "train loss=0.05522742381480614(Epoch= 5153)\n",
      "CV loss=0.11316246533354621(Epoch= 5154)\n",
      "train loss=0.05521215418288405(Epoch= 5154)\n",
      "CV loss=0.11322540386693933(Epoch= 5155)\n",
      "train loss=0.05520304574269699(Epoch= 5155)\n",
      "CV loss=0.11314179739436353(Epoch= 5156)\n",
      "train loss=0.05519970942935846(Epoch= 5156)\n",
      "CV loss=0.11339542896374989(Epoch= 5157)\n",
      "train loss=0.055194487386730835(Epoch= 5157)\n",
      "CV loss=0.11321736438358937(Epoch= 5158)\n",
      "train loss=0.05518274770243892(Epoch= 5158)\n",
      "CV loss=0.11337062171313664(Epoch= 5159)\n",
      "train loss=0.05518307318796137(Epoch= 5159)\n",
      "CV loss=0.11314511774711411(Epoch= 5160)\n",
      "train loss=0.05517881960039654(Epoch= 5160)\n",
      "CV loss=0.11318729248831748(Epoch= 5161)\n",
      "train loss=0.05517471159135888(Epoch= 5161)\n",
      "CV loss=0.11339370712729163(Epoch= 5162)\n",
      "train loss=0.05516766103644804(Epoch= 5162)\n",
      "CV loss=0.11327676037026543(Epoch= 5163)\n",
      "train loss=0.05515929878285593(Epoch= 5163)\n",
      "CV loss=0.11296509035744343(Epoch= 5164)\n",
      "train loss=0.055167168094143446(Epoch= 5164)\n",
      "CV loss=0.1132310664614666(Epoch= 5165)\n",
      "train loss=0.05515007434389673(Epoch= 5165)\n",
      "CV loss=0.1132098573589847(Epoch= 5166)\n",
      "train loss=0.055144697943184856(Epoch= 5166)\n",
      "CV loss=0.11323740566303646(Epoch= 5167)\n",
      "train loss=0.05513693162055264(Epoch= 5167)\n",
      "CV loss=0.11339536923840812(Epoch= 5168)\n",
      "train loss=0.05515142146457819(Epoch= 5168)\n",
      "CV loss=0.1132291187390769(Epoch= 5169)\n",
      "train loss=0.05513072312037761(Epoch= 5169)\n",
      "CV loss=0.11309079674430003(Epoch= 5170)\n",
      "train loss=0.055123432221555455(Epoch= 5170)\n",
      "CV loss=0.11325934170325981(Epoch= 5171)\n",
      "train loss=0.05511836088560885(Epoch= 5171)\n",
      "CV loss=0.11314741782738397(Epoch= 5172)\n",
      "train loss=0.055110776029911(Epoch= 5172)\n",
      "CV loss=0.11318240907626802(Epoch= 5173)\n",
      "train loss=0.05511031620293796(Epoch= 5173)\n",
      "CV loss=0.11306302206020813(Epoch= 5174)\n",
      "train loss=0.05510539085196493(Epoch= 5174)\n",
      "CV loss=0.11306286013720845(Epoch= 5175)\n",
      "train loss=0.05509447924194272(Epoch= 5175)\n",
      "CV loss=0.11320602716928124(Epoch= 5176)\n",
      "train loss=0.05509193795537901(Epoch= 5176)\n",
      "CV loss=0.11309181138820928(Epoch= 5177)\n",
      "train loss=0.055085546778792266(Epoch= 5177)\n",
      "CV loss=0.11319576560752961(Epoch= 5178)\n",
      "train loss=0.05507983905455825(Epoch= 5178)\n",
      "CV loss=0.11316486268690666(Epoch= 5179)\n",
      "train loss=0.05507762356922769(Epoch= 5179)\n",
      "CV loss=0.11321157044186121(Epoch= 5180)\n",
      "train loss=0.055070629999561714(Epoch= 5180)\n",
      "CV loss=0.11308949342197228(Epoch= 5181)\n",
      "train loss=0.055061474202213245(Epoch= 5181)\n",
      "CV loss=0.11300018665562551(Epoch= 5182)\n",
      "train loss=0.055070424436547645(Epoch= 5182)\n",
      "CV loss=0.11301575930044352(Epoch= 5183)\n",
      "train loss=0.055069708645749026(Epoch= 5183)\n",
      "CV loss=0.11306858517099645(Epoch= 5184)\n",
      "train loss=0.05505617108419073(Epoch= 5184)\n",
      "CV loss=0.11323270523964751(Epoch= 5185)\n",
      "train loss=0.0550550527948177(Epoch= 5185)\n",
      "CV loss=0.11329181151075701(Epoch= 5186)\n",
      "train loss=0.05504690142097492(Epoch= 5186)\n",
      "CV loss=0.11331621890393653(Epoch= 5187)\n",
      "train loss=0.05503924315252004(Epoch= 5187)\n",
      "CV loss=0.1129574964951747(Epoch= 5188)\n",
      "train loss=0.055029420205053095(Epoch= 5188)\n",
      "CV loss=0.11325854483613573(Epoch= 5189)\n",
      "train loss=0.05503445930729533(Epoch= 5189)\n",
      "CV loss=0.1130347726641793(Epoch= 5190)\n",
      "train loss=0.055014766613330005(Epoch= 5190)\n",
      "CV loss=0.1130781269888441(Epoch= 5191)\n",
      "train loss=0.0550125471529736(Epoch= 5191)\n",
      "CV loss=0.11314425060351313(Epoch= 5192)\n",
      "train loss=0.0550080590289688(Epoch= 5192)\n",
      "CV loss=0.11309506980103853(Epoch= 5193)\n",
      "train loss=0.05500021226503297(Epoch= 5193)\n",
      "CV loss=0.11313790617570554(Epoch= 5194)\n",
      "train loss=0.05500401388752287(Epoch= 5194)\n",
      "CV loss=0.11295055237441996(Epoch= 5195)\n",
      "train loss=0.054994089633537(Epoch= 5195)\n",
      "CV loss=0.11299633256168613(Epoch= 5196)\n",
      "train loss=0.054994582039644545(Epoch= 5196)\n",
      "CV loss=0.1131307625474221(Epoch= 5197)\n",
      "train loss=0.05499014812012987(Epoch= 5197)\n",
      "CV loss=0.11307833722729815(Epoch= 5198)\n",
      "train loss=0.05497828558906882(Epoch= 5198)\n",
      "CV loss=0.11300618577137947(Epoch= 5199)\n",
      "train loss=0.05496798058869005(Epoch= 5199)\n",
      "CV loss=0.11301901828145408(Epoch= 5200)\n",
      "train loss=0.05496694812687885(Epoch= 5200)\n",
      "CV loss=0.11304725044905875(Epoch= 5201)\n",
      "train loss=0.05496606393488173(Epoch= 5201)\n",
      "CV loss=0.11302919681601822(Epoch= 5202)\n",
      "train loss=0.054969672385625626(Epoch= 5202)\n",
      "CV loss=0.11300955747901734(Epoch= 5203)\n",
      "train loss=0.054954379398966(Epoch= 5203)\n",
      "CV loss=0.11328454616035465(Epoch= 5204)\n",
      "train loss=0.05497095475906473(Epoch= 5204)\n",
      "CV loss=0.11288561441427282(Epoch= 5205)\n",
      "train loss=0.0549439436011366(Epoch= 5205)\n",
      "CV loss=0.11298122740927344(Epoch= 5206)\n",
      "train loss=0.05493331852000462(Epoch= 5206)\n",
      "CV loss=0.11280099141510691(Epoch= 5207)\n",
      "train loss=0.054936784215887206(Epoch= 5207)\n",
      "CV loss=0.11293518066779257(Epoch= 5208)\n",
      "train loss=0.054929121934299135(Epoch= 5208)\n",
      "CV loss=0.11286692243511715(Epoch= 5209)\n",
      "train loss=0.054922588803495015(Epoch= 5209)\n",
      "CV loss=0.1129183531417273(Epoch= 5210)\n",
      "train loss=0.054913831131015375(Epoch= 5210)\n",
      "CV loss=0.11291999700872372(Epoch= 5211)\n",
      "train loss=0.05491561578385618(Epoch= 5211)\n",
      "CV loss=0.11298231244940568(Epoch= 5212)\n",
      "train loss=0.05490373324857943(Epoch= 5212)\n",
      "CV loss=0.11312970774474294(Epoch= 5213)\n",
      "train loss=0.0549047191271108(Epoch= 5213)\n",
      "CV loss=0.11270991675163151(Epoch= 5214)\n",
      "train loss=0.05490470822431075(Epoch= 5214)\n",
      "CV loss=0.11307136005006933(Epoch= 5215)\n",
      "train loss=0.05489774473508213(Epoch= 5215)\n",
      "CV loss=0.11278777356433872(Epoch= 5216)\n",
      "train loss=0.05488827470913962(Epoch= 5216)\n",
      "CV loss=0.1130649253552155(Epoch= 5217)\n",
      "train loss=0.05489896808304926(Epoch= 5217)\n",
      "CV loss=0.11284108721238366(Epoch= 5218)\n",
      "train loss=0.054874004974628375(Epoch= 5218)\n",
      "CV loss=0.11278153828885865(Epoch= 5219)\n",
      "train loss=0.05487510575009826(Epoch= 5219)\n",
      "CV loss=0.11288145388683116(Epoch= 5220)\n",
      "train loss=0.05486391950267032(Epoch= 5220)\n",
      "CV loss=0.11304862535499755(Epoch= 5221)\n",
      "train loss=0.05486981020518777(Epoch= 5221)\n",
      "CV loss=0.11291871007761581(Epoch= 5222)\n",
      "train loss=0.05485162809011944(Epoch= 5222)\n",
      "CV loss=0.11284507334719426(Epoch= 5223)\n",
      "train loss=0.054850104771564434(Epoch= 5223)\n",
      "CV loss=0.11284526619708612(Epoch= 5224)\n",
      "train loss=0.05484341878672577(Epoch= 5224)\n",
      "CV loss=0.112879838308949(Epoch= 5225)\n",
      "train loss=0.05484175710160379(Epoch= 5225)\n",
      "CV loss=0.11294985298924481(Epoch= 5226)\n",
      "train loss=0.05483525568115899(Epoch= 5226)\n",
      "CV loss=0.11294853950076805(Epoch= 5227)\n",
      "train loss=0.054830319103633035(Epoch= 5227)\n",
      "CV loss=0.11282241266927664(Epoch= 5228)\n",
      "train loss=0.054831706058146384(Epoch= 5228)\n",
      "CV loss=0.11288386756381574(Epoch= 5229)\n",
      "train loss=0.054818037903631114(Epoch= 5229)\n",
      "CV loss=0.11280599832644977(Epoch= 5230)\n",
      "train loss=0.054811132847312934(Epoch= 5230)\n",
      "CV loss=0.11277100999535777(Epoch= 5231)\n",
      "train loss=0.05481486872961683(Epoch= 5231)\n",
      "CV loss=0.11287809709307005(Epoch= 5232)\n",
      "train loss=0.05480340891239852(Epoch= 5232)\n",
      "CV loss=0.11262791135130315(Epoch= 5233)\n",
      "train loss=0.05481360902247461(Epoch= 5233)\n",
      "CV loss=0.11303445395046707(Epoch= 5234)\n",
      "train loss=0.05480495916765092(Epoch= 5234)\n",
      "CV loss=0.11277435880809289(Epoch= 5235)\n",
      "train loss=0.054796931093120345(Epoch= 5235)\n",
      "CV loss=0.11277931806107541(Epoch= 5236)\n",
      "train loss=0.05478668130526126(Epoch= 5236)\n",
      "CV loss=0.11273882538661956(Epoch= 5237)\n",
      "train loss=0.054783691783625243(Epoch= 5237)\n",
      "CV loss=0.11293998447486722(Epoch= 5238)\n",
      "train loss=0.05477760925267383(Epoch= 5238)\n",
      "CV loss=0.11269998959384725(Epoch= 5239)\n",
      "train loss=0.05476913236449685(Epoch= 5239)\n",
      "CV loss=0.11281279700136992(Epoch= 5240)\n",
      "train loss=0.05476069881986006(Epoch= 5240)\n",
      "CV loss=0.11276957154268943(Epoch= 5241)\n",
      "train loss=0.05475574708329277(Epoch= 5241)\n",
      "CV loss=0.11272308457189964(Epoch= 5242)\n",
      "train loss=0.05475605965211362(Epoch= 5242)\n",
      "CV loss=0.11278811374478356(Epoch= 5243)\n",
      "train loss=0.054749247350622615(Epoch= 5243)\n",
      "CV loss=0.11277938963970946(Epoch= 5244)\n",
      "train loss=0.05474464920279703(Epoch= 5244)\n",
      "CV loss=0.11288933810424273(Epoch= 5245)\n",
      "train loss=0.05474190920254558(Epoch= 5245)\n",
      "CV loss=0.11273967740898946(Epoch= 5246)\n",
      "train loss=0.05473220297129948(Epoch= 5246)\n",
      "CV loss=0.11277317333860899(Epoch= 5247)\n",
      "train loss=0.05472752089004473(Epoch= 5247)\n",
      "CV loss=0.11289489520187954(Epoch= 5248)\n",
      "train loss=0.05472554224112557(Epoch= 5248)\n",
      "CV loss=0.11274544737002692(Epoch= 5249)\n",
      "train loss=0.054726066684294986(Epoch= 5249)\n",
      "CV loss=0.11276750957512541(Epoch= 5250)\n",
      "train loss=0.054723535749987055(Epoch= 5250)\n",
      "CV loss=0.11267493391159444(Epoch= 5251)\n",
      "train loss=0.054707200677549486(Epoch= 5251)\n",
      "CV loss=0.11268725897587006(Epoch= 5252)\n",
      "train loss=0.05470445585565087(Epoch= 5252)\n",
      "CV loss=0.11271798839557873(Epoch= 5253)\n",
      "train loss=0.054698460220220166(Epoch= 5253)\n",
      "CV loss=0.11264463537098385(Epoch= 5254)\n",
      "train loss=0.05469538237180035(Epoch= 5254)\n",
      "CV loss=0.11266278858120118(Epoch= 5255)\n",
      "train loss=0.05469063571262472(Epoch= 5255)\n",
      "CV loss=0.11263936259178059(Epoch= 5256)\n",
      "train loss=0.0546826777035539(Epoch= 5256)\n",
      "CV loss=0.1125339357048403(Epoch= 5257)\n",
      "train loss=0.05468941326183884(Epoch= 5257)\n",
      "CV loss=0.11274292075848563(Epoch= 5258)\n",
      "train loss=0.05467380164269426(Epoch= 5258)\n",
      "CV loss=0.1127093630444235(Epoch= 5259)\n",
      "train loss=0.05466930225557178(Epoch= 5259)\n",
      "CV loss=0.11263452110920544(Epoch= 5260)\n",
      "train loss=0.05467629364267011(Epoch= 5260)\n",
      "CV loss=0.11265570298355843(Epoch= 5261)\n",
      "train loss=0.05466087940650078(Epoch= 5261)\n",
      "CV loss=0.11282057193099962(Epoch= 5262)\n",
      "train loss=0.054666080483061846(Epoch= 5262)\n",
      "CV loss=0.1125047170811384(Epoch= 5263)\n",
      "train loss=0.05465125903692749(Epoch= 5263)\n",
      "CV loss=0.1127339935620944(Epoch= 5264)\n",
      "train loss=0.0546458575147056(Epoch= 5264)\n",
      "CV loss=0.11260505282076211(Epoch= 5265)\n",
      "train loss=0.05464001214258027(Epoch= 5265)\n",
      "CV loss=0.11250612188605721(Epoch= 5266)\n",
      "train loss=0.05463928153773531(Epoch= 5266)\n",
      "CV loss=0.11252489390892789(Epoch= 5267)\n",
      "train loss=0.05463272119739335(Epoch= 5267)\n",
      "CV loss=0.11272987632654557(Epoch= 5268)\n",
      "train loss=0.05462492957175353(Epoch= 5268)\n",
      "CV loss=0.11251797652244869(Epoch= 5269)\n",
      "train loss=0.05461899254610668(Epoch= 5269)\n",
      "CV loss=0.11266527837473876(Epoch= 5270)\n",
      "train loss=0.05461821018955675(Epoch= 5270)\n",
      "CV loss=0.11265900068694518(Epoch= 5271)\n",
      "train loss=0.05461161315230307(Epoch= 5271)\n",
      "CV loss=0.11261915158148639(Epoch= 5272)\n",
      "train loss=0.05461232602579025(Epoch= 5272)\n",
      "CV loss=0.1124500519181872(Epoch= 5273)\n",
      "train loss=0.054605270114332(Epoch= 5273)\n",
      "CV loss=0.1125083601558614(Epoch= 5274)\n",
      "train loss=0.054597986150866334(Epoch= 5274)\n",
      "CV loss=0.11267284547603915(Epoch= 5275)\n",
      "train loss=0.05459020021569912(Epoch= 5275)\n",
      "CV loss=0.11271101365980057(Epoch= 5276)\n",
      "train loss=0.0545882860323372(Epoch= 5276)\n",
      "CV loss=0.11250476908889474(Epoch= 5277)\n",
      "train loss=0.05458042202121205(Epoch= 5277)\n",
      "CV loss=0.11263726191830317(Epoch= 5278)\n",
      "train loss=0.05457840976419407(Epoch= 5278)\n",
      "CV loss=0.1128235939306188(Epoch= 5279)\n",
      "train loss=0.054577309985240414(Epoch= 5279)\n",
      "CV loss=0.1125248246469607(Epoch= 5280)\n",
      "train loss=0.05457783175318728(Epoch= 5280)\n",
      "CV loss=0.11250261222979892(Epoch= 5281)\n",
      "train loss=0.05456193097235558(Epoch= 5281)\n",
      "CV loss=0.11265462793129807(Epoch= 5282)\n",
      "train loss=0.05456241585465232(Epoch= 5282)\n",
      "CV loss=0.11256690862585983(Epoch= 5283)\n",
      "train loss=0.05456243666432599(Epoch= 5283)\n",
      "CV loss=0.11260899252220741(Epoch= 5284)\n",
      "train loss=0.054547291462754076(Epoch= 5284)\n",
      "CV loss=0.1126113710456246(Epoch= 5285)\n",
      "train loss=0.054543610510759705(Epoch= 5285)\n",
      "CV loss=0.11245336745021073(Epoch= 5286)\n",
      "train loss=0.05453667613526131(Epoch= 5286)\n",
      "CV loss=0.11251238326125919(Epoch= 5287)\n",
      "train loss=0.05453155213280963(Epoch= 5287)\n",
      "CV loss=0.1125291334526746(Epoch= 5288)\n",
      "train loss=0.05453019481763085(Epoch= 5288)\n",
      "CV loss=0.11258654402104207(Epoch= 5289)\n",
      "train loss=0.05452647465339275(Epoch= 5289)\n",
      "CV loss=0.11253128282856303(Epoch= 5290)\n",
      "train loss=0.054521601779712804(Epoch= 5290)\n",
      "CV loss=0.11255942827411575(Epoch= 5291)\n",
      "train loss=0.05453712156260351(Epoch= 5291)\n",
      "CV loss=0.11238592467989975(Epoch= 5292)\n",
      "train loss=0.05451065381939135(Epoch= 5292)\n",
      "CV loss=0.11273956093878751(Epoch= 5293)\n",
      "train loss=0.054510855489118965(Epoch= 5293)\n",
      "CV loss=0.11256300408112793(Epoch= 5294)\n",
      "train loss=0.05449965943398964(Epoch= 5294)\n",
      "CV loss=0.11249847756474118(Epoch= 5295)\n",
      "train loss=0.05451570593220198(Epoch= 5295)\n",
      "CV loss=0.11250425699145816(Epoch= 5296)\n",
      "train loss=0.054488862876066795(Epoch= 5296)\n",
      "CV loss=0.11255965904466128(Epoch= 5297)\n",
      "train loss=0.05448747511741474(Epoch= 5297)\n",
      "CV loss=0.11263736587778195(Epoch= 5298)\n",
      "train loss=0.05448940001600358(Epoch= 5298)\n",
      "CV loss=0.11234161950519099(Epoch= 5299)\n",
      "train loss=0.054476363641409425(Epoch= 5299)\n",
      "CV loss=0.11243836345873864(Epoch= 5300)\n",
      "train loss=0.05447176918865218(Epoch= 5300)\n",
      "CV loss=0.11252101706331373(Epoch= 5301)\n",
      "train loss=0.05446565757752711(Epoch= 5301)\n",
      "CV loss=0.11255282680103902(Epoch= 5302)\n",
      "train loss=0.05446669647959068(Epoch= 5302)\n",
      "CV loss=0.11242169066815336(Epoch= 5303)\n",
      "train loss=0.05445459357573652(Epoch= 5303)\n",
      "CV loss=0.11232044135903423(Epoch= 5304)\n",
      "train loss=0.05445316130778713(Epoch= 5304)\n",
      "CV loss=0.1125777223723801(Epoch= 5305)\n",
      "train loss=0.054453362114254784(Epoch= 5305)\n",
      "CV loss=0.11237477122259192(Epoch= 5306)\n",
      "train loss=0.054442821803483504(Epoch= 5306)\n",
      "CV loss=0.11251511420494006(Epoch= 5307)\n",
      "train loss=0.05443765158513368(Epoch= 5307)\n",
      "CV loss=0.11253511443684125(Epoch= 5308)\n",
      "train loss=0.054440578558086254(Epoch= 5308)\n",
      "CV loss=0.11243709049757822(Epoch= 5309)\n",
      "train loss=0.05442681584978308(Epoch= 5309)\n",
      "CV loss=0.11249719480417845(Epoch= 5310)\n",
      "train loss=0.05442932205102722(Epoch= 5310)\n",
      "CV loss=0.11251748896280425(Epoch= 5311)\n",
      "train loss=0.0544233687852329(Epoch= 5311)\n",
      "CV loss=0.11239861861262145(Epoch= 5312)\n",
      "train loss=0.0544115407201481(Epoch= 5312)\n",
      "CV loss=0.11255073237953091(Epoch= 5313)\n",
      "train loss=0.05442162205526971(Epoch= 5313)\n",
      "CV loss=0.11247057663674176(Epoch= 5314)\n",
      "train loss=0.05440843597123589(Epoch= 5314)\n",
      "CV loss=0.11248497207289054(Epoch= 5315)\n",
      "train loss=0.05439947396261764(Epoch= 5315)\n",
      "CV loss=0.1125115284766919(Epoch= 5316)\n",
      "train loss=0.05439743026313268(Epoch= 5316)\n",
      "CV loss=0.1123941884242933(Epoch= 5317)\n",
      "train loss=0.05439439341535115(Epoch= 5317)\n",
      "CV loss=0.11245204924393667(Epoch= 5318)\n",
      "train loss=0.054386505120871315(Epoch= 5318)\n",
      "CV loss=0.11233476725280271(Epoch= 5319)\n",
      "train loss=0.05438761212101984(Epoch= 5319)\n",
      "CV loss=0.11238902760963332(Epoch= 5320)\n",
      "train loss=0.054388849385469784(Epoch= 5320)\n",
      "CV loss=0.1124942953704969(Epoch= 5321)\n",
      "train loss=0.05437132454060848(Epoch= 5321)\n",
      "CV loss=0.11227659954254793(Epoch= 5322)\n",
      "train loss=0.05437644491114369(Epoch= 5322)\n",
      "CV loss=0.11217976894434316(Epoch= 5323)\n",
      "train loss=0.054369657289251916(Epoch= 5323)\n",
      "CV loss=0.11229477622131913(Epoch= 5324)\n",
      "train loss=0.05436801243812009(Epoch= 5324)\n",
      "CV loss=0.11239641386046623(Epoch= 5325)\n",
      "train loss=0.0543530093830527(Epoch= 5325)\n",
      "CV loss=0.11229866625360448(Epoch= 5326)\n",
      "train loss=0.054349862524622365(Epoch= 5326)\n",
      "CV loss=0.11250803753612013(Epoch= 5327)\n",
      "train loss=0.05434679200655345(Epoch= 5327)\n",
      "CV loss=0.11224459785487909(Epoch= 5328)\n",
      "train loss=0.05434660206579213(Epoch= 5328)\n",
      "CV loss=0.11233580249992138(Epoch= 5329)\n",
      "train loss=0.05433549842510127(Epoch= 5329)\n",
      "CV loss=0.11236795039140773(Epoch= 5330)\n",
      "train loss=0.0543309564402752(Epoch= 5330)\n",
      "CV loss=0.11237887744892057(Epoch= 5331)\n",
      "train loss=0.05432439766753639(Epoch= 5331)\n",
      "CV loss=0.11258803780761573(Epoch= 5332)\n",
      "train loss=0.05433382545446919(Epoch= 5332)\n",
      "CV loss=0.11213397089015026(Epoch= 5333)\n",
      "train loss=0.054320817440161505(Epoch= 5333)\n",
      "CV loss=0.11236885844276737(Epoch= 5334)\n",
      "train loss=0.054313316723761954(Epoch= 5334)\n",
      "CV loss=0.11222121022490805(Epoch= 5335)\n",
      "train loss=0.054306948458050744(Epoch= 5335)\n",
      "CV loss=0.11243007988021253(Epoch= 5336)\n",
      "train loss=0.054305758886278935(Epoch= 5336)\n",
      "CV loss=0.11227768474810412(Epoch= 5337)\n",
      "train loss=0.05429481138198328(Epoch= 5337)\n",
      "CV loss=0.11235594728756501(Epoch= 5338)\n",
      "train loss=0.05430286415772077(Epoch= 5338)\n",
      "CV loss=0.11217547674422707(Epoch= 5339)\n",
      "train loss=0.05428903886363664(Epoch= 5339)\n",
      "CV loss=0.11228087919559392(Epoch= 5340)\n",
      "train loss=0.05428935468545171(Epoch= 5340)\n",
      "CV loss=0.11243395451679386(Epoch= 5341)\n",
      "train loss=0.0542787647336436(Epoch= 5341)\n",
      "CV loss=0.1123526860776756(Epoch= 5342)\n",
      "train loss=0.054273684608466516(Epoch= 5342)\n",
      "CV loss=0.11233553070900151(Epoch= 5343)\n",
      "train loss=0.05427386807215794(Epoch= 5343)\n",
      "CV loss=0.11239050085163556(Epoch= 5344)\n",
      "train loss=0.05427050221979024(Epoch= 5344)\n",
      "CV loss=0.11223113477617125(Epoch= 5345)\n",
      "train loss=0.05426139018471263(Epoch= 5345)\n",
      "CV loss=0.11228222142268483(Epoch= 5346)\n",
      "train loss=0.05426040379015571(Epoch= 5346)\n",
      "CV loss=0.11212653383505028(Epoch= 5347)\n",
      "train loss=0.054255159771732446(Epoch= 5347)\n",
      "CV loss=0.11228517882958922(Epoch= 5348)\n",
      "train loss=0.05424806635095239(Epoch= 5348)\n",
      "CV loss=0.11226175177500555(Epoch= 5349)\n",
      "train loss=0.05424230277778488(Epoch= 5349)\n",
      "CV loss=0.11223396973397709(Epoch= 5350)\n",
      "train loss=0.05425345862871145(Epoch= 5350)\n",
      "CV loss=0.11222577023217602(Epoch= 5351)\n",
      "train loss=0.05423183716228658(Epoch= 5351)\n",
      "CV loss=0.11224052759903083(Epoch= 5352)\n",
      "train loss=0.05422500462464219(Epoch= 5352)\n",
      "CV loss=0.11216837506404406(Epoch= 5353)\n",
      "train loss=0.05422300180160303(Epoch= 5353)\n",
      "CV loss=0.11212069950471346(Epoch= 5354)\n",
      "train loss=0.05422643131150424(Epoch= 5354)\n",
      "CV loss=0.11223993307031652(Epoch= 5355)\n",
      "train loss=0.054218695875085134(Epoch= 5355)\n",
      "CV loss=0.11217536476445988(Epoch= 5356)\n",
      "train loss=0.05421126434733905(Epoch= 5356)\n",
      "CV loss=0.11203067677156742(Epoch= 5357)\n",
      "train loss=0.054208451157030345(Epoch= 5357)\n",
      "CV loss=0.11218941987744804(Epoch= 5358)\n",
      "train loss=0.054197291130789436(Epoch= 5358)\n",
      "CV loss=0.1122131498259244(Epoch= 5359)\n",
      "train loss=0.054192703998685005(Epoch= 5359)\n",
      "CV loss=0.11217665689814278(Epoch= 5360)\n",
      "train loss=0.05419072680270626(Epoch= 5360)\n",
      "CV loss=0.1123328160245092(Epoch= 5361)\n",
      "train loss=0.05419279081479678(Epoch= 5361)\n",
      "CV loss=0.11215101637581132(Epoch= 5362)\n",
      "train loss=0.05417983767527174(Epoch= 5362)\n",
      "CV loss=0.11210609706894473(Epoch= 5363)\n",
      "train loss=0.05417441660994375(Epoch= 5363)\n",
      "CV loss=0.11207199731164237(Epoch= 5364)\n",
      "train loss=0.05417895314812387(Epoch= 5364)\n",
      "CV loss=0.11209660607373324(Epoch= 5365)\n",
      "train loss=0.05416580932693246(Epoch= 5365)\n",
      "CV loss=0.11214635247123819(Epoch= 5366)\n",
      "train loss=0.054163562483927716(Epoch= 5366)\n",
      "CV loss=0.11224163335712387(Epoch= 5367)\n",
      "train loss=0.0541562903734752(Epoch= 5367)\n",
      "CV loss=0.11216780613107694(Epoch= 5368)\n",
      "train loss=0.054165320488174275(Epoch= 5368)\n",
      "CV loss=0.11209102346265518(Epoch= 5369)\n",
      "train loss=0.05415187631597819(Epoch= 5369)\n",
      "CV loss=0.11213499735556617(Epoch= 5370)\n",
      "train loss=0.054150587670943696(Epoch= 5370)\n",
      "CV loss=0.11214809042167725(Epoch= 5371)\n",
      "train loss=0.0541425694747195(Epoch= 5371)\n",
      "CV loss=0.11210497617582693(Epoch= 5372)\n",
      "train loss=0.05414211233227086(Epoch= 5372)\n",
      "CV loss=0.11217238261190725(Epoch= 5373)\n",
      "train loss=0.05413316345493463(Epoch= 5373)\n",
      "CV loss=0.11219305935326503(Epoch= 5374)\n",
      "train loss=0.0541251809469933(Epoch= 5374)\n",
      "CV loss=0.11197182679886475(Epoch= 5375)\n",
      "train loss=0.05412602095608221(Epoch= 5375)\n",
      "CV loss=0.11223193955101728(Epoch= 5376)\n",
      "train loss=0.054118203450310924(Epoch= 5376)\n",
      "CV loss=0.1121429549735182(Epoch= 5377)\n",
      "train loss=0.05411114146081935(Epoch= 5377)\n",
      "CV loss=0.11206569738738005(Epoch= 5378)\n",
      "train loss=0.05410830451950037(Epoch= 5378)\n",
      "CV loss=0.11208237657715106(Epoch= 5379)\n",
      "train loss=0.054101126034816725(Epoch= 5379)\n",
      "CV loss=0.11210705777831864(Epoch= 5380)\n",
      "train loss=0.05409827849830161(Epoch= 5380)\n",
      "CV loss=0.11213212387381802(Epoch= 5381)\n",
      "train loss=0.0540953973413359(Epoch= 5381)\n",
      "CV loss=0.11221830390716897(Epoch= 5382)\n",
      "train loss=0.054097294547780866(Epoch= 5382)\n",
      "CV loss=0.11210073495277857(Epoch= 5383)\n",
      "train loss=0.054087522069187655(Epoch= 5383)\n",
      "CV loss=0.1121398267573419(Epoch= 5384)\n",
      "train loss=0.054080120762719404(Epoch= 5384)\n",
      "CV loss=0.11223139191896236(Epoch= 5385)\n",
      "train loss=0.05408031703414828(Epoch= 5385)\n",
      "CV loss=0.11202931400797939(Epoch= 5386)\n",
      "train loss=0.054073864434553934(Epoch= 5386)\n",
      "CV loss=0.11219849427167217(Epoch= 5387)\n",
      "train loss=0.054070911013438964(Epoch= 5387)\n",
      "CV loss=0.11192756249925781(Epoch= 5388)\n",
      "train loss=0.05407052900487949(Epoch= 5388)\n",
      "CV loss=0.11226601919604089(Epoch= 5389)\n",
      "train loss=0.054068773123949906(Epoch= 5389)\n",
      "CV loss=0.11187581785059222(Epoch= 5390)\n",
      "train loss=0.05406604701266012(Epoch= 5390)\n",
      "CV loss=0.1121346101014898(Epoch= 5391)\n",
      "train loss=0.054052373088765183(Epoch= 5391)\n",
      "CV loss=0.11198223094943552(Epoch= 5392)\n",
      "train loss=0.054049740201613045(Epoch= 5392)\n",
      "CV loss=0.11210011559209385(Epoch= 5393)\n",
      "train loss=0.054050504600883266(Epoch= 5393)\n",
      "CV loss=0.11210498039076545(Epoch= 5394)\n",
      "train loss=0.05404307264657107(Epoch= 5394)\n",
      "CV loss=0.11195163264211191(Epoch= 5395)\n",
      "train loss=0.05403180980483435(Epoch= 5395)\n",
      "CV loss=0.1119509476397754(Epoch= 5396)\n",
      "train loss=0.054039204973052445(Epoch= 5396)\n",
      "CV loss=0.11203425349893062(Epoch= 5397)\n",
      "train loss=0.05403913101993449(Epoch= 5397)\n",
      "CV loss=0.11206562767189811(Epoch= 5398)\n",
      "train loss=0.054017083387099596(Epoch= 5398)\n",
      "CV loss=0.11188017898900932(Epoch= 5399)\n",
      "train loss=0.054021229761740064(Epoch= 5399)\n",
      "CV loss=0.11201073581002993(Epoch= 5400)\n",
      "train loss=0.05401037044310955(Epoch= 5400)\n",
      "CV loss=0.11223313904799355(Epoch= 5401)\n",
      "train loss=0.05401846107964533(Epoch= 5401)\n",
      "CV loss=0.11194914321958044(Epoch= 5402)\n",
      "train loss=0.05400091421055322(Epoch= 5402)\n",
      "CV loss=0.11217280421582222(Epoch= 5403)\n",
      "train loss=0.054014786392919026(Epoch= 5403)\n",
      "CV loss=0.11206409512072196(Epoch= 5404)\n",
      "train loss=0.05399490593775105(Epoch= 5404)\n",
      "CV loss=0.1119282333291339(Epoch= 5405)\n",
      "train loss=0.05398784686558568(Epoch= 5405)\n",
      "CV loss=0.11186179675761965(Epoch= 5406)\n",
      "train loss=0.053983603513988745(Epoch= 5406)\n",
      "CV loss=0.11188859840193113(Epoch= 5407)\n",
      "train loss=0.05397819357306368(Epoch= 5407)\n",
      "CV loss=0.11198077588138604(Epoch= 5408)\n",
      "train loss=0.0539813585846751(Epoch= 5408)\n",
      "CV loss=0.11178825573197916(Epoch= 5409)\n",
      "train loss=0.0539753704402904(Epoch= 5409)\n",
      "CV loss=0.11194588581962582(Epoch= 5410)\n",
      "train loss=0.053964080849089296(Epoch= 5410)\n",
      "CV loss=0.11182628762858984(Epoch= 5411)\n",
      "train loss=0.05395963802764877(Epoch= 5411)\n",
      "CV loss=0.11203288953229462(Epoch= 5412)\n",
      "train loss=0.05395743804363861(Epoch= 5412)\n",
      "CV loss=0.1120757839176467(Epoch= 5413)\n",
      "train loss=0.05395846702776348(Epoch= 5413)\n",
      "CV loss=0.11189816091178728(Epoch= 5414)\n",
      "train loss=0.05395100774307387(Epoch= 5414)\n",
      "CV loss=0.11202827829128881(Epoch= 5415)\n",
      "train loss=0.05394704264013756(Epoch= 5415)\n",
      "CV loss=0.11191047819748376(Epoch= 5416)\n",
      "train loss=0.053945673773183614(Epoch= 5416)\n",
      "CV loss=0.11181588106754857(Epoch= 5417)\n",
      "train loss=0.05394034317256055(Epoch= 5417)\n",
      "CV loss=0.11206915270373144(Epoch= 5418)\n",
      "train loss=0.053935548633801275(Epoch= 5418)\n",
      "CV loss=0.11200905160355887(Epoch= 5419)\n",
      "train loss=0.05393420335969467(Epoch= 5419)\n",
      "CV loss=0.11185375555361793(Epoch= 5420)\n",
      "train loss=0.05392421189833961(Epoch= 5420)\n",
      "CV loss=0.11193133359459675(Epoch= 5421)\n",
      "train loss=0.053922624407107204(Epoch= 5421)\n",
      "CV loss=0.1117874377912874(Epoch= 5422)\n",
      "train loss=0.053912569131829(Epoch= 5422)\n",
      "CV loss=0.11184922466038516(Epoch= 5423)\n",
      "train loss=0.05390701071973836(Epoch= 5423)\n",
      "CV loss=0.11188133554920787(Epoch= 5424)\n",
      "train loss=0.05390563507279884(Epoch= 5424)\n",
      "CV loss=0.11175526064026817(Epoch= 5425)\n",
      "train loss=0.05391522936238376(Epoch= 5425)\n",
      "CV loss=0.11182021272365955(Epoch= 5426)\n",
      "train loss=0.05389545778337905(Epoch= 5426)\n",
      "CV loss=0.11195940389539746(Epoch= 5427)\n",
      "train loss=0.053893005464486624(Epoch= 5427)\n",
      "CV loss=0.11196538820798228(Epoch= 5428)\n",
      "train loss=0.053890099010361486(Epoch= 5428)\n",
      "CV loss=0.1117154782280705(Epoch= 5429)\n",
      "train loss=0.053893542229615385(Epoch= 5429)\n",
      "CV loss=0.11186377703022297(Epoch= 5430)\n",
      "train loss=0.0538765554838886(Epoch= 5430)\n",
      "CV loss=0.11173099308556242(Epoch= 5431)\n",
      "train loss=0.0538754741405007(Epoch= 5431)\n",
      "CV loss=0.1118063845148953(Epoch= 5432)\n",
      "train loss=0.05387371413407033(Epoch= 5432)\n",
      "CV loss=0.11196847985024397(Epoch= 5433)\n",
      "train loss=0.053867354486861335(Epoch= 5433)\n",
      "CV loss=0.11187684948338507(Epoch= 5434)\n",
      "train loss=0.05387460168743534(Epoch= 5434)\n",
      "CV loss=0.11163401637773565(Epoch= 5435)\n",
      "train loss=0.053861156552174834(Epoch= 5435)\n",
      "CV loss=0.1117996964071888(Epoch= 5436)\n",
      "train loss=0.053854564755021934(Epoch= 5436)\n",
      "CV loss=0.11179332518244639(Epoch= 5437)\n",
      "train loss=0.05384536939184717(Epoch= 5437)\n",
      "CV loss=0.11192103887085483(Epoch= 5438)\n",
      "train loss=0.05384551585965727(Epoch= 5438)\n",
      "CV loss=0.11174571395558823(Epoch= 5439)\n",
      "train loss=0.05383885253980625(Epoch= 5439)\n",
      "CV loss=0.11179884710352328(Epoch= 5440)\n",
      "train loss=0.05383467719007297(Epoch= 5440)\n",
      "CV loss=0.11191246134834651(Epoch= 5441)\n",
      "train loss=0.05383999701069991(Epoch= 5441)\n",
      "CV loss=0.11158946444926737(Epoch= 5442)\n",
      "train loss=0.05383707620615833(Epoch= 5442)\n",
      "CV loss=0.11182129597954496(Epoch= 5443)\n",
      "train loss=0.053824533785675956(Epoch= 5443)\n",
      "CV loss=0.11187514746871324(Epoch= 5444)\n",
      "train loss=0.053825000491351874(Epoch= 5444)\n",
      "CV loss=0.11197752710897116(Epoch= 5445)\n",
      "train loss=0.05382227272543657(Epoch= 5445)\n",
      "CV loss=0.11187130781776511(Epoch= 5446)\n",
      "train loss=0.05383234533495068(Epoch= 5446)\n",
      "CV loss=0.11193202650457945(Epoch= 5447)\n",
      "train loss=0.05381584595764502(Epoch= 5447)\n",
      "CV loss=0.11178422603317215(Epoch= 5448)\n",
      "train loss=0.05380476371082801(Epoch= 5448)\n",
      "CV loss=0.11185294952337485(Epoch= 5449)\n",
      "train loss=0.05380944575059334(Epoch= 5449)\n",
      "CV loss=0.11196394946523985(Epoch= 5450)\n",
      "train loss=0.05379864240781553(Epoch= 5450)\n",
      "CV loss=0.11155145462432076(Epoch= 5451)\n",
      "train loss=0.053800710923212494(Epoch= 5451)\n",
      "CV loss=0.11189818491153568(Epoch= 5452)\n",
      "train loss=0.05378814066622115(Epoch= 5452)\n",
      "CV loss=0.1116995176710741(Epoch= 5453)\n",
      "train loss=0.05378132761368146(Epoch= 5453)\n",
      "CV loss=0.11172952920531574(Epoch= 5454)\n",
      "train loss=0.05377603935733182(Epoch= 5454)\n",
      "CV loss=0.11180651318909988(Epoch= 5455)\n",
      "train loss=0.05376988297569918(Epoch= 5455)\n",
      "CV loss=0.11175062939298938(Epoch= 5456)\n",
      "train loss=0.05376577170314754(Epoch= 5456)\n",
      "CV loss=0.111768202453047(Epoch= 5457)\n",
      "train loss=0.053760681645215275(Epoch= 5457)\n",
      "CV loss=0.11179887141960967(Epoch= 5458)\n",
      "train loss=0.05375767412490662(Epoch= 5458)\n",
      "CV loss=0.11166133626748603(Epoch= 5459)\n",
      "train loss=0.05375961969360491(Epoch= 5459)\n",
      "CV loss=0.11166280872264833(Epoch= 5460)\n",
      "train loss=0.053757306126223724(Epoch= 5460)\n",
      "CV loss=0.11171638410505955(Epoch= 5461)\n",
      "train loss=0.053750788014779706(Epoch= 5461)\n",
      "CV loss=0.11167997640231012(Epoch= 5462)\n",
      "train loss=0.05374684245938553(Epoch= 5462)\n",
      "CV loss=0.11159799215062233(Epoch= 5463)\n",
      "train loss=0.05373538834473803(Epoch= 5463)\n",
      "CV loss=0.1118164129130646(Epoch= 5464)\n",
      "train loss=0.05373659195736412(Epoch= 5464)\n",
      "CV loss=0.11174202188286661(Epoch= 5465)\n",
      "train loss=0.05372849582842881(Epoch= 5465)\n",
      "CV loss=0.11179131896464449(Epoch= 5466)\n",
      "train loss=0.0537221483723816(Epoch= 5466)\n",
      "CV loss=0.11180634502424214(Epoch= 5467)\n",
      "train loss=0.05372097575953264(Epoch= 5467)\n",
      "CV loss=0.11162120684826618(Epoch= 5468)\n",
      "train loss=0.05371418904361576(Epoch= 5468)\n",
      "CV loss=0.11170448569694764(Epoch= 5469)\n",
      "train loss=0.0537148306235311(Epoch= 5469)\n",
      "CV loss=0.11178390921418914(Epoch= 5470)\n",
      "train loss=0.05370768426813286(Epoch= 5470)\n",
      "CV loss=0.11182198344314111(Epoch= 5471)\n",
      "train loss=0.0537061269428164(Epoch= 5471)\n",
      "CV loss=0.11169290378015387(Epoch= 5472)\n",
      "train loss=0.053699486586293456(Epoch= 5472)\n",
      "CV loss=0.11161912113002835(Epoch= 5473)\n",
      "train loss=0.053695728833658904(Epoch= 5473)\n",
      "CV loss=0.1118040462948276(Epoch= 5474)\n",
      "train loss=0.05369785861618127(Epoch= 5474)\n",
      "CV loss=0.11158586825861389(Epoch= 5475)\n",
      "train loss=0.053686125104473216(Epoch= 5475)\n",
      "CV loss=0.11158862253408251(Epoch= 5476)\n",
      "train loss=0.053684979649261014(Epoch= 5476)\n",
      "CV loss=0.11151772912987312(Epoch= 5477)\n",
      "train loss=0.05367864090550863(Epoch= 5477)\n",
      "CV loss=0.11170546631556935(Epoch= 5478)\n",
      "train loss=0.05367742638932489(Epoch= 5478)\n",
      "CV loss=0.11166180718359131(Epoch= 5479)\n",
      "train loss=0.0536835697635174(Epoch= 5479)\n",
      "CV loss=0.11163367606201666(Epoch= 5480)\n",
      "train loss=0.05366770252896229(Epoch= 5480)\n",
      "CV loss=0.11151938225926114(Epoch= 5481)\n",
      "train loss=0.05366295858134803(Epoch= 5481)\n",
      "CV loss=0.11156187663825978(Epoch= 5482)\n",
      "train loss=0.05365864140404529(Epoch= 5482)\n",
      "CV loss=0.11180103708812687(Epoch= 5483)\n",
      "train loss=0.053667366890150434(Epoch= 5483)\n",
      "CV loss=0.11148248835006004(Epoch= 5484)\n",
      "train loss=0.05365087698424523(Epoch= 5484)\n",
      "CV loss=0.11171793779953687(Epoch= 5485)\n",
      "train loss=0.05364343826726554(Epoch= 5485)\n",
      "CV loss=0.11156365456059039(Epoch= 5486)\n",
      "train loss=0.053638457349122995(Epoch= 5486)\n",
      "CV loss=0.11161182033791425(Epoch= 5487)\n",
      "train loss=0.05363521496383389(Epoch= 5487)\n",
      "CV loss=0.1117716037396936(Epoch= 5488)\n",
      "train loss=0.05364597020932091(Epoch= 5488)\n",
      "CV loss=0.11154712127163494(Epoch= 5489)\n",
      "train loss=0.05362830571355056(Epoch= 5489)\n",
      "CV loss=0.11146184067969828(Epoch= 5490)\n",
      "train loss=0.053630733666711464(Epoch= 5490)\n",
      "CV loss=0.11172716341930712(Epoch= 5491)\n",
      "train loss=0.05363534726597986(Epoch= 5491)\n",
      "CV loss=0.11151143520241007(Epoch= 5492)\n",
      "train loss=0.05361902821370789(Epoch= 5492)\n",
      "CV loss=0.11163784936498844(Epoch= 5493)\n",
      "train loss=0.05361135731301815(Epoch= 5493)\n",
      "CV loss=0.11151502470563918(Epoch= 5494)\n",
      "train loss=0.053607076986682815(Epoch= 5494)\n",
      "CV loss=0.11156774164524094(Epoch= 5495)\n",
      "train loss=0.05359931564245073(Epoch= 5495)\n",
      "CV loss=0.11173011621353632(Epoch= 5496)\n",
      "train loss=0.05360239885161938(Epoch= 5496)\n",
      "CV loss=0.11147872400036633(Epoch= 5497)\n",
      "train loss=0.05359704536512463(Epoch= 5497)\n",
      "CV loss=0.111488918171577(Epoch= 5498)\n",
      "train loss=0.053588557979519405(Epoch= 5498)\n",
      "CV loss=0.11159034829748887(Epoch= 5499)\n",
      "train loss=0.05359076892285844(Epoch= 5499)\n",
      "CV loss=0.11162285985193325(Epoch= 5500)\n",
      "train loss=0.05358957835680876(Epoch= 5500)\n",
      "CV loss=0.1114593426755502(Epoch= 5501)\n",
      "train loss=0.05358404718301639(Epoch= 5501)\n",
      "CV loss=0.1114672659125838(Epoch= 5502)\n",
      "train loss=0.05357184237631511(Epoch= 5502)\n",
      "CV loss=0.11146604229620738(Epoch= 5503)\n",
      "train loss=0.053571270603467946(Epoch= 5503)\n",
      "CV loss=0.11139451644390222(Epoch= 5504)\n",
      "train loss=0.053571749868202165(Epoch= 5504)\n",
      "CV loss=0.11151522232618169(Epoch= 5505)\n",
      "train loss=0.05356688291905852(Epoch= 5505)\n",
      "CV loss=0.1115980313272946(Epoch= 5506)\n",
      "train loss=0.05355685887045693(Epoch= 5506)\n",
      "CV loss=0.11134989648215521(Epoch= 5507)\n",
      "train loss=0.05357493574199446(Epoch= 5507)\n",
      "CV loss=0.1115070591860868(Epoch= 5508)\n",
      "train loss=0.05354915389350044(Epoch= 5508)\n",
      "CV loss=0.11145924680716408(Epoch= 5509)\n",
      "train loss=0.05354181232371998(Epoch= 5509)\n",
      "CV loss=0.11141910767351468(Epoch= 5510)\n",
      "train loss=0.05355127020156376(Epoch= 5510)\n",
      "CV loss=0.11157819539025432(Epoch= 5511)\n",
      "train loss=0.05354757155068127(Epoch= 5511)\n",
      "CV loss=0.1113961274368089(Epoch= 5512)\n",
      "train loss=0.05353812315543462(Epoch= 5512)\n",
      "CV loss=0.11166354236544744(Epoch= 5513)\n",
      "train loss=0.053530035094588715(Epoch= 5513)\n",
      "CV loss=0.1114736853609208(Epoch= 5514)\n",
      "train loss=0.053527450757763585(Epoch= 5514)\n",
      "CV loss=0.11155844441330787(Epoch= 5515)\n",
      "train loss=0.05352330775098466(Epoch= 5515)\n",
      "CV loss=0.11160040022314845(Epoch= 5516)\n",
      "train loss=0.05352256031135184(Epoch= 5516)\n",
      "CV loss=0.11146452911128221(Epoch= 5517)\n",
      "train loss=0.05351367148224112(Epoch= 5517)\n",
      "CV loss=0.1115376108279651(Epoch= 5518)\n",
      "train loss=0.053511809163601064(Epoch= 5518)\n",
      "CV loss=0.11146031772641259(Epoch= 5519)\n",
      "train loss=0.053502714512437446(Epoch= 5519)\n",
      "CV loss=0.11146220625821379(Epoch= 5520)\n",
      "train loss=0.05349962144946652(Epoch= 5520)\n",
      "CV loss=0.11131853291803293(Epoch= 5521)\n",
      "train loss=0.05350057838505166(Epoch= 5521)\n",
      "CV loss=0.11131354725310513(Epoch= 5522)\n",
      "train loss=0.05349438410380311(Epoch= 5522)\n",
      "CV loss=0.11150984538321507(Epoch= 5523)\n",
      "train loss=0.05349072072846511(Epoch= 5523)\n",
      "CV loss=0.11150457027636942(Epoch= 5524)\n",
      "train loss=0.05348300134122284(Epoch= 5524)\n",
      "CV loss=0.11148015222138609(Epoch= 5525)\n",
      "train loss=0.05347644059715805(Epoch= 5525)\n",
      "CV loss=0.11132734276529488(Epoch= 5526)\n",
      "train loss=0.05347658720881063(Epoch= 5526)\n",
      "CV loss=0.11158275932992998(Epoch= 5527)\n",
      "train loss=0.05347463382764652(Epoch= 5527)\n",
      "CV loss=0.11149835643960486(Epoch= 5528)\n",
      "train loss=0.05346624627065185(Epoch= 5528)\n",
      "CV loss=0.11139005726800418(Epoch= 5529)\n",
      "train loss=0.053461597885317465(Epoch= 5529)\n",
      "CV loss=0.11142694982480975(Epoch= 5530)\n",
      "train loss=0.05345918311447224(Epoch= 5530)\n",
      "CV loss=0.11146571761751878(Epoch= 5531)\n",
      "train loss=0.053457135850777424(Epoch= 5531)\n",
      "CV loss=0.11126523895092177(Epoch= 5532)\n",
      "train loss=0.05345794629691641(Epoch= 5532)\n",
      "CV loss=0.11118739005963593(Epoch= 5533)\n",
      "train loss=0.053476186363518904(Epoch= 5533)\n",
      "CV loss=0.11148195610120076(Epoch= 5534)\n",
      "train loss=0.0534461037226523(Epoch= 5534)\n",
      "CV loss=0.11169620792605918(Epoch= 5535)\n",
      "train loss=0.05345472477097754(Epoch= 5535)\n",
      "CV loss=0.11137503790983783(Epoch= 5536)\n",
      "train loss=0.05343258801948911(Epoch= 5536)\n",
      "CV loss=0.11127282839402372(Epoch= 5537)\n",
      "train loss=0.05343488816224137(Epoch= 5537)\n",
      "CV loss=0.1114009811668307(Epoch= 5538)\n",
      "train loss=0.05342882331682802(Epoch= 5538)\n",
      "CV loss=0.11133446371031597(Epoch= 5539)\n",
      "train loss=0.05342256118202879(Epoch= 5539)\n",
      "CV loss=0.11128723122324383(Epoch= 5540)\n",
      "train loss=0.05342165077017506(Epoch= 5540)\n",
      "CV loss=0.1113492046474199(Epoch= 5541)\n",
      "train loss=0.05341531976976097(Epoch= 5541)\n",
      "CV loss=0.11121854717669424(Epoch= 5542)\n",
      "train loss=0.05341213138003083(Epoch= 5542)\n",
      "CV loss=0.11140332499900459(Epoch= 5543)\n",
      "train loss=0.053406949086443466(Epoch= 5543)\n",
      "CV loss=0.11129659375452863(Epoch= 5544)\n",
      "train loss=0.053408536347507564(Epoch= 5544)\n",
      "CV loss=0.11125110874553643(Epoch= 5545)\n",
      "train loss=0.05339654536863421(Epoch= 5545)\n",
      "CV loss=0.1113271422706702(Epoch= 5546)\n",
      "train loss=0.053393198395567554(Epoch= 5546)\n",
      "CV loss=0.11136101152200828(Epoch= 5547)\n",
      "train loss=0.05339188290325689(Epoch= 5547)\n",
      "CV loss=0.11145142005973019(Epoch= 5548)\n",
      "train loss=0.05338957076503069(Epoch= 5548)\n",
      "CV loss=0.11127594053544382(Epoch= 5549)\n",
      "train loss=0.05339271278174165(Epoch= 5549)\n",
      "CV loss=0.11125718912096257(Epoch= 5550)\n",
      "train loss=0.05337772889487807(Epoch= 5550)\n",
      "CV loss=0.11144322804038392(Epoch= 5551)\n",
      "train loss=0.053390269673067585(Epoch= 5551)\n",
      "CV loss=0.11143603768451121(Epoch= 5552)\n",
      "train loss=0.053370602005838964(Epoch= 5552)\n",
      "CV loss=0.11136901098888627(Epoch= 5553)\n",
      "train loss=0.0533667022868789(Epoch= 5553)\n",
      "CV loss=0.11121873823656889(Epoch= 5554)\n",
      "train loss=0.053370628888175585(Epoch= 5554)\n",
      "CV loss=0.11131465704758498(Epoch= 5555)\n",
      "train loss=0.05335972764895743(Epoch= 5555)\n",
      "CV loss=0.11132948554468514(Epoch= 5556)\n",
      "train loss=0.05336025569160185(Epoch= 5556)\n",
      "CV loss=0.111226814449808(Epoch= 5557)\n",
      "train loss=0.05335435864654048(Epoch= 5557)\n",
      "CV loss=0.11129489010415182(Epoch= 5558)\n",
      "train loss=0.053344473178669506(Epoch= 5558)\n",
      "CV loss=0.11119515138787829(Epoch= 5559)\n",
      "train loss=0.05334827115725631(Epoch= 5559)\n",
      "CV loss=0.1111720963185267(Epoch= 5560)\n",
      "train loss=0.05333972662330851(Epoch= 5560)\n",
      "CV loss=0.11135491261561252(Epoch= 5561)\n",
      "train loss=0.053340213160619473(Epoch= 5561)\n",
      "CV loss=0.11125801136918438(Epoch= 5562)\n",
      "train loss=0.05333240306195359(Epoch= 5562)\n",
      "CV loss=0.11119435912322195(Epoch= 5563)\n",
      "train loss=0.053335153090444384(Epoch= 5563)\n",
      "CV loss=0.1110483068959677(Epoch= 5564)\n",
      "train loss=0.05333733602668019(Epoch= 5564)\n",
      "CV loss=0.11122077273851709(Epoch= 5565)\n",
      "train loss=0.053323044705101776(Epoch= 5565)\n",
      "CV loss=0.11117443799647847(Epoch= 5566)\n",
      "train loss=0.053326877491151566(Epoch= 5566)\n",
      "CV loss=0.11118008865916712(Epoch= 5567)\n",
      "train loss=0.05332105767382141(Epoch= 5567)\n",
      "CV loss=0.1112638193078116(Epoch= 5568)\n",
      "train loss=0.05330445980440082(Epoch= 5568)\n",
      "CV loss=0.11135214173584802(Epoch= 5569)\n",
      "train loss=0.053313695668761(Epoch= 5569)\n",
      "CV loss=0.11142703050795649(Epoch= 5570)\n",
      "train loss=0.05331670300583116(Epoch= 5570)\n",
      "CV loss=0.11123657880320117(Epoch= 5571)\n",
      "train loss=0.053305560489219764(Epoch= 5571)\n",
      "CV loss=0.11119400742974297(Epoch= 5572)\n",
      "train loss=0.05329162701000348(Epoch= 5572)\n",
      "CV loss=0.11118878518159009(Epoch= 5573)\n",
      "train loss=0.05329115047482646(Epoch= 5573)\n",
      "CV loss=0.11114470175379776(Epoch= 5574)\n",
      "train loss=0.05328543700962105(Epoch= 5574)\n",
      "CV loss=0.11130330469292933(Epoch= 5575)\n",
      "train loss=0.05327810246355254(Epoch= 5575)\n",
      "CV loss=0.11133967605771662(Epoch= 5576)\n",
      "train loss=0.053278531860036234(Epoch= 5576)\n",
      "CV loss=0.11124341240757174(Epoch= 5577)\n",
      "train loss=0.05327237360371696(Epoch= 5577)\n",
      "CV loss=0.1113611462244429(Epoch= 5578)\n",
      "train loss=0.053273318120709866(Epoch= 5578)\n",
      "CV loss=0.11112421253845932(Epoch= 5579)\n",
      "train loss=0.05326572435296603(Epoch= 5579)\n",
      "CV loss=0.1112397359737644(Epoch= 5580)\n",
      "train loss=0.05326377243426294(Epoch= 5580)\n",
      "CV loss=0.1112347873470122(Epoch= 5581)\n",
      "train loss=0.05325427994506762(Epoch= 5581)\n",
      "CV loss=0.11119581333435184(Epoch= 5582)\n",
      "train loss=0.05327202134205765(Epoch= 5582)\n",
      "CV loss=0.11129543660404015(Epoch= 5583)\n",
      "train loss=0.05325529289017203(Epoch= 5583)\n",
      "CV loss=0.11111966122435868(Epoch= 5584)\n",
      "train loss=0.0532445527939144(Epoch= 5584)\n",
      "CV loss=0.11119418151264546(Epoch= 5585)\n",
      "train loss=0.05324476896266882(Epoch= 5585)\n",
      "CV loss=0.11119740723643126(Epoch= 5586)\n",
      "train loss=0.053236846921762566(Epoch= 5586)\n",
      "CV loss=0.11109195707681628(Epoch= 5587)\n",
      "train loss=0.053234143655861606(Epoch= 5587)\n",
      "CV loss=0.11116920209746445(Epoch= 5588)\n",
      "train loss=0.05322765044224918(Epoch= 5588)\n",
      "CV loss=0.11112563911266954(Epoch= 5589)\n",
      "train loss=0.05322650384313129(Epoch= 5589)\n",
      "CV loss=0.11104422405715796(Epoch= 5590)\n",
      "train loss=0.05322321006739355(Epoch= 5590)\n",
      "CV loss=0.11108635923267812(Epoch= 5591)\n",
      "train loss=0.05322170007276962(Epoch= 5591)\n",
      "CV loss=0.11106348978265154(Epoch= 5592)\n",
      "train loss=0.053210927406092456(Epoch= 5592)\n",
      "CV loss=0.1111201806924028(Epoch= 5593)\n",
      "train loss=0.0532155473676951(Epoch= 5593)\n",
      "CV loss=0.11105945547311943(Epoch= 5594)\n",
      "train loss=0.05320892961513819(Epoch= 5594)\n",
      "CV loss=0.11120417852236454(Epoch= 5595)\n",
      "train loss=0.053210161563866445(Epoch= 5595)\n",
      "CV loss=0.1111482520051754(Epoch= 5596)\n",
      "train loss=0.05319642848095261(Epoch= 5596)\n",
      "CV loss=0.11124188956714132(Epoch= 5597)\n",
      "train loss=0.05319386678343809(Epoch= 5597)\n",
      "CV loss=0.11119299274036966(Epoch= 5598)\n",
      "train loss=0.0531911588390041(Epoch= 5598)\n",
      "CV loss=0.11120914272437611(Epoch= 5599)\n",
      "train loss=0.05318853771371841(Epoch= 5599)\n",
      "CV loss=0.11109568077988036(Epoch= 5600)\n",
      "train loss=0.05318003507459821(Epoch= 5600)\n",
      "CV loss=0.11120060835519381(Epoch= 5601)\n",
      "train loss=0.05318183175054969(Epoch= 5601)\n",
      "CV loss=0.11107418592088705(Epoch= 5602)\n",
      "train loss=0.05317309060421034(Epoch= 5602)\n",
      "CV loss=0.11103130257099128(Epoch= 5603)\n",
      "train loss=0.053186478323084686(Epoch= 5603)\n",
      "CV loss=0.11114288344675122(Epoch= 5604)\n",
      "train loss=0.05316744921368543(Epoch= 5604)\n",
      "CV loss=0.11111873277475302(Epoch= 5605)\n",
      "train loss=0.053166514790679145(Epoch= 5605)\n",
      "CV loss=0.11129107482804823(Epoch= 5606)\n",
      "train loss=0.05316598828568797(Epoch= 5606)\n",
      "CV loss=0.11101793660487867(Epoch= 5607)\n",
      "train loss=0.053158051217356984(Epoch= 5607)\n",
      "CV loss=0.11097994778257823(Epoch= 5608)\n",
      "train loss=0.053156913798678984(Epoch= 5608)\n",
      "CV loss=0.11110751880537637(Epoch= 5609)\n",
      "train loss=0.053149268880977305(Epoch= 5609)\n",
      "CV loss=0.1110994731346664(Epoch= 5610)\n",
      "train loss=0.05314606327436605(Epoch= 5610)\n",
      "CV loss=0.11105811783677029(Epoch= 5611)\n",
      "train loss=0.05314673283118388(Epoch= 5611)\n",
      "CV loss=0.11107915694570808(Epoch= 5612)\n",
      "train loss=0.05313504884969773(Epoch= 5612)\n",
      "CV loss=0.11112100334655342(Epoch= 5613)\n",
      "train loss=0.05313045310811253(Epoch= 5613)\n",
      "CV loss=0.11092154678569188(Epoch= 5614)\n",
      "train loss=0.053132872241358(Epoch= 5614)\n",
      "CV loss=0.11098730684479746(Epoch= 5615)\n",
      "train loss=0.053124179096417826(Epoch= 5615)\n",
      "CV loss=0.11100169463416223(Epoch= 5616)\n",
      "train loss=0.0531244963670646(Epoch= 5616)\n",
      "CV loss=0.11105482959062499(Epoch= 5617)\n",
      "train loss=0.05311909347908913(Epoch= 5617)\n",
      "CV loss=0.11112975751850537(Epoch= 5618)\n",
      "train loss=0.05311327704518746(Epoch= 5618)\n",
      "CV loss=0.11126287412641156(Epoch= 5619)\n",
      "train loss=0.053117757615589706(Epoch= 5619)\n",
      "CV loss=0.11111971522017225(Epoch= 5620)\n",
      "train loss=0.05310426046424939(Epoch= 5620)\n",
      "CV loss=0.11104695108095913(Epoch= 5621)\n",
      "train loss=0.05310020424301864(Epoch= 5621)\n",
      "CV loss=0.11093993055862636(Epoch= 5622)\n",
      "train loss=0.053098439663511895(Epoch= 5622)\n",
      "CV loss=0.11102118042768344(Epoch= 5623)\n",
      "train loss=0.053103187940832706(Epoch= 5623)\n",
      "CV loss=0.11095533652293207(Epoch= 5624)\n",
      "train loss=0.05309375508700499(Epoch= 5624)\n",
      "CV loss=0.11093980020821168(Epoch= 5625)\n",
      "train loss=0.05308604505993951(Epoch= 5625)\n",
      "CV loss=0.11102232987220706(Epoch= 5626)\n",
      "train loss=0.053084431794754916(Epoch= 5626)\n",
      "CV loss=0.11096590724662875(Epoch= 5627)\n",
      "train loss=0.05307781012647424(Epoch= 5627)\n",
      "CV loss=0.11095110053583063(Epoch= 5628)\n",
      "train loss=0.05307926772171978(Epoch= 5628)\n",
      "CV loss=0.11089048669459418(Epoch= 5629)\n",
      "train loss=0.053076235435965846(Epoch= 5629)\n",
      "CV loss=0.11094683269305719(Epoch= 5630)\n",
      "train loss=0.053068145109745626(Epoch= 5630)\n",
      "CV loss=0.11090825471434443(Epoch= 5631)\n",
      "train loss=0.053062133254591644(Epoch= 5631)\n",
      "CV loss=0.11091565507600612(Epoch= 5632)\n",
      "train loss=0.05306324384443273(Epoch= 5632)\n",
      "CV loss=0.11096887342176931(Epoch= 5633)\n",
      "train loss=0.05306190021054081(Epoch= 5633)\n",
      "CV loss=0.11086756562094147(Epoch= 5634)\n",
      "train loss=0.053059442810128844(Epoch= 5634)\n",
      "CV loss=0.11082049516712364(Epoch= 5635)\n",
      "train loss=0.05305483162629301(Epoch= 5635)\n",
      "CV loss=0.11109000519531972(Epoch= 5636)\n",
      "train loss=0.05304558389738852(Epoch= 5636)\n",
      "CV loss=0.1109113164089473(Epoch= 5637)\n",
      "train loss=0.05304218762886119(Epoch= 5637)\n",
      "CV loss=0.11110489475232038(Epoch= 5638)\n",
      "train loss=0.05304292473231346(Epoch= 5638)\n",
      "CV loss=0.11104812240397603(Epoch= 5639)\n",
      "train loss=0.05306978309276433(Epoch= 5639)\n",
      "CV loss=0.11090026046011855(Epoch= 5640)\n",
      "train loss=0.0530377299521335(Epoch= 5640)\n",
      "CV loss=0.11111156221215308(Epoch= 5641)\n",
      "train loss=0.05303818966030557(Epoch= 5641)\n",
      "CV loss=0.1108896773338501(Epoch= 5642)\n",
      "train loss=0.05302764852915737(Epoch= 5642)\n",
      "CV loss=0.11117326292678129(Epoch= 5643)\n",
      "train loss=0.05303486776804619(Epoch= 5643)\n",
      "CV loss=0.11103744986865227(Epoch= 5644)\n",
      "train loss=0.05302550719279237(Epoch= 5644)\n",
      "CV loss=0.11091134706111339(Epoch= 5645)\n",
      "train loss=0.05301247586510468(Epoch= 5645)\n",
      "CV loss=0.11080865215918505(Epoch= 5646)\n",
      "train loss=0.05301591082589295(Epoch= 5646)\n",
      "CV loss=0.11090527415867098(Epoch= 5647)\n",
      "train loss=0.053001139498318126(Epoch= 5647)\n",
      "CV loss=0.11095484038948733(Epoch= 5648)\n",
      "train loss=0.05300778526583359(Epoch= 5648)\n",
      "CV loss=0.11106872500961701(Epoch= 5649)\n",
      "train loss=0.05300038872289068(Epoch= 5649)\n",
      "CV loss=0.11084587058131518(Epoch= 5650)\n",
      "train loss=0.05299678598178201(Epoch= 5650)\n",
      "CV loss=0.110894865545573(Epoch= 5651)\n",
      "train loss=0.052990165393283134(Epoch= 5651)\n",
      "CV loss=0.1110321934753965(Epoch= 5652)\n",
      "train loss=0.05298911738021837(Epoch= 5652)\n",
      "CV loss=0.11091995357690894(Epoch= 5653)\n",
      "train loss=0.05297943143918623(Epoch= 5653)\n",
      "CV loss=0.11084291410889846(Epoch= 5654)\n",
      "train loss=0.052986489031272226(Epoch= 5654)\n",
      "CV loss=0.11082589945442459(Epoch= 5655)\n",
      "train loss=0.052971416189845875(Epoch= 5655)\n",
      "CV loss=0.11092864761600385(Epoch= 5656)\n",
      "train loss=0.05297105204485722(Epoch= 5656)\n",
      "CV loss=0.11088694969571566(Epoch= 5657)\n",
      "train loss=0.05296471949850318(Epoch= 5657)\n",
      "CV loss=0.11094203358164595(Epoch= 5658)\n",
      "train loss=0.05296247159684169(Epoch= 5658)\n",
      "CV loss=0.1110058262123347(Epoch= 5659)\n",
      "train loss=0.052972512387868854(Epoch= 5659)\n",
      "CV loss=0.11093847663228246(Epoch= 5660)\n",
      "train loss=0.05295521889671081(Epoch= 5660)\n",
      "CV loss=0.11082110668206958(Epoch= 5661)\n",
      "train loss=0.052951041303604174(Epoch= 5661)\n",
      "CV loss=0.11116995360503362(Epoch= 5662)\n",
      "train loss=0.05296753446325635(Epoch= 5662)\n",
      "CV loss=0.11097843618368158(Epoch= 5663)\n",
      "train loss=0.05294944886924346(Epoch= 5663)\n",
      "CV loss=0.11088259687880991(Epoch= 5664)\n",
      "train loss=0.05294349999275594(Epoch= 5664)\n",
      "CV loss=0.11086347623012777(Epoch= 5665)\n",
      "train loss=0.05295669562570357(Epoch= 5665)\n",
      "CV loss=0.11093750198453523(Epoch= 5666)\n",
      "train loss=0.052937204116805095(Epoch= 5666)\n",
      "CV loss=0.11093083669943524(Epoch= 5667)\n",
      "train loss=0.05293455960995458(Epoch= 5667)\n",
      "CV loss=0.110972054357876(Epoch= 5668)\n",
      "train loss=0.052934995945869506(Epoch= 5668)\n",
      "CV loss=0.1108586152638481(Epoch= 5669)\n",
      "train loss=0.05292513946601702(Epoch= 5669)\n",
      "CV loss=0.1107146957236004(Epoch= 5670)\n",
      "train loss=0.05291939412360694(Epoch= 5670)\n",
      "CV loss=0.110848150889017(Epoch= 5671)\n",
      "train loss=0.05292005906110205(Epoch= 5671)\n",
      "CV loss=0.11076715982612968(Epoch= 5672)\n",
      "train loss=0.05291107303619229(Epoch= 5672)\n",
      "CV loss=0.11091597405532304(Epoch= 5673)\n",
      "train loss=0.05290651060131129(Epoch= 5673)\n",
      "CV loss=0.11091347620096101(Epoch= 5674)\n",
      "train loss=0.052907303716555096(Epoch= 5674)\n",
      "CV loss=0.11089572470199618(Epoch= 5675)\n",
      "train loss=0.05290553681018267(Epoch= 5675)\n",
      "CV loss=0.11067579600643179(Epoch= 5676)\n",
      "train loss=0.05289956084033676(Epoch= 5676)\n",
      "CV loss=0.1108598135390307(Epoch= 5677)\n",
      "train loss=0.05289425244824476(Epoch= 5677)\n",
      "CV loss=0.11074585718168542(Epoch= 5678)\n",
      "train loss=0.05289302074239642(Epoch= 5678)\n",
      "CV loss=0.11065095660493954(Epoch= 5679)\n",
      "train loss=0.05289824876592453(Epoch= 5679)\n",
      "CV loss=0.11091699002699382(Epoch= 5680)\n",
      "train loss=0.05288608170459927(Epoch= 5680)\n",
      "CV loss=0.11084127631816444(Epoch= 5681)\n",
      "train loss=0.05288144831278935(Epoch= 5681)\n",
      "CV loss=0.11084285483996256(Epoch= 5682)\n",
      "train loss=0.05288351126564786(Epoch= 5682)\n",
      "CV loss=0.11082349805238195(Epoch= 5683)\n",
      "train loss=0.052868976296312285(Epoch= 5683)\n",
      "CV loss=0.11072922529369585(Epoch= 5684)\n",
      "train loss=0.05286796626077316(Epoch= 5684)\n",
      "CV loss=0.11081971961367001(Epoch= 5685)\n",
      "train loss=0.05286590854427228(Epoch= 5685)\n",
      "CV loss=0.11074088242311195(Epoch= 5686)\n",
      "train loss=0.05286411586882625(Epoch= 5686)\n",
      "CV loss=0.1108213068553407(Epoch= 5687)\n",
      "train loss=0.05285639591658909(Epoch= 5687)\n",
      "CV loss=0.11086318050807942(Epoch= 5688)\n",
      "train loss=0.0528575906573752(Epoch= 5688)\n",
      "CV loss=0.11073822537583702(Epoch= 5689)\n",
      "train loss=0.05285236199228412(Epoch= 5689)\n",
      "CV loss=0.11070467969948025(Epoch= 5690)\n",
      "train loss=0.0528445007941018(Epoch= 5690)\n",
      "CV loss=0.11062304575120324(Epoch= 5691)\n",
      "train loss=0.052847408504338715(Epoch= 5691)\n",
      "CV loss=0.11071895268406984(Epoch= 5692)\n",
      "train loss=0.05283779014514058(Epoch= 5692)\n",
      "CV loss=0.11072548782261021(Epoch= 5693)\n",
      "train loss=0.05283840261217389(Epoch= 5693)\n",
      "CV loss=0.11092938395850852(Epoch= 5694)\n",
      "train loss=0.052838609317782385(Epoch= 5694)\n",
      "CV loss=0.11072508761585757(Epoch= 5695)\n",
      "train loss=0.05283189478056867(Epoch= 5695)\n",
      "CV loss=0.11092714049542596(Epoch= 5696)\n",
      "train loss=0.05283338624048621(Epoch= 5696)\n",
      "CV loss=0.11078611418167783(Epoch= 5697)\n",
      "train loss=0.0528191682771465(Epoch= 5697)\n",
      "CV loss=0.1106875239012784(Epoch= 5698)\n",
      "train loss=0.05281789213174769(Epoch= 5698)\n",
      "CV loss=0.11058031525124967(Epoch= 5699)\n",
      "train loss=0.052824807031119136(Epoch= 5699)\n",
      "CV loss=0.11070917880770911(Epoch= 5700)\n",
      "train loss=0.05280661473943945(Epoch= 5700)\n",
      "CV loss=0.11072069001050361(Epoch= 5701)\n",
      "train loss=0.05282115170669146(Epoch= 5701)\n",
      "CV loss=0.11071150893017334(Epoch= 5702)\n",
      "train loss=0.052805869219728824(Epoch= 5702)\n",
      "CV loss=0.11076288673687355(Epoch= 5703)\n",
      "train loss=0.05279813597527178(Epoch= 5703)\n",
      "CV loss=0.11078714524551675(Epoch= 5704)\n",
      "train loss=0.052796477547218834(Epoch= 5704)\n",
      "CV loss=0.11071564882863029(Epoch= 5705)\n",
      "train loss=0.05279249507421627(Epoch= 5705)\n",
      "CV loss=0.11081633068801955(Epoch= 5706)\n",
      "train loss=0.05279281145590466(Epoch= 5706)\n",
      "CV loss=0.11076849731848068(Epoch= 5707)\n",
      "train loss=0.05278489879769216(Epoch= 5707)\n",
      "CV loss=0.11077452813702982(Epoch= 5708)\n",
      "train loss=0.05277877470403722(Epoch= 5708)\n",
      "CV loss=0.11094403096411687(Epoch= 5709)\n",
      "train loss=0.05278874330174564(Epoch= 5709)\n",
      "CV loss=0.11091463014020908(Epoch= 5710)\n",
      "train loss=0.05278610624141289(Epoch= 5710)\n",
      "CV loss=0.11060252652903667(Epoch= 5711)\n",
      "train loss=0.052771214194208584(Epoch= 5711)\n",
      "CV loss=0.11063618770824125(Epoch= 5712)\n",
      "train loss=0.05276614184656431(Epoch= 5712)\n",
      "CV loss=0.11064202711479895(Epoch= 5713)\n",
      "train loss=0.05276005468811134(Epoch= 5713)\n",
      "CV loss=0.11066026396078571(Epoch= 5714)\n",
      "train loss=0.05275949567516552(Epoch= 5714)\n",
      "CV loss=0.11080784440256519(Epoch= 5715)\n",
      "train loss=0.05275629696591157(Epoch= 5715)\n",
      "CV loss=0.11066462722555623(Epoch= 5716)\n",
      "train loss=0.05275131192072972(Epoch= 5716)\n",
      "CV loss=0.11065943393690503(Epoch= 5717)\n",
      "train loss=0.05274581450477231(Epoch= 5717)\n",
      "CV loss=0.11075594268527963(Epoch= 5718)\n",
      "train loss=0.052750603331285215(Epoch= 5718)\n",
      "CV loss=0.11060449032123823(Epoch= 5719)\n",
      "train loss=0.05274184542215565(Epoch= 5719)\n",
      "CV loss=0.11067615875975878(Epoch= 5720)\n",
      "train loss=0.052745818762397534(Epoch= 5720)\n",
      "CV loss=0.11058073587325776(Epoch= 5721)\n",
      "train loss=0.05273467997021313(Epoch= 5721)\n",
      "CV loss=0.11074322596234595(Epoch= 5722)\n",
      "train loss=0.05273428828664632(Epoch= 5722)\n",
      "CV loss=0.1109737874455406(Epoch= 5723)\n",
      "train loss=0.05275926853504592(Epoch= 5723)\n",
      "CV loss=0.11067888213007739(Epoch= 5724)\n",
      "train loss=0.052723981519103354(Epoch= 5724)\n",
      "CV loss=0.1105857170139423(Epoch= 5725)\n",
      "train loss=0.05271917258211636(Epoch= 5725)\n",
      "CV loss=0.11062407840167268(Epoch= 5726)\n",
      "train loss=0.05271587765856473(Epoch= 5726)\n",
      "CV loss=0.11067822663920387(Epoch= 5727)\n",
      "train loss=0.05271639066847819(Epoch= 5727)\n",
      "CV loss=0.11072712942685861(Epoch= 5728)\n",
      "train loss=0.052709377885857306(Epoch= 5728)\n",
      "CV loss=0.11059106432966076(Epoch= 5729)\n",
      "train loss=0.05270462959989681(Epoch= 5729)\n",
      "early stop\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "batch_size = 64\n",
    "loss = float(\"inf\")\n",
    "epsilon = 0.0001\n",
    "\n",
    "n_input = 784\n",
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 128\n",
    "n_out = 10\n",
    "\n",
    "# early stop logic\n",
    "best_cv_loss = float(\"inf\")\n",
    "patience = 30\n",
    "counter = 0\n",
    "tol = 1e-6 # accounts for floating point noise\n",
    "\n",
    "training_loss_history = []\n",
    "cv_loss_history = []\n",
    "\n",
    "i = 0\n",
    "while i < 10000: # epochs\n",
    "    randomized_indices = np.random.permutation(X_train.shape[0])\n",
    "    \n",
    "    for start_of_batch in range(0, X_train.shape[0], batch_size):\n",
    "        batch_indices = randomized_indices[start_of_batch : start_of_batch + batch_size]\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        loss, cache = forwardPass(X_train=X_batch, y_train=y_batch, weights=weights, lmbda=0.0005)\n",
    "        grads = backpropagation(cache=cache, X_train=X_batch, y_train=y_batch, weights=weights, lmbda=0.0005)\n",
    "        weights = update(alpha=0.001, cache = cache, weights=weights, grads=grads)\n",
    "\n",
    "        \n",
    "    cv_loss, cache = forwardPass(X_train=X_cv, y_train=y_cv, weights=weights, lmbda=0.0005)\n",
    "    print(\"CV loss=\", cv_loss, \"(Epoch= \", i + 1, \")\", sep=\"\")\n",
    "    train_loss, cache = forwardPass(X_train=X_train, y_train=y_train, weights=weights, lmbda=0.0005)\n",
    "    print(\"train loss=\", train_loss, \"(Epoch= \", i + 1, \")\", sep=\"\")\n",
    "    training_loss_history.append(train_loss)\n",
    "    cv_loss_history.append(cv_loss)\n",
    "\n",
    "    if cv_loss < best_cv_loss - tol:\n",
    "        best_cv_loss = cv_loss\n",
    "        best_weights = copy.deepcopy(weights)\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"early stop\")\n",
    "        weights = best_weights\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392f703",
   "metadata": {},
   "source": [
    "Plot training and cross validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d4ffb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2YAAAIjCAYAAABoNwiVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAtupJREFUeJzs3QeYE9X6BvA3yXZgl7r03nsTEOyKomDBLuoFvZb7t/desGMXO5ZrwYoVGxdUFEVBERDpvdelLlvYmvyf98xONltZll1S5v35HDMzmSSTmWSZL98537h8Pp8PIiIiIiIiEjTu4L20iIiIiIiIkAIzERERERGRIFNgJiIiIiIiEmQKzERERERERIJMgZmIiIiIiEiQKTATEREREREJMgVmIiIiIiIiQabATEREREREJMgUmImIiIiIiASZAjMRCWmXXHIJWrVqVanHPvDAA3C5XIhka9euNe/xnXfeCfamhK3SPif8zPGztz/c73wsj0NV0TF1tmOPPdY0EXEeBWYiUik8Ga1ImzZtmvZwCNm2bRtuvfVWdOrUCQkJCahRowb69u2LRx55BHv27EEoS0lJQVRUFC6++OIy10lLS0N8fDzOOusshLoPP/wQY8eORShhMFqzZk2EA5/Ph/feew9HH300ateubT7P3bt3x0MPPYSMjAyECjvQrkirygBfRMJPVLA3QETCE0+IAo0fPx4//PBDieWdO3c+qNd544034PV6K/XYe++9F3feeedBvX4k+euvvzB06FCkp6eb4IYBGc2ePRuPP/44fv31V3z//fcIVcnJyTjxxBPx1VdfITMz05yIF/fFF18gKyur3OCtIpYtWwa3213tgdnChQtx4403FlnesmVL7Nu3D9HR0dX6+uEsPz8fF154IT755BMcddRRJuvJz8P06dPx4IMP4tNPP8WPP/6Ihg0bBntT0aBBgxJ/F5955hls3LgRzz33XIl1Q/k7KCLVS4GZiFRK8RPfP/74wwRm+zshLuuEuiwHc3LK7AqbwGTDzjzzTHg8Hvz9998mYxbo0UcfNUFwedkJBjzMRgXTRRddhMmTJ+Prr7/GBRdcUGqwk5SUhGHDhh3U68TGxiJYmDmJi4sL2uuHgyeffNIEZcz+PvXUU/7lV155Jc477zwMHz7cZP/+97//HdLtKu3vG7PSxf8ufvzxx9i9e/dB/4AgIpFFXRlFpNpwnES3bt0wZ84c092IJyx33323uY9ZD548N2nSxJwEt23bFg8//LD5Jby8MWZ2t6Cnn34ar7/+unkcH9+vXz+TEdrf2CHOX3vttZg4caLZNj62a9eu5mS/OHbDPOyww8xJMl/ntddeq/C4Nf5yf+6556JFixbmNZo3b46bbrrJZEJK6zq2adMmczLJaf5qzhPO4vuCwRXXZ+DBrlujRo2qcPdDbjtf49lnny0RlBEzC8ww2rjPTz31VEyZMsXsAwZkfA5avXq1eW9169Y1x/Twww/Hd999V+I5X3zxRbNvuU6dOnXM8zBwCux2yGwRX4v7yM6IzZ07t8z3weCSJ7qBzxPY1XHq1Kk455xzzPNV9BiUprQxZosWLcLxxx9v9kWzZs1M98/SsrkV+Wzzu8F9tm7dOn83NvtzXtYYs59++slkh/j+efzPOOMMLFmypMg69udz5cqVZvu5Hj8vl156qQkaqgozUsy4cl/Ur1/fBBj8fAXaunWreV3uK+6Hxo0bm20O7K7HbO2QIUPMc/C5WrdujX//+9/lvjaPH4OxDh06YMyYMSXuP+2008x3g99p/mBE/Cy3adOm1OcbOHCg+WwGev/99/3vj59z/giwYcOGCv99q8oxZvw7xGPKQJTZwKZNm6JWrVrmc56amors7GzzPeL3h38/uM+5rLiKvCcRCS79lCwi1Wrnzp045ZRTzEkAT97srkU86eRJxM0332xuedJ5//33Y+/evUV+AS8LT8x5Yv+f//zHnLTwF3SOK2LQsL8s22+//Wa6vF199dXmBOeFF17A2WefjfXr16NevXpmHWaVTj75ZHMyyZMhnlRz7AqDpoqeuPJE+KqrrjLPOWvWLBOosPsS7wvE5+bJ6YABA0zAyS5Y7OrEE3o+3s5Y8aSW2/5///d/povol19+aU5AK4IZJp6Q8WTuQLrzjRgxwuzjK664Ah07djRj1AYNGmTe2/XXX2/e27vvvovTTz8dn332mQmciNk33s/Xu+GGG0y2bf78+fjzzz9NFzTi++BjGCh36dLFfFb4/hhs9OnTp9RtYlDC/cDH7dq1y5xg2iZMmGD2JbNqB3oM9odBxnHHHYe8vDzTPZbbwR8GSssgVuSzfc8995iT6sDubOWN7eJngt8jBhcMvhic8L0cccQRJpAtXiCHWSMGOQxceP+bb75pTtyfeOIJHCy+P57888cQPj8/E88//zx+//13871hMEj8TjGYve6668z2MXBmVp3fM3v+pJNOMt8p7lM+jkEbv5vl4WeE2SZ+rsrKiI8cORJvv/02vv32W/PDwfnnn2+W8ccbbreNgTGDt8C/Ocwe33fffWYfXn755di+fbvZ1wy+At9feX/fqgP3NT9v3FcMvLlN/FvHLrfcH/xc8L3w+PDY8zNXmfckIkHkExGpAtdcc42v+J+UY445xiwbN25cifUzMzNLLPvPf/7jS0hI8GVlZfmXjRo1yteyZUv//Jo1a8xz1qtXz7dr1y7/8q+++sos/+abb/zLRo8eXWKbOB8TE+NbuXKlf9k///xjlr/44ov+ZaeddprZlk2bNvmXrVixwhcVFVXiOUtT2vsbM2aMz+Vy+datW1fk/fH5HnrooSLr9u7d29e3b1///MSJE816Tz75pH9ZXl6e76ijjjLL33777XK3p06dOr6ePXv6Kor7nM87efLkIstvvPFGs3z69On+ZWlpab7WrVv7WrVq5cvPzzfLzjjjDF/Xrl3LfY2kpCTzuTlQ3333ndmG1157rcjyww8/3Ne0aVP/NlT0GJT2OeH757Ep/r7//PNP/7KUlBTzHricn8sD/WwPGzasyGe7+Gc88Jj26tXLl5yc7Nu5c2eRz63b7faNHDmyxHv597//XeQ5zzzzTPOd2R++5xo1apR5f05OjtmObt26+fbt2+df/u2335rXvf/++8387t27zfxTTz1V5nN9+eWXZp2//vrLdyDGjh1rHsfHl4V/G7jOWWedZeZTU1N9sbGxvltuuaXIevw+BX4e1q5d6/N4PL5HH320yHoLFiww3/3A5eX9fdufso69/bxstp9//tm8Dvc5979txIgRZttPOeWUIo8fOHBgkec+kPckIsGlrowiUq3YhYm/rhcXmGlg5mvHjh2mmxYzHEuXLt3v8/IXcHaPs/GxxIzZ/gwePNhko2w9evRAYmKi/7HMujBDwa6F7I5ma9eunfl1vCIC3x8rxPH9MdPE2JC/UBfH7FEgvp/A9zJp0iSTHbAzaMTxYsxGVASzNcwOHgj+6s5MXiBuR//+/XHkkUf6lzHTw7E9zHYsXrzYLOMv8MwGFe9eGojrMIO2efPmA9ouO8sS2J1xzZo1JlvADJ9dtONAj0F5+L6ZeeF7t3Eb7OxcVX62i9uyZQvmzZtnuiYGZgj5uWXXT25bRT5PzO7wc3Aw2PWQmS5mmwPHwbHrJrvI2l1auQ9iYmJMNzxmc0pjZ2mY1crNza3wNnCfUnmfZ/s++/3y+83vLrsDWr/PFGZZeVzZ3ZWYrWP3VGaWeNzs1qhRI7Rv3x4///xzhf6+VQdm/AJ7AzDDzvdSvOsnl7OLIrO7lXlPIhI8CsxEpFpxPARP0IpjFyd2e+P4F5408STXHgjPLl77Y59I2ewgrayTwPIeaz/efixPPNlVjIFYcaUtKw27a9kn0va4sWOOOabU98cT3OJdJAO3x+5yxW6Vxbu7sXthRXAf2ye0BxKYFcftKO017eqbvJ/uuOMOs60MZHjyd80115iuboHY/ZRVCTn2i+uxK1ZgMMrqkexCaDd2vyIGqAzMOYbMHtdkB2mBgdKBHIP94fvi+yiutH1xsJ/t0l67rNfifudJdvHy8Afz/ajstjAws+9nwMJukyy+we597DLH483jaOOxYHdHdhXmGDN2UWX3w9LGR5UWdJX3eS4teONnhgHLzJkzzfyqVavM+DAut61YscIEOzzWPG6BjV1s+behIn/fqkPxY8rPF/H7U3w5AzH7s3ag70lEgkdjzESkWpU2BocFK3hSxpNWjtti9orBCcfC8IS+IuXxmS0qTeCv4dXx2Ipgxo2ZDI6B4vvhCSvHJDGIYKBQ/P2VtT1VidvArEtOTk6FTyQPpgIjAwaOUWM2hEUYPv/8c7zyyitm3AtPxIm/4DOTw7FyLBHOcT48mecv/MxucLydva5dRt4uHMFA56WXXsJHH31kCqXwluPUevXqValjUFWq4rNdFar7M14RLEjBQhwstMMiMhzjxHFSHHPXu3dvMzaUYwWZ6fzmm2/MOsz+cHwll5U15s7+EYBjFpnVLg3vI34mbNwWFuhg1oyZU94yu8oCMTYeH24XA8rS9mHxbTqUVUrLOqb7O9YH+p5EJHgUmInIIcfuTexWxRNw/pIe2B0tFLBIAk+mOcC+uNKWFbdgwQIsX77cFMVg9yMbCx9UFoMSVhxkFinwRIrBT0XwpJSZAgZI7O53MNtR2mvaXfR4v42BELMRbAwIWZyFRQjuuusufzc4ZgHZLY6Nv9yz6AfXYWDGfRfYZTLwJJjdtRj0MFPGAIxZKj6uuo4B3xczD8UV3xcH8tmuSHVP+7VLey17vzPbxH19KARuCytUBuKywONPPEa33HKLadx/DJwZeLFCoI1dCdl4/Hg8mfVkOXkWqSgNPxPsBsl1WUSltGCD11W0qzHauI84z8IvrE7Kboz8YSCwuzK3lwENs8Ws+hgJIvE9iUQqdWUUkUPOPpEK/PWeJ+7MqITK9nEcGn/pDxz/xKCsItdFKu39cZqV6yqLF4bmmJFXX33Vv4xZIVZWqwiOOWIQxBNkBizFMShi+feKbAerG9rdwYjd6FihkJX27AwFg5NAzNLxPu4Hjifithfv1seAmCfJdlc2ViDkcbAbKxAG4gk8x4qNHj3aBDl2tcfqOAZ838zi8L3b2LXygw8+qPRnm4FCRbo28rgxoGGQGXh5BHYDZaaR23aosKw8j9O4ceOKdDnk94Ld4uzrx3E8HStxFg8Q2LXQfhy7VRbP4NkZz/K6MzLrxSwpA0EGZsVxnBsrE3J8JAO+QPyRgN9pVqn8559/inRjJP54wGPITG3xbeN88c91OIjE9yQSqZQxE5FDjt2IOOaFpd5ZUp0n1e+9994h7Wa1PxzvxJNeBgMsuMFAgl3neN0idgksD7vN8SSUJ4/sOsdubcxUHcz4Hma8uC0slc3ufAxymJWp6Jgl7m92GeRJPE9+2RWQ1zQidrNjV0Bez2l/+PpclxktHjuO32LAwIwQ36NdeIMFOlhcgNvMMUY8aef+44k7T84ZYPD6Viyn37NnT5MFZMEVFgthRqUi+B7YXZDXDePrBJaMr+pjcPvtt5vPKC+hwDLtdrl8ZojsbnMH+tnm/mfWhmX1WcKd+4DHuTTs5sl9zmN02WWX+cvlczwRP6tViYFzaUE6jzUzm+xuyoIX7LLJ7KtdLp/7n9eJIwb/J5xwgumuys8qxwXy88d17QuD83PDgJXj8XisOC6Ml1ngsdpfsMnPIYNybgt/JOBYNWZUWUqf2Th2d+TzF8fn5eePnwsGK3xcIG4H3zuzuvyesask1+fnm9vPIjd8bDiJxPckErGCXBVSRCK8XH5ZJdN///13U948Pj7e16RJE9/tt9/umzJlinkOlofeX7n80spwcznLhe+vXH5pJdqLl0enqVOnmrL1LK/ftm1b35tvvmnKbcfFxe13fyxevNg3ePBgX82aNX3169f3XXHFFf6y/IFl0MsqT17atrNU+r/+9S9fYmKiKdPO6b///rtC5fJtmzdv9t10002+Dh06mPfBEu4sy8+S2SwpHrg/WNK7NKtWrfKdc845vtq1a5vn6N+/vymXHoil7I8++mhTop1lyrn/brvtNv9rZGdnm3mW8K9Vq5bZB5x+5ZVXfAeiX79+5v2X9riKHoOKlMun+fPnm8803zPL8j/88MO+//73vyXK5Vf0s52enu678MILzX7kffbnvLRy+fTjjz/6jjjiCPO8/Azwkg58j4Hs97J9+/Yiy/lcxbezNPblG0prPIa2CRMmmO8Gj23dunV9F110kW/jxo3++3fs2GG+Z506dTLHlp/XAQMG+D755BP/OnPnzjUl31u0aGGeh2X4Tz31VN/s2bN9FcHLIvB9cZ9wf/C48O/Ngw8+aPZtWbitfD/8bJTl888/9x155JFm29n4Pvh+li1bVqG/b9VRLv/TTz8t9ZgWv9xAWZ+BirwnEQkuF/8X7OBQRCRc8NdmjmcqbbyRiIiISGVpjJmISBnYXSwQgzFeM+rYY4/VPhMREZEqpYyZiEg5RRdYWp1FKHh9JhbeYFECjm0p7ZpWIiIiIpWl4h8iImVgoQcWuuBFcXnBXBZeeOyxxxSUiYiISJVTxkxERERERCTINMZMREREREQkyBSYiYiIiIiIBJnGmJXC6/Vi8+bN5gKMvDioiIiIiIg4k8/nQ1paGpo0aQK3u/ryWgrMSsGgrHnz5tW200VEREREJLxs2LABzZo1q7bnV2BWCmbK7J2fmJiIYGfvtm/fjgYNGlRrhC7Bp2PtHDrWzqFj7Rw61s6hY10tOxXYsMGaNskRH7D7b2u+Tm/A7QnqeTgrM7ds2dIfI1QXBWalsLsvMigLhcAsKyvLbIcCs8imY+0cOtbOoWPtHDrWzqFjXQ3y84G1a63pLl24AFj/qzXf4kjAE4NgHuu4uDgzX91DnJSCERERERERCTIFZiIiIiIiIkGmwExERERERCTINMZMRCJWfn4+cnNzEWrYZ53bxX7rGjsa2XSsnaO8Yx0dHQ2PJzjFC0QkfCgwE5GIlJ6ejo0bN5prj4QabhNP4nhNFF0rMbLpWDtHecea8yyxXbNmzaBtn4iEPgVmIhKRmTIGZQkJCeZSE6EW/PAELi8vD1FRUSG3bVK1dKydo6xjzeUst82/Se3bt1fmTETKpMBMRCIOuxPxZIhBWXx8PEKNTtadQ8faOco71vxbtHbtWvO3SV0aRUrB7r/9+xdOwwW0urBg3jnhinPeqYg4jrJRIhIK9LdIZL9fEqBhw8AFQGIHx+02VWUUEREREREJMmXMREREREQkeLxeYNMma7ppU3YOBlIXWPNJ3QG3M6qaKmMmIhLBWrVqhbFjx1Z4/WnTppluV3v27IFTvfPOO6hdu3bIvs4DDzyAXr16Vcs2iYgEBSsoz5tnNU778oENE63GaYdQYCYiEgIYDJXXeDJeGX/99ReuvPLKCq8/aNAgbNmyBUlJSahOCgBD2/z583HUUUchLi4OzZs3x5NPPrnfx6xfvx7Dhg0z1VCTk5Nx2223mWIYxY97nz59EBsbi3bt2pngtLiXX37Z/KDA1x4wYABmzZpV5P5jjz22xPfj//7v/6rgXYuIBJe6MoqIhAAGQ7YJEybg/vvvx7Jly/zLAq9/xOpvvCQAq7/tD6vBHYiYmBg0atTogB4jkWXv3r046aSTMHjwYIwbNw4LFizAv//9b5PdKyvI5+eRQRk/OzNmzDCf55EjR5oLKz/22GNmnTVr1ph1GER98MEHmDp1Ki6//HI0btwYQ4YM8X/2b775ZvO6DMqY7eV9/C4w2LNdccUVeOihh/zzDAZFRMKdMmYiEvEYyGTm5AWlVfQC1zyhtRuzVcwC2PNLly5FrVq18L///Q99+/Y12YbffvsNq1atwhlnnIGGDRuawK1fv3748ccfy+3KyOd98803ceaZZ5qTWV5X6euvvy4zk2V3t5syZQo6d+5sXufkk08uEkgyK3L99deb9erVq4c77rgDo0aNwvDhwyt9zHbv3m1O7OvUqWO285RTTsGKFSv8969btw6nnXaaub9GjRro2rUrJk2a5H/sRRdd5L9cAt/j22+/Xeltqeh+fuSRR8w2c52WLVua/crrV5111lnm+PXo0QOzZ88u8fwTJ04028gMEYOQDRs2FLn/8ccfN6/N57jsssuQlZVVIit64oknon79+uazc8wxx2Du3LmVfr8MmnJycvDWW2+Z/XrBBReY4/vss8+W+Zjvv/8eixcvxvvvv2+6WfJ4Pfzwwyb7xeciBlutW7fGM888Yz5L1157Lc455xw899xz/ufhazDouvTSS9GlSxfzGB5/bksgLgv8ziQmJlb6/YqIhAplzEQk4u3LzUeX+6cE5bUXPzQECTFV86f2zjvvxNNPP402bdqYgIQn8EOHDsWjjz5qgrXx48ebYIXZhRYtWpT5PA8++KDpmvbUU0/hxRdfNEEMA526deuWun5mZqZ53ffeew9utxsXX3wxbr31VnMCT0888YSZZvDDE+7nn3/eBBvHHXdcpd/rJZdcYgIxBjc86Wawx/fKk39mYa655hpzwv/rr7+awIzL7azifffdZ+YZyDJYWblyJfbt21fpbUlPT6/QfmaAwewQX5/T//rXv0zXUAZr3H88fpxetGiRv3w69y2fl8/JbOXVV19tAqHff//d3P/JJ5+YbqwMcI488khzDF544QXzGbClpaWZQJjHkj8EMPDh9nL/MZgjBkrTp08v8z0ykOR20cyZM3H00Ueb7bExYORxZtDLz15xfEz37t1NABn4mKuuuso8b+/evc06zMIF4jo33nijmebxnDNnDu666y7//fy88TF8bCB+3hgEMijjseA+V9ZMRMKdAjMRkTDBrlvMjNgYSPXs2dM/zwzFl19+aYIZZiPKC3pGjBhhphlI8ESf43iYCSsNL4rLzEXbtm3NPJ87sBsZAwKeTDMLRy+99JI/e1UZdkDG4ISBjX0izrFODPjOPfdcM57p7LPPNsEABQYqvI+BwGGHHebPZh0M7uOK7GcGQ//5z3/MNLuivvrqq2YbmBVit1MGlwMHDsS2bdv83UW5b7m/2G2P3n33XRPc8nj079/fZDuZJWMjZuWYrQvMmh1//PFFtvf111832ctffvkFp556qlnGLGl5wSmDXdvWrVtNZiuQHXDxvtICMy4PDMqKP6a8ddh1ktvGoI9dIktbh1lj24UXXmgCySZNmpixcNyvDJK/+OKLMt+fiEg4UGAW6tZMR+y2dUD8YCCpSbC3RiQsxUd7TOYqWK9dVexAIzCTw2zKd999Z7oWskshT3AZmJSHXepszDYxI5WSklLm+sxE2EEZcUyQvX5qaqoJNBhE2Dwej+ly6WX540pYsmSJCWTsYIXYRbJjx47mPmLXOmZj2IWOGRUGafb74nLOszsfx0qxS6Ud4FVGRfdz4H61gws7cAxcxn1nB2Z8n+waaevUqZMJqvg+uU95W7ywBYO7n3/+2T/P/X/vvfeabqh8bgY3zMQFbl9TU346MgSOc+P+5efxhBNOMF1OAz+nIiLhRoFZiHNNfQB1Ns+Ft+5HCsxEKvs9crmqrDthMDGICsTuhD/88IPpJscKdxxPxeyMPaanItkRe/+UF0SVtn5Fx85VFxaNYDc4BksMzsaMGWO68F133XWm2x67ZjJrx/3Dk3Z2feR+qoyK7ufA/WR3VSxtWWUD1rKwG+POnTtNF1JmktjdksFb4PYdSFdGBo0M9gLZ82UVhuHy4tUTiz+mrOflDwPcpwzo2Upbp7yCNHYAzy6rCsxEwpTbDfTtWzgNF9Di3IL58P/3u6JU/CPEbd5jdT1ZsyMj2JsiIiGGXf3YLZFdCJk54Mnr2rVrD+k2sNgEM0EsQGFjxuZgik+wKx+zUn/++ad/GQMPdldjQQgbuzYym8QubLfccgveeOMN/30s/MGAheOQ2B2Q3ftCcT/zfQYWBOF7ZOEV7gPibeB+oD/++KPE9jGDyK6ULNbBwGzHjh1F1mFXxnnz5pXZArueMqjj2D12s7QxMGXGsrRujPZjWL0xMPPKxzDoso8Z12ElxkBch8uJY9qYaQ1ch0Es5+11SsPtJ2bORCRM8YerJk2sxmmXG6jd1WqcdoigvlP+4eegXfYT5y+JHDtQHv7DWNr1ffgPkY3dTYrfz64h4Sojx7qo3t6swn8gRUSIlfwYlPDE9J9//jFjb6o6G1MRzFIxY/XVV1+ZwOKGG24w44XsDFF5eDIfGCDwffB9sQoiq/Ox+iSXseAIu+NxObFgBCtFsgQ7g0B27bODGY7v4rYwg8Is0Lfffuu/L9T2MzNq3H8Mvlj4gv/OHX744f6uodyXrEjIwirLly/H6NGj/ZmtwO1jURB2e+TzsJgLM1CBuO+Y7SurMWNm4/tjkMRxbXwtlrBnNo5l7G0cYxf4byu7jDIAY8ET7iMeG3avZKaSgSIxiF69ejVuv/12M2bslVdeMcVNbrrpJv/z8DUYYHOsHd8Pu6VmZGSYKo3E7ooc48d9xeCY4/xYUIXFSgK7koqIhKOg5gb5x5YDqnl9FJYT3h/+w8CywYG/NPLxHAgeiIFaYCnjilzrJ1QFt7OQiIQylhbn30+On2L1QRZBYCGFQ42vy8IOPEFmVzSOAWI3Q07vD0+oA/Ex/NvOQIRBCYtXsEse12NWx+4ayKwcT/o3btxosjIsXGKXXWdQwWIkPHFngMILJX/88cdFLlDMgiClXdz4UO9njt/j8zEY2rRpk9nW//73v/77zz//fBOMMJhhwQ+OnWOwwsDHxvW5z3nhZmYRWdCF3S8PJgvK7qHcv8xg8T0z2A0c28WxhYHX2eNxYwDMbWN2i91umbEMLBLDgiLsespAjP+eN2vWzGTy7GuY2e+Xlxjg6/EzxdL7kydP9o/P47Hlv+/MgvIcgu+X+4RBoIiEMXaP31JwGRaT/fYBqdaYYiR1dkzWzOUL9kCBAvxllb/AHch1b5hhY0DHX0ztX/uYMeNyu2tDZfAfXP7DxH94gn1tlGWP9EPHvOX4+4hX0PvEi4K6LVK9+As8uwHxIqosES2VxxNY/l3giSCvDRVq+GeXwQd/NKpIVikcP8vMUJ133nkmuxFq+O8FLxnA7FR1i/RjLRU71qH+N0kOjP69rgb5+YDdpXroUC4AFlkXp0fXuwFP4eU7gnGs+b1lV+7qjg3CN5VU8Cshq3EFdsGwSy2zeyR3In+5Yxeb8q7pk52dbZrN/iWUByMY3YICuQL+4Ad7W6R68fjqOFftvrRbKLK3K1S370Cw0AYzLLywMf+Wsvw7T0JZkj/U3h+75vGHN3a5O1TbFknHWip3rO2/RaFwXiEHT/9eVwN+L7wF3w1z62X2yMz6OO/yOuJYh21gtnnzZnPx0A8//LBEdSZ2T+EgZZY15q+i7BqycOFC/4U2i2PgxvWKY3eKwGvFBIO34G87u2yUV85awh+/9Pwlhn8AlDE7OCxawP3JX6/ZQg2PMbviUSRkUbiv+Xf3tttuM++N3cnZ/Yxjn0Jt//PfBo5POlQnyJF2rKVyx5rfA37eWMSmeJVTCT/697oa5OcjNjXVTGbzfNeVj5oZVuG79O0pgDsmqOdmh+q8LGwDMw4M5rVeind9ZElgGwcCM1BjRo0DjO0LdBbHsQiBg5qZMWO/dVb1CnZXxtSCP+41EhJMFzeJXPzy8x9zfu4UmB0c/qCSlpZmuhSF8hjTSDlBY/csVgaUyD/WUrljzb9D/LvO6/GpK2P407/X1YA/aiQlWdPmfDcfru3WJWISGiQHtSsjz83sIkbVLXTPWPbzqxSrVLErCgcCl4fBW4cOHUx1rrJwZ5e2w/lHNNgnyL6Czoz8UAR7W6T62cdZx/rgcP8FVmYNxb9h9naF4vZJ1dGxdo7yjrX9t0h/3yOHjmcVY7dFd8F5rrn1WWXzua85H8Rz4EN5Dh6WZ/q//PKLCbTKyoAFSk9PNxWtwv36JhqZICIiIiISuYIamDFosq9dQxwszun169f7uxiy/HJpRT/YRbFbt24l7mOJYAZuLJM8Y8YMc0FQlvHlIPRwZP/mZg+AFBERERGRyBPUroyzZ8/Gcccd55+3x3nx2iccSM7iHXaQZuMAvM8//9xcA6U0vKYNgzAOsOVYnSOPPBJ//PGHmQ5HdldGEREREZGIxG6LvXoVTsMDNC+oI+Ha/zUxI0VQAzNe5LO88sGlXfyTZY4zMzPLfEzgRUQjifJlIiIiIhKROIarefOiy+oUBGoOEpZjzJxJoZmIiIiISKQKy6qMjqKejCIiIiISydiDLiUloFy+D0grqKheqx1LM8IJnPEuI4GKf4g4wtatW3HdddehTZs25jIevKbiaaedhqlTpyInJwf169fH448/XupjH374YTRs2NBcYLuskr8TJ06s5ncgIiJygLxeYNYsq3Hamwes/dBqnHYIBWZhU/xDXRlFIh2ryfbt2xc//fQTnnrqKSxYsACTJ082RZKuueYac93Giy++GG+//XaJx3K8LsflspKtLmYsIiISfhSYhQklzEQO8guUkxGcdgBf3quvvtpktWbNmoWzzz4bHTp0QNeuXU3FWlaXJV6/cfny5fjtt9+KPJaXCVm9enWFru9YGq/Xi4ceegjNmjUzmbpevXqZoNDGbN21115rrgkZFxeHli1bYsyYMQW714cHHngALVq0MI9t0qQJrr/++kpth4iIiFNpjFmY0FAzkYOQmwk81iQ4u/DuzUBMjf2utmvXLhMIPfroo6hRo+T6tWvXNrfdu3dHv3798NZbb5nLgdiYRRs0aBA6depUqc3kJUieeeYZvPbaa+jdu7d5/tNPPx2LFi1C+/bt8cILL+Drr7/GJ598YgKwDRs2mEa8hMlzzz1nquIykGR3zH/++adS2yEiIuJUCsxCnK5jJuIMK1euNJmnigRWzIrdeuutJliqWbMm0tLS8Nlnn5n5ynr66adxxx134IILLjDzTzzxBH7++WeMHTsWL7/8srmmJAM0BoPM6jFjZuN9jRo1wuDBg003SgZu/fv3r/S2iIiIOJECszChEWYiByE6wcpcBeu1K6C8azoWN2LECNx0000me/Xvf/8bEyZMgNvtxvnnn1+pTdy7dy82b96MI444oshyztuZr0suuQQnnngiOnbsiJNPPhmnnnoqTjrpJHPfueeeawI4FizhfUOHDjUFS6Ki9E+MiIhIRWmMWdh0YVRoJlL5L5LL6k4YjMbXrgBmo5iJWrp06X7XTUxMxDnnnOMvAsLb8847z2TPqkufPn2wZs0aU/lx37595vW4DcTKkcuWLcMrr7yC+Ph4M1bu6KOPLrM6pIiIiJSkwCzEqSujiDPUrVsXQ4YMMd0GMzIySty/Z8+eEt0ZWQDk22+/xYwZMypd9MMO9Fiw4/fffy+ynPNdunQpsh6zcm+88YbJ0nFsGcfGEQMyZsnYnXLatGmYOXOmqSopIiKyX/wRs3t3q3Ha5QGaDLUapx1C/UzChcoyikQ8BmXsPsjxWayQ2KNHD+Tl5eGHH37Aq6++iiVLlvjXZUaqXbt2pjw+x6Wx8EdFMOs1b968Etm62267DaNHj0bbtm1NRUZm4bjeBx98YNZ59tlnTUVGFgZht8lPP/3UjCtjURKW6c/Pz8eAAQOQkJCA999/3wRqgePQREREyuR2A61aFV1W33ljlRWYhQ11ZRSJdByjNXfuXFOZ8ZZbbsGWLVvQoEEDc20zBmaB2O2R48vuvvtu3HXXXRV+DZbeL2769OmmvH1qaqp53ZSUFJMpYxVGBm1Uq1YtPPnkk1ixYgU8Ho+pDDlp0iQTpDE440Wv+dwM0Fg58ptvvkG9evWqYK+IiIg4g8t3ICPOHYID4ZOSksxJCrvuBNPiR49Al9yFmNP/WfQdWvmuShL6eB0pnhAnJyebk12pvKysLJMZat26tbnmVqjhn11mwlgcgwGWRC4da+co71iH+t8kOTD697oaMBzZZXWNR926VkIiY701X6MF4HIH9Vjze1unTp1qjw109iciIiIiIsHj9QIzZliN0948YPU7VuO0QygwC5fiH0psioiIiIhELAVmYUL9TUVEREREIpcCs1BXkDBzKTITEREREYlYCsxCngoDiIiIiIhEOgVmYUIJMxERERGRyKXALGwoNBMRERERiVS6wHS4VGUUkSB8AX3Azp1AejpQsybACybrumMiIiJVi/+2dulSOA0P0PjEgnmPY/a2MmbhQuXyRQ6dPXuA558H2rcHGjQAWre2bjnP5bxfREREqobbDbRtazVOuz1AgyOsxmmHUGAW8uyMmboyihwSU6YAzZoBN90ErF5d9D7Ocznv53oiYWTZsmVo1KgR0tLSEEpatWqFsWPHIlxNnjwZvXr1gpcXxRUROQgKzEREbAy2hg0D9u2zstTFM9X2Mt7P9ao4OLvkkkvgcrn8rV69ejj55JMxf/78KnuNBx54wJxEVsTevXtxzz33oFOnToiLizMn9YMHD8YXX3wBn8+H7t274//+7/9Kfex7772H2NhY7NixIyxPxl9++WWzjXzfAwYMwKxZs8pdf9GiRTj77LPNY3jsSntvv/76K0477TQ0adLErDNx4sQS63C/3n///WjcuDHi4+PN/l6xYkWJ9b777juzXVynTp06GD58+H7f01133YXrrrsOtWrVQrjj5zjwu8LGz2lxM2fOxPHHH48aNWogMTERRx99NPbx+1tg7ty5OPHEE1G7dm3zfbvyyiuRzq7LAaZOnYpBgwaZ/cbvwB133IG8vDz//fyORkdH44MPPqjmdy0Swfhv6549VjP/1nqBzE1W47RDKDALE8qXiVQz/mNw9tnWPwj7++Wb93M9rl/F3Rp5krdlyxbTeEIYFRWFU089FYfanj17zMno+PHjzQk9T2AZWJx//vm4/fbbkZqaissuuwwff/xxkRNd29tvv43TTz8d9evXR7iZMGECbr75ZowePdq87549e2LIkCFISUkp8zGZmZlo06YNHn/8cXPyXpqMjAzzXAz6yvLkk0/ihRdewLhx4/Dnn3+agIKvnZWV5V/n888/x7/+9S9ceuml+Oeff/D777/jwgsvLPc9rV+/Ht9++60J/iNF165d/d8Vtt9++61EUMbv00knnWQC67/++gvXXnst3OwmBWDz5s0m8G3Xrp3Z18x8McAO3Efcv0OHDjXP8/fff5vPxtdff40777yzyGvxMS+++OIheuciEYj/rk6fbjVOe/OAlW9YjdNO4ZMSUlNTGQeZ22Cb/+jRPt/oRN9fX78a7E2Rapafn+/bsmWLuZWDs2/fPt/ixYvNbYWNHevzuVx2Tqxijes///wBb5/X6/Xl5OSY20CjRo3ynXHGGUWWTZ8+3fw9SklJ8S9bv36979xzz/UlJSX56tSp4zv99NN9a9as8d//888/+/r16+dLSEgw6wwaNMi3du1a39tvv22eK7BxWWmuuuoqX40aNXybNm0qcV9aWpovNzfXt337dl9MTIzvvffeK3L/6tWrfS6Xy/e///2vzH3QsmVL33PPPVfm/a+88oqvTZs2vujoaF+HDh1848ePL7L/Ro8e7WvevLl5/caNG/uuu+46//0vv/yyr127dr7Y2FhfcnKy7+yzz/YdiP79+/uuueYa/zy/k02aNPGNGTOmQo8PfG9lHWvu+y+//LLIMq7TqFEj31NPPeVftmfPHvM+PvroIzPP/d60aVPfm2++eUDvic952GGHlVjOz9eRRx7pi4uL8zVr1szsx/T09CLv5aGHHvJdcMEF5vPE/fDSSy8VeY5169aZzyA/L7Vq1TKfza1btxZZ5+uvvzavz/dSr1493/Dhw4u8xqOPPuq79NJLfTVr1jTH9bXXXiv3/fD49+zZs9x1BgwY4Lv33nvLvJ+vwc9H4N/c+fPnm2OzYsUKM3/XXXeV2G98L9xfe/fuLbIP+LglS5aUONaV/pskIUv/XleDvDx+uazG6bxsn++f0VbjdJCP9e7duw9JbKCMmYgIz5Mr+2v3Cy9UW3Eedql6//33zS/67GZFubm5JoPCblXTp0832ZKaNWuaX/RzcnJMFyt2azvmmGNMF0hmDdg9i129mO265ZZbimQauKw4jpVhJuyiiy4y3e6K4+sxk8ds2BlnnIG33nqryP3vvPMOmjVrZjIVlfHll1/ihhtuMNu6cOFC/Oc//zHZoZ9//tmfMXruuefw2muvmW5+7BLIbpU0e/ZsXH/99XjooYfMmCpmQdh9LXDbuC/Kwn04Z84ck0mxMcPCee7L6rRmzRps3bq1yGsnJSWZLov2azODt2nTJrNNvXv3Nl0eTznlFLOfysPPymGHHVZk2apVq8znhl0w+VlhNohZJ2aVAj311FMm08eMETNFPDY//PCD/7PCz8CuXbvwyy+/mOWrV68u8rlit8szzzzTZJ74HMwE9+/fv8hrPPPMM2b7eP/VV1+Nq666yhy/8vDY8/PJTCU/q8wK2pjdZBYsOTnZZH4bNmxovhOBWbXs7GzExMT4M2jErqFkr8d12J01ENdhBpOfE1uLFi3Ma/D7KCJSadUa9oWpUMqYLXjMypjN+koZs0inX+CqzgH/Or19+4Flyoq3HTuqLGPm8XhM5oGNf4eYDZozZ45/HWanOnbsWOSx2dnZvvj4eN+UKVN8O3fuNI+bNm1apTMN27ZtM8/x7LPP7ve9TJ482WTHmCWz3xszIOVlKvaXMWOG74orriiyjFmYoUOHmulnnnnGZNG4D4v7/PPPfYmJiUWyGYG++OILs//Kwgwh3/uMGTOKLL/ttttMJq06M2a///67Wb558+YS7/28884z08yccZ0WLVr4PvvsM9/s2bN9I0aMMFkoHvuy8Jgz8xXosssu81155ZUlMmhut9v/3eF7Ofnkk4usc/755/tOOeUUM/3999+bzyyzuLZFixaZbZw1a5aZHzhwoO+iiy4qd39dfPHF/nnuK2ayXn217H/3Jk2a5Pvkk098//zzj/kM8jW4T+zjPnPmTLMNdevW9b311lu+uXPn+m688UaTYV2+fLlZZ+HChb6oqCjfk08+ab5Du3btMtlVPu6xxx4z6/A7xf3x4Ycf+vLy8nwbN270HXXUUWYdLgvUu3dv33333aeMmQPo3+tqoIyZoYxZiNN1zEQOgWKD/Q9YFVa5O+644zBv3jzTOC6G2TFmRNatW+cf87Jy5UqTMWPmiq1u3brmF3xmQDjN8S58HAtNPP/88yYzdiCsuKFiWDiB2TGOKSNmQ5i5YIarspYsWYIjjjiiyDLOczmde+65ZlwbMyVXXHGFybDZxRi4PS1btjT3cRwWCzJw/JeNmZulS5ciXNmV/1iUhZmuvn37mn3PLOCnn35a5uO4v4pnfvhZYgbR/hyx8XPD12D2zjZw4MAij+O8fSx427x5c9NsXbp0McU07HX4WT7hhBPKfV89evTwT/O9cJxeeWP6+J3g54CP4zZPmjTJjIv85JNPiuwnO9vK7CKzrB07dvRneJk5fvfdd022LiEhwbxm69atTebLzqIx68uMIYvcsJhNhw4dTOaPAjNtdiYt8LMmInKgFJiFDZX/EKk2vHj0wajCKncs9sCui2z9+vXDm2++aYpGvPHGG/7ujTwZt4M3uy1fvtxfAIIn6uz6xi5c7J7Gk8k//vijwtvQoEEDc2JdkQCGJ6cMBHmCy5NhvjaDSwZG1YVBALu5vfLKK+ZkmF3f2F2R3TwZsLK730cffWS6+bHCIbvh8aS9Itg90+PxYNu2bUWWc76soh5VxX7+8l6b78kOfmwMGLi/A7vylfa+du/eXWQZP0sMXAI/RwzW2EWwLa8lVEXs7oHlYVXDQAzODqT8PD+v/JzzR4uy9hN17ty5yH7id4bdR9k9dOfOnaba4/bt24t8flkIhp8fPo5VRtl1k4p/xtmdk98dEZHKUmAWLnSBaZHqw/FbPBEtZ+xRqbg+H1e3bnVtmTlBZfBjVz7s06ePOXHm2Bk7gLMbxyPZmCFgNcUZM2agW7du+PDDD81yjqnJz88v9zX5ehdccIHJNrFyXXE8oQ8sF86MxIYNG0wZfWavWK3xYPDkufhYHc4HnmTzZJ8ZQVYwnDZtmglEFyxYYO7j+DeO02KFQ46dWrt2LX766acKvTb3DwNfZv5sDBA4XzxzVNWYrWEAFvjavGQBx0rZr81tYyAWOP6KASnfIzOFZeHnYfHixUWW8bPEZcU/R2zcD7biQT3neYyItzz2bDY+JwMZ+3gxqxX4nqoDP5PMGNsBGS9bwPFnxcep8QeM0vYTs2TMGPKHDGYWmXkt/j3k8/Fzx6CfPw5w/9nsjHVFL0UhIlKaqFKXSgg5wBNFEanE18wFXHeddfHoA3X99Qce0JWDxQb4Cz4xw/HSSy+Zk04GIcQiB+xaxV/tWeCC3QjZzZFBEcvY8yT99ddfN6Xq7RNTBnIjR470n7CymxqzI3wsM0w80S/u0UcfNQEPC09wmoUZmNVgEYkxY8aY0uPMUtgBBa8VxSIjfK6zzjqrQu+VWQpuRyCeNN92220477zzTDDBAOubb74x7+/HH38067D7HYNLbhu7oLFACk+Y+ViWhGfxCWbQeH0vdnFjYMUubMTAkQFredlAZkhGjRpl3jOLVPCaZMxaBnbP5P5s2rSp2Rd20RA78OG0/d6YAeU+Jx5HO6ND9nFg91MWj+DJ/4033ohHHnkE7du3N/v1vvvuM8fRvk4Zr8XFbnUs5c/ggO+Znwdi176ysLvf5ZdfbvYbM4LE63EdfvjhptgH7+O28j2wgAc/d4FBMYNcbgPvY5dJFvQgHh8WXuHnkvuJATszmCy0YRcb4bayKyOzcAz4uQ6PC1+/sm699VbzneD7548HfA2+rxEjRpj7uS/5OeJyZkwZMDGry+P+2Wef+Z+H75OZZQZlfG98DC95YH+2ifuXRVL4gwU/h7yfXSbt/WgHq/zsc3+KSCXw39EOHQqn4QEaHlswX/hdi3jWUDMJ1eIf8x871ir+MfHlYG+KVDMNJq46lSpNvXu3z1ejhs/ndles4AfX4/p83AEqr/hHYCl7lh5n2XsWeQjE0r0jR4701a9f35QfZ1l5Fsvg3yyWKWcpchYNYaEDFla4//77/SXBs7KyTIGD2rVrl1su3y7Vfuedd/rat29vnqthw4a+wYMHm6IVxbedhRD4fFdffXWF9gG3q3jpfja79H555fL5+iyFziIfLJJy+OGH+3788Ud/8YpjjjnGXEaABVF69OjhmzBhgv+x9iUD9ufFF180xST4vln0448//ihyP1+Dx8vGyxWU9n64nn2seRmD0tYJfB6uxwIS3Nc8tieccIJv2bJlRV6bz3fLLbeYAhn8jPCYsJBFeVhmn6XuWSgjEAt0nHjiiaZMPfcl9xdL1wcepwcffNAUIGG5fJbzf77YJSIqUi6fRVl69epl9ic/t2eddVa5hWBYrISFasrCAiT2Z5yXD+D8ypUrS6zHSxzwMgDcdhYI4ecj0L/+9S9TIITPw/ce+DmzHXfcceayEyyRz88dC48UxyIqbKV9r0nl8iOL/r12jvxDXC7fxf8FOzgMNew6wi5BvIAqf50MpvljjkeP7Dn4q9dj6Df8mqBui1Qv/qrPwe7solZ8ULkcGHYrYjaCGYfiBQ/KNWUKMGzY/i8yzePDX/QmTWJ1gAM+PPyzy6wBu9yVV7pdwl8oHWte2JoXR57Cz3kFMdvHLB6blI7jzpiRZRaZWczSjnWl/yZJSNK/18471nFxcaYXRnXHBjr7ExGxDRnCiy5xAJMVeBU/kbaX8f5KBmUiwcJCH+zimVaFVUQFZnwfC9Ew6BKRSuIPomlpVrP7pmSlWM1BOSQFZqFOP6aLHPrgbONGYOxYll0reh/nuXzTJgVlEnaYyWGZfY4rlKrDsXSlXahdRA4Ae6lMm2Y1TntzgeWvWI3TDqHiH2ETmTnn1wKRoOPAfxb1YEGQXbusX/B4Msvqi+p6KA7LBomIyKGhwExEpCwMwlhKn01ERESkGqkrY5hQvkxEREREJHIpMAuTgMylyExEREREJGIpMAsTPuXMREREREQilgKzkKfiHyIiIiIikU7FP0REREREJLjFttq2LZyGB2gwqGDe45gjo4xZuNAYMxFxYKl2l8uFefPmBXtTRESkOrndQJcuVuO02wM0PslqnHYIBWYhzqeujCKOcckll5hAxG716tXDySefjPnz51fZazzwwAPo1atXlT2fiIiIVA0FZiIiIYSB2JYtW0ybOnUqoqKicOqppwZ7s0RERKqPzwdkZlqN02w5e6zGaYdQYBbiTDdbEaka+fllN6+34uuyVWTdSoiNjUWjRo1MY2brzjvvxIYNG7B9+3b/Opw/77zzULt2bdStWxdnnHGG6fZnmzZtGvr3748aNWqYdY444gisW7cO77zzDh588EH8888//qwcl5XlzTffROfOnREXF4dOnTrhlVdeKdHN8OOPP8agQYPMOt26dcMvv/xS5Dk4z23h+2rcuLF5P3l5ef77vV4vnnzySbRr186s06JFCzz66KNFnmP16tU47rjjkJCQgJ49e2LmzJmV2rciIhKi+G/w1KlW47Q3F1g61mqcdggV/wibrowictAmTSr7vuRkYMCAwvkpU8oOrurVAwYVDEqmH38EcnJKrnfaaQeztUhPT8f7779vghZ2a6Tc3FwMGTIEAwcOxPTp001G7ZFHHvF3eXS73Rg+fDiuuOIKfPTRR8jJycGsWbNMEHX++edj4cKFmDx5Mn7kNgNISkoq9bU/+OAD3H///XjppZfQu3dv/P333+Y5GeyNGjXKv95tt92GsWPHokuXLnj22Wdx2mmnYc2aNWZ7N23ahKFDh5oumuPHj8fSpUvNczCIY5dKuuuuu/DGG2/gueeew5FHHmkyhVwv0D333IOnn34a7du3N9MjRozAypUrzXsXERGJFPpXTUQkhHz77beoWbOmmc7IyDBZJi5jwEUTJkwwWSZmsxhs0dtvv20yY8yUHXbYYUhNTTXdH9sWVLhi1svG52ZAw4xceUaPHo1nnnkGZ511lplv3bo1Fi9ejNdee61IYHbttdfi7LPPNtOvvvqqCfr++9//4vbbbzcZtubNm5vgjtvKrNvmzZtxxx13mKCP7+/5558399vPyW1mgBbo1ltvxbBhw8w0M35du3Y1gRmfT0REJFIoMAsTPgf1rxWpNkOHVrzf8JAhFX/ewYNRVdhljwEO7d692wQ3p5xyisl6tWzZ0nRDZFBSq1atIo/LysrCqlWrcNJJJ5kMFbNqJ554IgYPHmy6PTLAqygGTHyuyy67zGS4bOyCWDzDxsydjQEfA8MlS5aYed7yfjuAJHarZCZw48aN2Lp1K7Kzs3HCCSeUuz09evTwT9vvIyUlRYGZiIhElKCOMfv1119Nt5cmTZqYf7gnTpxY7vr8NTiwYpnd+I97oJdffhmtWrUy3WUGDBhgTmjCl3VC41K9fJGD5/GU3QoyUhVal60i61YCuwqy6yJbv379TGaMgRK7+xGDmr59+5oS8oFt+fLluPDCC/0ZNI7D4tgvZtg6dOiAP/74o8LbwNcgvmbga7Ab5IE8z/7Ex8dXaL3o6Gj/tB3kMWsoIiISSYIamPFkgwO5GUgdiGXLlvmrlrElc2xIAZ6E3HzzzaYbzty5c83z85dj/roazpQvE3EmBiLsxrhv3z4z36dPH6xYscL83bMDOLsFZrM4Lozjt2bMmGGKcnz44YdmeUxMDPL3U5ikYcOG5gczFt0o/hrs0hgoMFBjRm3OnDn+rpO8ZYAYmPH//fffTbavWbNmZswYgzNWnxQREXG6oHZlZPcctgPFExKOpygNB5+z682ll15q5seNG4fvvvsOb731lqkGFm4KT2cUmok4Abv22b0A2JWR46+YwWLvArrooovw1FNPmUqMDz30kAlwWHHxiy++MOO6WBzk9ddfx+mnn26CK/6QxUBu5MiR5vHsTcDiHMyA8bEMklgNsTiO5br++utNsMfCItyu2bNnm23ij182/rDGAItBGAt48P5///vf5r6rr77aFAa57rrrzFg0bgt/NOPjGWyyVwPHm3G7GTCymyOrTy5atMh0oxQREXGSsBxjxhLSPEngr8Cs7MV/zInVx/hrLX8ltvEff46xKK+8Mp+LzbZ3715/V5mQ6S7DyqGhsi1SLXh8mVnQca66fWm3UGRvV/HtY/EMexwVgyYWuPjkk09wzDHHmHWZYWIJev7QxMIcaWlpaNq0KY4//nizPjNrrGr47rvvYufOnea5GCBdeeWV5vF8DIM4jmXbs2eP+dGKY9KKY2DE12I1RFZeZBfL7t2744YbbiiyX8eMGYPHH3/cBHrMqH311VemIiPvZ2DIH8YYeLH3Akv7M2hjZUX78ffeey88Ho8pBsLCINze//znP0Veo/h08WWhrqxjLZGnrGNtf15D6rxCKk3/XlcDfmdatCicprp9C+YLyuk74Fi7fCHyLwW763z55ZemzHNZ+GurXXWMgRTHXrz33nv4888/Tfce/qPOExR23QkckM6TAp7IcL3SMLjjr8PFccxG8QH2h9rmt/+FPtmzMK3t7eh0on5BjmT80rOaHjMUdgU+qRxmjbgvWSyDWZlQwz+77E7IgCSwMEY44XXMOHaNY3j5Y5lE7rGWgz/WLM7DzDb/vgeOmZTwpH+vnXes3W63+aGU04mJidX2emGVMevYsaNpNg5sZ+Uwdp9hgFZZzLAFds1hxowlnhs0aFCtO78itritAgI8uQwcSyeR+eXnP+b83CkwOzg8CWImiVUCQ/laV+F8gmbv11Dfx6EinI+1HPyx5neEf9eZTQ7FH4vkwOjfa+cd69hSuvxXh7D/17R///747bffzHT9+vXNL1Xbtm0rsg7ny7tmD3d2aTucf0RD5QTZ7bK2R5xR6EHH+uBw/wVWbg3FX9bt7QrF7auIwO0P1/dwKETCsZaDP9b290R/3yOHjmc1yMmxbmNirO6M+ZnWvCeh5CVtgnCsD4WwP9PnuAZ7PAYHj7OMdGCFL0a6nA/s2hhWCj6HIdHfVESkAIuI8ERU3RhFROSgsVrwlClW47Q3F1j8lNU47RBBzZix0hgvlGqzK4VxgHiLFi1MF8NNmzZh/Pjx5n5W92Kp5q5du5quShxj9tNPP+H777/3Pwe7JI4aNcqMQ2M2jY9hWX67SmP4UWQmIiIiIhLpghqYsfQyK4PZ7HFeDKzeeecdc42y9evX++9n1cVbbrnFBGsJCQno0aMHfvzxxyLPcf7555tyy6zwxZLT/DWXVc54XR4REREREZFQFNTA7Nhjjy23fDCDs0Csrsi2P7xeDltksPvUqjOjiIiIiEikCvsxZo4RGlc1EBERERGRaqDATEREREREJMgUmIU4n7oyioiIiIhEPAVmIiJS7eOJb7zxRsfs5QceeECXERARORC8Tlnz5lbjtMsN1OllNU47hHPeaZjTCDORyMeKsldddZW5XAgvet+oUSMMGTIEv//+e5ELXU6cOBFOvG4aL38SqdauXVvkouh2++OPP4oEuKWtM2zYMHN/bm4u7rjjDnTv3h01atRAkyZNMHLkSGzevLnIa82dOxcnnngiateujXr16uHKK680l68REQkaXsC5Vy+rcdodBTQfbjVOO4QCszCpyuhSZCYS8c4++2z8/fffePfdd7F8+XJ8/fXX5mR8586dwd40OUR4CRheKsZuffv29d/3xRdfFLlv4cKF8Hg8OPfcc839mZmZJui67777zC3XX7ZsGU4//XT/czBIGzx4MNq1a4c///zTXE5m0aJFuOSSS3SMRUSCTIGZiDhHfk7ZzZt3AOvmVmzdA7Bnzx5Mnz4dTzzxhLk2Y8uWLdG/f3/cdddd/hNrZo3ozDPPNJkSe55effVVtG3bFjExMejYsSPee++9Is/P9bnOKaecgvj4eLRp0wafffZZiYzNxx9/jEGDBiEuLg7dunXDL7/8UuR5GAzwOWrWrGmuD/mvf/0LO3bs8N+fkZFhsjS8v3HjxnjmmWf2+95XrVqFM844wzwfH9evXz8ToNgYnK5btw433XSTP0tU3n68/PLL0aBBAyQmJuL444/HP//8U6Kb4WuvvYbmzZuba2Ked955SE1N9a/j9Xrx0EMPoVmzZiZzaV8PM9DGjRsxYsQI1K1b12SnDjvsMBPoBOIxaN26NerXr2/WTUtL2+++YAaLmVK7RUdH++/jawXe98MPP5jttwOzpKQks4zvh5+Bww8/HC+99BLmzJnjvybot99+a57z5ZdfNutwX48bNw6ff/45Vq5cud/tExGpNvn5VrOrkdv/ljqoMrkCs1BXcP7hU2dGkYO36LGy27oJRddd8lTZ6679oOi6y8aWvt4BYEDCxm6K2dnZpa7z119/mdu3337bZEzs+S+//BI33HADbrnlFhM4/ec//8Gll16Kn3/+ucjjmUlhVo6BykUXXYQLLrgAS5YsKbLObbfdZp6HmbuBAwfitNNO82fsGPQw0Onduzdmz55tgpVt27aZQCDw8QzmvvrqK3z//feYNm2ayd6Uh93ohg4diqlTp5rXPfnkk83r2sEEMz8Mkhgs2dmisjBISUlJwf/+9z8TkPTp0wcnnHACdu3a5V+HAcgnn3yCb775xrwHvubVV1/tv//55583AeXTTz+N+fPnm+6kDI5XrFjh395jjjkGmzZtMllN7k9eY5MBXWCwyWPJ1+At98njjz+O/eHrJCcn48gjjzTPXZ7//ve/5hgyMCwLA04Gsuy2SPxsMXh3s6tQAQbq9Ntvv+13+0REqgUDskmTrMZp/gBq/1ta/MfQCKbALMTZvxG4HPRrgYgTRUVF4Z133jHdGHkSfcQRR+Duu+82gYGNWSDi/cyY2PMMINgVjcFFhw4dcPPNN+Oss84yy4sHLcwmcZ2HH37YZHlefPHFIutce+21Jnjr3LmzybAxC8MAgJh9YVD22GOPoVOnTmb6rbfeMgEgu14yYOG6fF0GQxzrxPeTl1csG1lMz549TTDJDF379u3NtjH7ZwcmzBSxy16tWrX82aLSMLCYNWsWPv30U/Pe+FzcFu6vwOxgVlYWxo8fbzJhRx99tNkHzBRu3brVvz85VotBD7NKzGJyXXuM24cffmjGAzLgYgDFboEMThnI2hik8XjyPXGdiy++2ASeZWFQzmCQ2/7dd9+ZxwwfPrzM4Izvk0E4j2dZ+D75PpitY/aQGFjzfT711FPIycnB7t27ceedd5r7ygt4RUSk+jlnNJ2ISNe7y94Hxas+db6tnHWLdaXrWDUVBxkQsZADuzSy6AOzPk8++STefPPNcscAMevFAg6BGNgx8xMoMHCw5+fNm1fmOgwWGeDYWTVmhhiEMYgojhmiffv2mZP9AQMG+JczqGJwUx4GdOxiyICEwQEDOT6XnTGrKG4fn4vdAQPxubh9NhZXadq0aZH3zECK47HYNZDjsLj/AnHe7hLJfcaglO+tLOxmykDSV/CjGrt1MpNXFnZ3ZEBtYxdDbgcDqMAxYjYGwAx82d21NCwEwmCRr88A29a1a1cTLPO12E2WAe/1119vupEGZtFEROTQU2AW8qwTQOXLRKqAJyb46+4Hx3axYh4bux4yIzJ69OiQKM7AoIddDJlBKo6BR2XHKN16661mbBQzVcw+sWvdOeecY4K8A90+bge7TxZnd+WrCnbXv/IEjg0jdicM7OpYEQxwuV+K4zg+ZvjYtbO8oIzj8n766Sd/tsx24YUXmsZuqOwGyW179tlnzbhDEREJHv08FjYUmok4UZcuXcyJeOAJf749OLoAux0GltQnzvOxgQJLr9vzfGxZ6zBzxXFa9jocr8UKfswGMYAKbDzBZ/dDbl9gEQx2lWM3x/JwWxl4sqgJs0DsqshiJIE4Lqr4+y6O28duesz0Fd8+ZqRszMQFlpDne2a2iJk9BjEsM1/e/uzRo4fJmgWOW6sOfA0GmsWxuyPHirF7ZFlBGcfDsYBK8exhILvYyoQJE/w/CIiISPAoYyYiEgJYYINjwP7973+bE392g2OBDXZlZMVCG4MijlVi1zpWDKxTp44puMGTcXavYyl0FpxgwYzAyoZkj73i+KUPPvjAjFOyx4/ZWK2PY7MYjD333HMmsOI20TXXXIM33njDjFlisQt25WOWjNkbdrfkSf5ll11mtocBAYtY3HPPPfvtIsfX4/YyG8fsDTOFxbNLfN+//vqrGffF9x0YaNn43tktkWOzuN84lo4BGLtIMujjeycGIaNGjTIZur1795qufNx/9tg1bj+zlAw0ObaMxVYYJHGfEd8/x9nxdcaMGWOCJxYQYUBXvLtoRbF7IYNPHkPi/uD4Pe7X4njM+NrFgy4GZcw0stgKqy8ykLXHzfFY8fntsYKsvMnjxYwc3y8Lk1RlVlFERA6cArNQZ49lUcJMJKLxJJld1xgMcTwUT7JZzv2KK64wRUBsLBDB8UEMkDhOipklnqRzPBkDDVZnZIl2BhMsMx/owQcfNEEUi4QwmPjoo49KZNV4gs7GQISZJhafsIMgO5PEghInnXSSydqwrD+rKNrBF8dE2V0eGVyywmNgKfrSsBsdgz8GC3wtPj8DpkDstscCIQyW+Lr22K1ADOomTZpkgkFWpWSBDgZbLPDB7JCN74vFUVgJklmvU089Fa+88or/fgZq3GZuO8eFcR9xPzCAJAY4rDjJ+/kczCxyHQa1B4NFT9j9kBk/FldhJouBViCOg2ORE75+cXaVSGJAGYhjA+3PAwNyBp48TnwdXjqAlz0QEZHgcvlK+9fN4XhCwEpk/Ie5eN/8Q23O06ejb/ov+KPjnTh8xF1B3RapXswQ8CSQWQYNwj84rEa3Zs0aE6AwOxJq+GeXJ/M8AS/vmlxVia/DsvoM4krDAI/7i5mf4if1kYRFRlhNsXjRk0g61hIc5R3rUP+bJAdG/15XA/aSmFtwaZU+fbgA2PCFNd/8LMAdFdRjze8te6hUd2ygjJmIiIiIiAQPe10cdljgAqBl4TUynULFP0Ke/aubEpsiIiIiIpFKGTMREQfYX691FtdwQs92dmVkExERCTUKzEKcz86YOeCESUREREQciJdDmTTJmh46lAuARY9Z813vrtLrhYYydWUMGwrMRA74W6MfNEQkBOhvkYhUhAKzEKcaXiIHzuPxmNucnBztPhEJOvtvkf23SUSkNOrKGC5dGUWkwliuOiEhwVzHKjo6OuQuP6AS6s6hY+0cZR1rltvm3yL+TeJ9IiJl0V8IEYk4PCniBZR53SBesDcUT+B4ssaAUde2imw61s5R3rHmshYtWuj7LiLlUmAW6pQwE6mUmJgYtG/fPiS7M/LkbefOnahXr17IZfOkaulYO0d5x5p/j/RdF5H9UWAWLlTEQOSA8UQoLi4uJE/g2MWS26aTtcimY+0cOtYicrAUmImIiIiISPCw+29ycuE06xPWal8w75yeJQrMwoRP5fJFREREJBKx+++AAYELgNYXwWmcE4KGeVVGlwIzEREREZGIpcBMREREREQkyNSVMeRZGTPV/hARERGRiJSfD0yZYk0PGcIFwJKnrPnOtwGeGDiBArMwoar5IiIiIhLRwVkgby6cRl0ZQ1yxa1SKiIiIiEgEUmAWJsU/fMHeEBERERERqTYKzMKGQjMRERERkUilwExERERERCTIFJiFvILrmKkso4iIiIhIxFJVRhERERERCa569YpWv6vZqnDaIRSYhToHfRhFRERExIE8HmDQoMAFQJtL4DTqyhg2VPxDRERERCRSKTATEREREREJMnVlDJfrmClhJiIiIiKRKD8f+PFHa3rwYC4Alo215jveCHhi4AQKzMKES10ZRURERCRS5eQUnc/LhNOoK2OIU+kPEREREZHIp8AsxNk9GNWTUUREREQkcikwCxsKzUREREREIpUCMxERERERkSBTYBYuF5hWWUYRERERkYilqowiIiIiIhJctWsXTUwkNCmcdggFZmFyHTMRERERkYjk8QBHHRW4AGh3JZxGXRnDhop/iIiIiIhEKgVmIU75MhERERGRyBfUwOzXX3/FaaedhiZNmsDlcmHixInlrv/FF1/gxBNPRIMGDZCYmIiBAwdiypQpRdZ54IEHzHMFtk6dOiHsuzIqYSYiIiIikSg/H/jxR6tx2psLLB1rNU47RFADs4yMDPTs2RMvv/xyhQM5BmaTJk3CnDlzcNxxx5nA7u+//y6yXteuXbFlyxZ/++233xD+FJmJiIiISITat89qdjXynD1Wc1Bl8qAW/zjllFNMq6ixY8cWmX/sscfw1Vdf4ZtvvkHv3r39y6OiotCoUaMKP292drZptr1795pbr9drWkjwWdsjkYvH1+fz6Tg7gI61c+hYO4eOtXPoWFfLToVp9jS8cBUEZD7Ou7yOONZhXZWROyktLQ1169YtsnzFihWme2RcXJzp7jhmzBi0aNGizOfh/Q8++GCJ5du3b0dWVhaCKZ/pXAaPOdlISUkJ6rZI9X+eU1NTzR8At1vDPyOZjrVz6Fg7h461c+hYV4P8fMSmpprJbJ7vuvJRMyPDzKdvTwHcMQjmsT5U52VhHZg9/fTTSE9Px3nnnedfNmDAALzzzjvo2LGj6cbIgOuoo47CwoULUatWrVKf56677sLNN99cJGPWvHlz/1i2YFrH8qEAYmJikJycHNRtker/8nNMJD93Cswim461c+hYO4eOtXPoWFcDJiKSkqxpc76bD9f2GmY2oUEy4IkJ6rlZbGzsIXm9sA3MPvzwQxN0sStjYMAS2DWyR48eJlBr2bIlPvnkE1x22WWlPhd3dmk7nCfHQT9Bdrn919YL+rZIteOXPyQ+d1LtdKydQ8faOXSsnUPHuoqx26K74NzH3Pr8F5Z2cT6I50X2sT4UwjIw+/jjj3H55Zfj008/xeDBg8tdt3bt2ujQoQNWrlyJsOaggY8iIiIiIk4Tdj/Lf/TRR7j00kvN7bBhw/a7Prs6rlq1Co0bNz4k2yciIiIiIgeoVi2rEbNlcQ2sVpA5c4KgZswYNAVmstasWYN58+aZYh4s1sGxX5s2bcL48eP93RdHjRqF559/3nRR3Lp1q1keHx+PpIJ+qbfeeqspoc/ui5s3b8bo0aPh8XgwYsQIhCUHfRhFRERExIFYU+HYYwMXAB2ugdMENWM2e/ZsU+beLnXPAhycvv/++808i3esX7/ev/7rr7+OvLw8XHPNNSYDZrcbbrjBv87GjRtNEMbiHywKUq9ePfzxxx+moEJYU1dGEREREZGIFdSM2bHHHmtKg5eF1RUDTZs2rULjz0RERERERMJJWBb/cBZ1ZRQRERGRCC+XP326NX3UUdYFpVe+bs23uxJwR8MJFJiFDVVlFBEREZEIlZZWOM0edVnbC6cdIuyqMjqNcz6KIiIiIiLOpcAsXChCExERERGJWArMwoYiMxERERGRSKXALNTpOmYiIiIiIhFPgVnYUMZMRERERCRSqSqjiIiIiIgEV3x80R5jMbULpx1CgVnIc86HUUREREQcyOMBBg8OXAB0uhFOo66M4UI9GUVEREREIpYCMxERERERkSBTV8aQp66MIiIiIhLB8vOBGTOs6UGDAJcXWP22Nd/mUsAdDSdQYBY21JdRRERERCLUnj2F0z4fkLm5cNoh1JUx1DmoEo2IiIiIiFMpMAsXDvq1QERERETEaRSYhQ0FZiIiIiIikUqBWchTV0YRERERkUinwExERERERCTIVJUx1ClhJiIiIiKRLiam6HxUApxGgVnIU2QmIiIiIhHM4wGGDAlcAHS5HU6jrozhQlUZRUREREQilgIzERERERGRIFNXRhERERERCZ78fODPP63pAQMAlxdY+4E13+oiwB3tiKOjwCxs6DpmIiIiIhKhdu4sOoQnfW3htEOoK2OI87lU/ENEREREJNIpMBMREREREQkyBWbhwkFpXBERERERp1FgFvLUlVFEREREJNIpMAsbypiJiIiIiEQqVWUMccqXiYiIiEjE83iKzjukRH4gBWYhzqfQTEREREQiPSgbOjRwAdDtHjiNujKGDXVlFBERERGJVArMQp2uYyYiIiIiEvHUlVFERERERILH6wX++sua7tePC4B1E6z5lucDbmeELM54l5FA1zETERERkUg9z01JKZxmYJa2omDeC6dQV8aQp7qMIiIiIiKRToGZiIiIiIhIkCkwC5Ny+S4HpXFFRERERJxGgVmYVGVUh0YRERERkcilwCxsLjCt65iJiIiIiEQqBWbhch0zdWUUEREREYlYKpcf6lxW7Oxi2VARERERkUjj8QCnnRa4AOjxAJxGGbMwCczUk1FEREREJHIpMAt56sooIiIiIhLp1JUxXDJmSpmJiIiISCTyeoG5c63pPn24ANjwhTXf/CzA7YyQxRnvMhKo+IeIiIiIRCKfD9iypXCagVnqYmu+2XA4hboyhkvxD/MhFRERERGRSKTALGy6Mqoqo4iIiIhIpFJgFjbXMVPGTEREREQkUikwC5vrmCkwExERERGJVEENzH799VecdtppaNKkCVwuFyZOnLjfx0ybNg19+vRBbGws2rVrh3feeafEOi+//DJatWqFuLg4DBgwALNmzUL4Z8zUlVFEREREJFIFNTDLyMhAz549TSBVEWvWrMGwYcNw3HHHYd68ebjxxhtx+eWXY8qUKf51JkyYgJtvvhmjR4/G3LlzzfMPGTIEKSkpCO8LTCtjJiIiIiISqYJaLv+UU04xraLGjRuH1q1b45lnnjHznTt3xm+//YbnnnvOBF/07LPP4oorrsCll17qf8x3332Ht956C3feeWepz5udnW2abe/evebW6/WaFlyFGbPgb4tUJx5fn8+n4+wAOtbOoWPtHDrWzqFjXU09xE4+uXDa5wE62+ftHus6Zw441mF1HbOZM2di8ODBRZYxIGPmjHJycjBnzhzcdddd/vvdbrd5DB9bljFjxuDBBx8ssXz79u3IyspCMGXn5Jpbb35e+Gb9pEL4pU9NTTV/APi5lcilY+0cOtbOoWPtHDrWzjvW7kN0XhZWgdnWrVvRsGHDIss4zwzXvn37sHv3buTn55e6ztKlS8t8XgZy7P5o4/M1b94cDRo0QGJiIoJpVVycufV43EhOTg7qtkj1f/k51pKfOwVmkU3H2jl0rJ1Dx9o5dKydd6xjY2MPyeuFVWBWXbizS9vhPDkO+gmy/wLT3uBvi1Q7fvlD4nMn1U7H2jl0rJ1Dx9o5dKyrGLsKzp9vTffoYV2/d9O31nzTUwF3VNCP9aEQVmd/jRo1wrZt24os4zyzWvHx8ahfvz48Hk+p6/Cx4YgfBouKf4iIiIhIBGKRuw0brMZpViPfPc9qDqpMHlaB2cCBAzF16tQiy3744QeznGJiYtC3b98i6zAFyXl7nbDj8li3qsooIiIiIhKxghqYpaenm7L3bHY5fE6vX7/eP/Zr5MiR/vX/7//+D6tXr8btt99uxoy98sor+OSTT3DTTTf51+FYsTfeeAPvvvsulixZgquuusqU5berNIadgoQZuzKKiIiIiEhkCuoYs9mzZ5trktnsAhyjRo0yF47esmWLP0gjlspn6XsGYs8//zyaNWuGN998018qn84//3xTTfH+++83xUJ69eqFyZMnlygIEm4ZM5e6MoqIiIiIRKygBmbHHnusKQ1eFgZnpT3m77//Lvd5r732WtMign+ImTJmIiIiIiKRKqzGmDmRyx5jJiIiIiIiEUuBWagrqMroYtlQERERERGJSJXqyrhhwwZTxp1jvGjWrFn48MMP0aVLF1x55ZVVvY2O5gq4jpmIiIiISMTxeAC7ZgSnfW6gy23WvDsaTlGpjNmFF16In3/+2UyzwMaJJ55ogrN77rkHDz30UFVvo7P5r2MmIiIiIhKhYmKsZp//RtWwmoPOhSsVmC1cuBD9+/c30yxX361bN8yYMQMffPBBqQU75GAUdGVUxkxEREREJGJVqitjbm4uYmNjzfSPP/6I008/3Ux36tTJlLiXKlTQlREqly8iIiIikcjrBRYtsqa7duUCYMsUa77xEMAd1ELyoZ0x69q1K8aNG4fp06fjhx9+wMknn2yWb968GfXq1avqbXQ0l9u+jpnGmImIiIhIBOLls9autRqn2VNs519Wc1CvsUoFZk888QRee+01c02xESNGoGfPnmb5119/7e/iKFVclbGc672JiIiIiEh4q1RekAHZjh07sHfvXtSpU8e/nBUZExISqnL7xD/gUYGZiIiIiEikqlTGbN++fcjOzvYHZevWrcPYsWOxbNkyJCcnV/U2Opqr4BApYyYiIiIiErkqFZidccYZGD9+vJnes2cPBgwYgGeeeQbDhw/Hq6++WtXb6Gxu+wLTypiJiIiIiESqSgVmc+fOxVFHHWWmP/vsMzRs2NBkzRisvfDCC1W9jY7mchUU/3DQwEcREREREaepVGCWmZmJWrVqmenvv/8eZ511FtxuNw4//HAToEnVcWmMmYiIiIhIxKtU8Y927dph4sSJOPPMMzFlyhTcdNNNZnlKSgoSExOrehudreA6ZurKKCIiIiIRye0GTjihcJq5o043FsxHwykqlTG7//77ceutt6JVq1amPP7AgQP92bPevXtX9TY6nD3GTF0ZRURERCQCsYdYQoLVOM0WU9tq/t5jka9SGbNzzjkHRx55JLZs2eK/hhmdcMIJJosmVcdlfjVQVUYRERERkUhWqcCMGjVqZNrGjRvNfLNmzXRx6ergtg6RWxkzEREREYlEXi+wdKk13amTdf3ebVOt+YYnAG6rGF6kq1RXRq/Xi4ceeghJSUlo2bKlabVr18bDDz9s7pOqPELWB9Hjy9NuFREREZHI4/MBq1ZZjdO+fGD7DKtx2iEqlTG755578N///hePP/44jjjiCLPst99+wwMPPICsrCw8+uijVb2djuUrGPDohnM+lCIiIiIiTlOpwOzdd9/Fm2++idNPP92/rEePHmjatCmuvvpqBWZVyBNlHSJlzEREREREIlelujLu2rULnUz/z6K4jPdJ1XF5Ysytx0FpXBERERERp6lUYMZKjC+99FKJ5VzGzJlUHbenIGOmrowiIiIiIhGrUl0Zn3zySQwbNgw//vij/xpmM2fOxIYNGzBp0qSq3kZH80RbY8zUlVFEREREJHJVKmN2zDHHYPny5eaaZXv27DHtrLPOwqJFi/Dee+9V/VY6mNvuyqiMmYiIiIhIxKr0dcyaNGlSosjHP//8Y6o1vv7661WxbcKAzFOQMVNgJiIiIiKRyO0Gjj22cJq5ow5XF8xb58JOUOnATA4Nd0FXxijoOmYiIiIiEoFcLqBWraLL4pLhNJXqyiiHjsdf/EMX7hYRERERiVTKmIU4T3SsuY1mV0ZeCZ2/KIiIiIiIRAqvF1ixwppu3x6AD9g+3ZpvcBSLLsAJDigwY4GP8rAIiFTPBaYNbz4XaBeLiIiISORg8mH5cmu6XTuACYlt06z5+oNM3zEnOKCz/KSkpP3eP3LkyIPdJgkQFWVVZTS8uQrMREREREScHpi9/fbb1bclUip3VGElGm9eLtzR8dpTIiIiIiIRRsU/Qlx0dGHGLC8vN6jbIiIiIiIi1UOBWZhUZaT83JygbouIiIiIiFQPBWYhLirKjVyfNeAxL18ZMxERERGRSKTALMRFud3IK6hE41XGTEREREQkIqn2eojzuF3IKgjMNMZMRERERCKO2w0cdVThNFxAuysK5p0TrjjnnYaxXH/GLCvYmyIiIiIiUrVcLqB27cAFQEJTx+1ldWUMA1mINbf5OfuCvSkiIiIiIlINlDELA9mwSuYrYyYiIiIiEcfrBdassaZbtwbgA3b+Yc3XOxxwW73HIp0CszCQ7SrImGVnBHtTRERERESqls8HLF5sTbdqxbNeYMsP1nzdfqy64Ig9rq6MYSDHZWXM8rIyg70pIiIiIiJSDRSYhYEcZcxERERERCKaArMwkGcHZrkq/iEiIiIiEokUmIWBXLc9xkxdGUVEREREIpECszCQVxCY+XIUmImIiIiIRCIFZmEgzxNnbn3qyigiIiIiEpFULj8M5Hviza0rR+XyRURERCTCuN3AoEGF03ABbS4pmHdOuOKcdxrGcjw1zK07Jy3YmyIiIiIiUrVcLqBevcAFQE1ez8xZ1JUxDORF1zS3npy9wd4UERERERGpBsqYhYHc6FrmNjpXGTMRERERiTBeL7B+vTXdogUrKwC75ljzdfsCbg+cICQyZi+//DJatWqFuLg4DBgwALNmzSpz3WOPPRYul6tEGzZsmH+dSy65pMT9J598MsJWjBWYReUpMBMRERGRCOPzAQsWWI3Tvnxg8ySrcdohgp4xmzBhAm6++WaMGzfOBGVjx47FkCFDsGzZMiQnJ5dY/4svvkBOTo5/fufOnejZsyfOPffcIusxEHv77bf987GxVsn5cOSKSzS3MXnpwd4UERERERGJxMDs2WefxRVXXIFLL73UzDNA++677/DWW2/hzjvvLLF+3bp1i8x//PHHSEhIKBGYMRBr1KhRhbYhOzvbNNvevdZYLq/Xa1ow8fXdcVbGLC4vPejbI9WHx9bn8+kYO4COtXPoWDuHjrVz6FhXy06FafY0vHAxc8ZkGuddXkcc66AGZsx8zZkzB3fddZd/mdvtxuDBgzFz5swKPcd///tfXHDBBahRw6pcaJs2bZrJuNWpUwfHH388HnnkEdQrUu2l0JgxY/Dggw+WWL59+3ZkZWUhmPhByEW0mY7zZiAlJSWo2yPVe6xTU1PNHwB+DyRy6Vg7h461c+hYO4eOdTXIz0dsaqqZzOa5risfNTOsy0Slb08B3DEI5rE+VOdlQQ3MduzYgfz8fDRs2LDIcs4vXbp0v4/nWLSFCxea4Kx4N8azzjoLrVu3xqpVq3D33XfjlFNOMcGex1Ny8CADQ3anDMyYNW/eHA0aNEBiotWNMFj4gUis39hMxyIHyXVqAdHWdc0ksvBYczwkP3cKzCKbjrVz6Fg7h461c+hYV4P8fCApyZo2Q5ny4dpuJV0SGiQDnpignpsdqiFRQe/KeDAYkHXv3h39+/cvspwZNBvv79GjB9q2bWuyaCeccEKJ5+HOLm2H8+Q4FE6Q42vURrYvCrGuPLj37QRiWa1GIhG//KHyuZPqpWPtHDrWzqFj7Rw61lWM3RbdBec+5tZnXduM+5rzQTwvso/1oRDUs7/69eubDNa2bduKLOf8/saHZWRkmPFll1122X5fp02bNua1Vq5ciXBUMy4KO1DwK0L69mBvjoiIiIiIRFJgFhMTg759+2Lq1KlFUoacHzhwYLmP/fTTT03Bjosvvni/r7Nx40ZTvbFxY6tLYLipEePBDp8VmHnTigaxIiIiIiJhjRmp/v2tZjJkUUCrC63GaYcIen8pju1644038O6772LJkiW46qqrTDbMrtI4cuTIIsVBArsxDh8+vERBj/T0dNx22234448/sHbtWhPknXHGGWjXrp0pwx+OEuMKA7N9u7cEe3NERERERKoOuy02bGg1TrvcQGIHq3HaIYIegp5//vmm+uH999+PrVu3olevXpg8ebK/IMj69etL9OvkNc5+++03fP/99yWej10j58+fbwK9PXv2oEmTJjjppJPw8MMPh+21zKI9bqR6apvprD1bULT+pIiIiIiIhLugB2Z07bXXmlYaFuwormPHjqakeGni4+MxZcoURJp90fWAXCA3VV0ZRURERCSC8DphmzZZ002bWsU/UhdY80ndAXfJquqRKCQCM9m/rLj6JjBDmroyioiIiEgEYcJl3jxrukkTUy4fGyZa84ld2CcOTuCcTpthLiuBH1IgKn1zsDdFRERERESqmAKzMJGfyLQuEL9PGTMRERERkUijwCxMeJKam9saubuA3Kxgb46IiIiIiFQhBWZhomadBtjni7Fm9hYMjhQRERERkYigwCxMNKmTgM2+gmu2pW4M9uaIiIiIiEgVUmAWJpokxWGTr741s2d9sDdHRERERESqkMrlh4kmteMx29cIR2MB8rav0IETERERkcjgdgN9+xZOwwW0OLdg3jnhinPeaZhLjIvCRrdVmTF761IdOBERERGJDC5XwfXL/AuA2l3hNOrKGCZcLhdSa7a2pneuDPbmiIiIiIhIFVJgFkZya7czt3Fp64D83GBvjoiIiIjIwfP5gM2brcZpnxfYs8hqnHYIBWZhJL5eM2T6YuH25QG71wV7c0REREREDp7XC8yZYzVOe/OA9Z9ajdMOocAsjLSsXwtrfI2smR3Lg705IiIiIiJSRRSYhZG2DWpiqa+5NbN1frA3R0REREREqogCszDSNrkm5nvbmmnfxjnB3hwREREREakiCszCSPM68VjssgIz76Y51uBIEREREREJewrMwkiUx430Ol2Q6/PAs28nkLoh2JskIiIiIiJVQIFZmGmRXKdwnNmmucHeHBERERERqQIKzMJMmwaF48ywWYGZiIiIiIQ5lwvo1ctqnHZ5gObDrcZph1BgFoaVGf/xtbFmlDETERERkXDndgPNm1uN024PUKeX1TjtEArMwky3pomFlRk3z7MuwiciIiIiImFNgVmYadegJjZGtUCGLxaunDRg+9Jgb5KIiIiISOWx0vi2bVbjtM8L7F1uNU47hAKzMKzM2LlpHczztrMWrJ8Z7E0SEREREak89gCbNctqnPbmAWs/tBqnHUKBWRjq0aw2/vR2tmbWTg/25oiIiIiIyEFSYBaGejRLwgxvF2tm9S+ANz/YmyQiIiIiIgdBgVmYZszm+dpht68msG8XsG5GsDdJREREREQOggKzMNSqXgIS4uLwff5h1oLFXwV7k0RERERE5CAoMAtDLpfLZM3+5+1vLVjytcrmi4iIiIiEMQVmYapfq7r43dsNme4aQPo2YMOfwd4kERERERGpJAVmYeqoDvWRiyj84O1rLVg8MdibJCIiIiJy4FwuoHt3q3Ha5QGaDLUapx1CgVmY6tmsNpLio/FVTkF3xj/HAXk5wd4sEREREZED43YDrVpZjdNuD1C/v9U47RAKzMKUx+3Cke3qm+6Mfit/COYmiYiIiIhIJSkwC2NHd6iPbMTgp9jB1oJ5HwZ7k0REREREDozPB+zcaTVO+7xA+lqrcdohFJiFsaM7NDC3T6cVBGZLvwN2rAjuRomIiIiIHAivF5gxw2qc9uYBq9+xGqcdQoFZGGucFI/2yTWx2NsCKQ2P4s8NwKw3gr1ZIiIiIiJygBSYhbljCrJmn8WcUdidMSs1uBslIiIiIiIHRIFZmDule2Nz+/K6pvDW7wjkpClrJiIiIiISZhSYhbk+LWqjae14ZOT4sKD15dbCnx4G9m4J9qaJiIiIiEgFKTALcy6XC6f2sLJmb+7uXXjHB+cGb6NEREREROSAKDCLAKf2aGJupyzdgfRh46yF2xYCu9cGd8NERERERKRCFJhFgG5NE03LyfPi/cz+QJ3WVoXG9860rgUhIiIiIhKqXC6gSxercdrlARqfaDVOO4QCswjpzjhyYCsz/d7Mdcg/910uBXatBv54JdibJyIiIiJSNrcbaNvWapx2e4AGR1iN0w6hwCxCnN6zCeokRGPTnn34aU8joMvp1h2/Pw+kbQv25omIiIiISDkUmEWIuGgPzu/Xwky/O2MtcPpLQHwdIH0b8M311lXURURERERCDYfe7NljNU77vEDmJqtx2iEUmEWQiwa0gNsF/LZyB1budQGjvgE8scDyycD4ggyaiIiIiEgoYQJh+nSrcdqbB6x8w2qcdggFZhGked0EnNC5oZkeP3Md0Kg7cNQt1p1rpwMb5wR3A0VEREREpFQKzCLMJYOsIiCfzN6AlLQs4JjbgYR61p0fX6gS+iIiIiIiIUiBWYQZ1LYeereojaxcL8ZNW22VHL1+HlCvHZC+FXjtaCBzV7A3U0REREREAigwi8DS+TcN7mCmP/hzHbbtzQLiEoGRXwOxSUBWKvBkayAvJ9ibKiIiIiIiBRSYRaCj2tfHYS3rIDvPi7E/rrAWJjUFTnq4cKUvLge8+UHbRhERERERCbHA7OWXX0arVq0QFxeHAQMGYNasWWWu+84775isUGDj4wL5fD7cf//9aNy4MeLj4zF48GCsWFEQoDgA98kdp3Qy0xP+Wo+lW/dad/QdBZzylDW9+CvgiyuAfOdUuhERERERCVVBD8wmTJiAm2++GaNHj8bcuXPRs2dPDBkyBCkpKWU+JjExEVu2bPG3devWFbn/ySefxAsvvIBx48bhzz//RI0aNcxzZmVlwSn6taqLU7o1gtcHPPD1IhOsGgOuBM55C3BHAws/Bz4eAeQ6Z7+IiIiISIhhTYQOHazGaZcHaHis1TjtEEEPzJ599llcccUVuPTSS9GlSxcTTCUkJOCtt94qNyPUqFEjf2vY0CoRTwxAxo4di3vvvRdnnHEGevTogfHjx2Pz5s2YOHEinOSeYZ0RF+3GH6t34bsFWwrv6HY2cN671gd9xffAow2BfXuCuakiIiIi4lRuN9Cxo9U47Q4IzDjtEFHBfPGcnBzMmTMHd911l3+Z2+02XQ9nzpxZ5uPS09PRsmVLeL1e9OnTB4899hi6du1q7luzZg22bt1qnsOWlJRkukjyOS+44IISz5ednW2abe9eq+sfn58tmPj6DDYrsx1NkuLwf0e3wdipK/Hod0twdPv6qBlbcMg7nAJc8BHcH51nZn3vDIPv3PFA3dZV/RbkEBxrCS861s6hY+0cOtbOoWPtHN5DfG4W1MBsx44dyM/PL5LxIs4vXbq01Md07NjRZNOYCUtNTcXTTz+NQYMGYdGiRWjWrJkJyuznKP6c9n3FjRkzBg8++GCJ5du3bw9690d+EPg++aFg0HqghneuhU/+isHm1Czc//nfuHNwy8I7k3oi/phHkPTLvXBtWwjXS32w45wvkVe/S9W+CTkkx1rCh461c+hYO4eOtXPoWFcDnw+ujAxrskYNc+vO3Wnt7+h6VvfGIB7rQ3VeFtTArDIGDhxomo1BWefOnfHaa6/h4YcDqg4eAGbsOM4tMGPWvHlzNGjQwIxnCyZ+INh1k9tS2Q/F0+dF48I3Z2Hiwh04rW9LHNcxufDO5Gvg7XkaXOOOhCsnDfUmXQ7fOe8ArY6sujchh+xYS3jQsXYOHWvn0LF2Dh3rapCfD9jF/4YO5QK4Fo8zs74udwGeGATzWMfGxkZ+YFa/fn14PB5s27atyHLOc+xYRURHR6N3795YuXKlmbcfx+dgVcbA5+zVq1epz8GdXdoO58lxKJwg8wNxMNsyqF0D/PuI1njr9zW464uFmHLj0ahTI+ADXrcVcNn3wKsD4crcCdf404Dj7gGOvi1ov1A41cEeawkfOtbOoWPtHDrWzqFjXcVYpM5dcO5jbn3+c1CXGXPmDvqxPhSCevYXExODvn37YurUqUUiU84HZsXKw66QCxYs8AdhrVu3NsFZ4HMyA8bqjBV9zkh0+8kd0bZBDaSkZePerxYWVmm0NewC3L0F6DnCmv/5UeCjC4D07UHZXhERERERJwn6z/LsQvjGG2/g3XffxZIlS3DVVVchIyPDVGmkkSNHFikO8tBDD+H777/H6tWrTXn9iy++2JTLv/zyy/1R7Y033ohHHnkEX3/9tQna+BxNmjTB8OHD4VRx0R48e14veNwufDd/Cz6dvbHkSjEJwJnjgCFjrIqNyycDT7cDZpddIVNERERERCJgjNn5559vimzwgtAszsHuhpMnT/YX71i/fn2R9OHu3btNeX2uW6dOHZNxmzFjhim1b7v99ttNcHfllVdiz549OPLII81zFr8QtdP0bF4bt5zUAU9OXob7vlqIdg1rok+LOiVXHHg10Kwf8N5wICcd+PYmIHMncOQtQU0li4iIiIhEKpevRJ82YddHlthnFZZQKP7Bi20nJydXSf9Wr9eHK8bPxtSlKahXIwYTrzkCzesmlL7y7nXA20OBvQXZtZZHAKe9ANRvd9DbIdV/rCV06Vg7h461c+hYO4eOdTUV/5g0qUjxDyx6zJrvendQi3/w3IzJHSaEqjs20Nmfw7jdLrwwoje6NknEzowcXPL2LKRm5pa+cp2WwM2LgBMfBqJrAOt+B17qC0x7nJ/UQ73pIiIiIiIRS4GZA9WIjcJ/R/VDo8Q4rNqegas+mIOcvHICrSOuB676DUi2LuKNaWOsbo671hyybRYRERGRCMUKjG3bWo3TrHXQYJDVOO0QCswcqlFSHN66pB9qxHgwY9VO3PPlgpKVGgPVbQP832/ACfcDnlhgzS/Aq0cAvz8P5JeRcRMRERER2R8O4ejSxWqmPL4HaHyS1TjtEArMHKxLk0S8dGEfuF3Ap3M24pVpq8p/AL8oR91iBWgcb5abAfxwP/DK4cCqnw/VZouIiIiIRBwFZg53XKdkPHi61UXxqSnL8PU/m/f/oAYdgFHfAsOeBWJqATtXWl0bP70ESN1U/RstIiIiIpGDvbYyM63GabacPVZzUJ1CBWaCfw1shcuObG32xK2f/oPfV+6owCfHDfS7DLhpAdDvCnYOBhZ9CbwyEPjtOSA3S3tWRERERPaPReWmTrUap725wNKxVuO0QygwE+PuoZ1xUpeGpgjIv9/5C78u316xPRNfBxj2NHDlNKBJHyA7FfjxAeDRhsCUe7R3RUREREQqQIGZGB63Cy9e2BsndEpGdp4Xl4+fjZ+XpVR87zTpBVz2A3DGK0CtJtaymS8BDyQBS77VXhYRERERKYcCM/GLjfLg1Yv7+jNn/xk/B1MWba34HvJEAb0vAq6eWfRCgBMuAr64UuX1RURERETKoMBMioiJcuPli/pgaPdGyMn34qr35+DjWesPbC/F1wbu2w5c+UvhsvkTgJcOA767FUjbpr0uIiIiIhJAgZmUEO1x44ULeuO8w5rB6wPu/GIBnp6yrPzrnJXVvfGBVGv8WdsTAG8e8NcbwAu9gKkPA1mp2vsiIiIiIgrMpCxRHjeeOLsHrj+hvZl/6eeVuGnCPGTn5R/4TmvSG/jXF8Cob4CmfYHcTGD608DzPYEZLwK5+3QgRERERMTRlDGTMrlcLtx8Ygc8eXYPRLldmDhvM0a9NQt7MnMqt9daHw1cPhU4/32gfgdg327g+3uBF/sC059RgCYiIiLiRC4X0KqV1TjtcgP1+lmN0w7hnHcqlXZev+Z465J+qBkbhT9W78KwF37D8m1plXsyftk6nwZcNRM442UgsRmwdxMw9SHghT7Ae2cCedk6WiIiIiJOwevjdu9uNU67o4Cmw6zGaYdQYCYVcnSHBvj0/waiZb0EbNqzD2e+/DsmLzyAio2lVnC8GLhuDnDSo9aytM3Aqp+sLo4zX1YGTUREREQcQ4GZVFjnxon48uojMLBNPWTk5OP/3p+DMZOWIDffW/m9GB0HDLoWuHszMOg6a1naFmDK3cALvYE57wB5lew6KSIiIiLhISfHasSCc3kZVjvQ4nNhTIGZHJC6NWLw3mX9cdmRrc38a7+uxgWv/2GyaAclpgZw0iPAnRuAY+4sDNC+uQF4poNVJCRrr46WiIiISKTJzwemTLEap725wOKnrMZph1BgJpWq2HjfqV3w6kV9UCs2CnPW7cbQ56dj8sItB7834xKB4+4C7k0BhjwGJNQvLBLyXFfg+/uA9BQdNRERERGJKArMpNJO6d4Y311/FHo2S0Lqvlz83/tzcdcX85GRnXfwezUqFhh4DXDzEmDYM1YVx+y9wIwXrCqODNRSN+roiYiIiEhEUGAmB6VFvQR8dtUg/N8xbc38R7M2YOgL0/HX2l1Vs2ejYoB+lwNX/wmM+Bho1L0gQHsRGNsD+OwyYMs/VfNaIiIiIiJBosBMDlq0x407T+mED68YgCZJcVi3MxPnjpuJe75cYDJpVYKlUzueAlz5KzBiAtDqKMCXDyz8DHjtaGD8cGDVz44aICoiIiIikUOBmVSZQW3rY/JNR+P8w5qb+Q/+XI8Tn/0F/1uwBb6qCphMgHYycMm3wJW/AN3O4cXRgNU/A+8NB8YdCfz1XyAno2peT0RERETkEFBgJlUqMS4aT5zTAx9dcTja1K+BlLRsXPXBXFwxfg42H2zlxuKa9ALO+S9wwzyg/3+A6BrAtoXAdzcDjzUBnusObFtcta8pIiIiIlINFJhJtRjYth4m3XAUrj++HaI9Lvy4ZJvJnr3z+xrke6u4u2GdVsDQJ4GbFwEnPgzUbmEtT10PvHYU8Nm/gXUz1c1RREREJBS5XEDz5lbjtMsN1OllNU47hHPeqRxycdEe3HxSR1O5sW/LOuai1A98sxhnvToDS7ZUwzXJ4usAR1wPXDfXKhhC3jxg4efA2ydbQdrc8UBOZtW/toiIiIhUfqhKr15W47Q7Cmg+3GqcdggFZlLtOjSshU//MxCPDO9mrnv2z4Y9OPXF3/D4/5YiM6cKSusX54m2Suw/kGqNQ+szEoiKB7YuAL6+Dni2MzD5bmDnqqp/bRERERGRSlBgJoeE2+3CxYe3xI+3HINTujUy3RnH/bIKxz/9Cz6fsxHequ7eGDgO7fQXgZsXF3RzbAlk7QH+eNm6HtpbJwMLvwC83up5fRERERHZv/x8qxGLxuXnWM1BFbcVmMkh1TAxDq9e3BdvjDwMzerEY+veLNzy6T8Y/srvVXfts9Ik1LW6OV7/t1Vuv+UR/NYD62cCn10KPFwP+OkRIG1r9W2DiIiIiJTEgGzSJKtx2psLLHrMapx2CAVmEhQndmmIH28+Bref3BE1Y6Mwf2OqufbZ1R/Mwfqd1TgGzO2xyu1fOgm4djbQbjAQFQf4vMCvTwHPdgE+GQWsnAp4C361ERERERGpZgrMJKjFQa4+th1+vvVYjOjfAm4XMGnBVgx+9heMmbQEqZnV/AtJ/fbAxZ8Dt60Cjr0baNrXumj14onA+2cBz3UFpj4MbF1YvdshIiIiIo6nwEyCrkGtWIw5q7spr39ku/rIyffitV9X46gnf8Ir01ZiX041Z65iawLH3gFc8VNBsZBRQHQCkLYFmP40MO4I4O2hwILPgLzs6t0WEREREXEkBWYSMjo1SsR7l/XHW5ccho4Na2FvVh6enLwMxzz1M96dsRY5eYegQIcpFvICcMc64Jy3gbYnWMvX/Q58fhnwdHvgmxt0XTQRERERqVLOuTCAhAWXy4XjOzXEMR2S8dW8TXj2h+XYuHsfRn+9CG9MX41rj2uHs/o0Q0xUNf+mEBUDdDvLarvXAvM+BOa+B6RtBua8YzVWeOxxPtDzAqBe2+rdHhERERGJaMqYSUjyuF0mAPvplmPx8BldTXdHBmh3frEAxxZk0LJyD1FxjjqtgOPuBm5aCIz8Guh1ERBTE9izDvj1SeDFPsAbJwCz3gAydh6abRIRERGRiKLATEIaM2P/GtgKv952HO4d1hnJtWKxOTXLZNCOfOJnvP7rKmRkV8NFqsuq6NjmGGD4K8CtK4Cz/2tVdXS5gU2zgUm3As90AD4aASz+SuPRRERERCrC5QIaN7Yap3luldTFapx2CJfP56CrtlXQ3r17kZSUhNTUVCQmJgZ1W7xeL1JSUpCcnAy32zkfzLIwS/bpnI0YN20VNu3ZZ5bVTojGZUe0xshBrZAUH33oNyptG7DwM+Cfj4Gt8wuXxyUBXc8Eeo4Amg+w/tCUQ8faOXSsnUPH2jl0rJ1Dx9p5xzouLg516tSp9thAgVkpFJiFvtx8L778exNe+Xkl1hZc96xWbBRGDWqFfx/ZGnVrxARnw1KWWAHagk+BvZsKl9drB3Q7G+h2DtCgQ6kP1R9659Cxdg4da+fQsXYOHWvn8CowCz4FZuEjL9+L7xZswcs/r8TybelmWXy0Bxcf3gKXH9UGDRPjgrNhXi+wdjrw9/vAkq+BvKzC+xp1tzJpHU4Bkjv7M2n6Q+8cOtbOoWPtHDrWzqFj7RxeBWbBp8As/Hi9Pny/eBte+nkFFm7aa5ZFe1w4rUcTXHZUa3RtkhS8jcvaCyybZF0HbfXPgDdgTFxSc6D3xUDn0+Ct3wkp27er26oD6B9159Cxdg4da+fQsa4G+fnApEnW9NChXAAsesya73o34IlxRGCmcvkSEdxuF07u1ghDujbEtOXb8erPqzBr7S588fcm0wa1rYcrjmqDYzo0MOseUnGJVkl9NlZtXDzRKg6y5lcgdQMwbYxprrptUbPF8UDf84GmffmmDu12ioiIiEjQKDCTiLsO2nEdk02bt2EP3py+Gv9buBUzVu00rV1yTVx6RCuc2bspEmKC8PGvUQ/od5nV0lOsAG3lVGDVT3DtWoWau1YB894AEpsCnU41mTS0GAh49FUVERERiWQ625OI1at5bbx0YR9s3J2Jd35fi4//2oCVKem458uFeOJ/S3FB/xb41+Et0bxuQnA2sGYy0P8Kq2Wnwbv8e2TP+wxxG36Fi4VDZr1mtYR6QMehQOfTrXL9UbHB2V4RERERqTYKzCTiNauTgHtP7YIbBrfHJ7M3motTr9+Vidd/XW0yasd3aoh/DWyJo9rVP/TdHG2xtUxBkNQGRyC2biJc7Oa45Btg2XdA5k7g7/esFlML6DAE6HyqVTwkOkjFTURERESkSikwE8eoFReNy45sjUsGtcK0ZSl4Z8ZaTF+xAz8u2WZay3oJuHhAS5zTtxnqBKvcPkXFAR1Ptlr+88C6360gbem3QNoW65ppbHABbY8DWh4BdBkO1G8XvG0WERERkYOiwEwcx+N24YTODU1j18YP/lyHz+ZsxLqdmXh00hI89f0ynNqjMS4a0AJ9WtQx49aCt7FRVvdFtlOeBDbNscrvz/vAyqSt+slqPz1sXSut3YlAp6FA88OBqCAGlyIiIiJyQBSYiaOxGMjo07ritiEd8fW8zRg/cx0Wb9mLL+ZuMq1Dw5oY0b+FKRZSOyHIgQ6rNDbvZ7UTHwJ2rABWTAGW/Q/YMAvYudJqf74KxCYBbY4G2h4PtB8CJDUN7raLiIiIlIU/gicnF07DDdRqXzDvnCrVLp/P5wv2RoQaXcfMufh1+HvDHnz053p8M38zsnK9ZnmMx40TOifjrD7NcGzHBoj2uEPruii8Vtrqadb10lb+CGRsL3p//Y5AuxOAticALQcBMUEqeCKGroHjHDrWzqFj7Rw61s7h1XXMRIKH3RbZfZHtvtO64Ku/N+HDWRuwZMteU3afrW6NGJzeswnO7tMM3ZomBrerY+C10rqcbjVvvtXlkQVEln4HbJkH7FhmtT9eATyxQKsjgTbHWl0kG3bXNdNEREREgkwZs1IoYybFLd7M7o0bMXHeZuxIz/YvZ1dHZtHY1bFhYlxo/gKXucsK0lZNBVb+BOzdWPT+Gg0KgrTjrICtTsuqe20plX5tdQ4da+fQsXYOHWvn8B7ijJkCs1IoMJOy5OV7MX3lDnw+ZyO+X7wNOXlWV0dW2T+iXX2TRRvStRHiYzyh+YeePZe3L7UKhqz+xar4mJNedJ3aLYH2J1oXtmbFx8TG1bMtDqZ/1J1Dx9o5dKydQ8e6GuTnA1OmWNNDhnABsOQpa77zbYAnxhGBWUiMpnv55ZfRqlUr86YHDBiAWbNmlbnuG2+8gaOOOsrsHLbBgweXWP+SSy4x3csC28knn3wI3olEuiiPG8d1TDYXrv7rnsF4/Kzu6NeqDrw+mNL7N06Yh8Me+QG3ffoP/li9E17eEXKDazsDA68BLvoEuH0NMGICcMQNQLP+gDsK2LMO+OtN4PPLgGc7Ac/3AiZeA/z9AbBrjRXciYiIiFR1cJafXzjvzbWagwS9KuOECRNw8803Y9y4cSYoGzt2LIYMGYJly5aZzEFx06ZNw4gRIzBo0CATyD3xxBM46aSTsGjRIjRtWlh5joHY22+/7Z+PjY09ZO9JnCEpPhoX9G9h2rqdGVYlx783YsOuffh0zkbTmtaOx1l9mprujq3r10DIYUl9+5pplJ0OrPkFWDPdyqZtWwjsXmO1ee9b69RqbBUQMe0Iq7BIdWX4RERERBwi6F0ZGYz169cPL730kj9l2Lx5c1x33XW488479/v4/Px8kznj40eOHOnPmO3ZswcTJ06s1DapK6NUFr9Of63dbcajfTd/C9Ky8/z3dW+ahGE9GmNY98ZoXjchPLpGZKVapfgZpK2bAWyaW/LXq/i6Bd0eC4K1Rj2s669JmULyWEu10LF2Dh1r59CxrgbMlE2aZE0PHWp1ZVz0mDXf9W7HdGUM6tlTTk4O5syZg7vuusu/jCcp7J44c+bMCj1HZmYmcnNzUbdu3RKZNZ70cCcef/zxeOSRR1CvXr1SnyM7O9u0wMDMPhhswcTX58l+sLdDKu6wlrVNu//Uzvhh8TZ88fdmTF+xHQs2pZr2+P+WWkFa90YY2r0RmtVJCN1jHVPLKrHPRrmZVnC2bgZc62eYoM21bxew7DurMTiNqQk0HwBfi8OBFoOApn2AqIMrjBJpQvJYS7XQsXYOHWvn0LGulp0K0+xpeJk9MrM+zru8jjjWQQ3MduzYYTJeDRs2LLKc80uXLq3Qc9xxxx1o0qSJCeYCuzGeddZZaN26NVatWoW7774bp5xyign2PJ6SRRnGjBmDBx98sMTy7du3IysrC8HEDwKjc34o9Mt6+BnQOAoDGrfA7szGmLZyD6au2I25G9MKg7TJy9ClYQJO6FAXR7dJRE1khf6xTugAdGa7BMjPQfSOxYjeMhsxBc2dk2YqQLpYBZJ/UOFCXp22yGvQFdlNByK3cT/k12pacAFJZ9L32jl0rJ1Dx9o5dKyrQX4+YlNTzWR2SgrgykfNjAwzn749BXDHBPU8/FCdl4V1f6PHH38cH3/8scmOMcVou+CCC/zT3bt3R48ePdC2bVuz3gknFPzyH4AZO45zC8yYsTtlgwYNqjVdWdEPBIuXcFtC+mRdysXRkh1bNcV/BgPb07IxZdFWTFqwFX+u3YXF2zJNe3E60K5+PE7uBpzUtRG6NgmRa6TtT+NmQPeTrGlvPrwpi4H1M+Fi10feZqQgevdK0+KXf+V/mI/l+ZseBl+T3lZWrWbRH2gimb7XzqFj7Rw61s6hY11NXRmTkqxpU2MiH67t1tj8hAbJQe3KyHOxQ1WrIqiBWf369U0Ga9u2bUWWc75Ro0blPvbpp582gdmPP/5oAq/ytGnTxrzWypUrSw3MuLNL2+EMhEIhGOIHIlS2RQ5ew6R4jBzU2rSUtCxMXsggbQtmrdmFlTv24aVpq01rkhRnArSTujREv9Z1Ee0Jg+PPz2iTnlY7/P+sCo5pW4At84G1LCgyA9j8t5VHW/0zsPpn+EPPxGZAUwZpfYEmfYAmvYC4gj/SEUjfa+fQsXYOHWvn0LGuYjxfaNCg8FyCJwe1Wlv7mr3dgngObB/riA/MYmJi0LdvX0ydOhXDhw/3R6acv/baa8t83JNPPolHH30UU6ZMwWGHHbbf19m4cSN27tyJxo11PSYJLcm14jByYCvTdqZn4atZq/DHxn2m9P7m1Cy8M2OtaYlxUTihc0MTpB3doQFqxIZJspsZv8QmVrMrP+ZkWFUfUzdYQdqmOcD2ZdaFr9mWfFP4+PodrCCNGTUGbA27AdEaryYiIhJRGHwNGhS4AGhzCZwm6Gd37EI4atQoE2D179/flMvPyMjApZdeau5npUWWwec4MGJ5/Pvvvx8ffvihufbZ1q1bzfKaNWualp6ebsaLnX322SbrxjFmt99+O9q1a2fK8IuEqjoJMRjapR4uOTYZOfk+/LZiB75fvBU/LknBrowcfPn3JtNiotw4sl19E6Qd1ykZDRPDLFCJqVEYpNmy04At/1hBGouLsKWuB3Yst9r8j631XB6gRgOgcU+gzbFWwJbcBYgLbpdjERERkbAPzM4//3xTZIPBFoOsXr16YfLkyf6CIOvXry+SPnz11VdNNcdzzjmnyPOMHj0aDzzwgOkaOX/+fLz77rumZD4Lg/A6Zw8//LCuZSZhIy7ag8FdGpqW7/Vh7vrd+H7RVny/eBvW7czET0tTTKNOjWrhmI4NcGyHZPRtWccEbmEnthbQ6kir2dK3F2bUNjNYmwNk7gTStwIr2KYUrlunVUH3x95W0Na4BxBfJyhvRURERCQsr2MWinQdMwnV66Lw67p8Wzp+WLwVPyxJwfyNe0y3bFuNGA8GtauPYzo0MK2066WFLXu8GsepbV0AsMjI1oVA2ubS109qATTqBiR3trJqjXtZAVwIXGNN18BxDh1r59Cxdg4d62oq/vHjj9a0qbSeDywba813vFHXMROR0MMBqB0b1TLt2uPbmy6OvEbaL8u349fl27EjPcdcO42N2jSo4Q/SDm9Tz2TiwpY9Xq37OVazZe6yMmtb5lm3m/+xukHabVnBBSuJ11Or09oK2JhZq98RaNDBCuJUXEdERCR4cnKKzudlBmtLgib4Px2LSKXVrRGDM3o1Nc3r9WHxlr0mSPtl2XbMWb8bq7dnmPb272sRG+U2wRmDNBYQadugRniU49+fhLpAuxOsZtu3B9i2yMqsbV9iZda2LQTysqx5tgWfFq4fXcMK0Bowu9ap8DapuaOvtyYiIiKHjgIzkQjhdrvQrWmSadcc1w57s3IxY+UOTFtmZdS2pGZZQdvy7Wb95FqxGNS2Hga1rY+BbetFVrfH+NpAqyOsZvN6gd1rgJ0rrfL9W+cDO1cBO1cAuRkF2TaW8g8QUxNo0LFkwJbo7Atki4iISNVTYCYSoRLjonFyt8amcWzaipR0k0mbtjwFf63djZS0bEyct9k0alYnHgPb1MOgdvUwsE19NEoKs2qP+8OuivXaWq1DQIXW/DwrYOOYtZSlVjaNtwzgctILKkXOKRmwsUskx6+xpD+fs3ZLK+vGQiYiIiIiB0iBmYgDsMtih4a1TLvi6DbIys03lR5nrtqJGat24p8Ne7Bx9z58OmejadSmfg2TSWPr37quueZaRGIxkPrtrdbljMLl+blWRs0O1OzbXausgG3bAqsVV7ORFajVbQPUa1cwzdYaiI4/pG9NREREwocCMxEHYhEQdmFkuwVARnYe/lq7CzNX7zTB2sJNqVi9I8O0D/5cbx7Tsl6CKcffr1VdHNayDto2qGm6T0YsT7TVbZGta8DyvBwgZRGwe52VVePFsfess663tm+3Vc6fbd3vxZ7QBSQ1MwGbq15bJEQ3BFr2sAJCZtuiYg7xGxQREZFQosBMRFAjNgrHdkw2jVL35WLWml2YsWqHCdSWbUsz109j+2LuJrNO7YRo9G1RB4cxUGtVB92bJoV31ceKYgDF66WxFceiI8yombFrqwqmVwI7VwPZqUDqBtNca36BuST2TBReOLt2i8IukQzg6rS0sm4sQFKj/iF+kyIiIodY7dqF0xzHndCkcNohFJiJSAlJ8dE4sUtD04iFROau240563abzNq8DXuwJzMXU5emmEYxHje6N0sy2TQGa8yusWqko7DoSNO+Vit+DTZeHNsEaavg27kS2ZsXITZjE1y7VgO5mdY4N7bSxCVZARuDN94yaPPftgBiahyStyciIlItPB7gqKMCFwDtrnTczlZgJiIVKiQSmFHLzfdi8ea9JkhjsDZ73W5sT8s202yv/brarMeS/Ie1rIu+reqgT4s6ZtxaRHd/LAt/7WPWi63F4fB5vdhTcDFxc8kCXji7SIZtldUtkrcZKUBWqlVFkq00CfWtAK1W48JsG6/5VquJtbxmQ12nTUREJMQpMBORAxbtcaNn89qmXX4UE0I+rN+VidlrGaTtMresArlqe4ZpE2ZvMI+rFReFns1qo1fBY3nboFass4+AfeFsttaBvxYWyMkA9qy3xrTxluPZ2Mz8Oitoy9xhtbK4o4Gkpla3SAZqDN7sLpMM5mo1sqpJOqi7iIiISKhRYCYiB41Zn5b1aph2dt9mZtmezBx/Nm322l1YsCkVaVl5+G3lDtNsTWvHo1vTRDNGrWvTJHNbv6bDg7VA7KbIsvxspeG4NgZovJh27j5rHBuDtrStwN5NVvPmArvXWq0svMh2rYaFgZp9yyqTgfOxNavtrYqIiEPl5wM//2xNH3cc4PICy1+25jtcY/3A6AAKzESkWtROiMEJnRuaRnn5XlNE5J8NqZi3Ybe5XZ6Shk179pk2ZdE2/2MbJcYVXCzbCtg43TAxQsv1V8W4NrbGPUu/n9dpS9sMpG4E9ljFR6zgbS2wd4sVwLEwCS+yzfFubOWJqVUQqAW2UoK4mAi6YLmIiFS/ffuKjs3O2VM47RAKzETk0Pyx8bjRtUmSaRcOaGGWpWfnYcHGVCzanGpK9DOrxhL9W/dmmfbjksJgjV0euzVJLAjYrNYkKc4aoyXlX6fNFA1pAbQsYx12l2SAllZQ6t9MFwRtgS0nzWo72VaUv9djk8oI3pKtMW92UxdKERERQ4GZiARNzdgo/0Wsbbym2uIte02gtnCTdbsiJc0UF/l52XbTbKz62LVJYsHFs2uifcNaaJ9cE7XinNHloUq7S7JUP1t5stOAtG1W0JZecFsiiNtiVZlkFo5tx7LynzMqHoivA9SoZwVqCfWsYiasRMlxdwl1gRrJ1i2DupiaGgsnIiIRSYGZiITcNdV4EWs2276cfCzZuheLCrJqDNiWb0vDrowcTF+xw7RAzKS1LwjWrKCtFtol1zTPLQeB2S22+u3KXoddTkwAt7UwgNu7uXA6PaXwNnsvkLcPSGPbDGDB/rfBEwPE17UCNQZxiU2tapdxBV06TWBX0Lic6+ri3SIiEgZ0liIiIS8+xmPK7bPZsnLzTXDGsv3Lt6WbrBrnt+3NxubULNN+WV6YXaNmdeJNkNaeAVtyYcDG55cqwq6lcYlWa9Ch/HXZhZIBWtYe6zZju3W9t4wdhQHcvl1AZkHjOLj8HKu7JVtFxSYWBHL1C4M2ZunYzZPZuBoNrAwdG9fjfeZ+ZV5FROTQUWAmImEpLtqDHs1qmxYoNTPXBGksNLJiW7oJ1hi47UjPxsbd+0z7qeCi2HYc0aJuAtonF2bYGLi1bVDTvIZUcxfKuq0rtq4ZCJ5RGKjxlsEbM3O8VACrUzLAy9hpBXdsXMfntTJzbOVVpSx1+2oBCQVBGqcZuJnulDUKM3S85XJ72r5Vlk5ERA6QAjMRiShJCdE4rFVd0wLtzsixgrSUdCzfamXXeK01dodctzPTtMBiI7wONsv/c8xax0YM1qzArXX9GoiNUsB2yDGCZql+NhYyqQiv1wrW7EDNzsbZ07y8ALtY8mLenOY14RjM8ZbsYie8ftyBik4wAZsrLgl1PTXgqlWvMHBj2Wdm5tgt0y6AwsZKlszucZr3cZpZPRERJ6hVq1jviwaF0w6hv/gi4gh1asRgQJt6pgViJm15QHaNt8y2pe7LxZodGaZ9v7gwYPO4XWhVLwGt69dEy3oJZpoBXKt6NdCkdpypPikhwu0u6MLIIL19xR/nzbcycHZ2joEaAzRm6BjQ5WUVZOhSrcDPztbtKyh4QiyAkpsJV9oWxHD+AHpelgjwYgu6hvKW2Tp2xeRtVJwVqJr5mlYwZ7qRJlnz9vpRsQryRCS0eTzAsccGLrCuX+YwCsxExNF4MWu2QW3r+5f5fD5TBZJdIK3MWpp/mhfJXrU9w7TiotwuNK+bUBCw1TBdJFvVtwK35nUSEBOloC0suD1WlUi2A8Wgjt0mC4I1b+ZupG5bj6Q4F9z2cjuws7tYZqdbtzmsZplmTcNXJMA7oDF15V1EnBk4O2NnxtHFFAR7BZ9/LmfAZ4I5ZvFqWEGdCQJrWV00mflj4Md1uDxa1xgUkYPE7uo7dwLp6UDNmkC9eo7KlNkUmImIFMNroyUnxpl2ZPuiARuvr8as2rpdmVi3IwNrTTfIDDOfk+f1Z9mAooVH2DWySe34goAtHvVifeja0mcybwzgVIAkgoI6u3gIeb3IrpkCJCdbGbyKnqB486wgjVk5O1izp9n1MicdyMuxlpviKAWZvLzswvU5Jo/Nl289L4un5Ba8BsflVRWX2wr6GKDx8gfmlgFbQrFlZd1y3fiKP4ZFWRx4wiYSkfbsAd59F3jxRWDVqsLlbdsC110HjBoF1C46ljySKTATETmAgK1xUrxpxXm9PmxLy8LaHVagZgds9m1mTr6/+Ijf9I3+yUaJcf5MW/O68SbzxiqSzeokoEHNWLgZ2YkzMOhg8OHvhnkQGOTl5xYGa2y8Fh2f3w72eD+7YnJdBnic52UMmMljto6BIKth8jkYDHJdBnx2Vo8FVuzxeIcCA8EiAdv+Ar9itwz+igSDFQkEC4JqBYQiVWfKFODss4HMzJL3rV4N3HQjcPftwGdfAKcMc8SeV2AmIlIFGDjZQVvgBbPtTNuO9Bx/oLZ2RzqWbdqFrRles2xvVp7JxLH9uWZXieeO8bjR1ARpVmuSFG+yb1zWtHY8GibGqZuklI6BBLsfRgV0zWxcBTvLDviYhWMXTJOtywJyebvPKqbCZpbt75bdNbP2v25gIGiyfyW7E1crD7t31rQCO7gKu3ZyeVQMXJ4Y1M4DXDHRhV1GWdDF3B8QANqPYcBnWgzgjrJuTYu2uolymsEjXytwXf9z8HHRFc/EioRaUDZsmPW3hK04X8GyfTnAaWcA330HDBmCSKfATETkEGTaGtSKNY3VIr1eL1JSUpCcnAy32409mTmFGbYdmdiwOxMbTduHLalZyMkP7CJZ2vMDybViTZBmAraC28ZJcSZQbJQUh3o1YpR1k2oI+HjB78LrC1YbEwjmBAR8+wKCwMDbYsFcuesWuy2+LgPAQPnZQGY2gJ2l7xJexgNBwODMDujMvKdkJpEY7PE+Zv/MYxjUeQqCu6iCYNJuBfeZdQKXR5WzLOC5ijyfvSzw+Srw/NxOZSgjt/siM2Wm23ax71lxjM+4HtffuDHiuzUqMBMRCbLaCTHoxda85D84efleE5wxSLMCtn3YsmcfNqfuw+Y9Wdi0Z58Z28YLa7PNXb+n1NeI9riQXCvOBGlsjRMLp9mNkre8XwVKJHQDQWaeYq1LDlQ3OyPIAI3dNzlOz5/ZY9Dms7p0stgLx/Xl58Cbm4W0PTtQKyEObnYRZUaNj/E/T7Y1z+djkMflHEvIgDNwmvczG2gXkmGQYp4jG/AWrBfILGP2EpHHHzTagV9AoFck8PMcRNAY8FhmNhkJcLn5zMVbt/bjTTfjGHP8YzOygZ1JBfcx4OWtuyCoDFxWEAwzUDZdYl3W8/AzxIyqCUDZ7KDZXcqygttICVY5pozdF0vLlJWGwRvXHz8euP56RDIFZiIiIYzl9znejG0g6pU6tm1nRg42M1jbs88EalbAlomtqVkmqNueno3cfJ+5j60s/Pe+Xo1Yk2lj98iGibEmWEtOjDXj3JIL5uvXjNFlAcQ5GcGK8nqxLyUFtQ6k0EtlMGBjoGYHdyYwLJjmSb8JIgOzhQXfea7HLCAbA0AGeHwuExAWBHyBzSzj/QH35dv324FksccUea6Ax/qfq7TH5pbMThYPOvk+QgiP7iHIE5fBVTRYY7dYZkZ5fE3Q5irobuuzxony8hkMRO0gkceBnxEGmvzxoETgF9DKWs7PDwNZBlb8sYTPTeZ17ICyYDv9wagdVLqAJ16teFAW6IUXrIIgkRCclkGBmYhImI9ts7tJ9iwl40a5+V5T/p9B2ra9VrC2NXUftu7NLrjNwrbUbNNlktd1Y1uwqeB6XGUGcDFowKCt4LWT7ZZYuIyXIagRq39mRKr2S19wgh1JlylgRoTBQpkBYvFlxQLEEsv291ylBY251nhJ/oHjcgYO7BLrD2YLAsj8HPi8+cjNzkR0dAxcDIDs5+R7MM9lT+cVvrfAcZL+98tsrNda375vv3wFjy2otspttK+faOM1GG3F7wu2TC+wJf3AH+fzWVUbd+2ySulHKP2LKSIS4aI9bjPmjK0szLztyswxWTaTadubhe17s5CSlm0aA7uUtCxTxCTfaxUzYVuypfzXjo/2oH6tGP/14qxmzderGWMydJyvWyPGdOnkBbxFxGFMhtFdOE4uxPm8XuwqGCfsqqrsqCmCURAEmsCuYJpBmFlWbN5ez3SFzSnMUvF5TJfZLCubVjxw5HqcZ6BY/PmKvE4Zr8dbHidzuQ5WYmXWLK5wXTNmzH4vAe/JXrZlN1Nfld9PaWkKzEREJPIzb3bg1K1pUpnrMSjblZFjgjQ7YDNBW0AQZwK4tBzsy803bcOufabtdxtcQJ0EK0hjY+BmpguW1SlYHrhOXHRBFxoRkXBmuv7x71lBUZRItWPHwQVmtWohkiljJiIiFeYJ6DrZdT/rZmTn+btGbk/L8U/vTM/BzgxOW8sY6O3JzIXXBzNejq2iEmI8JlCrnRCNpHircTrRno6PKbKct7yvVmyUqlSKiBxq7IbIi0fzOmUHMs7M5QLatAHqHuS1HUOcAjMREakWHF/G1rJejf2uy3FwuzNzTNDGQM0EaOnZ2J2RY7pY7s7INcu5Dm/Z8rw+c+HuzJzyi5qUlZ1jgFa7IGgz0wkM4qKKBHTW8qJBH7tn8hIIIiJygPi3kwU8brrpwHfd9ddHdOEPUmAmIiIhMQ7OVICsVbGCBrxod1p2ngncGMSl7svF3n25JvPG6dSAabN8X45/WXae12TnOM124Nvq8gdqhQFbYSCXGBflv60R40FuZgbSXelIjI8xgSqzfArsRMSxRo0C7rkH2Ldv/9cxI47ji48HRo5EpFNgJiIiYYeBTWIcg5/oCmXkAmXl5hcEawVBXGbgtBXAmSDOXlawDm+ZpeOlB+ziJxW3tEi2jgEau1PWZPDGW87HWbc1Y6NRM9Zj7jPTcda6xdfjvK47JyJhhxeJ/vxzYNgwK+gqLzhzFVz37YsvIv7i0qTATEREHIUFQ9hY2v9AMEuXkZMfEMzlmAAvMDvHYC4tKw9pWYW3ezNzkJnrRXp2nsnUsVn35QEHWcmagRmDtoRYD2rEWMGaaTEeJJj5glvOxxa9jWeLtu5nFo/7JKFgGYvBiIhUmyFDgO++A84+27p4NPkCxpyZLos+IC4K+PxL4KSTHHEwFJiJiIhUMEtnZbSi0LScSw8E8nq9SLHLartcyMr1Ii07F+lZeSZQ4y27ZLJQCufTApab22LT1v255nkoJ8+LnXnszlm1hzA2ym2CNAZtcdGcjgoI5AKDOus2viC4s6YDl1sBn9XcZhmn+fzqzinicAzONm4Exo+3Lh69alXhfW1aA2d3AE7vCRx+IpxCgZmIiMghwEDEBDQxHiQfZMXnvHwvMrLzrSDPBHb5JrjLzGHgxoIonM5HJu8zBVKsdaxiKVYgyEsZcH6fWWZd2sDGcXhsuysxBq+iYjxuE6DFFLSi0x5zf2nL/fMlHu8p8VyxXCea61r3lfp6HgWJIkHD7oks6nH11cCECda4s9NPB+omAYvHOO7AKDATEREJM1EeN5IS2KruekfsqslMHAM3cw26gICt6LQV9JW+TtHHsmXn5Zvn5dg+jtGz5eR7TUM2gu5gg0TeRrtdyMnKRN3amYiLtsb/KUgUqSB2XUxMtBpL6qPwhyInUWAmIiIiRTJ61YWXRWDgllOQkeOtNZ1fOJ3vRXauFbSVuM9+jFkn37ot67n8j/eW+vhAVRskbjqoR9uZvuKBYPEgkZVMWSGUt1G8dRfcFixn8G6m3fa0y1yHMMrNW3fBLe8rY7nbZcYaFs67S1k/YDnnPQWPcxXery6rIhWnwExEREQOCStocAd9b3u9Pn8wVm6QaN9XgSCR83vTM+CKikFOvq/yQWLB8lDIJFYFBmdFA7myA0MT0HlKBoglHlcQAAbeXyJIDHyMp+znKhp8Bm5TadtgPb/b5cOe3VnI9GQg2uPxP7/HVRiM2vMsOhi4XKQ8CsxERETEUXgyHue2CpFUlcBCL26ejVdjkJhd0C2UYw0ZBPLWupSD17S8fOuyDmbay8f7kO+11vH6uD7nfWbeuvUWnc8vudxbZP2i9/O1ysL72Q7k4hKRLDBgYyBqTTPYQ2FQ5w/krGVcz11kujDgsx5rPSZwfU4HPs5VMF34+MJ5e117u+x1recsfH37tQqfx1rXek37cSWfN3BbylzHm49G6/cCLmDnP5vhcXvRZleeyRq3hnMoMBMRERGJoCAxGIoGbuUFdFaAWG5wyPt8xR5TYv3C5+L69nL/4/ID7i/tccWeL7+sIDVwecDr5Oblg7lOJjzNvNdbpNp7WczrsAy8M4dQ7Udt62b9/IL5gWjToAZ+OjoGTqHATEREREQOOsCM8V//LryDzMpmR1lAh/Vt8gsyk3agyOC0xHKvzwRydiDJ5fZ9vN6ymfZxHS6z5v3PU7Bu4POa9Qpe39qOwscFbpc9bb8Wt8F67aKvYT/Ofk5rvvA92I+z5q3AnP8Frhv4vL5iywK3w573+ecLlzVOqtilSSKFAjMRERERkYNkdUO0ug6KVIYCMxERERERCZ78fODPP63pAQMAlxdY+4E13+oiwF11lwYJZQrMREREREQkuHbuLJxmv8b0tYXTDhH8mrUiIiIiIiIOp8BMREREREQkyBSYiYiIiIiIBJkCMxERERERkSBTYCYiIiIiIhJkIRGYvfzyy2jVqhXi4uIwYMAAzJo1q9z1P/30U3Tq1Mms3717d0yaNKnI/bwg3f3334/GjRsjPj4egwcPxooVK6r5XYiIiIiISKV4PFazsUS+Q8rkh0xgNmHCBNx8880YPXo05s6di549e2LIkCHmiuqlmTFjBkaMGIHLLrsMf//9N4YPH27awoUL/es8+eSTeOGFFzBu3Dj8+eefqFGjhnnOrKysQ/jORERERERkvxiQDR1qNROgxQDd7rEapx0i6IHZs88+iyuuuAKXXnopunTpYoKphIQEvPXWW6Wu//zzz+Pkk0/Gbbfdhs6dO+Phhx9Gnz598NJLL/mzZWPHjsW9996LM844Az169MD48eOxefNmTJw48RC/OxERERERkRC/wHROTg7mzJmDu+66y7/M7XabroczZ84s9TFczgxbIGbD7KBrzZo12Lp1q3kOW1JSkukiycdecMEFJZ4zOzvbNNvevXvNrdfrNS2Y+PoMNoO9HVL9dKydQ8faOXSsnUPH2jl0rJ3De4jPw4MamO3YsQP5+flo2LBhkeWcX7p0aamPYdBV2vpcbt9vLytrneLGjBmDBx98sMTy7du3B737Iz8Iqamp5kPBoFUil461c+hYO4eOtXPoWDuHjnW17FREz5tnJnN79eICxKV8Zeazks8A3FFBPdaH6hw8qIFZqGDGLjALx4xZ8+bN0aBBAyQmJgZ12/iBcLlcZlsUmEU2HWvn0LF2Dh1r59Cxdg4d62qQnw/k5lrTDRpwAVzbrXoTiQ3qB22cmX2sY2NjIz8wq1+/PjweD7Zt21ZkOecbNWpU6mO4vLz17VsuY1XGwHV6mQi8JO7s0nY4A6FQCIb4gQiVbZHqpWPtHDrWzqFj7Rw61s6hY13FfD6eeFvT5tbHnWzta84H8RzYPtaHQlDP9GNiYtC3b19MnTq1SGTK+YEDB5b6GC4PXJ9++OEH//qtW7c2wVngOsyAsTpjWc8pIiIiIiISTEHvysguhKNGjcJhhx2G/v37m4qKGRkZpkojjRw5Ek2bNjXjwOiGG27AMcccg2eeeQbDhg3Dxx9/jNmzZ+P111/3R7U33ngjHnnkEbRv394Eavfddx+aNGliyuqLiIiIiIiEmqAHZueff74pssELQrM4B7sbTp482V+8Y/369UXSh4MGDcKHH35oyuHffffdJvhiRcZu3br517n99ttNcHfllVdiz549OPLII81z8oLUIiIiIiIiocblY7k/KYJdH1lin1VYQqH4By+2nZycrDFmEU7H2jl0rJ1Dx9o5dKydQ8e6mop/TJpkTfMi08gHFj1mzXe9O6jFP3gezuROnTp1qj02CHrGLBTZsap9PbNg4gciLS3NfCBU/COy6Vg7h461c+hYO4eOtXPoWFdTYJaZaU2b8+98ID27cD6IgRnPw3ntZarufJYCs1LwABBL5ouIiIiISLA8HlIxAnvVVRd1ZSwjOt68eTNq1apliokEk31NtQ0bNgS9W6VULx1r59Cxdg4da+fQsXYOHWvnHev169ebmIDFBKuzB5syZqXgDm/WrBlCCYMyBWbOoGPtHDrWzqFj7Rw61s6hY+0cSUlJh+Q8XFcsFhERERERCTIFZiIiIiIiIkGmwCzExcbGYvTo0eZWIpuOtXPoWDuHjrVz/H979wIUVfk+cPwVAYXwgqJAliID4S1IxRLRmsIRsdF07DrkoDU5IBZNZaTlpemiU43dRikttRkdmXSCyAQivJSOimaIt1DzkmMhGmpgSirvf573P3t+u2j9LH9xluP3M7Punj2vuwceztn32fO+zyHW1w9iff1o1cT9cIp/AAAAAIDNOGMGAAAAADYjMQMAAAAAm5GYAQAAAIDNSMwAAAAAwGYkZl5u3rx5KiIiQrVu3VrdcccdqqyszO5Nwl/45ptv1MiRI82V4eUK8fn5+R7rtdZqxowZKjw8XAUEBKihQ4eq/fv3e7SpqalRqamp5kKG7du3V48//riqq6vzaFNRUaGGDBli/i7kivRvvPEGcWlis2fPVgMGDFBt2rRRnTt3VqNHj1aVlZUebc6fP68yMzNVx44dVVBQkBo7dqw6fvy4R5uffvpJ3XvvvSowMNC8zpQpU9TFixc92qxbt07169fPVIWKiopSS5YsaZKfEf8vJydHxcbGWheTTUhIUIWFhdavhzg705w5c8xx/Omnn7aeI9bOMWvWLBNf91uPHj2s9cTaWY4dO6YeffRR83ks/a9bb71Vbdu2zfv6ZxpeKzc3V/v7++tFixbp3bt36yeeeEK3b99eHz9+3O5Nw59YvXq1fvHFF/Vnn32mZffKy8vzWD9nzhzdrl07nZ+fr3fs2KFHjRqlu3fvrs+dO2e1GT58uI6Li9ObN2/W3377rY6KitKPPPKItf7MmTM6NDRUp6am6l27dunly5frgIAA/eGHHxKXJpScnKwXL15sYlBeXq5HjBihu3btquvq6qw26enp+uabb9alpaV627ZteuDAgXrQoEHW+osXL+o+ffrooUOH6u+//978/YSEhOipU6dabQ4ePKgDAwP1M888o/fs2aPff/993bJlS11UVES8m0hBQYH+8ssv9b59+3RlZaWeNm2a9vPzM7Enzs5UVlamIyIidGxsrM7KyrKeZ592jpkzZ+revXvrX375xbqdOHHCWk+snaOmpkZ369ZNjx8/Xm/ZssV8rhYXF+sDBw54Xf+MxMyL3X777TozM9NavnTpkr7xxhv17Nmzbd0uXJ3GiVlDQ4MOCwvTb775pvXc6dOndatWrczOK6TjLf9v69atVpvCwkLdokULfezYMbM8f/58HRwcrOvr66022dnZOiYmhtDYqLq62sRu/fr1Vmyl875ixQqrzd69e02bTZs2mWVJxHx8fHRVVZXVJicnR7dt29aK7/PPP286D+4eeughkxjCPrIPfvTRR8TZgWpra3V0dLQuKSnRd911l5WYsU87LzGTTvaVEGtnyc7O1oMHD/7T9d7UP2Moo5f6448/1HfffWdOpbr4+PiY5U2bNtm6bfhnDh06pKqqqjxi2q5dOzNE1RVTuZfT4/Hx8VYbaS+x37Jli9XmzjvvVP7+/lab5ORkM4zu1KlThMcmZ86cMfcdOnQw97L/XrhwwSPeMkyma9euHvGW4RShoaEesfztt9/U7t27rTbur+Fqw3HAHpcuXVK5ubnq7NmzZkgjcXYeGX4sw4sb73fE2nlkqJpMPYiMjDRD1GRouSDWzlJQUGD6VQ888ICZMtC3b1+1cOFCr+yfkZh5qZMnT5oOgHuHTciy/PGg+XHF7a9iKvdy0HDn6+trOvvuba70Gu7vgabV0NBg5qEkJiaqPn36WLGQg7McyBvH6u/E8s/aSPJ27ty5f/Xnwn/s3LnTzBOUeX7p6ekqLy9P9erVizg7jCTd27dvN3NIG2OfdhbpdMt83aKiIjOPVDrnMjeotraWWDvMwYMHTYyjo6NVcXGxysjIUE899ZT65JNPvK5/5vuPf0oAgPUN+65du9SGDRv4jThUTEyMKi8vN2dGV65cqdLS0tT69evt3iz8Dx09elRlZWWpkpISM3EfzpaSkmI9luI+kqh169ZNffrpp6b4A5z15Wl8fLx6/fXXzbKcMZPP7A8++MAcy70JZ8y8VEhIiGrZsuVlFdxkOSwszLbtwj/nittfxVTuq6urPdZLhT6pBOTe5kqv4f4eaDqTJ09Wq1atUmvXrlU33XST9bzEQoYknz59+rJY/Z1Y/lkbqQpF56HpyNlPqYjZv39/czYlLi5Ovfvuu8TZQWT4mhx/pQKqfBMuN0m+33vvPfNYvvlmn3YuGd1wyy23qAMHDrBfO0x4eLgZ4eCuZ8+e1tBVb+qfkZh5cSdAOgClpaUeGb8sy7wGND/du3c3O6Z7TGU4moxNdsVU7qUjLx0ElzVr1pjYy7d5rjZSll/mL7nIN7zyjX5wcHCT/kzXM6nvIkmZDGmTGEl83cn+6+fn5xFvGWcuHwTu8ZYhcu4He4mlJF2uDxFp4/4arjYcB+wl+2R9fT1xdpCkpCSzP8qZUddNvmWXuUeux+zTziVlz3/88UfTief47SyJiYmXXc5m37595gyp1/XP/nZpEzRpuXypCLNkyRJTDWbixImmXL57BTd4XzUvKXsuN9m95s6dax4fOXLEKscqMfz88891RUWFvu+++65YjrVv376mpOuGDRtMdTD3cqxSKUjKsY4bN86UY5W/EymnTrn8ppWRkWFK665bt86j3PLvv//uUW5ZSuivWbPGlMtPSEgwt8bl8ocNG2ZK7ksJ/E6dOl2xXP6UKVNMVcd58+ZRLr+JvfDCC6ba5qFDh8x+K8tSieurr74izg7nXpVRsE87x7PPPmuO37Jfb9y40Vy2RC5XIhV2BbF21uUvfH199Wuvvab379+vly1bZj5Xly5darXxlv4ZiZmXk2sWScdOrmcm5fPl2gnwXmvXrjUJWeNbWlqaVZJ1+vTpZseVpDspKclcF8ndr7/+anb0oKAgUzZ9woQJJuFzJ9fYkNKv8hpdunQxBxQ0rSvFWW5ybTMXOaBPmjTJlM+Vg/OYMWNM8ubu8OHDOiUlxVzrRDoF0lm4cOHCZX9Xt912mzkOREZGerwH/n2PPfaYuQaO/P4lcZb91pWUCeJ8/SRmxNo55LIj4eHhZr+Wz1FZdr+uFbF2li+++MJ8ESr9ph49eugFCxZ4rPeW/lkL+efaThACAAAAAK4Fc8wAAAAAwGYkZgAAAABgMxIzAAAAALAZiRkAAAAA2IzEDAAAAABsRmIGAAAAADYjMQMAAAAAm5GYAQAAAIDNSMwAALhGLVq0UPn5+fweAQD/GIkZAKBZGz9+vEmMGt+GDx9u96YBAHDVfK++KQAA3kmSsMWLF3s816pVK9u2BwCAv4szZgCAZk+SsLCwMI9bcHCwWSdnz3JyclRKSooKCAhQkZGRauXKlR7/f+fOneqee+4x6zt27KgmTpyo6urqPNosWrRI9e7d27xXeHi4mjx5ssf6kydPqjFjxqjAwEAVHR2tCgoKrHWnTp1SqampqlOnTuY9ZH3jRBIAcH0jMQMAON706dPV2LFj1Y4dO0yC9PDDD6u9e/eadWfPnlXJyckmkdu6datasWKF+vrrrz0SL0nsMjMzTcImSZwkXVFRUR7v8fLLL6sHH3xQVVRUqBEjRpj3qampsd5/z549qrCw0LyvvF5ISEgT/xYAAN6shdZa270RAABcyxyzpUuXqtatW3s8P23aNHOTM2bp6ekmGXIZOHCg6tevn5o/f75auHChys7OVkePHlU33HCDWb969Wo1cuRI9fPPP6vQ0FDVpUsXNWHCBPXqq69ecRvkPV566SX1yiuvWMleUFCQScRkmOWoUaNMIiZn3QAAuBLmmAEAmr27777bI/ESHTp0sB4nJCR4rJPl8vJy81jOYMXFxVlJmUhMTFQNDQ2qsrLSJF2SoCUlJf3lNsTGxlqP5bXatm2rqqurzXJGRoY5Y7d9+3Y1bNgwNXr0aDVo0KBr/KkBAE5CYgYAaPYkEWo8tPB/ReaEXQ0/Pz+PZUnoJLkTMr/tyJEj5kxcSUmJSfJkaORbb731r2wzAKD5YY4ZAMDxNm/efNlyz549zWO5l7lnMvzQZePGjcrHx0fFxMSoNm3aqIiICFVaWnpN2yCFP9LS0sywy3feeUctWLDgml4PAOAsnDEDADR79fX1qqqqyuM5X19fq8CGFPSIj49XgwcPVsuWLVNlZWXq448/NuukSMfMmTNN0jRr1ix14sQJ9eSTT6px48aZ+WVCnpd5ap07dzZnv2pra03yJu2uxowZM1T//v1NVUfZ1lWrVlmJIQAA5nOLXwMAoLkrKioyJezdydmuH374waqYmJubqyZNmmTaLV++XPXq1cusk/L2xcXFKisrSw0YMMAsy3ywuXPnWq8lSdv58+fV22+/rZ577jmT8N1///1XvX3+/v5q6tSp6vDhw2Zo5JAhQ8z2AADgQlVGAICjyVyvvLw8U3ADAABvxRwzAAAAALAZiRkAAAAA2Iw5ZgAAR9Na270JAAD8V5wxAwAAAACbkZgBAAAAgM1IzAAAAADAZiRmAAAAAGAzEjMAAAAAsBmJGQAAAADYjMQMAAAAAGxGYgYAAAAAyl7/B9XSHwNfCjvhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_loss_history, label=\"Training Loss, lambda=0.0005\")\n",
    "plt.plot(cv_loss_history, label=\"CV Loss\")\n",
    "\n",
    "# Mark the best CV loss point\n",
    "best_epoch = cv_loss_history.index(min(cv_loss_history))\n",
    "plt.scatter(best_epoch, best_cv_loss, color='red', s=100, zorder=5, label=f'Best CV Loss: {best_cv_loss:.4f} (epoch {best_epoch + 1})')\n",
    "plt.axvline(x=best_epoch, color='red', linestyle='--', alpha=0.3, label='Best epoch')\n",
    "\n",
    "# Mark where training actually stopped\n",
    "plt.axvline(x=len(cv_loss_history) - 1, color='orange', linestyle='--', alpha=0.5, label=f'Stopped at epoch {len(cv_loss_history)}')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Cross-Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed810c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.99821428571428 %\n",
      "CV accuracy: 98.02857142857142 %\n",
      "relative gap between CV and Train loss: 109.83178360080399 %\n",
      "\n",
      "Training error rate is approximately 0.0017857142857167219 %\n",
      "CV error rate is approximately 1.971428571428575 %\n",
      "Bayes/human error rate for mnist datset is approximately 0.2%\n"
     ]
    }
   ],
   "source": [
    "def inference(X, *, weights=weights):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    W3 = weights['W3']\n",
    "    b3 = weights['b3']\n",
    "    W4 = weights['W4']\n",
    "    b4 = weights['b4'] \n",
    "\n",
    "    # First Hidden Layer\n",
    "    z_1 = X @ W1 + b1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    z_2 = a_1 @ W2 + b2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    # Third Hidden Layer\n",
    "    z_3 = a_2 @ W3 + b3\n",
    "    a_3 = np.maximum(0, z_3)\n",
    "\n",
    "    # Logits before softmax\n",
    "    z_4 = a_3 @ W4 + b4\n",
    "\n",
    "    # Softmax activation applied\n",
    "    shifted = z_4 - np.max(z_4, axis=1, keepdims=True)\n",
    "    z_4_exp = np.exp(shifted)\n",
    "    p = z_4_exp / np.sum(z_4_exp, axis=1, keepdims=True)\n",
    "\n",
    "    return np.argmax(p, axis=1)\n",
    "\n",
    "train_accuracy = np.mean(inference(X_train, weights=weights) == y_train)\n",
    "print(\"Training accuracy:\", train_accuracy*100, \"%\")\n",
    "\n",
    "cv_accuracy = np.mean(inference(X_cv, weights=weights) == y_cv)\n",
    "print(\"CV accuracy:\", cv_accuracy*100, \"%\")\n",
    "\n",
    "gap = cv_loss - train_loss\n",
    "gap_percentage = (gap / train_loss) * 100\n",
    "print(\"relative gap between CV and Train loss:\", gap_percentage, \"%\")\n",
    "print()\n",
    "\n",
    "print(\"Training error rate is approximately\", 100 - train_accuracy*100, \"%\")\n",
    "print(\"CV error rate is approximately\", 100 - cv_accuracy*100, \"%\")\n",
    "print(\"Bayes/human error rate for mnist datset is approximately 0.2%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5974d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 56000\n",
      "CV set size: 7000\n",
      "Test set size: 7000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"CV set size:\", X_cv.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9e158f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 1\n",
      "Index = 18652\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(X_train.shape[0])\n",
    "\n",
    "img = X_train[idx].reshape(28, 28)\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label: {y_train[idx]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Predicted Label:\", inference(X_train)[idx])\n",
    "print(\"Index =\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a328b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking accuracy on test set\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
