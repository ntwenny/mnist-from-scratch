# MNIST Neural Network From Scratch

This notebook documents my process of implementing a fully-connected neural network
from first principles using only NumPy (no ML frameworks).

Current state:
- Vectorized forward propagation and backpropagation
- Softmax cross-entropy loss with numerical stabilization
- He initialization and ReLU activations
- Mini-batch gradient descent with early stopping
- Manual learning-rate and regularization tuning

This repo will be expanded with experiment logs and cleaned modules.